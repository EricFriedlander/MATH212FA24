---
title: "AE 11: Cross validation"
date: "Oct 23, 2023"
editor: visual
---

::: callout-important
Go to the [course GitHub organization](https://github.com/sta210-fa23) and locate your `ae-11` repo to get started.

Render, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, October 26 at 11:59pm.
:::

## Model statistics function

You will use this function to calculate $Adj. R^2$, AIC, and BIC in the cross validation.

```{r}
#| label: model-stats-function
calc_model_stats <- function(x) {
  glance(extract_fit_parsnip(x)) |>
    select(adj.r.squared, AIC, BIC)
}
```

## Packages

```{r}
#| label: load-pkgs
#| message: false
 
library(tidyverse)
library(tidymodels)
library(knitr)
```

## Load data and relevel factors

```{r}
#| label: load-data
#| message: false

tips <- read_csv("data/tip-data.csv")

tips <- tips |>
  mutate(Age = factor(Age, levels = c("Yadult", "Middle", "SenCit")), 
         Meal = factor(Meal, levels = c("Lunch", "Dinner", "Late Night"))
  )
```

## Split data into training and testing

Split your data into testing and training sets.

```{r}
#| label: initial-split

set.seed(10232023)
tips_split <- initial_split(tips)
tips_train <- training(tips_split)
tips_test <- testing(tips_split)
```

## Specify model

Specify a linear regression model. Call it `tips_spec`.

```{r}
#| label: specify-model

tips_spec <- linear_reg() |>
  set_engine("lm")

tips_spec
```

# Model 1

## Create recipe

Create a recipe to use `Party`, `Age`, and `Meal` to predict `Tip`. Call it `tips_rec1`.

```{r}
#| label: create-recipe

tips_rec1 <- recipe(Tip ~ Party + Age + Meal,
                    data = tips_train) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors())

tips_rec1
```

## Preview recipe

```{r}
#| label: preview-recipe

prep(tips_rec1) |>
  bake(tips_train) |>
  glimpse()
```

## Create workflow

Create the workflow that brings together the model specification and recipe. Call it `tips_wflow1`.

```{r}
#| label: create-wflow

tips_wflow1 <- workflow() |>
  add_model(tips_spec) |>
  add_recipe(tips_rec1)

tips_wflow1
```

# Cross validation

## Create folds

Create 5 folds.

```{r}
#| label: cv-tenfold

# make 10 folds
set.seed(10232023)
folds <- vfold_cv(tips_train, v = 5)
```

## Conduct cross validation

Conduct cross validation on the 5 folds.

```{r}
#| label: conduct-cv

# Fit model and performance statistics for each iteration
tips_fit_rs1 <- tips_wflow1 |>
  fit_resamples(resamples = folds, 
                control = control_resamples(extract = calc_model_stats))
```

## Take a look at `tips_fit_rs1`

```{r}
tips_fit_rs1
```

## Summarize assessment CV metrics

Summarize assessment metrics from your CV iterations These statistics are calculated using the *assessment set*.

```{r}
#| label: cv-summarize

collect_metrics(tips_fit_rs1, summarize = TRUE)
```

::: callout-tip
Set `summarize = FALSE` to see the individual $R^2$ and RMSE for each iteration.
:::

## Summarize model fit CV metrics

Summarize model fit statistics from your CV iterations These statistics are calculated using the *analysis set.*

```{r}
#| label: cv-model-fit
map_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>
  summarise(mean_adj_rsq = mean(adj.r.squared), 
            mean_aic = mean(AIC), 
            mean_bic = mean(BIC))
```

::: callout-tip
Run the first line of code `map_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]])` to see the individual $Adj. R^2$, AIC, and BIC for each iteration.
:::

# Another model - Model 2

Create the recipe for a new model that includes `Party`, `Age`, `Meal`, and `Alcohol` (an indicator for whether the party ordered alcohol with the meal). Conduct 10-fold cross validation and summarize the metrics.

## Model 2: Recipe

```{r}
#| label: model2-recipe

# add code here
```

## Model 2: Model building workflow

```{r}
#| label: model2-workflow

# add code here
```

## Model 2: Conduct CV

::: callout-note
We will use the same folds as the ones used for Model 1. Why should we use the same folds to evaluate and compare both models?
:::

```{r}
#| label: model2-cv

# add code here
```

## Model 2: Summarize assessment CV metrics

```{r}
#| label: model2-assessment

# add code here
```

## Model 2: Summarize model fit CV metrics

```{r}
#| label: model2-model-fit

# add code here
```

# Compare and choose a model

-   Describe how the two models compare to each other based on cross validation metrics.

-   Which model do you choose for the final model? Why?

## Fit the selected model

Fit the selected model using the entire training set.

```{r}
#| label: fit-selected-model

# add code here
```

::: callout-tip
See [notes](https://sta210-fa23.netlify.app/slides/12-model-workflow#/fit-model-to-training-data) for example code.
:::

## Evaluate the performance of the selected model on the testing data

### Calculate predicted values

```{r}
#| label: predicted-values

# add code here
```

### Calculate $RMSE$

```{r}
#| label: rmse-selected-model

# add code here
```

::: callout-tip
See notes [notes](https://sta210-fa23.netlify.app/slides/12-model-workflow#/make-predictions-for-testing-data) for example code.
:::

-   How does the model performance on the testing data compare to its performance on the training data?

-   Is this what you expected? Why or why not?

# Submission

::: callout-important
To submit the AE:

-   Render the document to produce the PDF with all of your work from today's class.
-   Push all your work to your `ae-11` repo on GitHub. (You do not submit AEs on Gradescope).
:::
