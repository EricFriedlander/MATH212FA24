{
  "hash": "723a1ffd887c90e82153c36b2bc9a335",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Model comparison\"\nauthor: \"Prof. Eric Friedlander\"\ndate: \"2024-10-23\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ğŸ”— MAT 212 - Fall 2024 -  Schedule](https://mat212fa24.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\n  cache: false\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n## Announcements\n\n-   See Ed Discussion for upcoming events and internship opportunities\n\n-   Statistics Experience due Mon, Nov 20 at 11:59pm\n\n-   Please submit [mid-semester feedback](https://duke.qualtrics.com/jfe/form/SV_86XWKdUFjvStYmW) by Friday\n\n-   Prof. Tackett office hours Fridays 1:30 - 3:30pm for the rest of the semester\n\n## Topics\n\n::: nonincremental\n-   ANOVA for multiple linear regression and sum of squares\n-   Comparing models with $R^2$ vs. $R^2_{adj}$\n-   Comparing models with AIC and BIC\n-   Occam's razor and parsimony\n:::\n\n## Computational setup\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\nâœ” dplyr     1.1.4     âœ” readr     2.1.5\nâœ” forcats   1.0.0     âœ” stringr   1.5.1\nâœ” ggplot2   3.5.1     âœ” tibble    3.2.1\nâœ” lubridate 1.9.3     âœ” tidyr     1.3.1\nâœ” purrr     1.0.2     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\nâœ– dplyr::filter() masks stats::filter()\nâœ– dplyr::lag()    masks stats::lag()\nâ„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(broom)\nlibrary(yardstick)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'yardstick'\n\nThe following object is masked from 'package:readr':\n\n    spec\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggformula)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nLoading required package: scales\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nLoading required package: ggridges\n\nNew to ggformula?  Try the tutorials: \n\tlearnr::run_tutorial(\"introduction\", package = \"ggformula\")\n\tlearnr::run_tutorial(\"refining\", package = \"ggformula\")\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.2.0 â”€â”€\nâœ” dials        1.3.0     âœ” rsample      1.2.1\nâœ” infer        1.0.7     âœ” tune         1.2.1\nâœ” modeldata    1.4.0     âœ” workflows    1.1.4\nâœ” parsnip      1.2.1     âœ” workflowsets 1.1.0\nâœ” recipes      1.1.0     \nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nâœ– scales::discard() masks purrr::discard()\nâœ– dplyr::filter()   masks stats::filter()\nâœ– recipes::fixed()  masks stringr::fixed()\nâœ– dplyr::lag()      masks stats::lag()\nâœ– yardstick::spec() masks readr::spec()\nâœ– recipes::step()   masks stats::step()\nâ€¢ Use tidymodels_prefer() to resolve common conflicts.\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n```\n\n\n:::\n\n```{.r .cell-code}\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))\n```\n:::\n\n\n\n# Introduction\n\n## Data: Restaurant tips\n\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n\n::: {.cell layout-align=\"center\"}\n\n:::\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 169 Ã— 4\n     Tip Party Meal   Age   \n   <dbl> <dbl> <chr>  <chr> \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# â„¹ 159 more rows\n```\n\n\n:::\n:::\n\n\n\n## Variables\n\n**Predictors**:\n\n::: nonincremental\n-   `Party`: Number of people in the party\n-   `Meal`: Time of day (Lunch, Dinner, Late Night)\n-   `Age`: Age category of person paying the bill (Yadult, Middle, SenCit)\n:::\n\n**Outcome**: `Tip`: Amount of tip\n\n## Outcome: `Tip`\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-comparison-cv-old_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n## Predictors\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-comparison-cv-old_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n## Relevel categorical predictors\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntips <- tips |>\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )\n```\n:::\n\n\n\n## Predictors, again\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-comparison-cv-old_files/figure-html/unnamed-chunk-8-1.png){fig-align='center' width=3000}\n:::\n:::\n\n\n\n## Outcome vs. predictors\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n![](20-comparison-cv-old_files/figure-html/unnamed-chunk-9-1.png){fig-align='center' width=3600}\n:::\n:::\n\n\n\n## Fit and summarize model {.midi}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntip_fit <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |   -0.170|     0.366|    -0.465|   0.643|\n|Party       |    1.837|     0.124|    14.758|   0.000|\n|AgeMiddle   |    1.009|     0.408|     2.475|   0.014|\n|AgeSenCit   |    1.388|     0.485|     2.862|   0.005|\n\n\n:::\n:::\n\n\n\n. . .\n\n<br>\n\n::: question\nIs this the best model to explain variation in tips?\n:::\n\n## Another model summary\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(tip_fit$fit) |>\n  tidy() |>\n  kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|term      |  df|   sumsq|  meansq| statistic| p.value|\n|:---------|---:|-------:|-------:|---------:|-------:|\n|Party     |   1| 1188.64| 1188.64|    285.71|    0.00|\n|Age       |   2|   38.03|   19.01|      4.57|    0.01|\n|Residuals | 165|  686.44|    4.16|        NA|      NA|\n\n\n:::\n:::\n\n\n\n# Analysis of variance (ANOVA)\n\n## Analysis of variance (ANOVA)\n\n<br>\n\n![](images/13/model-anova.png){fig-align=\"center\"}\n\n## ANOVA\n\n-   **Main Idea:** Decompose the total variation on the outcome into:\n    -   the variation that can be explained by the each of the variables in the model\n\n    -   the variation that **can't** be explained by the model (left in the residuals)\n-   If the variation that can be explained by the variables in the model is greater than the variation in the residuals, this signals that the model might be \"valuable\" (at least one of the $\\beta$s not equal to 0)\n\n## ANOVA output in R[^1]\n\n[^1]: [Click here](anova-table.html) for explanation about the way R calculates sum of squares for each variable.\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nanova(tip_fit$fit) |>\n  tidy() |>\n  kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|term      |  df|   sumsq|  meansq| statistic| p.value|\n|:---------|---:|-------:|-------:|---------:|-------:|\n|Party     |   1| 1188.64| 1188.64|    285.71|    0.00|\n|Age       |   2|   38.03|   19.01|      4.57|    0.01|\n|Residuals | 165|  686.44|    4.16|        NA|      NA|\n\n\n:::\n:::\n\n\n\n## ANOVA output, with totals\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: There was 1 warning in `mutate()`.\nâ„¹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n|term      |  df|   sumsq|meansq  |statistic |p.value |\n|:---------|---:|-------:|:-------|:---------|:-------|\n|Party     |   1| 1188.64|1188.64 |285.71    |0       |\n|Age       |   2|   38.03|19.01   |4.57      |0.01    |\n|Residuals | 165|  686.44|4.16    |          |        |\n|Total     | 168| 1913.11|        |          |        |\n\n\n:::\n:::\n\n\n\n## Sum of squares\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> sumsq </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Party </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 1188.64 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Age </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 38.03 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Residuals </td>\n   <td style=\"text-align:right;\"> 165 </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 686.44 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Total </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 1913.11 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n-   $SS_{Total}$: Total sum of squares, variability of outcome, $\\sum_{i = 1}^n (y_i - \\bar{y})^2$\n-   $SS_{Error}$: Residual sum of squares, variability of residuals, $\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2$\n-   $SS_{Model} = SS_{Total} - SS_{Error}$: Variability explained by the model\n:::\n:::\n\n## Sum of squares: $SS_{Total}$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> sumsq </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Party </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1188.64 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Age </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 38.03 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Residuals </td>\n   <td style=\"text-align:right;\"> 165 </td>\n   <td style=\"text-align:right;\"> 686.44 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;background-color: rgba(217, 227, 228, 255) !important;\"> Total </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 168 </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 1913.11 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n<br>\n\n<center>\n\n$SS_{Total}$: Total sum of squares, variability of outcome\n\n<br>\n\n$\\sum_{i = 1}^n (y_i - \\bar{y})^2$ = 1913.11\n\n</center>\n\n## Sum of squares: $SS_{Error}$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> sumsq </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> Party </td>\n   <td style=\"text-align:right;\"> 1 </td>\n   <td style=\"text-align:right;\"> 1188.64 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Age </td>\n   <td style=\"text-align:right;\"> 2 </td>\n   <td style=\"text-align:right;\"> 38.03 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;background-color: rgba(217, 227, 228, 255) !important;\"> Residuals </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 165 </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 686.44 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Total </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 1913.11 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n<br>\n\n<center>\n\n$SS_{Error}$: Residual sum of squares, variability of residuals\n\n<br>\n\n$\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2$ = 686.44\n\n</center>\n\n## Sum of squares: $SS_{Model}$\n\n\n\n::: {.cell layout-align=\"center\"}\n::: {.cell-output-display}\n`````{=html}\n<table>\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> df </th>\n   <th style=\"text-align:right;\"> sumsq </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;background-color: rgba(217, 227, 228, 255) !important;\"> Party </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 1 </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 1188.64 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;background-color: rgba(217, 227, 228, 255) !important;\"> Age </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 2 </td>\n   <td style=\"text-align:right;background-color: rgba(217, 227, 228, 255) !important;\"> 38.03 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Residuals </td>\n   <td style=\"text-align:right;\"> 165 </td>\n   <td style=\"text-align:right;\"> 686.44 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> Total </td>\n   <td style=\"text-align:right;\"> 168 </td>\n   <td style=\"text-align:right;\"> 1913.11 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n<br>\n\n<center>\n\n$SS_{Model}$: Variability explained by the model\n\n<br>\n\n$SS_{Total} - SS_{Error}$ = 1226.67\n\n</center>\n\n## R-squared, $R^2$\n\n**Recall**: $R^2$ is the proportion of the variation in the response variable explained by the regression model.\n\n. . .\n\n$$\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}} = 1 - \\frac{686.44}{1913.11} = 0.641\n$$\n\n. . .\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nglance(tip_fit)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.6411891\n```\n\n\n:::\n:::\n\n\n\n# Model comparison\n\n## R-squared, $R^2$\n\n-   $R^2$ will always increase as we add more variables to the model + If we add enough variables, we can always achieve $R^2=100\\%$\n-   If we only use $R^2$ to choose a best fit model, we will be prone to choose the model with the most predictor variables\n\n## Adjusted $R^2$\n\n-   **Adjusted** $R^2$: measure that includes a penalty for unnecessary predictor variables\n-   Similar to $R^2$, it is a measure of the amount of variation in the response that is explained by the regression model\n-   Differs from $R^2$ by using the mean squares (sumsq/df) rather than sums of squares and therefore adjusting for the number of predictor variables\n\n## $R^2$ and Adjusted $R^2$\n\n$$R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}$$\n\n<br>\n\n. . .\n\n$$R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}$$\n\nwhere\n\n-   $n$ is the number of observations used to fit the model\n\n-   $p$ is the number of terms (not including the intercept) in the model\n\n## Using $R^2$ and Adjusted $R^2$\n\n-   Adjusted $R^2$ can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\n-   Use $R^2$ when describing the relationship between the response and predictor variables\n\n## Comparing models with $R^2_{adj}$ {.smaller}\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntip_fit_1 <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n    data = tips)\n\nglance(tip_fit_1) |> \n  select(r.squared, adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 2\n  r.squared adj.r.squared\n      <dbl>         <dbl>\n1     0.674         0.664\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntip_fit_2 <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |> \n  select(r.squared, adj.r.squared)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 2\n  r.squared adj.r.squared\n      <dbl>         <dbl>\n1     0.683         0.662\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n::: question\n1.  Why did we not use the full `recipe()` workflow to fit Model 1 or Model 2?\n2.  Which model would we choose based on $R^2$?\n3.  Which model would we choose based on Adjusted $R^2$?\n4.  Which statistic should we use to choose the final model - $R^2$ or Adjusted $R^2$? Why?\n\nVote on Ed Discussion \\[[10:05am lecture](https://edstem.org/us/courses/44523/discussion/462744)\\]\\[[1:25pm lecture](https://edstem.org/us/courses/44523/discussion/462751)\\]\n:::\n\n## AIC & BIC\n\nEstimators of prediction error and *relative* quality of models:\n\n. . .\n\n**Akaike's Information Criterion (AIC)**: $$AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)$$ <br>\n\n. . .\n\n**Schwarz's Bayesian Information Criterion (BIC)**: $$BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)$$\n\n## AIC & BIC\n\n$$\n\\begin{aligned} \n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1) \n\\end{aligned}\n$$\n\n. . .\n\n<br>\n\nFirst Term: Decreases as *p* increases\n\n## AIC & BIC\n\n$$\n\\begin{aligned} \n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1) \n\\end{aligned}\n$$\n\n<br>\n\nSecond Term: Fixed for a given sample size *n*\n\n## AIC & BIC\n\n$$\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)} \n\\end{aligned}\n$$\n\n<br>\n\nThird Term: Increases as *p* increases\n\n## Using AIC & BIC\n\n$$\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)} \n\\end{aligned}\n$$\n\n-   Choose model with the smaller value of AIC or BIC\n\n-   If $n \\geq 8$, the **penalty** for BIC is larger than that of AIC, so BIC tends to favor *more parsimonious* models (i.e. models with fewer terms)\n\n## Comparing models with AIC and BIC\n\n::: columns\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntip_fit_1 <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) |> \n  select(AIC, BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 2\n    AIC   BIC\n  <dbl> <dbl>\n1  714.  736.\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntip_fit_2 <- linear_reg() |>\n  set_engine(\"lm\") |>\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |> \n  select(AIC, BIC)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 2\n    AIC   BIC\n  <dbl> <dbl>\n1  720.  757.\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n::: question\n1.  Which model would we choose based on AIC?\n\n2.  Which model would we choose based on BIC?\n:::\n\n## Commonalities between criteria\n\n-   $R^2_{adj}$, AIC, and BIC all apply a penalty for more predictors\n-   The penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\n-   Goal: **Parsimony**\n\n## Parsimony and Occam's razor {.small}\n\n-   The principle of **parsimony** is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, *the correct explanation is the simplest explanation*[^2]\n\n-   Called **Occam's razor** because he \"shaved\" his explanations down to the bare minimum\n\n-   Parsimony in modeling:\n\n    ::: nonincremental\n    -   models should have as few parameters as possible\n    -   linear models should be preferred to non-linear models\n    -   experiments relying on few assumptions should be preferred to those relying on many\n    -   models should be pared down until they are *minimal adequate*\n    -   simple explanations should be preferred to complex explanations\n    :::\n\n[^2]: Source: The R Book by Michael J. Crawley.\n\n## In pursuit of Occam's razor\n\n-   Occam's razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\n\n-   Model selection follows this principle\n\n-   We only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\n\n-   In other words, we prefer the simplest best model, i.e. **parsimonious** model\n\n## Alternate views {.midi}\n\n> Sometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n>\n> <br>\n>\n> Radford Neal - Bayesian Learning for Neural Networks[^3]\n\n[^3]: Suggested blog post: [Occam](https://statmodeling.stat.columbia.edu/2012/06/26/occam-2/) by Andrew Gelman\n\n## Other concerns with our approach {.midi}\n\n-   All criteria we considered for model comparison require making predictions for our data and then uses the prediction error ($SS_{Error}$) somewhere in the formula\n-   But we're making prediction for the data we used to build the model (estimate the coefficients), which can lead to **overfitting**\n-   Instead we should\n    -   split our data into testing and training sets\n\n    -   \"train\" the model on the training data and pick a few models we're genuinely considering as potentially good models\n\n    -   test those models on the testing set\n\n    -   ...and repeat this process multiple times\n\n# Cross validation\n\n## Spending our data\n\n-   We have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.\n-   However, we usually need to understand the effectiveness of the model [*before*]{.underline} *using the test set*.\n-   Typically we can't decide on *which* final model to take to the test set without making model assessments.\n-   **Remedy:** Resampling to make model assessments on training data in a way that can generalize to new data.\n\n## Resampling for model assessment\n\n**Resampling is only conducted on the** <u>**training**</u> **set**. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\n-   The model is fit with the **analysis set**. Model fit statistics such as $R^2_{Adj}$, AIC, and BIC are calculated based on this fit.\n-   The model is evaluated with the **assessment set**.\n\n## Resampling for model assessment\n\n![](images/16/resampling.svg){fig-align=\"center\"}\n\n<br>\n\nImage source: Kuhn and Silge. [Tidy modeling with R](https://www.tmwr.org/).\n\n## Analysis and assessment sets\n\n-   Analysis set is analogous to training set.\n-   Assessment set is analogous to test set.\n-   The terms *analysis* and *assessment* avoids confusion with initial split of the data.\n-   These data sets are mutually exclusive.\n\n## Cross validation\n\nMore specifically, **v-fold cross validation** -- commonly used resampling technique:\n\n-   Randomly split your **training** **data** into ***v*** partitions\n-   Use ***v-1*** partitions for analysis, and the remaining 1 partition for analysis (model fit + model fit statistics)\n-   Repeat ***v*** times, updating which partition is used for assessment each time\n\n. . .\n\nLet's give an example where `v = 3`...\n\n## To get started...\n\n**Split data into training and test sets**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nset.seed(345)\n\ntips_split <- initial_split(tips)\ntips_train <- training(tips_split)\ntips_test <- testing(tips_split)\n```\n:::\n\n\n\n## To get started...\n\n**Create recipe**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntips_rec1 <- recipe(Tip ~ Party + Age + Meal, \n                    data = tips_train)\n\ntips_rec1\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Recipe â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Inputs \n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nNumber of variables by role\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\noutcome:   1\npredictor: 3\n```\n\n\n:::\n:::\n\n\n\n## To get started... {.midi}\n\n**Specify model**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntips_spec <- linear_reg() |>\n  set_engine(\"lm\")\n```\n:::\n\n\n\n. . .\n\n**Create workflow**\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntips_wflow1 <- workflow() |>\n  add_model(tips_spec) |>\n  add_recipe(tips_rec1)\n\ntips_wflow1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nâ•â• Workflow â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\nPreprocessor: Recipe\nModel: linear_reg()\n\nâ”€â”€ Preprocessor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n0 Recipe Steps\n\nâ”€â”€ Model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n```\n\n\n:::\n:::\n\n\n\n## Cross validation, step 1\n\nRandomly split your **training** **data** into 3 partitions:\n\n<br>\n\n![](images/16/three-CV.svg){fig-align=\"center\"}\n\n## Tips: Split training data\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nfolds <- vfold_cv(tips_train, v = 3)\nfolds\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n#  3-fold cross-validation \n# A tibble: 3 Ã— 2\n  splits          id   \n  <list>          <chr>\n1 <split [84/42]> Fold1\n2 <split [84/42]> Fold2\n3 <split [84/42]> Fold3\n```\n\n\n:::\n:::\n\n\n\n## Cross validation, steps 2 and 3\n\n::: nonincremental\n-   Use *v-1* partitions for analysis, and the remaining 1 partition for assessment\n-   Repeat *v* times, updating which partition is used for assessment each time\n:::\n\n![](images/16/three-CV-iter.svg){fig-align=\"center\"}\n\n## Tips: Fit resamples {.midi}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntips_fit_rs1 <- tips_wflow1 |>\n  fit_resamples(folds)\n\ntips_fit_rs1 \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 Ã— 4\n  splits          id    .metrics         .notes          \n  <list>          <chr> <list>           <list>          \n1 <split [84/42]> Fold1 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n2 <split [84/42]> Fold2 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n3 <split [84/42]> Fold3 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]>\n```\n\n\n:::\n:::\n\n\n\n## Cross validation, now what?\n\n-   We've fit a bunch of models\n-   Now it's time to use them to collect metrics (e.g., \\$R\\^2\\$, AIC, RMSE, etc. ) on each model and use them to evaluate model fit and how it varies across folds\n\n## Collect $R^2$ and RMSE from CV\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Produces summary across all CV\ncollect_metrics(tips_fit_rs1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 Ã— 6\n  .metric .estimator  mean     n std_err .config             \n  <chr>   <chr>      <dbl> <int>   <dbl> <chr>               \n1 rmse    standard   2.10      3  0.243  Preprocessor1_Model1\n2 rsq     standard   0.591     3  0.0728 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n<br>\n\nNote: These are calculated using the *assessment* data\n\n## Deeper look into $R^2$ and RMSE\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_metrics1 <- collect_metrics(tips_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 6 Ã— 5\n  id    .metric .estimator .estimate .config             \n  <chr> <chr>   <chr>          <dbl> <chr>               \n1 Fold1 rmse    standard       2.04  Preprocessor1_Model1\n2 Fold1 rsq     standard       0.736 Preprocessor1_Model1\n3 Fold2 rmse    standard       1.71  Preprocessor1_Model1\n4 Fold2 rsq     standard       0.509 Preprocessor1_Model1\n5 Fold3 rmse    standard       2.54  Preprocessor1_Model1\n6 Fold3 rsq     standard       0.528 Preprocessor1_Model1\n```\n\n\n:::\n:::\n\n\n\n## Better tabulation of $R^2$ and RMSE from CV\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_metrics1 |>\n  mutate(.estimate = round(.estimate, 3)) |>\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) |>\n  kable(col.names = c(\"Fold\", \"RMSE\", \"R-squared\"))\n```\n\n::: {.cell-output-display}\n\n\n|Fold  |  RMSE| R-squared|\n|:-----|-----:|---------:|\n|Fold1 | 2.036|     0.736|\n|Fold2 | 1.709|     0.509|\n|Fold3 | 2.545|     0.528|\n\n\n:::\n:::\n\n\n\n## How does RMSE compare to y? {.small}\n\n::: columns\n::: {.column width=\"50%\"}\nCross validation RMSE stats:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ncv_metrics1 |>\n  filter(.metric == \"rmse\") |>\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 4\n    min   max  mean    sd\n  <dbl> <dbl> <dbl> <dbl>\n1  1.71  2.54  2.10 0.421\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\nTraining data tips:\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\ntips_train |>\n  summarise(\n    min = min(Tip),\n    max = max(Tip),\n    mean = mean(Tip),\n    sd = sd(Tip)\n  )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 Ã— 4\n    min   max  mean    sd\n  <dbl> <dbl> <dbl> <dbl>\n1     0  19.5  4.87  3.37\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n## Calculate $R^2_{Adj}$, AIC, and BIC for each fold {.midi}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\n# Function to get Adj R-sq, AIC, BIC\ncalc_model_stats <- function(x) {\n  glance(extract_fit_parsnip(x)) |>\n    select(adj.r.squared, AIC, BIC)\n}\n\n# Fit model and calculate statistics for each fold\ntips_fit_rs1 <-tips_wflow1 |>\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))\n\ntips_fit_rs1\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 Ã— 5\n  splits          id    .metrics         .notes           .extracts       \n  <list>          <chr> <list>           <list>           <list>          \n1 <split [84/42]> Fold1 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble [1 Ã— 2]>\n2 <split [84/42]> Fold2 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble [1 Ã— 2]>\n3 <split [84/42]> Fold3 <tibble [2 Ã— 4]> <tibble [0 Ã— 3]> <tibble [1 Ã— 2]>\n```\n\n\n:::\n:::\n\n\n\n## Collect $R^2_{Adj}$, AIC, BIC from CV {.midi}\n\n\n\n::: {.cell layout-align=\"center\"}\n\n```{.r .cell-code}\nmap_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) |>\n  bind_cols(Fold = tips_fit_rs1$id)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 Ã— 4\n  adj.r.squared   AIC   BIC Fold \n          <dbl> <dbl> <dbl> <chr>\n1         0.550  369.  386. Fold1\n2         0.659  377.  394. Fold2\n3         0.718  337.  354. Fold3\n```\n\n\n:::\n:::\n\n\n\n<br>\n\nNote: These are based on the model fit from the *analysis* data\n\n## Cross validation in practice\n\n::: incremental\n-   To illustrate how CV works, we used `v = 3`:\n\n    ::: nonincremental\n    -   Analysis sets are 2/3 of the training set\n    -   Each assessment set is a distinct 1/3\n    -   The final resampling estimate of performance averages each of the 3 replicates\n    :::\n\n-   This was useful for illustrative purposes, but `v = 3` is a poor choice in practice\n\n-   Values of `v` are most often 5 or 10; we generally prefer 10-fold cross-validation as a default\n:::\n\n## Recap\n\n-   ANOVA for multiple linear regression and sum of squares\n-   Comparing models with\n    -   $R^2$ vs.Â $R^2_{Adj}$\n    -   AIC and BIC\n-   Occam's razor and parsimony\n\n<!-- -->\n\n-   Cross validation for\n\n    -   model evaluation\n\n    -   model comparison\n",
    "supporting": [
      "20-comparison-cv-old_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}