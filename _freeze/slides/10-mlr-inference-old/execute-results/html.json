{
  "hash": "5136e8125e616ba0180173cd8e5956b8",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"MLR: Inference, conditions, and transformations\"\nauthor: \"Prof. Eric Friedlander\"\ndate: \"2024-08-28\"\ndate-format: \"MMM DD, YYYY\"\nfooter: \"[ðŸ”— MAT 212 - Fall 2024 -  Schedule](https://mat212fa24.netlify.app/schedule)\"\nlogo: \"../images/logo.png\"\nformat: \n  revealjs:\n    theme: slides.scss\n    multiplex: false\n    transition: fade\n    slide-number: false\n    incremental: false \n    chalkboard: true\nhtml-math-method:\n  method: mathjax\n  url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js\"\nexecute:\n  freeze: auto\n  echo: true\n  cache: true\nknitr:\n  opts_chunk: \n    R.options:      \n    width: 200\nbibliography: references.bib\n---\n\n\n\n\n\n<!-- ## Announcements -->\n\n<!-- -   Project propsal due -->\n\n<!--     -   Friday, October 27 (Tuesday labs) -->\n\n<!--     -   Sunday, October 29 (Thursday labs) -->\n\n<!-- -   HW 03 due Wednesday, November 1 -->\n\n<!--     -   released after Section 002 lecture -->\n\n## Topics\n\n::: nonincremental\n-   Inference for multiple linear regression\n-   Checking model conditions\n-   Variable transformations\n:::\n\n## Computational setup\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Attaching packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels 1.2.0 â”€â”€\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâœ” dials        1.3.0     âœ” rsample      1.2.1\nâœ” infer        1.0.7     âœ” tune         1.2.1\nâœ” modeldata    1.4.0     âœ” workflows    1.1.4\nâœ” parsnip      1.2.1     âœ” workflowsets 1.1.0\nâœ” recipes      1.1.0     âœ” yardstick    1.3.1\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nâ”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidymodels_conflicts() â”€â”€\nâœ– scales::discard()        masks purrr::discard()\nâœ– dplyr::filter()          masks stats::filter()\nâœ– recipes::fixed()         masks stringr::fixed()\nâœ– kableExtra::group_rows() masks dplyr::group_rows()\nâœ– dplyr::lag()             masks stats::lag()\nâœ– yardstick::spec()        masks readr::spec()\nâœ– Hmisc::src()             masks dplyr::src()\nâœ– recipes::step()          masks stats::step()\nâœ– Hmisc::summarize()       masks dplyr::summarize()\nâœ– parsnip::translate()     masks Hmisc::translate()\nâ€¢ Use tidymodels_prefer() to resolve common conflicts.\n```\n\n\n:::\n\n```{.r .cell-code}\nlibrary(ggformula)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))\n```\n:::\n\n\n\n# Inference for multiple linear regression\n\n\n## Data: `rail_trail` {.smaller}\n\n::: nonincremental\n-   The Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\n-   Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\nRows: 90 Columns: 7\nâ”€â”€ Column specification â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nDelimiter: \",\"\nchr (2): season, day_type\ndbl (5): volume, hightemp, avgtemp, cloudcover, precip\n\nâ„¹ Use `spec()` to retrieve the full column specification for this data.\nâ„¹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n```\n\n\n:::\n\n::: {.cell-output-display}\n\n\n| volume| hightemp| avgtemp|season | cloudcover| precip|day_type |\n|------:|--------:|-------:|:------|----------:|------:|:--------|\n|    501|       83|    66.5|Summer |        7.6|   0.00|Weekday  |\n|    419|       73|    61.0|Summer |        6.3|   0.29|Weekday  |\n|    397|       74|    63.0|Spring |        7.5|   0.32|Weekday  |\n|    385|       95|    78.0|Summer |        2.6|   0.00|Weekend  |\n|    200|       44|    48.0|Spring |       10.0|   0.14|Weekday  |\n|    375|       69|    61.5|Spring |        6.6|   0.02|Weekday  |\n|    417|       66|    52.5|Spring |        2.4|   0.00|Weekday  |\n|    629|       66|    52.0|Spring |        0.0|   0.00|Weekend  |\n|    533|       80|    67.5|Summer |        3.8|   0.00|Weekend  |\n|    547|       79|    62.0|Summer |        4.1|   0.00|Weekday  |\n\n\n:::\n:::\n\n\n\nSource: [Pioneer Valley Planning Commission](http://www.fvgreenway.org/pdfs/Northampton-Bikepath-Volume-Counts%20_05_LTA.pdf) via the **mosaicData** package.\n\n## Variables {.smaller}\n\n**Outcome**:\n\n`volume` estimated number of trail users that day (number of breaks recorded)\n\n. . .\n\n**Predictors**\n\n::: nonincremental\n-   `hightemp` daily high temperature (in degrees Fahrenheit)\n-   `avgtemp` average of daily low and daily high temperature (in degrees Fahrenheit)\n-   `season` one of \"Fall\", \"Spring\", or \"Summer\"\n-   `cloudcover` measure of cloud cover (in oktas)\n-   `precip` measure of precipitation (in inches)\n-   `day_type` one of \"weekday\" or \"weekend\"\n:::\n\n# Conduct a hypothesis test for $\\beta_j$\n\n## [Review: Simple linear regression (SLR)]{.r-fit-text}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(volume ~ hightemp, data = rail_trail, alpha = 0.5) |> \n  gf_lm()  |> \n  gf_labs(x = \"High temp (F)\", y = \"Number of riders\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nâ„¹ Please use the `linewidth` aesthetic instead.\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n## SLR model summary\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_slr_fit <- lm(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit) |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |   -17.08|     59.40|     -0.29|    0.77|\n|hightemp    |     5.70|      0.85|      6.72|    0.00|\n\n\n:::\n:::\n\n\n\n\n## SLR hypothesis test {.midi}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |   -17.08|     59.40|     -0.29|    0.77|\n|hightemp    |     5.70|      0.85|      6.72|    0.00|\n\n\n:::\n:::\n\n\n\n1.  **Set hypotheses:** $H_0: \\beta_1 = 0$ vs. $H_A: \\beta_1 \\ne 0$\n\n. . .\n\n2.  **Calculate test statistic and p-value:** The test statistic is $t= 6.72$ . The p-value is calculated using a $t$ distribution with 88 degrees of freedom. The p-value is $\\approx 0$ .\n\n. . .\n\n3.  **State the conclusion:** The p-value is small, so we reject $H_0$. The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders, i.e. there is a linear relationship between high temperature and number of daily riders.\n\n## Multiple linear regression\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_mlr_main_fit <- lm(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit) |> kable(digits = 2)\n```\n\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value|\n|:------------|--------:|---------:|---------:|-------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|\n\n\n:::\n:::\n\n\n\n## Multiple linear regression\n\nThe multiple linear regression model assumes $$Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)$$\n\n. . .\n\nFor a given observation $(x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)$, we can rewrite the previous statement as\n\n$$y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)$$\n\n------------------------------------------------------------------------\n\n## Estimating $\\sigma_\\epsilon$\n\nFor a given observation $(x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)$ the residual is \n$$\n\\begin{aligned}\ne_i &= y_{i} - \\hat{y_i}\\\\\n&= y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\n\\end{aligned}\n$$\n\n. . .\n\nThe estimated value of the regression standard error , $\\sigma_{\\epsilon}$, is\n\n$$\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}$$\n\n. . .\n\nAs with SLR, we use $\\hat{\\sigma}_{\\epsilon}$ to calculate $SE_{\\hat{\\beta}_j}$, the standard error of each coefficient. See [Matrix Form of Linear Regression](https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf) for more detail.\n\n## MLR hypothesis test: hightemp {.midi}\n\n1.  **Set hypotheses:** $H_0: \\beta_{hightemp} = 0$ vs. $H_A: \\beta_{hightemp} \\ne 0$, given `season` is in the model\n\n. . .\n\n2.  **Calculate test statistic and p-value:** The test statistic is $t = 6.43$. The p-value is calculated using a $t$ distribution with 86 $(n - p - 1)$ degrees of freedom. The p-value is $\\approx 0$.\n\n. . .\n\n3.  **State the conclusion:** The p-value is small, so we reject $H_0$. The data provide strong evidence that high temperature for the day is a useful predictor in a model that already contains the season as a predictor for number of daily riders.\n\n## The model for `season = Spring` {.smaller}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value|\n|:------------|--------:|---------:|---------:|-------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|\n\n\n:::\n:::\n\n\n\n<br>\n\n. . .\n\n$$\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 1 - 76.84 \\times 0 \\\\\n&= -120.10 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n$$\n\n## The model for `season = Summer` {.smaller}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value|\n|:------------|--------:|---------:|---------:|-------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|\n\n\n:::\n:::\n\n\n\n<br>\n\n. . .\n\n$$\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 1 \\\\\n&= -202.07 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n$$\n\n## The model for `season = Fall` {.smaller}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term         | estimate| std.error| statistic| p.value|\n|:------------|--------:|---------:|---------:|-------:|\n|(Intercept)  |  -125.23|     71.66|     -1.75|    0.08|\n|hightemp     |     7.54|      1.17|      6.43|    0.00|\n|seasonSpring |     5.13|     34.32|      0.15|    0.88|\n|seasonSummer |   -76.84|     47.71|     -1.61|    0.11|\n\n\n:::\n:::\n\n\n\n<br>\n\n. . .\n\n$$\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 0 \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n$$\n\n## The models\n\nSame slope, different intercepts\n\n-   `season = Spring`: $-120.10 + 7.54 \\times \\texttt{hightemp}$\n-   `season = Summer`: $-202.07 + 7.54 \\times \\texttt{hightemp}$\n-   `season = Fall`: $-125.23 + 7.54 \\times \\texttt{hightemp}$\n\n## Interaction terms\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 24px; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -10.53 </td>\n   <td style=\"text-align:right;\"> 166.80 </td>\n   <td style=\"text-align:right;\"> -0.06 </td>\n   <td style=\"text-align:right;\"> 0.95 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hightemp </td>\n   <td style=\"text-align:right;\"> 5.48 </td>\n   <td style=\"text-align:right;\"> 2.95 </td>\n   <td style=\"text-align:right;\"> 1.86 </td>\n   <td style=\"text-align:right;\"> 0.07 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> seasonSpring </td>\n   <td style=\"text-align:right;\"> -293.95 </td>\n   <td style=\"text-align:right;\"> 190.33 </td>\n   <td style=\"text-align:right;\"> -1.54 </td>\n   <td style=\"text-align:right;\"> 0.13 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> seasonSummer </td>\n   <td style=\"text-align:right;\"> 354.18 </td>\n   <td style=\"text-align:right;\"> 255.08 </td>\n   <td style=\"text-align:right;\"> 1.39 </td>\n   <td style=\"text-align:right;\"> 0.17 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hightemp:seasonSpring </td>\n   <td style=\"text-align:right;\"> 4.88 </td>\n   <td style=\"text-align:right;\"> 3.26 </td>\n   <td style=\"text-align:right;\"> 1.50 </td>\n   <td style=\"text-align:right;\"> 0.14 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hightemp:seasonSummer </td>\n   <td style=\"text-align:right;\"> -4.54 </td>\n   <td style=\"text-align:right;\"> 3.75 </td>\n   <td style=\"text-align:right;\"> -1.21 </td>\n   <td style=\"text-align:right;\"> 0.23 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n::: question\nDo the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms.\n:::\n\n# Confidence interval for $\\beta_j$\n\n## Confidence interval for $\\beta_j$ {.midi}\n\n-   The $C\\%$ confidence interval for $\\beta_j$ $$\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)$$ where $t^*$ follows a $t$ distribution with $n - p - 1$ degrees of freedom.\n\n-   **Generically**: We are $C\\%$ confident that the interval LB to UB contains the population coefficient of $x_j$.\n\n-   **In context:** We are $C\\%$ confident that for every one unit increase in $x_j$, we expect $y$ to change by LB to UB units, holding all else constant.\n\n## Confidence interval for $\\beta_j$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(rt_mlr_main_fit, conf.int = TRUE) |>\n  kable(\"html\", digits = 2) |> kable_styling(font_size = 24)\n```\n\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 24px; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n   <th style=\"text-align:right;\"> conf.low </th>\n   <th style=\"text-align:right;\"> conf.high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -125.23 </td>\n   <td style=\"text-align:right;\"> 71.66 </td>\n   <td style=\"text-align:right;\"> -1.75 </td>\n   <td style=\"text-align:right;\"> 0.08 </td>\n   <td style=\"text-align:right;\"> -267.68 </td>\n   <td style=\"text-align:right;\"> 17.22 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hightemp </td>\n   <td style=\"text-align:right;\"> 7.54 </td>\n   <td style=\"text-align:right;\"> 1.17 </td>\n   <td style=\"text-align:right;\"> 6.43 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 5.21 </td>\n   <td style=\"text-align:right;\"> 9.87 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> seasonSpring </td>\n   <td style=\"text-align:right;\"> 5.13 </td>\n   <td style=\"text-align:right;\"> 34.32 </td>\n   <td style=\"text-align:right;\"> 0.15 </td>\n   <td style=\"text-align:right;\"> 0.88 </td>\n   <td style=\"text-align:right;\"> -63.10 </td>\n   <td style=\"text-align:right;\"> 73.36 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> seasonSummer </td>\n   <td style=\"text-align:right;\"> -76.84 </td>\n   <td style=\"text-align:right;\"> 47.71 </td>\n   <td style=\"text-align:right;\"> -1.61 </td>\n   <td style=\"text-align:right;\"> 0.11 </td>\n   <td style=\"text-align:right;\"> -171.68 </td>\n   <td style=\"text-align:right;\"> 18.00 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n## CI for `hightemp` {.midi}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 24px; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n   <th style=\"text-align:right;\"> conf.low </th>\n   <th style=\"text-align:right;\"> conf.high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -125.23 </td>\n   <td style=\"text-align:right;\"> 71.66 </td>\n   <td style=\"text-align:right;\"> -1.75 </td>\n   <td style=\"text-align:right;\"> 0.08 </td>\n   <td style=\"text-align:right;\"> -267.68 </td>\n   <td style=\"text-align:right;\"> 17.22 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hightemp </td>\n   <td style=\"text-align:right;\"> 7.54 </td>\n   <td style=\"text-align:right;\"> 1.17 </td>\n   <td style=\"text-align:right;\"> 6.43 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 5.21 </td>\n   <td style=\"text-align:right;\"> 9.87 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> seasonSpring </td>\n   <td style=\"text-align:right;\"> 5.13 </td>\n   <td style=\"text-align:right;\"> 34.32 </td>\n   <td style=\"text-align:right;\"> 0.15 </td>\n   <td style=\"text-align:right;\"> 0.88 </td>\n   <td style=\"text-align:right;\"> -63.10 </td>\n   <td style=\"text-align:right;\"> 73.36 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> seasonSummer </td>\n   <td style=\"text-align:right;\"> -76.84 </td>\n   <td style=\"text-align:right;\"> 47.71 </td>\n   <td style=\"text-align:right;\"> -1.61 </td>\n   <td style=\"text-align:right;\"> 0.11 </td>\n   <td style=\"text-align:right;\"> -171.68 </td>\n   <td style=\"text-align:right;\"> 18.00 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n<br>\n\nWe are 95% confident that for every degree Fahrenheit the day is warmer, the number of riders increases by 5.21 to 9.87, on average, holding season constant.\n\n## CI for `seasonSpring` {.midi}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n`````{=html}\n<table class=\"table\" style=\"font-size: 24px; margin-left: auto; margin-right: auto;\">\n <thead>\n  <tr>\n   <th style=\"text-align:left;\"> term </th>\n   <th style=\"text-align:right;\"> estimate </th>\n   <th style=\"text-align:right;\"> std.error </th>\n   <th style=\"text-align:right;\"> statistic </th>\n   <th style=\"text-align:right;\"> p.value </th>\n   <th style=\"text-align:right;\"> conf.low </th>\n   <th style=\"text-align:right;\"> conf.high </th>\n  </tr>\n </thead>\n<tbody>\n  <tr>\n   <td style=\"text-align:left;\"> (Intercept) </td>\n   <td style=\"text-align:right;\"> -125.23 </td>\n   <td style=\"text-align:right;\"> 71.66 </td>\n   <td style=\"text-align:right;\"> -1.75 </td>\n   <td style=\"text-align:right;\"> 0.08 </td>\n   <td style=\"text-align:right;\"> -267.68 </td>\n   <td style=\"text-align:right;\"> 17.22 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> hightemp </td>\n   <td style=\"text-align:right;\"> 7.54 </td>\n   <td style=\"text-align:right;\"> 1.17 </td>\n   <td style=\"text-align:right;\"> 6.43 </td>\n   <td style=\"text-align:right;\"> 0.00 </td>\n   <td style=\"text-align:right;\"> 5.21 </td>\n   <td style=\"text-align:right;\"> 9.87 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> seasonSpring </td>\n   <td style=\"text-align:right;\"> 5.13 </td>\n   <td style=\"text-align:right;\"> 34.32 </td>\n   <td style=\"text-align:right;\"> 0.15 </td>\n   <td style=\"text-align:right;\"> 0.88 </td>\n   <td style=\"text-align:right;\"> -63.10 </td>\n   <td style=\"text-align:right;\"> 73.36 </td>\n  </tr>\n  <tr>\n   <td style=\"text-align:left;\"> seasonSummer </td>\n   <td style=\"text-align:right;\"> -76.84 </td>\n   <td style=\"text-align:right;\"> 47.71 </td>\n   <td style=\"text-align:right;\"> -1.61 </td>\n   <td style=\"text-align:right;\"> 0.11 </td>\n   <td style=\"text-align:right;\"> -171.68 </td>\n   <td style=\"text-align:right;\"> 18.00 </td>\n  </tr>\n</tbody>\n</table>\n\n`````\n:::\n:::\n\n\n\n<br>\n\nWe are 95% confident that the number of riders on a Spring day is lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant.\n\n. . .\n\n::: question\nIs `season` a significant predictor of the number of riders, after accounting for high temperature?\n:::\n\n# Inference pitfalls\n\n## Large sample sizes\n\n::: callout-caution\nIf the sample size is large enough, the test will likely result in rejecting $H_0: \\beta_j = 0$ even $x_j$ has a very small effect on $y$.\n\n::: nonincremental\n-   Consider the **practical significance** of the result not just the statistical significance.\n\n-   Use the confidence interval to draw conclusions instead of relying only p-values.\n:::\n:::\n\n## Small sample sizes\n\n::: callout-caution\nIf the sample size is small, there may not be enough evidence to reject $H_0: \\beta_j=0$.\n\n::: nonincremental\n-   When you fail to reject the null hypothesis, **DON'T** immediately conclude that the variable has no association with the response.\n\n-   There may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n:::\n:::\n\n# Conditions for inference\n\n## Full model {.smaller}\n\nIncluding all available predictors\n\n**Fit:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_full_fit <- lm(volume ~ ., data = rail_trail)\n```\n:::\n\n\n\n. . .\n\n**Summarize:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(rt_full_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 8 Ã— 5\n  term            estimate std.error statistic p.value\n  <chr>              <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n```\n\n\n:::\n:::\n\n\n\n. . .\n\n**Augment:**\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrt_full_aug <- augment(rt_full_fit)\n```\n:::\n\n\n\n## Model conditions\n\n1.  **Linearity:** There is a linear relationship between the response and predictor variables.\n\n2.  **Constant Variance:** The variability about the least squares line is generally constant.\n\n3.  **Normality:** The distribution of the residuals is approximately normal.\n\n4.  **Independence:** The residuals are independent from each other.\n\n\n## Checking Linearity\n\n-   Look at a plot of the residuals vs. predicted values\n\n-   Look at a plot of the residuals vs. each predictor\n\n-   Linearity is met if there is no discernible pattern in each of these plots\n\n## Residuals vs. predicted values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_point(.resid ~ .fitted, data = rt_full_aug, alpha = 0.7) |> \n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") |> \n  gf_labs(x = \"Predicted values\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/main_res_pred-1.png){width=672}\n:::\n:::\n\n\n\n## Residuals vs. each predictor\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-19-1.png){width=672}\n:::\n:::\n\n\n\n## Checking linearity\n\n-   The plot of the residuals vs. predicted values looked OK\n\n-   The plots of residuals vs. `hightemp` and `avgtemp` appear to have a parabolic pattern.\n\n-   The linearity condition does not appear to be satisfied given these plots.\n\n. . .\n\n::: question\nGiven this conclusion, what might be a next step in the analysis?\n:::\n\n## Checking constant variance\n\n::: question\nDoes the constant variance condition appear to be satisfied?\n:::\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\n\n## Checking constant variance\n\n-   The vertical spread of the residuals is not constant across the plot.\n\n-   The constant variance condition is not satisfied.\n\n. . .\n\n::: question\nWe will talk about to address this later in the notes.\n:::\n\n## Checking normality\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\n\nThe distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size 90 is sufficiently large to relax this condition if it was not satisfied.\n\n## Checking independence\n\n-   We can often check the independence condition based on the context of the data and how the observations were collected.\n\n-   If the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\n\n-   If there is a grouping variable lurking in the background, check the residuals based on that grouping variable.\n\n## Checking independence {.midi}\n\nResiduals vs. order of data collection:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\ngf_line(.resid ~ 1:nrow(rt_full_aug), data = rt_full_aug) |> \n  gf_point()  |> \n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\")  |> \n  gf_labs(x = \"Order of data collection\", y = \"Residuals\")\n```\n\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n\n## Checking independence\n\n-   No clear pattern in the residuals vs. order of data collection plot.\n\n-   Independence condition appears to be satisfied, as far as we can evaluate it.\n\n# Multicollinearity\n\n## What is multicollinearity\n\n**Multicollinearity** is the case when two or more predictor variables are strongly correlated with one another\n\n## Example\n\nLet's assume the true population regression equation is $y = 3 + 4x$\n\n. . .\n\nSuppose we try estimating that equation using a model with variables $x$ and $z = x/10$\n\n$$\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n$$\n\n## Example\n\n$$\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x$$\n\n-   We can set $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$ to any two numbers such that $\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4$\n\n-   Therefore, we are unable to choose the \"best\" combination of $\\hat{\\beta}_1$ and $\\hat{\\beta}_2$\n\n## Why multicollinearity is a problem\n\n-   When we have perfect collinearities, we are unable to get estimates for the coefficients\n\n    -   When we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate\n\n    -   In other words, we lose precision in our estimates of the regression coefficients\n\n    -   This impedes our ability to use the model for inference\n\n    -   It is also difficult to interpret the model coefficients\n\n## Detecting Multicollinearity {.midi}\n\nMulticollinearity may occur when...\n\n-   There are very high correlations $(r > 0.9)$ among two or more predictor variables, especially when the sample size is small\n\n<!-- -->\n\n-   One (or more) predictor variables is an almost perfect linear combination of the others\n\n-   There is a quadratic term in the model without mean-centering the variable first\n\n-   There are interactions between two or more continuous variables\n\n    -   Can reduce this by mean-centering the variables first\n\n-   There is a categorical predictor with very few observations in the baseline level\n\n## Detecting multicollinearity in the EDA\n\n-   Look at a correlation matrix of the predictor variables, including all indicator variables\n    -   Look out for values close to 1 or -1\n-   Look at a scatterplot matrix of the predictor variables\n    -   Look out for plots that show a relatively linear relationship\n-   Look at the distribution of categorical predictors and avoid setting the baseline to a category with very few observations\n\n\n## Detecting Multicollinearity (VIF)\n\n**Variance Inflation Factor (VIF)**: Measure of multicollinearity in the regression model\n\n$$VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}$$\n\nwhere $R^2_{X_j|X_{-j}}$ is the proportion of variation $X$ that is explained by the linear combination of the other explanatory variables in the model.\n\n## Detecting Multicollinearity (VIF)\n\n-   Typically $VIF > 5$ indicates concerning multicollinearity\n\n-   Variables with similar values of VIF are typically the ones correlated with each other\n\n-   Use the `vif()` function in the **rms** R package to calculate VIF\n\n## VIF for rail trail model\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nvif(rt_full_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n```\n\n\n:::\n:::\n\n\n\n<br>\n\n. . .\n\n`hightemp`, `avgtemp`, and `seasonSummer` are correlated. \n\n## Solutions for Multicollinearity\n\n1. Drop some predictors\n2. Combine some predictors\n3. Discount the individual coefficients and t-tests (i.e. predictions are meaningful but coefficients, tests, and confidence intervals are not)\n\n. . .\n\nLet's try removing `hightemp` and `avgtemp` (separately)\n\n## Model without `hightemp` {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm1 <- lm(volume ~ . - hightemp, data = rail_trail)\n  \nm1 |>\n  tidy() |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term            | estimate| std.error| statistic| p.value|\n|:---------------|--------:|---------:|---------:|-------:|\n|(Intercept)     |   76.071|    77.204|     0.985|   0.327|\n|avgtemp         |    6.003|     1.583|     3.792|   0.000|\n|seasonSpring    |   34.555|    34.454|     1.003|   0.319|\n|seasonSummer    |   13.531|    55.024|     0.246|   0.806|\n|cloudcover      |  -12.807|     3.488|    -3.672|   0.000|\n|precip          | -110.736|    44.137|    -2.509|   0.014|\n|day_typeWeekend |   48.420|    22.993|     2.106|   0.038|\n\n\n:::\n:::\n\n\n\n## Model without `avgtemp` {.smaller}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nm2 <- lm(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 |>\n  tidy() |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term            | estimate| std.error| statistic| p.value|\n|:---------------|--------:|---------:|---------:|-------:|\n|(Intercept)     |    8.421|    74.992|     0.112|   0.911|\n|hightemp        |    5.696|     1.164|     4.895|   0.000|\n|seasonSpring    |   31.239|    32.082|     0.974|   0.333|\n|seasonSummer    |    9.424|    47.504|     0.198|   0.843|\n|cloudcover      |   -8.353|     3.435|    -2.431|   0.017|\n|precip          |  -98.904|    42.137|    -2.347|   0.021|\n|day_typeWeekend |   37.062|    22.280|     1.663|   0.100|\n\n\n:::\n:::\n\n\n\n## Choosing a model {.midi}\n\nModel without **hightemp**:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| adj.r.squared|    AIC|    BIC|\n|-------------:|------:|------:|\n|          0.42| 1087.5| 1107.5|\n\n\n:::\n:::\n\n\n\nModel without **avgtemp**:\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n| adj.r.squared|     AIC|     BIC|\n|-------------:|-------:|-------:|\n|          0.47| 1079.05| 1099.05|\n\n\n:::\n:::\n\n\n\n. . .\n\nBased on Adjusted $R^2$, AIC, and BIC, the model without **avgtemp** is a better fit. Therefore, we choose to remove **avgtemp** from the model and leave **hightemp** in the model to deal with the multicollinearity.\n\n## Selected model (for now)\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term            | estimate| std.error| statistic| p.value|\n|:---------------|--------:|---------:|---------:|-------:|\n|(Intercept)     |    8.421|    74.992|     0.112|   0.911|\n|hightemp        |    5.696|     1.164|     4.895|   0.000|\n|seasonSpring    |   31.239|    32.082|     0.974|   0.333|\n|seasonSummer    |    9.424|    47.504|     0.198|   0.843|\n|cloudcover      |   -8.353|     3.435|    -2.431|   0.017|\n|precip          |  -98.904|    42.137|    -2.347|   0.021|\n|day_typeWeekend |   37.062|    22.280|     1.663|   0.100|\n\n\n:::\n:::\n\n\n\n# Variable transformations\n\n## Topics\n\n-   Log transformation on the response variable\n\n-   Log transformation on the predictor variable\n\n## Residuals vs. fitted for the selected model {.midi}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-29-1.png){width=672}\n:::\n:::\n\n\n\nThe constant variance condition is not satisfied. We can transform the response variable to address the violation in this condition.\n\n# Log transformation on the response variable\n\n## Identifying a need to transform $Y$ {.midi}\n\n-   Typically, a \"fan-shaped\" residual plot indicates the need for a transformation of the response variable $Y$\n    -   There are multiple ways to transform a variable, e.g., $\\sqrt{Y}$, $1/Y$, $\\log(Y)$\n    -   $\\log(Y)$ the most straightforward to interpret, so we use that transformation when possible\n\n. . .\n\n-   When building a model:\n    -   Choose a transformation and build the model on the transformed data\n    -   Reassess the residual plots\n    -   If the residuals plots did not sufficiently improve, try a new transformation!\n\n## Log transformation on $Y$\n\n-   If we apply a log transformation to the response variable, we want to estimate the parameters for the statistical model\n\n$$\n\\log(Y) = \\beta_0+ \\beta_1 X_1 + \\dots +\\beta_pX_p + \\epsilon, \\hspace{10mm} \\epsilon \\sim N(0,\\sigma^2_\\epsilon)\n$$\n\n-   The regression equation is\n\n$$\\widehat{\\log(Y)} = \\hat{\\beta}_0+ \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_pX_p$$\n\n## Log transformation on $Y$\n\nWe want to interpret the model in terms of the original variable $Y$, not $\\log(Y)$, so we need to write the regression equation in terms of $Y$\n\n$$\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}$$\n\n::: callout-note\nThe predicted value $\\hat{Y}$ is the predicted *median* of $Y$. Note, when the distribution of $Y|X_1, \\ldots, X_p$ is symmetric, then the median equals the mean. See the slides in the [appendix] for more detail.\n:::\n\n## Model interpretation {.midi}\n\n$$\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}$$\n\n. . .\n\n-   **Intercept**: When $X_1 = \\dots = X_p =0$, $Y$ is expected to be $\\exp\\{\\hat{\\beta}_0\\}$\n\n-   **Slope:** For every one unit increase in $X_j$, the $Y$ is expected to multiply by a factor of $\\exp\\{\\hat{\\beta}_j\\}$, holding all else constant\n\n::: question\nWhy is the interpretation in terms of a multiplicative change?\n:::\n\n## Model for $log(volume)$ {.midi}\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n#fit model\nlog_rt_fit <- lm(log(volume) ~ hightemp + season + cloudcover + precip + day_type, data = rail_trail)\n\ntidy(log_rt_fit) |>\n  kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term            | estimate| std.error| statistic| p.value|\n|:---------------|--------:|---------:|---------:|-------:|\n|(Intercept)     |    4.738|     0.219|    21.667|   0.000|\n|hightemp        |    0.018|     0.003|     5.452|   0.000|\n|seasonSpring    |    0.026|     0.094|     0.283|   0.778|\n|seasonSummer    |   -0.047|     0.139|    -0.338|   0.736|\n|cloudcover      |   -0.025|     0.010|    -2.452|   0.016|\n|precip          |   -0.294|     0.123|    -2.397|   0.019|\n|day_typeWeekend |    0.064|     0.065|     0.987|   0.327|\n\n\n:::\n:::\n\n\n\n## Interpretation of model for $\\log(volume)$ {.midi}\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term            | estimate| std.error| statistic| p.value|\n|:---------------|--------:|---------:|---------:|-------:|\n|(Intercept)     |    4.738|     0.219|    21.667|   0.000|\n|hightemp        |    0.018|     0.003|     5.452|   0.000|\n|seasonSpring    |    0.026|     0.094|     0.283|   0.778|\n|seasonSummer    |   -0.047|     0.139|    -0.338|   0.736|\n|cloudcover      |   -0.025|     0.010|    -2.452|   0.016|\n|precip          |   -0.294|     0.123|    -2.397|   0.019|\n|day_typeWeekend |    0.064|     0.065|     0.987|   0.327|\n\n\n:::\n:::\n\n\n\n::: question\n-   Interpret the intercept in terms of (1) `log(volume)` and (2) `volume`.\n\n-   Interpret the coefficient of `hightemp` in terms of (1) `log(volume)` and (2) `volume`.\n:::\n\n## Residuals for model with $\\log(volume)$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\n\n## Compare residual plots\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\n\n\n# Log transformation on a predictor variable\n\n## Log Transformation on $X$\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-34-1.png){width=672}\n:::\n:::\n\n\n\nTry a transformation on $X$ if the scatterplot shows some curvature but the variance is constant for all values of $X$\n\n## Respiratory Rate vs. Age {.midi}\n\n-   A high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a \"high\" rate, we first want to understand the relationship between a child's age and their respiratory rate.\n\n-   The data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the **Sleuth3** R package and is originally form a 1994 publication \"Reference Values for Respiratory Rate in the First 3 Years of Life\".\n\n-   **Variables**:\n\n    -   `Age`: age in months\n    -   `Rate`: respiratory rate (breaths per minute)\n\n## Rate vs. Age\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](10-mlr-inference-old_files/figure-html/unnamed-chunk-35-1.png){width=672}\n:::\n:::\n\n\n\n## Model with Transformation on $X$ {.midi}\n\nSuppose we have the following regression equation:\n\n$$\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\log(X)$$\n\n. . .\n\n-   **Intercept:** When $X = 1$ $(\\log(X) = 0)$, $Y$ is expected to be $\\hat{\\beta}_0$ (i.e. the mean of $Y$ is $\\hat{\\beta}_0$)\n\n-   **Slope:** When $X$ is multiplied by a factor of $\\mathbf{C}$, the mean of $Y$ is expected to increase by $\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(C)}$ units\n\n    -   **Example**: when $X$ is multiplied by a factor of 2, $Y$ is expected to increase by $\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(2)}$ units\n\n## Model interpretation\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n\n\n|term        | estimate| std.error| statistic| p.value|\n|:-----------|--------:|---------:|---------:|-------:|\n|(Intercept) |   50.135|     0.632|    79.330|       0|\n|log(Age)    |   -5.982|     0.263|   -22.781|       0|\n\n\n:::\n:::\n\n\n\n$$\\hat{\\text{Rate}} = 50.135 - 5.982 \\times \\log\\text{(Age)}$$\n\n::: question\n-   Interpret the intercept in the context of the data.\n\n-   Interpret the slope in terms of age multiplying by 2 in the context of the data.\n:::\n\n## Learn more\n\nSee [Log Transformations in Linear Regression](https://github.com/sta210-sp20/supplemental-notes/blob/master/log-transformations.pdf) for more details about interpreting regression models with log-transformed variables.\n\n# Appendix\n\n## Why $Median(Y|X)$ instead of $\\mu_{Y|X}$\n\nSuppose we have a set of values\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(3, 5, 6, 8, 10, 14, 19)\n```\n:::\n\n\n\n<br>\n\n. . .\n\n::: columns\n::: {.column width=\"50%\"}\nLet's calculate $\\overline{\\log(x)}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_x <- log(x)\nmean(log_x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.066476\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\nLet's calculate $\\log(\\bar{x})$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nxbar <- mean(x)\nlog(xbar)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.228477\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n<br>\n\n. . .\n\nNote: $\\overline{\\log(x)} \\neq \\log(\\bar{x})$\n\n## Why $Median(Y|X)$ instead of $\\mu_{Y|X}$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(3, 5, 6, 8, 10, 14, 19)\n```\n:::\n\n\n\n<br>\n\n. . .\n\n::: columns\n::: {.column width=\"50%\"}\nLet's calculate $\\text{Median}(\\log(x))$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_x <- log(x)\nmedian(log_x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.079442\n```\n\n\n:::\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\nLet's calculate $\\log(\\text{Median}(x))$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian_x <- median(x)\nlog(median_x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 2.079442\n```\n\n\n:::\n:::\n\n\n:::\n:::\n\n<br>\n\n. . .\n\nNote: $\\text{Median} (\\log(x)) = \\log(\\text{Median}(x))$\n\n## Mean, Median, and Log\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nx <- c(3, 5, 6, 8, 10, 14, 19)\n```\n:::\n\n\n\n$$\\overline{\\log(x)} \\neq \\log(\\bar{x})$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(log_x) == log(xbar)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] FALSE\n```\n\n\n:::\n:::\n\n\n\n. . .\n\n$$\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))$$\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian(log_x) == log(median_x)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] TRUE\n```\n\n\n:::\n:::\n\n\n\n## Mean and median of $\\log(Y)$\n\n-   Recall that $Y = \\beta_0 + \\beta_1 X$ is the **mean** value of the response at the given value of the predictor $X$. This doesn't hold when we log-transform the response variable.\n\n-   Mathematically, the mean of the logged values is **not** necessarily equal to the log of the mean value. Therefore at a given value of $X$\n\n. . .\n\n$$\n\\begin{aligned}\\exp\\{\\text{Mean}(\\log(Y|X))\\} \\neq \\text{Mean}(Y|X) \\\\[5pt]\n\\Rightarrow \\exp\\{\\beta_0 + \\beta_1 X\\} \\neq \\text{Mean}(Y|X) \\end{aligned}\n$$\n\n## Mean and median of $\\log(y)$\n\n-   However, the median of the logged values **is** equal to the log of the median value. Therefore,\n\n$$\\exp\\{\\text{Median}(\\log(Y|X))\\} = \\text{Median}(Y|X)$$\n\n. . .\n\n-   If the distribution of $\\log(Y)$ is symmetric about the regression line, for a given value $X$, we can expect $Mean(Y)$ and $Median(Y)$ to be approximately equal.\n\n\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/kePrint-0.0.1/kePrint.js\"></script>\n<link href=\"../site_libs/lightable-0.0.1/lightable.css\" rel=\"stylesheet\" />\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}