[
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMon, Wed, Fri 10:00 - 10:50am\nBoone 132\n\n\nOffice Hours\nMonday\n2:30-3:30pm\n\n\nOffice Hours\nTuesday\n9:45-10:45am\n\n\nOffice Hours\nWednesday\n1:30-2:30pm\n\n\nOffice Hours\nThursday\n1-2pm\n\n\nFinal Presentation\nWednesday, December 11th\n8:30-11am\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\nInstructor: Professor Eric Friedlander Office: Boone 126B Email: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-info",
    "href": "syllabus.html#course-info",
    "title": "Syllabus",
    "section": "",
    "text": "Lecture\nMon, Wed, Fri 10:00 - 10:50am\nBoone 132\n\n\nOffice Hours\nMonday\n2:30-3:30pm\n\n\nOffice Hours\nTuesday\n9:45-10:45am\n\n\nOffice Hours\nWednesday\n1:30-2:30pm\n\n\nOffice Hours\nThursday\n1-2pm\n\n\nFinal Presentation\nWednesday, December 11th\n8:30-11am\n\n\n\nOffice hours are also available by appointment, just email me!\n\n\n\nInstructor: Professor Eric Friedlander Office: Boone 126B Email: efriedlander@collegeofidaho.edu",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-learning-objectives",
    "href": "syllabus.html#course-learning-objectives",
    "title": "Syllabus",
    "section": "Course Learning Objectives",
    "text": "Course Learning Objectives\nBy the end of the semester, you will be able to… - analyze real-world data to answer questions about multivariable relationships. - use R to fit and evaluate linear and logistic regression models. - assess whether a proposed model is appropriate and describe its limitations. - use Quarto to write reproducible reports and GitHub for version control and collaboration. - effectively communicate statistical results through writing and oral presentations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-community",
    "href": "syllabus.html#course-community",
    "title": "Syllabus",
    "section": "Course community",
    "text": "Course community\n\nCollege of Idaho Honor Code\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code.\n\n\nDisability Accommodation Statement\nThe College of Idaho seeks to provide an educational environment that is accessible to the needs of students with disabilities. The College provides reasonable services to enrolled students who have a documented permanent or temporary physical, psychological, learning, intellectual, or sensory disability that qualifies the student for accommodations under the Americans with Disabilities Act or section 504 of the Rehabilitation Act of 1973. If you have, or think you may have, a disability that impacts your performance as a student in this class, you are encouraged to arrange support services and/or accommodations through the Department of Accessibility and Learning Excellence located in McCain 201B and available via email at accessibility@collegeofidaho.edu. Reasonable academic accommodations may be provided to students who submit appropriate and current documentation of their disability. Accommodations can be arranged only through this process and are not retroactively applied. More information can be found on the DALE webpage (https://www.collegeofidaho.edu/accessibility).\n\n\nCommunication\nAll lecture notes, assignment instructions, an up-to-date schedule, and other course materials may be found on the course website, mat212fa24.netlify.app.\nPeriodic announcements will be sent via email and will also be available through Canvas and grades will be stored in the Canvas gradebook. Please check your email regularly to ensure you have the latest announcements for the course.\n\n\nIn class agreements\nIf we discuss/agree to something in class or office hours which requires action from me (e.g. “you may turn in your homework late due to a sporting event”), you MUST send me a follow-up message. If you don’t, I will almost certainly forget, and our agreement will be considered null and void.\n\n\nGetting help in the course\n\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.\nI am here to help you be successful in the course. You are encouraged to attend office hours to ask questions about the course content and assignments. Many questions are most effectively answered as you discuss them with others, so office hours are a valuable resource. You are encouraged to use them!\nOutside of class and office hours, any general questions about course content or assignments should be posted on the class discussion forum on Canvas. There is a chance another student has already asked a similar question, so please check the other posts in Canvas before adding a new question. If you know the answer to a question posted in the discussion forum, you are encouraged to respond!\n\n\n\nEmail\nIf you have questions about assignment extensions or accommodations, please email efriedlander@collegeofidaho.edu. Please see Late work policy for more information. If you email me, please include “MAT 212” in the subject line. Barring extenuating circumstances, I will respond to MAT 212 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.\nCheck out the Support page for more resources.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#textbook",
    "href": "syllabus.html#textbook",
    "title": "Syllabus",
    "section": "Textbook",
    "text": "Textbook\nThe official textbook for this course is:\n\nStat2: Modeling with Regression and ANOVA, 2nd ed. by Cannon et al.\n\nIn addition, readings may be assigned from the following texts (all freely available online).\n\nR for Data Science by Garret Grolemund and Hadley Wickham\nIntroduction to Modern Statistics by Mine Çetinkaya-Rundel and Johanna Hardin\nTidy modeling with R by Max Kuhn and Julia Silge\nBeyond Multiple Linear Regression by Paul Roback and Julie Legler",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#lectures",
    "href": "syllabus.html#lectures",
    "title": "Syllabus",
    "section": "Lectures",
    "text": "Lectures\nLectures are designed to be interactive, so you gain experience applying new concepts and learning from each other. My role as instructor is to introduce you to new methods, tools, and techniques, but it is up to you to take them and make use of them. A lot of what you do in this course will involve writing code, and coding is a skill that is best learned by doing. Therefore, as much as possible, you will be working on a variety of tasks and activities throughout each lecture and lab. You are expected to prepare for class by completing assigned readings, attend all lecture sessions, and meaningfully contribute to in-class exercises and discussion. Additionally, some lectures will feature application exercises that will be graded based on completing what we do in class.\nYou are expected to bring a laptop, tablet, or Chromebook to each class so that you can participate in the in-class exercises. Please make sure your device is fully charged before you come to class, as the number of outlets in the classroom may not be sufficient to accommodate everyone.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#activities-assessment",
    "href": "syllabus.html#activities-assessment",
    "title": "Syllabus",
    "section": "Activities & Assessment",
    "text": "Activities & Assessment\nYou will be assessed based on five components: application exercises, homework, exams, project, and oral R quiz\n\nApplication Exercises\nParts of some lectures will be dedicated to working on Application Exercises (AEs). These exercises which give you an opportunity to practice applying the statistical concepts and code introduced in the prepare assignment. These AEs are due within three days of the corresponding lecture period. Specifically, AEs from Monday lectures are due Thursday by 11:59p MT, AEs from Wednesday lectures are due Saturday by 11:59p MT, and AEs from Friday lectures are due Monday at 12:59p MT.\nBecause these AEs are for practice, they will be graded based on making a good-faith effort in attempting all questions covered in class. You are welcome to, but not required, to work on AEs beyond lecture. Successful on-time effort on at least 80% of AEs will result in full credit for AEs in the final course grade. Because AEs are intended for in-class activities, there are no extensions given on AEs.\n\n\nHomework\nIn homework, you will apply what you’ve learned during lecture to complete data analysis tasks. You may discuss homework assignments with other students; however, homework should be completed and submitted individually. Similar to lab assignments, homework must be typed up using Quarto and submitted as a .Qmd file in Canvas.\n\n\nExams\nThere will be two exams in this course. Each exam will include a closed-notes in-class component and may contain an open-note take-home component. Through these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. The exams will focus on both conceptual understanding of the content and application through analysis and computational tasks. The content of the exam will be related to the content in reading assignments, lectures, application exercises, and homework assignments. More detail about the exams will be given during the semester.\n\n\nProject\nThe purpose of the final project is to apply what you’ve learned throughout the semester to analyze an interesting data-driven research question. The project will be completed with your in pairs, and each team will present their work through a written report and poster presentation taking place during the final exam period. More information about the project will be provided during the semester.\n\n\nOral R Quiz\n\nThe purpose of this Quiz is to test your knowledge of the basic R functions we use regularly, R syntax, and your R “workflow”. Do you know the basic functions? If you run into a (common) error, can you debug it yourself? Can you interpret the R Help menu? Do you understand common terminology (e.g. “argument”, “function”, “output”)?\nEach student will take sign up for a 20-minute time slot to take the Quiz via Teams. Slots will open in early September and close on November 15th.\nThe R Quiz will be graded: “Proficient” (5 points), “In Progress” (3 points), or “Unsatisfactory” (0 points)\nEach student who earns “In Progress” or below is allowed one retake before November 15th, but this retake must be at least one week after their first attempt.\nThe specific questions asked during the Oral R Quiz will be different for every student and for every attempt, although similar in style and difficulty.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#grading",
    "href": "syllabus.html#grading",
    "title": "Syllabus",
    "section": "Grading",
    "text": "Grading\nThe final course grade will be calculated as follows:\n\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n25%\n\n\nFinal project\n30%\n\n\nExam 01\n15%\n\n\nExam 02\n20%\n\n\nApplication Exercises\n5%\n\n\nOral R Quiz\n5%\n\n\n\n\n\nThe final letter grade will be determined based on the following thresholds:\n\n\n\n\nLetter Grade\nFinal Course Grade\n\n\n\n\nA\n&gt;= 93\n\n\nA-\n90 - 92.99\n\n\nB+\n87 - 89.99\n\n\nB\n83 - 86.99\n\n\nB-\n80 - 82.99\n\n\nC+\n77 - 79.99\n\n\nC\n73 - 76.99\n\n\nC-\n70 - 72.99\n\n\nD+\n67 - 69.99\n\n\nD\n63 - 66.99\n\n\nD-\n60 - 62.99\n\n\nF\n&lt; 60",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#five-tips-for-success",
    "href": "syllabus.html#five-tips-for-success",
    "title": "Syllabus",
    "section": "Five tips for success",
    "text": "Five tips for success\nYour success on this course depends very much on you and the effort you put into it. The course has been organized so that the burden of learning is on you. I will help you by providing you with materials and answering questions and setting a pace, but for this to work you must do the following:\n\nComplete all the preparation work before class.\nAsk questions. As often as you can. In class, out of class. Ask me, ask your friends, ask the person sitting next to you. This will help you more than anything else. If you get a question wrong on an assessment, ask why. If you’re not sure about the homework, ask. If you hear something on the news that sounds related to what we discussed, ask. If the reading is confusing, ask.\nDo the readings.\nDo the homework. The earlier you start, the better. It’s not enough to just mechanically plow through the exercises. You should ask yourself how these exercises relate to earlier material, and imagine how they might be changed (to make questions for an exam, for example.)\nDon’t procrastinate. The content builds upon what was taught in previous weeks, so if something is confusing to you in Week 2, Week 3 will become more confusing, Week 4 even worse, etc. Don’t let the week end with unanswered questions. But if you find yourself falling behind and not knowing where to begin asking, come to office hours and I can help you identify a good (re)starting point.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#course-policies",
    "href": "syllabus.html#course-policies",
    "title": "Syllabus",
    "section": "Course policies",
    "text": "Course policies\n\nAcademic honesty\nTL;DR: Don’t cheat!\n\nThe homework assignments must be completed individually but you are welcome to discuss the assignment with classmates (e.g., discuss what’s the best way for approaching a problem, what functions are useful for accomplishing a particular task, etc.). However you may not directly share (i.e. via copy/paste or copying) answers to homework questions (including and especially any code) with anyone other than myself.\nYou may not discuss or otherwise work with others on the exams. Unauthorized collaboration or using unauthorized materials will be considered a violation for all students involved. More details will be given closer to the exam date.\nFor the projects, collaboration within teams is not only allowed, but expected. Communication between teams at a high level is also allowed however you may not share code or components of the project across teams.\nReusing code: Unless explicitly stated otherwise, you may make use of online resources (e.g. StackOverflow) for coding examples on assignments. If you directly use code from an outside source (or use it as inspiration), you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nUse of artificial intelligence (AI): You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course:1 (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\nAI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\nNo AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments. In general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content.\n\n\nIf you are unsure if the use of a particular resource complies with the academic honesty policy, please ask.\nRegardless of course delivery format, it is the responsibility of all students to understand and follow all College of Idaho policies, including academic integrity (e.g., completing one’s own work, following proper citation of sources, adhering to guidance around group work projects, and more). Ignoring these requirements is a violation of the Honor Code.\n\n\nLate work policy\nThe due dates for assignments are there to help you keep up with the course material and to ensure the teaching team can provide feedback within a timely manner. I understand that things come up periodically that could make it difficult to submit an assignment by the deadline.\n\nLate Homework: There will be a 5% deduction for each 24-hour period the assignment is late for the first two days. After 2 days, students will receive a 30% reduction. No homework will be accepted after it is returned to the class\nLate Application Exercises: AEs are due three days after the class they are assigned. No late work is accepted for application exercises, since these are designed as in-class activities to help you prepare for homework.\nSchool-Sponsored Events/Illness: If an application exercise, exam, or final project must be missed due to a school-sponsored event, you must let me know at least a week ahead of time so that we can schedule a time for you to make up the work before you leave. If you must miss a exam of the final project due to illness, you must let me know before class that day so that we can schedule a time for you to take a make-up quiz or exam. Failure to adhere to this policy will result in a 35% penalty the corresponding assignment.\n\n\n\nRegrade Requests\nRegrade requests must be submitted via email within a week of when an assignment is returned. Regrade requests will be considered if there was an error in the grade calculation or if you feel a correct answer was mistakenly marked as incorrect. Requests to dispute the number of points deducted for an incorrect response will not be considered. Note that by submitting a regrade request, the entire question will be graded which could potentially result in losing points.\nNo grades will be changed after the final project presentations.",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#important-dates",
    "href": "syllabus.html#important-dates",
    "title": "Syllabus",
    "section": "Important dates",
    "text": "Important dates\n\nAug 21: Classes begin\nSep 4: Last day to drop\nOct 14 - 18: Fall break\nNov 18: Last day to withdraw with W\nNov 28 - 29: Thanksgiving recess\nDec 6: Classes end\nDec 11: Final presentation",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "syllabus.html#footnotes",
    "href": "syllabus.html#footnotes",
    "title": "Syllabus",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎↩︎",
    "crumbs": [
      "Syllabus"
    ]
  },
  {
    "objectID": "stats-experience.html",
    "href": "stats-experience.html",
    "title": "Statistics Experience",
    "section": "",
    "text": "The world of statistics and data science is vast and continually growing! The goal of the statistics experience assignments is to help you engage with the statistics and data science communities outside of the classroom.\nYou may submit the statistics experience assignment anytime between now and the deadline.\nEach experience has two parts:\n1️⃣ Have a statistics experience.\n2️⃣ Make a slide reflecting on your experience.\nYou must complete both parts to receive credit. The statistics experience will count as a homework grade."
  },
  {
    "objectID": "stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "href": "stats-experience.html#part-1-experience-statistics-outside-of-the-classroom",
    "title": "Statistics Experience",
    "section": "Part 1: Experience statistics outside of the classroom",
    "text": "Part 1: Experience statistics outside of the classroom\nComplete an activity in one of the categories below. Under each category are suggested activities. You do not have to do one these suggested activities. You are welcome to find other activities as long as they are related to statistics/data science and they fit in one of the six categories. If there is an activity you’d like to do but you’re not sure if it qualifies for the statistics experience, just ask!\n\nCategory 1: Attend a talk or conference\nAttend an talk, panel, or conference related to statistics or data science. If you are attending a single talk or panel, it must be at least 30 minutes to count towards the statistics experience. The event can be in-person or online.\n\n\nCategory 2: Talk with a statistician/ data scientist\nTalk with someone who uses statistics in their daily work. This could include a professor, professional in industry, graduate student, etc.\n\n\nCategory 3: Listen to a podcast / watch video\nListen to a podcast or watch a video about statistics and data science. The podcast or video must be at least 30 minutes to count towards the statistics experience. A few suggestions are below:\n\nStats + Stories Podcast\nCausal Inference Podcast\nFiveThirtyEight Model Talk\nrstudio::conf talks\n\n2022 conference\n2021 conference\n2020 conference\n\n\nThis list is not exhaustive. You may listen to other podcasts or watch other statistics/data science videos not included on this list. Ask Professor Tackett if you are unsure whether a particular podcast or video will count towards the statistics experience.\n\n\nCategory 4: Participate in a data science competition or challenge\nParticipate in a statistics or data science competition. You can participate individually or with a team.\n\n\nCategory 5: Read a book on statistics/data science\nThere are a lot of books about statistics, data science, and related topics. A few suggestions are below. If you decide to read a book that isn’t on this list, ask Professor Tackett to make sure it counts toward the experience. Many of these books are available through Duke library.\n\nWeapons of Math Destruction by Cathy O’Neil\nHow Charts Lie: Getting Smarter about Visual Information by Alberto Cairo\nThe Theory that Would Not Die by Sharon Bertsch McGrayne\nThe Art of Statistics: How to learn from data by David Spiegelhalter\nThe Signal and the Noise: Why so many predictions fail - but some don’t by Nate Silver\nList of books about data science ethics\n\nThis list is not exhaustive.\n\n\nCategory 6: TidyTuesday\nYou may also participate in a TidyTuesday challenge. New data sets are announced on Monday afternoons.You can find more information about TidyTuesday and see the data in the TidyTuesday GitHub repo.\nA few guidelines:\n✅ Create a GitHub repo for your TidyTuesday submission. Your repo should include\n\nThe R Markdown file with all the code needed to reproduce your visualization.\nA README that includes an image of your final visualization and a short summary (~ 1 paragraph) about your visualization.\n\n✅ The visualization should include features or customization that are beyond what we’ve done in class .\n✅ Include the link to your GitHub repo in the slide summarizing your experience.\n\n\nCategory 7: CURV - connecting, uplifting, and recognizing voices\nCURV is a project by Dr. Jo Hardin at Pomona College to highlight statisticians and data scientists from groups who have been historically marginalized in the discipline. We will highlight some of the scholars in the CURV data base through the “Statistician of the Day” during lecture each week.\nThere are two options for this statistics experience:\n1️⃣. Present the Statistician of the Day. Learn about one of the scholars in the CURV data base and present what you learn in class.\nA few guidelines:\n\nLet Professor Tackett know at least one week in advance of when you’d like to present a Statistician of the Day. They will take place at the beginning of lectures on Mondays.\nCreate 1 - 2 slides about the scholar.\nPresent at the beginning of lecture for both lectures. If you are unable to attend the beginning of both lectures, you can present in-person during your lecture period and provide a short video for the other lecture period.\n\n2️⃣ Contribute to the CURV data base. If there is a scholar you would like to suggest for the data base, submit your suggestion as an issue or pull request on the CURV GitHub repo and create a sample CURV page.\nA few guidelines:\n✅ Create a draft of the CURV page for your suggested scholar. For reference, click here for the CURV page for W.E.B. Du Bois. The page must be created in a Quarto document.\n\n\n\n\n\n\nTip\n\n\n\nYou can find the Quarto documents for current scholars in the data base in the CURV GitHub repo. You can use one of these as a template to format your page.\n\n\n✅ Make a pull request to the CURV GitHub repo to add the .qmd file for your suggested scholar, OR open an issue with a link to the .qmd file for your suggested scholar. You can ask a member of the teaching team if you have questions about how to do this.\n✅ Include the URL to your pull request or issue in your one-slide reflection."
  },
  {
    "objectID": "stats-experience.html#part-2-reflect-on-your-experience",
    "href": "stats-experience.html#part-2-reflect-on-your-experience",
    "title": "Statistics Experience",
    "section": "Part 2: Reflect on your experience",
    "text": "Part 2: Reflect on your experience\nMake one slide summarizing and reflecting on your experience. Submit the slide as a PDF on Gradescope.\nInclude the following on your slide:\n\nName and brief description of the event/podcast/competition/etc.\nSomething you found new, interesting, or unexpected\nHow the event/podcast/competition/etc. connects to something we’ve done in class.\nCitation or link to web page for event/competition/etc.\n\nClick here to see a template to help you get started on your slide. Your slide does not have to follow this exact format; it just needs to include the information mentioned above and be easily readable (i.e. use a reasonable font size!). Creativity is encouraged!"
  },
  {
    "objectID": "stats-experience.html#submission",
    "href": "stats-experience.html#submission",
    "title": "Statistics Experience",
    "section": "Submission",
    "text": "Submission\nSubmit the reflection as a PDF under the Statistics Experience assignment on Gradescope by Mon, Nov 20 at 11:59pm. Standard homework late policy applies."
  },
  {
    "objectID": "slides/xx-slr-outliers.html#computational-set-up",
    "href": "slides/xx-slr-outliers.html#computational-set-up",
    "title": "SLR: Transformations",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(ggformula)   # for creating plots using formulas\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(moderndive)  # for house_price dataset\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#data-house_prices",
    "href": "slides/xx-slr-outliers.html#data-house_prices",
    "title": "SLR: Transformations",
    "section": "Data: house_prices",
    "text": "Data: house_prices\n\n\nContains house sale prices for King County, which includes Seattle, from homes sold between May 2014 and May 2015\nObtained from Kaggle.com\n\n\n\nglimpse(house_prices)\n\nRows: 21,613\nColumns: 21\n$ id            &lt;chr&gt; \"7129300520\", \"6414100192\", \"5631500400\", \"2487200875\", …\n$ date          &lt;date&gt; 2014-10-13, 2014-12-09, 2015-02-25, 2014-12-09, 2015-02…\n$ price         &lt;dbl&gt; 221900, 538000, 180000, 604000, 510000, 1225000, 257500,…\n$ bedrooms      &lt;int&gt; 3, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 5, 4, 3, 4, 2,…\n$ bathrooms     &lt;dbl&gt; 1.00, 2.25, 1.00, 3.00, 2.00, 4.50, 2.25, 1.50, 1.00, 2.…\n$ sqft_living   &lt;int&gt; 1180, 2570, 770, 1960, 1680, 5420, 1715, 1060, 1780, 189…\n$ sqft_lot      &lt;int&gt; 5650, 7242, 10000, 5000, 8080, 101930, 6819, 9711, 7470,…\n$ floors        &lt;dbl&gt; 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1…\n$ waterfront    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ view          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,…\n$ condition     &lt;fct&gt; 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4,…\n$ grade         &lt;fct&gt; 7, 7, 6, 7, 8, 11, 7, 7, 7, 7, 8, 7, 7, 7, 7, 9, 7, 7, 7…\n$ sqft_above    &lt;int&gt; 1180, 2170, 770, 1050, 1680, 3890, 1715, 1060, 1050, 189…\n$ sqft_basement &lt;int&gt; 0, 400, 0, 910, 0, 1530, 0, 0, 730, 0, 1700, 300, 0, 0, …\n$ yr_built      &lt;int&gt; 1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 20…\n$ yr_renovated  &lt;int&gt; 0, 1991, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ zipcode       &lt;fct&gt; 98178, 98125, 98028, 98136, 98074, 98053, 98003, 98198, …\n$ lat           &lt;dbl&gt; 47.5112, 47.7210, 47.7379, 47.5208, 47.6168, 47.6561, 47…\n$ long          &lt;dbl&gt; -122.257, -122.319, -122.233, -122.393, -122.045, -122.0…\n$ sqft_living15 &lt;int&gt; 1340, 1690, 2720, 1360, 1800, 4760, 2238, 1650, 1780, 23…\n$ sqft_lot15    &lt;int&gt; 5650, 7639, 8062, 5000, 7503, 101930, 6819, 9711, 8113, …"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#variables",
    "href": "slides/xx-slr-outliers.html#variables",
    "title": "SLR: Transformations",
    "section": "Variables",
    "text": "Variables\n\nOutcome\n\nprice: the sale price\n\nPredictor\n\nsqft_living: the square footage of the home"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#recap-fit-the-model",
    "href": "slides/xx-slr-outliers.html#recap-fit-the-model",
    "title": "SLR: Transformations",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\nhp_fit &lt;- lm(price ~ sqft_living, data = house_prices)\ntidy(hp_fit) |&gt;  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-43580.74\n4402.69\n-9.90\n0\n\n\nsqft_living\n280.62\n1.94\n144.92\n0\n\n\n\n\n\n\nModel: \\(\\text{price} = -43580.74 + 280.62\\times\\text{sqft_living}\\)\nInterpretation: If the square footage of the house increases by 1, the price increases by and average of $280.62 and a (theoretical) house with 0 square feet with cost $-43,580.74."
  },
  {
    "objectID": "slides/xx-slr-outliers.html#recap-fit-the-model-1",
    "href": "slides/xx-slr-outliers.html#recap-fit-the-model-1",
    "title": "SLR: Transformations",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\n\nCode\ngf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#recap-model-conditions",
    "href": "slides/xx-slr-outliers.html#recap-model-conditions",
    "title": "SLR: Transformations",
    "section": "Recap: Model conditions",
    "text": "Recap: Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other\n\n\n\nHow should we check these assumptions?"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#recap-residual-histogram",
    "href": "slides/xx-slr-outliers.html#recap-residual-histogram",
    "title": "SLR: Transformations",
    "section": "Recap: Residual Histogram",
    "text": "Recap: Residual Histogram\n\n\nCode\nhp_aug &lt;- augment(hp_fit)\n\ngf_histogram(~.resid, data = hp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Residual Histogram\")"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#recap-qq-plot-of-residuals",
    "href": "slides/xx-slr-outliers.html#recap-qq-plot-of-residuals",
    "title": "SLR: Transformations",
    "section": "Recap: QQ-Plot of Residuals",
    "text": "Recap: QQ-Plot of Residuals\n\n\nCode\ngf_qq(~.resid, data = hp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#recap-residuals-vs.-fitted-values",
    "href": "slides/xx-slr-outliers.html#recap-residuals-vs.-fitted-values",
    "title": "SLR: Transformations",
    "section": "Recap: Residuals vs. Fitted Values",
    "text": "Recap: Residuals vs. Fitted Values\n\n\nCode\ngf_point(.resid ~ .fitted, data = hp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )\n\n\n\n:::"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#are-model-conditions-satisfied",
    "href": "slides/xx-slr-outliers.html#are-model-conditions-satisfied",
    "title": "SLR: Transformations",
    "section": "Are model conditions satisfied?",
    "text": "Are model conditions satisfied?\n\nLinearity: ❓\nConstant variance: ❌\nNormality: ❌\nIndependence: ✅"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#what-to-do-when-regression-conditions-are-violated",
    "href": "slides/xx-slr-outliers.html#what-to-do-when-regression-conditions-are-violated",
    "title": "SLR: Transformations",
    "section": "What to do when regression conditions are violated",
    "text": "What to do when regression conditions are violated\nExamples:\n\nLack of normality in residuals\nPatterns in residuals\nHeteroscedasticity (nonconstant variance)\nOutliers: influential points, large residuals"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#data-transformations",
    "href": "slides/xx-slr-outliers.html#data-transformations",
    "title": "SLR: Transformations",
    "section": "Data Transformations",
    "text": "Data Transformations\nCan be used to:\n\nAddress nonlinear patterns\nStabilize variance\nRemove skewness from resiudals\nMinimize effects of outliers"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#common-transformations",
    "href": "slides/xx-slr-outliers.html#common-transformations",
    "title": "SLR: Transformations",
    "section": "Common Transformations",
    "text": "Common Transformations\nFor either the response \\(Y\\) or predictor \\(X\\):\n\nLogarithm \\(Z \\to \\log(Z)\\)\n\nNote: “log” means “log base \\(e\\)”\n\nSquare Root \\(Z \\to \\sqrt{Z}\\)\nExponential \\(Z \\to e^Z\\)\nPower functions \\(Z \\to Z^2, Z^3, Z^4, \\ldots\\)\nReciprocal \\(Z \\to 1/Z\\)"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#example-planets",
    "href": "slides/xx-slr-outliers.html#example-planets",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nplanets &lt;- read_csv(\"data/planets.csv\")\n\n\n\\(Y =\\) length of the “year” for planets\n\\(X =\\) distance from the Sun"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#example-planets-1",
    "href": "slides/xx-slr-outliers.html#example-planets-1",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Distance ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#example-planets-2",
    "href": "slides/xx-slr-outliers.html#example-planets-2",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(log(Distance) ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#example-planets-3",
    "href": "slides/xx-slr-outliers.html#example-planets-3",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(Distance ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#example-planets-4",
    "href": "slides/xx-slr-outliers.html#example-planets-4",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Distance) ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#example-mammal-species",
    "href": "slides/xx-slr-outliers.html#example-mammal-species",
    "title": "SLR: Transformations",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nSpeciesArea &lt;- read_csv(\"data/SpeciesArea.csv\")\n\n\n\\(Y =\\) number of mammal species on an island\n\\(X =\\) area of the island"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#example-mammal-species-1",
    "href": "slides/xx-slr-outliers.html#example-mammal-species-1",
    "title": "SLR: Transformations",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Species ~ Area, data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#example-mammal-species-2",
    "href": "slides/xx-slr-outliers.html#example-mammal-species-2",
    "title": "SLR: Transformations",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Species) ~ log(Area), data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#why-a-log-transformation",
    "href": "slides/xx-slr-outliers.html#why-a-log-transformation",
    "title": "SLR: Transformations",
    "section": "Why a Log Transformation?",
    "text": "Why a Log Transformation?\n\nSome relationship are multiplicative (not linear)\n\nExample: Area of a circle\n\\[\n\\begin{aligned}\nA &= \\pi r^2 \\text{ (not linear)}\\\\\n\\log(A) &= \\log(\\pi r^2)\n= \\log(\\pi) + 2\\log(r)\\\\\n\\log(A) &= \\beta_0 + \\beta_1\\times \\log(r)\\\\\n\\implies & \\log(A) \\text{ is a linear function of } \\log(r)\n\\end{aligned}\n\\]\nLook for:\n\nStrongly right-skewed distributions\nCurvative in scatterplot\nIncreasing variability in scatterplot"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#back-to-house_sales",
    "href": "slides/xx-slr-outliers.html#back-to-house_sales",
    "title": "SLR: Transformations",
    "section": "Back to house_sales",
    "text": "Back to house_sales\n\n\nCode\np1 &lt;- gf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np2 &lt;- gf_point(log(price) ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np3 &lt;- gf_point(price ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np4 &lt;- gf_point(log(price) ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\n(p1 + p2)/ (p3 + p4)"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#fitting-transformed-models",
    "href": "slides/xx-slr-outliers.html#fitting-transformed-models",
    "title": "SLR: Transformations",
    "section": "Fitting Transformed Models",
    "text": "Fitting Transformed Models\n\n\n\nlogprice_model &lt;- lm(log(price) ~ sqft_living, data = house_prices)\ntidy(logprice_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.2184641\n0.0063741\n1916.8830\n0\n\n\nsqft_living\n0.0003987\n0.0000028\n142.2326\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &= 12.22  + 3.99\\times 10^{-4}\\times X\\\\\nY &= e^{12.22 + 3.99\\times 10^{-4}\\times X}\\\\\n&= 202805\\times e^{3.99\\times 10^{-4}\\times X}\n\\end{aligned}\n\\]\n\n\nloglog_model &lt;- lm(log(price) ~ log(sqft_living), data = house_prices)\ntidy(loglog_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.729916\n0.0470620\n143.0011\n0\n\n\nlog(sqft_living)\n0.836771\n0.0062233\n134.4587\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &=6.73 + 0.837\\times \\log(X)\\\\\n\\log(Y) &= \\log(e^{6.73})  + \\log(X^{0.837})\\\\\nY &= 873.15\\times X^{0.837}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#residuals-histograms",
    "href": "slides/xx-slr-outliers.html#residuals-histograms",
    "title": "SLR: Transformations",
    "section": "Residuals Histograms",
    "text": "Residuals Histograms\n\n\nCode\nlp_aug &lt;- augment(logprice_model)\nll_aug &lt;- augment(loglog_model)\n\np1 &lt;- gf_histogram(~.resid, data = lp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log Price Residuals\")\n\np2 &lt;- gf_histogram(~.resid, data = ll_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log-Log Residuals\")\n\n(p1 + p2)"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#qq-plots-of-residuals",
    "href": "slides/xx-slr-outliers.html#qq-plots-of-residuals",
    "title": "SLR: Transformations",
    "section": "QQ-Plots of Residuals",
    "text": "QQ-Plots of Residuals\n\n\nCode\np1 &lt;- gf_qq(~.resid, data = lp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log Price QQ\")\n\np2 &lt;- gf_qq(~.resid, data = ll_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log-Log QQ\")\n\np1 + p2"
  },
  {
    "objectID": "slides/xx-slr-outliers.html#residuals-vs.-fitted-values",
    "href": "slides/xx-slr-outliers.html#residuals-vs.-fitted-values",
    "title": "SLR: Transformations",
    "section": "Residuals vs. Fitted Values",
    "text": "Residuals vs. Fitted Values\n\n\nCode\np1 &lt;- gf_point(.resid ~ .fitted, data = lp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log Price Model\"\n  )\n\np2 &lt;- gf_point(.resid ~ .fitted, data = ll_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log-Log Model\"\n  )\n\np1 + p2\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/lab-04.html#goals",
    "href": "slides/lab-04.html#goals",
    "title": "Lab 04",
    "section": "Goals",
    "text": "Goals\n\nMeet your team!\nTeam agreement\nLab 04: Alumni jobs"
  },
  {
    "objectID": "slides/lab-04.html#meet-your-team",
    "href": "slides/lab-04.html#meet-your-team",
    "title": "Lab 04",
    "section": "Meet your team!",
    "text": "Meet your team!\n\nClick here to find your team.\nSit with your team."
  },
  {
    "objectID": "slides/lab-04.html#team-name-agreement",
    "href": "slides/lab-04.html#team-name-agreement",
    "title": "Lab 04",
    "section": "Team name + agreement",
    "text": "Team name + agreement\n\nCome up with a team name. You can’t have the same name as another group in the class, so be creative!\n\nYour TA will get your team name by the end of lab.\n\nFill out the team agreement. The goals of the agreement are to…\n\nGain a common understanding of the team’s goals and expectations for collaboration\nMake a plan for team communication\nMake a plan for working outside of lab"
  },
  {
    "objectID": "slides/lab-04.html#team-workflow",
    "href": "slides/lab-04.html#team-workflow",
    "title": "Lab 04",
    "section": "Team workflow",
    "text": "Team workflow\n\nOnly one team member should type at a time. There are markers in today’s lab to help you determine whose turn it is to type.\n\nEvery team member should still be engaged in discussion for all questions, even if it’s not your turn type.\n\nDon’t forget to pull to get your teammates’ updates before making changes to the .qmd file.\n\n\n\n\n\n\nImportant\n\n\nOnly one submission per team on Gradescope. Read the submission instructions carefully!"
  },
  {
    "objectID": "slides/lab-04.html#team-workflow-in-action",
    "href": "slides/lab-04.html#team-workflow-in-action",
    "title": "Lab 04",
    "section": "Team workflow, in action",
    "text": "Team workflow, in action\n\nComplete the “Workflow: Using Git and GitHub as a team” section of the lab in your teams.\nRaise your hand if you have any questions about the workflow.\n\n\n\n\n−+\n05:00"
  },
  {
    "objectID": "slides/lab-04.html#tips-for-working-on-a-team",
    "href": "slides/lab-04.html#tips-for-working-on-a-team",
    "title": "Lab 04",
    "section": "Tips for working on a team",
    "text": "Tips for working on a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other.\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/lab-02.html#reminders",
    "href": "slides/lab-02.html#reminders",
    "title": "Lab 02",
    "section": "Reminders",
    "text": "Reminders\n\n\nSelect the pages corresponding to each exercise when you when you submit the assignment on Gradescope.\n\nClick here for written and video instructions on submitting an assignment and marking pages on Gradescope.\n\nIn your write up:\n\nWrite all narrative in complete sentences.\nInclude an informative title and axis labels on graphs.\nWrite responses in the context of the data.\nDescribe distribution using shape, center, spread, and potential outliers. Describe relationships between variables using strength, direction, and shape."
  },
  {
    "objectID": "slides/lab-02.html#axis-labels-and-titles",
    "href": "slides/lab-02.html#axis-labels-and-titles",
    "title": "Lab 02",
    "section": "Axis labels and titles",
    "text": "Axis labels and titles\n\nBelow is a graph of association between flipper length in millimeters and body mass in grams of three species of penguins in Palmer Station, Antarctica. What are informative title and axis labels for this graph?"
  },
  {
    "objectID": "slides/lab-02.html#code-style",
    "href": "slides/lab-02.html#code-style",
    "title": "Lab 02",
    "section": "Code style",
    "text": "Code style\nWhich code chunk would you rather read?\n\n# code chunk 1\npenguins|&gt;filter(!is.na(flipper_length_mm))|&gt;group_by(species)|&gt;summarise(min=min(flipper_length_mm),mean=mean(flipper_length_mm),sd=sd(flipper_length_mm),max=max(flipper_length_mm),n=n())\n\n\n\n\n# code chunk 2\npenguins |&gt; \n  filter(!is.na(flipper_length_mm)) |&gt; \n  group_by(species) |&gt; \n  summarise(min = min(flipper_length_mm), \n            mean = mean(flipper_length_mm), \n            max = max(flipper_length_mm),\n            n = n())"
  },
  {
    "objectID": "slides/lab-02.html#code-style-contd",
    "href": "slides/lab-02.html#code-style-contd",
    "title": "Lab 02",
    "section": "Code style cont’d",
    "text": "Code style cont’d\nMake code easier to read and debug by\n\nPutting each element on a different line (start a new line after + and |&gt;)\nPutting spaces before and after operators (+, -, *, =, |&gt; )\nIn general, avoiding long lines of code, i.e. lines longer than 120 characters.\n\nSee the Tidyverse Style Guide for more tips on code styling."
  },
  {
    "objectID": "slides/lab-02.html#todays-lab",
    "href": "slides/lab-02.html#todays-lab",
    "title": "Lab 02",
    "section": "Today’s lab",
    "text": "Today’s lab\n\nRemember to use a reproducible workflow with regular commits (and informative commit messages).\nPush all updated files after each commit! The Git pane should be clear after each Render -&gt; Commit -&gt; Push.\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/lab-00.html#meet-your-ta",
    "href": "slides/lab-00.html#meet-your-ta",
    "title": "Welcome to STA 210 Labs!",
    "section": "Meet your TA!",
    "text": "Meet your TA!"
  },
  {
    "objectID": "slides/lab-00.html#meet-each-other",
    "href": "slides/lab-00.html#meet-each-other",
    "title": "Welcome to STA 210 Labs!",
    "section": "Meet each other!",
    "text": "Meet each other!\n\n\nGet into groups of 4 - 5.\nIntroduce yourself - Name, year, major\nChoose a reporter\n\nNeed help choosing? Person with birthday closest to today’s date.\n\nIdentify 8 things everyone in the group has in common\n\nNot being a Duke student\nNot clothes (e.g., we’re all wearing socks)\nNot body parts (e.g., we all have a nose)\n\nReporter will share list with the class.\n\n\n\n\n\n−+\n06:00"
  },
  {
    "objectID": "slides/lab-00.html#what-to-expect-in-lab",
    "href": "slides/lab-00.html#what-to-expect-in-lab",
    "title": "Welcome to STA 210 Labs!",
    "section": "What to expect in lab",
    "text": "What to expect in lab\n\nIntroduction to the lab assignment (~ 5 - 10 minutes)\nWork on the lab assignment (individual for the first few weeks, but in teams for the rest of the semester)\nLab instructions will be posted on the course website\nStart each lab by finding your assignment repo in the course GitHub organization\n\nMore on the computing tools during lecture"
  },
  {
    "objectID": "slides/lab-00.html#todays-lab",
    "href": "slides/lab-00.html#todays-lab",
    "title": "Welcome to STA 210 Labs!",
    "section": "Today’s lab",
    "text": "Today’s lab\nThe rest of the today’s lab is focused on setting up the computing for the course and completing the class survey. Click the link below for the Lab 00 instructions.\n🔗 sta210-fa23.netlify.app/labs/lab-00.html\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/23-exam-02-review.html#announcements",
    "href": "slides/23-exam-02-review.html#announcements",
    "title": "Exam 02 review",
    "section": "Announcements",
    "text": "Announcements\n\nDue dates\n\nHW 05 due TODAY at 11:59pm\n\nProject\n\n(Optional) Round 1 submission due Fri, Dec 01 at 11:59pm\nPresentations in lab Dec 5 & 7\n\nExam 02 dates\n\nIn-class: Mon, Dec 4\nTake-home: Mon, Dec 4 - Wed, Dec 6\nReview: Wed, Nov 29\nNo Ed Discussion or office hours Dec 4 - 6\n\nClick here to access lecture recordings. Available until Mon, Dec 04 at 9am\nPlease fill out course evaluations"
  },
  {
    "objectID": "slides/23-exam-02-review.html#course-ta-evaluations",
    "href": "slides/23-exam-02-review.html#course-ta-evaluations",
    "title": "Exam 02 review",
    "section": "Course + TA evaluations",
    "text": "Course + TA evaluations\n\nCourse and TA evaluations are now available (will receive email about TA evals)\nIf there is at least 80% on the course evaluations and TA evaluations, everyone in the class will get 1 percentage point (i.e. 0.5 out of 50) on Exam 02.\n\nThis will be determined separately for the Sections 001 and 002."
  },
  {
    "objectID": "slides/23-exam-02-review.html#exam-02",
    "href": "slides/23-exam-02-review.html#exam-02",
    "title": "Exam 02 review",
    "section": "Exam 02",
    "text": "Exam 02\n\n15% of final course grade\n50 points total\n\nin-class: 35 points\ntake-home: 15 points\n\nIn-class: 75 minutes during Mon, Dec 4 lecture\nTake-home:\n\nReleased: Mon, Dec 4 ~ 3pm\nDue: Wed, Oct 6 at 11:59pm\nSimilar in format to a lab/ HW"
  },
  {
    "objectID": "slides/23-exam-02-review.html#content-weeks-5---14",
    "href": "slides/23-exam-02-review.html#content-weeks-5---14",
    "title": "Exam 02 review",
    "section": "Content: Weeks 5 - 14",
    "text": "Content: Weeks 5 - 14\n\n\n\nFitting, interpreting, comparing, and evaluating multiple linear regression models\nFeature engineering + model workflow using tidymodels\nCross validation\nInference for multiple linear regression\nProbabilities, odds, and odds ratios\n\n\n\nFitting, interpreting, comparing, and evaluating logistic regression models\n\n\n\nUsing logistic regression models for prediction and classification\nFitting, interpreting, comparing, and evaluating multinomial logistic regression models\nUsing multinomial logistic regression models for prediction and classification"
  },
  {
    "objectID": "slides/23-exam-02-review.html#outline-of-in-class-portion",
    "href": "slides/23-exam-02-review.html#outline-of-in-class-portion",
    "title": "Exam 02 review",
    "section": "Outline of in-class portion",
    "text": "Outline of in-class portion\n\nClosed-book, closed-note.\n8-10 questions, some with multiple parts\nQuestion types:\n\nMultiple choice\nShort answer (no more than 3 sentences)\nTrue/ False.\n\nIf false, write 1 - 2 sentence justification about why it is false. Must get justification correct to receive credit.\n\nEvaluate a response.\n\nLast page will have analysis output.\nJust need a pencil or pen. No calculator permitted on exam."
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#announcements",
    "href": "slides/21-multinomial-logistic.html#announcements",
    "title": "Multinomial Logistic Regression",
    "section": "Announcements",
    "text": "Announcements\n\nDue dates\n\nDraft report due in GitHub repo on 9am on your lab day\nHW 04 due TODAY at 11:59pm\nStatistics experience due Mon, Nov 20 at 11:59pm\n\nNext week:\n\n(Optional) Project meetings Nov 20 & 21. Click here to sign up. Must sign up by Fri, Nov 17\nNo class meetings next week\n\nClick here to access lecture recordings. Available until Mon, Dec 04 at 9am\n\n\n🍂 Have a good Thanksgiving break! 🍂"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#topics",
    "href": "slides/21-multinomial-logistic.html#topics",
    "title": "Multinomial Logistic Regression",
    "section": "Topics",
    "text": "Topics\n\n\nConditions for logistic regression AE\nMultinomial logistic regression\nInterpret model coefficients\nInference for a coefficient \\(\\beta_{jk}\\)"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#computational-setup",
    "href": "slides/21-multinomial-logistic.html#computational-setup",
    "title": "Multinomial Logistic Regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(NHANES) #data set\nlibrary(knitr)\nlibrary(patchwork)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#generalized-linear-models-glms",
    "href": "slides/21-multinomial-logistic.html#generalized-linear-models-glms",
    "title": "Multinomial Logistic Regression",
    "section": "Generalized Linear Models (GLMs)",
    "text": "Generalized Linear Models (GLMs)\n\nIn practice, there are many different types of outcome variables:\n\n\nBinary: Win or Lose\nNominal: Democrat, Republican or Third Party candidate\nOrdered: Movie rating (1 - 5 stars)\nand others…\n\n\nPredicting each of these outcomes requires a generalized linear model, a broader class of models that generalize the multiple linear regression model\n\n\n\n\n\n\n\nNote\n\n\nRecommended reading for more details about GLMs: Generalized Linear Models: A Unifying Theory."
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#binary-outcome-logistic",
    "href": "slides/21-multinomial-logistic.html#binary-outcome-logistic",
    "title": "Multinomial Logistic Regression",
    "section": "Binary outcome (Logistic)",
    "text": "Binary outcome (Logistic)\n\nGiven \\(P(y_i=1|x_i)= \\hat{\\pi}_i\\hspace{5mm} \\text{ and } \\hspace{5mm}P(y_i=0|x_i) = 1-\\hat{\\pi}_i\\)\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}_i}{1-\\hat{\\pi}_i}\\Big) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i}\n\\]\nWe can calculate \\(\\hat{\\pi}_i\\) by solving the logit equation:\n\\[\n\\hat{\\pi}_i = \\frac{\\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i}\\}}{1 + \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i}\\}}\n\\]"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#binary-outcome-logistic-1",
    "href": "slides/21-multinomial-logistic.html#binary-outcome-logistic-1",
    "title": "Multinomial Logistic Regression",
    "section": "Binary outcome (Logistic)",
    "text": "Binary outcome (Logistic)\n\n\nSuppose we consider \\(y=0\\) the baseline category such that\n\\[\nP(y_i=1|x_i) = \\hat{\\pi}_{i1} \\hspace{2mm}  \\text{ and } \\hspace{2mm} P(y_i=0|x_i) = \\hat{\\pi}_{i0}\n\\]\nThen the logistic regression model is\n\\[\n\\log\\bigg(\\frac{\\hat{\\pi}_{i1}}{1- \\hat{\\pi}_{i1}}\\bigg) = \\log\\bigg(\\frac{\\hat{\\pi}_{i1}}{\\hat{\\pi}_{i0}}\\bigg) = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i\n\\]\nSlope, \\(\\hat{\\beta}_1\\): When \\(x\\) increases by one unit, the odds of \\(y=1\\) versus the baseline \\(y=0\\) are expected to multiply by a factor of \\(\\exp\\{\\hat{\\beta}_1\\}\\)\nIntercept, \\(\\hat{\\beta}_0\\): When \\(x=0\\), the predicted odds of \\(y=1\\) versus the baseline \\(y=0\\) are \\(\\exp\\{\\hat{\\beta}_0\\}\\)"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#multinomial-outcome-variable",
    "href": "slides/21-multinomial-logistic.html#multinomial-outcome-variable",
    "title": "Multinomial Logistic Regression",
    "section": "Multinomial outcome variable",
    "text": "Multinomial outcome variable\n\nSuppose the outcome variable \\(y\\) is categorical and can take values \\(1, 2, \\ldots, K\\) such that \\((K &gt; 2)\\)\nMultinomial Distribution:\n\\[\nP(y=1) = \\pi_1, P(y=2) = \\pi_2, \\ldots, P(y=K) = \\pi_K\n\\]\nsuch that \\(\\sum\\limits_{k=1}^{K} \\pi_k = 1\\)"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#multinomial-logistic-regression",
    "href": "slides/21-multinomial-logistic.html#multinomial-logistic-regression",
    "title": "Multinomial Logistic Regression",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\n\n\nIf we have an explanatory variable \\(x\\), then we want to fit a model such that \\(P(y = k) = \\pi_k\\) is a function of \\(x\\)\nChoose a baseline category. Let’s choose \\(y=1\\). Then,\n\\[\n\\log\\bigg(\\frac{\\pi_{ik}}{\\pi_{i1}}\\bigg) = \\beta_{0k} + \\beta_{1k} x_i\n\\]\nIn the multinomial logistic model, we have a separate equation for each category of the outcome relative to the baseline category\n\nIf the outcome has \\(K\\) possible categories, there will be \\(K-1\\) equations as part of the multinomial logistic model"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#multinomial-logistic-regression-1",
    "href": "slides/21-multinomial-logistic.html#multinomial-logistic-regression-1",
    "title": "Multinomial Logistic Regression",
    "section": "Multinomial Logistic Regression",
    "text": "Multinomial Logistic Regression\n\nSuppose we have a outcome variable \\(y\\) that can take three possible outcomes that are coded as “A”, “B”, “C”\nLet “A” be the baseline category. Then\n\\[\n\\begin{aligned}\n\\log\\bigg(\\frac{\\pi_{iB}}{\\pi_{iA}}\\bigg) &= \\beta_{0B} + \\beta_{1B}x_i \\\\[10pt]\n\\log\\bigg(\\frac{\\pi_{iC}}{\\pi_{iA}}\\bigg) &= \\beta_{0C} + \\beta_{1C} x_i\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#nhanes-data",
    "href": "slides/21-multinomial-logistic.html#nhanes-data",
    "title": "Multinomial Logistic Regression",
    "section": "NHANES Data",
    "text": "NHANES Data\n\nNational Health and Nutrition Examination Survey is conducted by the National Center for Health Statistics (NCHS)\nThe goal is to “assess the health and nutritional status of adults and children in the United States”\nThis survey includes an interview and a physical examination"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#nhanes-data-1",
    "href": "slides/21-multinomial-logistic.html#nhanes-data-1",
    "title": "Multinomial Logistic Regression",
    "section": "NHANES Data",
    "text": "NHANES Data\n\nWe will use the data from the NHANES R package\nContains 75 variables for the 2009 - 2010 and 2011 - 2012 sample years\nThe data in this package is modified for educational purposes and should not be used for research\nOriginal data can be obtained from the NCHS website for research purposes\nType ?NHANES in console to see list of variables and definitions"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#variables",
    "href": "slides/21-multinomial-logistic.html#variables",
    "title": "Multinomial Logistic Regression",
    "section": "Variables",
    "text": "Variables\nGoal: Use a person’s age and whether they do regular physical activity to predict their self-reported health rating.\n\nOutcome:\n\nHealthGen: Self-reported rating of participant’s health in general. Excellent, Vgood, Good, Fair, or Poor.\n\nPredictors:\n\nAge: Age at time of screening (in years). Participants 80 or older were recorded as 80.\nPhysActive: Participant does moderate to vigorous-intensity sports, fitness or recreational activities."
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#the-data",
    "href": "slides/21-multinomial-logistic.html#the-data",
    "title": "Multinomial Logistic Regression",
    "section": "The data",
    "text": "The data\n\nnhanes_adult &lt;- NHANES |&gt;\n  filter(Age &gt;= 18) |&gt;\n  select(HealthGen, Age, PhysActive) |&gt;\n  filter(!(is.na(HealthGen))) |&gt;\n  mutate(obs_num = 1:n())\n\n\nglimpse(nhanes_adult)\n\nRows: 6,710\nColumns: 4\n$ HealthGen  &lt;fct&gt; Good, Good, Good, Good, Vgood, Vgood, Vgood, Vgood, Vgood, …\n$ Age        &lt;int&gt; 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 50, 33, 60, 56, 56,…\n$ PhysActive &lt;fct&gt; No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, …\n$ obs_num    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#exploratory-data-analysis",
    "href": "slides/21-multinomial-logistic.html#exploratory-data-analysis",
    "title": "Multinomial Logistic Regression",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#exploratory-data-analysis-1",
    "href": "slides/21-multinomial-logistic.html#exploratory-data-analysis-1",
    "title": "Multinomial Logistic Regression",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#model-in-r",
    "href": "slides/21-multinomial-logistic.html#model-in-r",
    "title": "Multinomial Logistic Regression",
    "section": "Model in R",
    "text": "Model in R\nUse the multinom_reg() function with the \"nnet\" engine:\n\nhealth_fit &lt;- multinom_reg() |&gt;\n  set_engine(\"nnet\") |&gt;\n  fit(HealthGen ~ Age + PhysActive, data = nhanes_adult)"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#model-result",
    "href": "slides/21-multinomial-logistic.html#model-result",
    "title": "Multinomial Logistic Regression",
    "section": "Model result",
    "text": "Model result\n\nhealth_fit\n\nparsnip model object\n\nCall:\nnnet::multinom(formula = HealthGen ~ Age + PhysActive, data = data, \n    trace = FALSE)\n\nCoefficients:\n      (Intercept)           Age PhysActiveYes\nVgood   1.2053460  0.0009101848    -0.3209047\nGood    1.9476261 -0.0023686122    -1.0014925\nFair    0.9145492  0.0030462534    -1.6454297\nPoor   -1.5211414  0.0221905681    -2.6556343\n\nResidual Deviance: 17588.88 \nAIC: 17612.88"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#model-output",
    "href": "slides/21-multinomial-logistic.html#model-output",
    "title": "Multinomial Logistic Regression",
    "section": "Model output",
    "text": "Model output\n\nWhat function do we use to get the model summary, i.e., coefficient estimates.\n\n\n\ntidy(health_fit)\n\n# A tibble: 12 × 6\n   y.level term           estimate std.error statistic  p.value\n   &lt;chr&gt;   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 Vgood   (Intercept)    1.21       0.145       8.33  8.42e-17\n 2 Vgood   Age            0.000910   0.00246     0.369 7.12e- 1\n 3 Vgood   PhysActiveYes -0.321      0.0929     -3.45  5.51e- 4\n 4 Good    (Intercept)    1.95       0.141      13.8   1.39e-43\n 5 Good    Age           -0.00237    0.00242    -0.977 3.29e- 1\n 6 Good    PhysActiveYes -1.00       0.0901    -11.1   1.00e-28\n 7 Fair    (Intercept)    0.915      0.164       5.57  2.61e- 8\n 8 Fair    Age            0.00305    0.00288     1.06  2.90e- 1\n 9 Fair    PhysActiveYes -1.65       0.107     -15.3   5.69e-53\n10 Poor    (Intercept)   -1.52       0.290      -5.24  1.62e- 7\n11 Poor    Age            0.0222     0.00491     4.52  6.11e- 6\n12 Poor    PhysActiveYes -2.66       0.236     -11.3   1.75e-29"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#model-output-with-ci",
    "href": "slides/21-multinomial-logistic.html#model-output-with-ci",
    "title": "Multinomial Logistic Regression",
    "section": "Model output, with CI",
    "text": "Model output, with CI\n\ntidy(health_fit, conf.int = TRUE)\n\n# A tibble: 12 × 8\n   y.level term         estimate std.error statistic  p.value conf.low conf.high\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Vgood   (Intercept)   1.21e+0   0.145       8.33  8.42e-17  0.922     1.49   \n 2 Vgood   Age           9.10e-4   0.00246     0.369 7.12e- 1 -0.00392   0.00574\n 3 Vgood   PhysActiveY… -3.21e-1   0.0929     -3.45  5.51e- 4 -0.503    -0.139  \n 4 Good    (Intercept)   1.95e+0   0.141      13.8   1.39e-43  1.67      2.22   \n 5 Good    Age          -2.37e-3   0.00242    -0.977 3.29e- 1 -0.00712   0.00238\n 6 Good    PhysActiveY… -1.00e+0   0.0901    -11.1   1.00e-28 -1.18     -0.825  \n 7 Fair    (Intercept)   9.15e-1   0.164       5.57  2.61e- 8  0.592     1.24   \n 8 Fair    Age           3.05e-3   0.00288     1.06  2.90e- 1 -0.00260   0.00869\n 9 Fair    PhysActiveY… -1.65e+0   0.107     -15.3   5.69e-53 -1.86     -1.43   \n10 Poor    (Intercept)  -1.52e+0   0.290      -5.24  1.62e- 7 -2.09     -0.952  \n11 Poor    Age           2.22e-2   0.00491     4.52  6.11e- 6  0.0126    0.0318 \n12 Poor    PhysActiveY… -2.66e+0   0.236     -11.3   1.75e-29 -3.12     -2.19"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#model-output-with-ci-1",
    "href": "slides/21-multinomial-logistic.html#model-output-with-ci-1",
    "title": "Multinomial Logistic Regression",
    "section": "Model output, with CI",
    "text": "Model output, with CI\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nVgood\n(Intercept)\n1.205\n0.145\n8.325\n0.000\n0.922\n1.489\n\n\nVgood\nAge\n0.001\n0.002\n0.369\n0.712\n-0.004\n0.006\n\n\nVgood\nPhysActiveYes\n-0.321\n0.093\n-3.454\n0.001\n-0.503\n-0.139\n\n\nGood\n(Intercept)\n1.948\n0.141\n13.844\n0.000\n1.672\n2.223\n\n\nGood\nAge\n-0.002\n0.002\n-0.977\n0.329\n-0.007\n0.002\n\n\nGood\nPhysActiveYes\n-1.001\n0.090\n-11.120\n0.000\n-1.178\n-0.825\n\n\nFair\n(Intercept)\n0.915\n0.164\n5.566\n0.000\n0.592\n1.237\n\n\nFair\nAge\n0.003\n0.003\n1.058\n0.290\n-0.003\n0.009\n\n\nFair\nPhysActiveYes\n-1.645\n0.107\n-15.319\n0.000\n-1.856\n-1.435\n\n\nPoor\n(Intercept)\n-1.521\n0.290\n-5.238\n0.000\n-2.090\n-0.952\n\n\nPoor\nAge\n0.022\n0.005\n4.522\n0.000\n0.013\n0.032\n\n\nPoor\nPhysActiveYes\n-2.656\n0.236\n-11.275\n0.000\n-3.117\n-2.194"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#fair-vs.-excellent-health",
    "href": "slides/21-multinomial-logistic.html#fair-vs.-excellent-health",
    "title": "Multinomial Logistic Regression",
    "section": "Fair vs. Excellent Health",
    "text": "Fair vs. Excellent Health\nThe baseline category for the model is Excellent.\n\nThe model equation for the log-odds a person rates themselves as having “Fair” health vs. “Excellent” is\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}_{Fair}}{\\hat{\\pi}_{Excellent}}\\Big) = 0.915  + 0.003 ~ \\text{age} - 1.645 ~ \\text{PhysActive}\n\\]"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#interpretations",
    "href": "slides/21-multinomial-logistic.html#interpretations",
    "title": "Multinomial Logistic Regression",
    "section": "Interpretations",
    "text": "Interpretations\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}_{Fair}}{\\hat{\\pi}_{Excellent}}\\Big) = 0.915  + 0.003 ~ \\text{age} - 1.645 ~ \\text{PhysActive}\n\\]\nFor each additional year in age, the odds a person rates themselves as having fair health versus excellent health are expected to multiply by 1.003 (exp(0.003)), holding physical activity constant.\n\n\nThe odds a person who does physical activity will rate themselves as having fair health versus excellent health are expected to be 0.193 (exp(-1.645)) times the odds for a person who doesn’t do physical activity, holding age constant."
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#interpretations-1",
    "href": "slides/21-multinomial-logistic.html#interpretations-1",
    "title": "Multinomial Logistic Regression",
    "section": "Interpretations",
    "text": "Interpretations\n\\[\n\\log\\Big(\\frac{\\hat{\\pi}_{Fair}}{\\hat{\\pi}_{Excellent}}\\Big) = 0.915  + 0.003 ~ \\text{age} - 1.645 ~ \\text{PhysActive}\n\\]\nThe odds a 0 year old person who doesn’t do physical activity rates themselves as having fair health vs. excellent health are 2.497 (exp(0.915)).\n\n\n\n\n\n\n\nWarning\n\n\nNeed to mean-center age for the intercept to have a meaningful interpretation!"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#hypothesis-test-for-beta_jk",
    "href": "slides/21-multinomial-logistic.html#hypothesis-test-for-beta_jk",
    "title": "Multinomial Logistic Regression",
    "section": "Hypothesis test for \\(\\beta_{jk}\\)",
    "text": "Hypothesis test for \\(\\beta_{jk}\\)\nThe test of significance for the coefficient \\(\\beta_{jk}\\) is\nHypotheses: \\(H_0: \\beta_{jk} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_{jk} \\neq 0\\)\nTest Statistic: \\[z = \\frac{\\hat{\\beta}_{jk} - 0}{SE(\\hat{\\beta_{jk}})}\\]\nP-value: \\(P(|Z| &gt; |z|)\\),\nwhere \\(Z \\sim N(0, 1)\\), the Standard Normal distribution"
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#confidence-interval-for-beta_jk",
    "href": "slides/21-multinomial-logistic.html#confidence-interval-for-beta_jk",
    "title": "Multinomial Logistic Regression",
    "section": "Confidence interval for \\(\\beta_{jk}\\)",
    "text": "Confidence interval for \\(\\beta_{jk}\\)\n\n\nWe can calculate the C% confidence interval for \\(\\beta_{jk}\\) using \\(\\hat{\\beta}_{jk} \\pm z^* SE(\\hat{\\beta}_{jk})\\), where \\(z^*\\) is calculated from the \\(N(0,1)\\) distribution.\nInterpretation: We are \\(C\\%\\) confident that for every one unit change in \\(x_{j}\\), the odds of \\(y = k\\) versus the baseline will multiply by a factor of \\(\\exp\\{\\hat{\\beta}_{jk} - z^* SE(\\hat{\\beta}_{jk})\\}\\) to \\(\\exp\\{\\hat{\\beta}_{jk} + z^* SE(\\hat{\\beta}_{jk})\\}\\), holding all else constant."
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#interpreting-cis-for-beta_jk",
    "href": "slides/21-multinomial-logistic.html#interpreting-cis-for-beta_jk",
    "title": "Multinomial Logistic Regression",
    "section": "Interpreting CIs for \\(\\beta_{jk}\\)",
    "text": "Interpreting CIs for \\(\\beta_{jk}\\)\n\ntidy(health_fit, conf.int = TRUE) |&gt;\n  filter(y.level == \"Fair\") |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nFair\n(Intercept)\n0.915\n0.164\n5.566\n0.00\n0.592\n1.237\n\n\nFair\nAge\n0.003\n0.003\n1.058\n0.29\n-0.003\n0.009\n\n\nFair\nPhysActiveYes\n-1.645\n0.107\n-15.319\n0.00\n-1.856\n-1.435\n\n\n\n\n\n\n\nWe are 95% confident, that for each additional year in age, the odds a person rates themselves as having fair health versus excellent health will multiply by 0.997 (exp(-0.003)) to 1.009 (exp(0.009)) , holding physical activity constant."
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#interpreting-cis-for-beta_jk-1",
    "href": "slides/21-multinomial-logistic.html#interpreting-cis-for-beta_jk-1",
    "title": "Multinomial Logistic Regression",
    "section": "Interpreting CIs for \\(\\beta_{jk}\\)",
    "text": "Interpreting CIs for \\(\\beta_{jk}\\)\n\ntidy(health_fit, conf.int = TRUE) |&gt;\n  filter(y.level == \"Fair\") |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\nFair\n(Intercept)\n0.915\n0.164\n5.566\n0.00\n0.592\n1.237\n\n\nFair\nAge\n0.003\n0.003\n1.058\n0.29\n-0.003\n0.009\n\n\nFair\nPhysActiveYes\n-1.645\n0.107\n-15.319\n0.00\n-1.856\n-1.435\n\n\n\n\n\n\nWe are 95% confident that the odds a person who does physical activity will rate themselves as having fair health versus excellent health are 0.156 (exp(-1.856 )) to 0.238 (exp(-1.435)) times the odds for a person who doesn’t do physical activity, holding age constant."
  },
  {
    "objectID": "slides/21-multinomial-logistic.html#recap",
    "href": "slides/21-multinomial-logistic.html#recap",
    "title": "Multinomial Logistic Regression",
    "section": "Recap",
    "text": "Recap\n\nIntroduce multinomial logistic regression\nInterpret model coefficients\nInference for a coefficient \\(\\beta_{jk}\\)\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/19-logistic-compare.html#announcements",
    "href": "slides/19-logistic-compare.html#announcements",
    "title": "Logistic Regression: Model comparison",
    "section": "Announcements",
    "text": "Announcements\n\nProject draft due in your GitHub repo at 9am on\n\nNovember 14 (Tuesday labs)\nNovember 16 (Thursday labs)\nWill do peer review in lab those days\n\nTeam Feedback #1 due Friday, November 10 at 11:5pm\n\nReceived email from teammates\n\nHW 04 due Wednesday, November 15 at 11:59pm\n\nReleased later today"
  },
  {
    "objectID": "slides/19-logistic-compare.html#topics",
    "href": "slides/19-logistic-compare.html#topics",
    "title": "Logistic Regression: Model comparison",
    "section": "Topics",
    "text": "Topics\nComparing logistic regression models using\n\nDrop-in-deviance test\nAIC\nBIC"
  },
  {
    "objectID": "slides/19-logistic-compare.html#computational-setup",
    "href": "slides/19-logistic-compare.html#computational-setup",
    "title": "Logistic Regression: Model comparison",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\nlibrary(kableExtra)  # for table embellishments\nlibrary(Stat2Data)   # for empirical logit\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/19-logistic-compare.html#risk-of-coronary-heart-disease",
    "href": "slides/19-logistic-compare.html#risk-of-coronary-heart-disease",
    "title": "Logistic Regression: Model comparison",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\ncurrentSmoker: 0 = nonsmoker, 1 = smoker"
  },
  {
    "objectID": "slides/19-logistic-compare.html#data-prep",
    "href": "slides/19-logistic-compare.html#data-prep",
    "title": "Logistic Regression: Model comparison",
    "section": "Data prep",
    "text": "Data prep\n\nheart_disease &lt;- read_csv(here::here(\"slides\", \"data/framingham.csv\")) |&gt;\n  select(age, education, TenYearCHD, totChol, currentSmoker) |&gt;\n  drop_na() |&gt; #consider the limitations of doing this\n  mutate(\n    high_risk = as.factor(TenYearCHD),\n    education = as.factor(education),\n    currentSmoker = as.factor(currentSmoker)\n  )\n\nheart_disease\n\n# A tibble: 4,086 × 6\n     age education TenYearCHD totChol currentSmoker high_risk\n   &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;fct&gt;    \n 1    39 4                  0     195 0             0        \n 2    46 2                  0     250 0             0        \n 3    48 1                  0     245 1             0        \n 4    61 3                  1     225 1             1        \n 5    46 3                  0     285 1             0        \n 6    43 2                  0     228 0             0        \n 7    63 1                  1     205 0             1        \n 8    45 2                  0     313 1             0        \n 9    52 1                  0     260 0             0        \n10    43 1                  0     225 1             0        \n# ℹ 4,076 more rows"
  },
  {
    "objectID": "slides/19-logistic-compare.html#modeling-risk-of-coronary-heart-disease",
    "href": "slides/19-logistic-compare.html#modeling-risk-of-coronary-heart-disease",
    "title": "Logistic Regression: Model comparison",
    "section": "Modeling risk of coronary heart disease",
    "text": "Modeling risk of coronary heart disease\nUsing age and education:\n\nrisk_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/19-logistic-compare.html#model-output",
    "href": "slides/19-logistic-compare.html#model-output",
    "title": "Logistic Regression: Model comparison",
    "section": "Model output",
    "text": "Model output\n\ntidy(risk_fit, conf.int = TRUE) |&gt; \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\\[\n\\small{\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.508 + 0.076 ~ \\text{age} - 0.245 ~ \\text{ed2} - 0.236 ~ \\text{ed3} - 0.024 ~ \\text{ed4}}\n\\]\n\nShould we add currentSmoker to this model?"
  },
  {
    "objectID": "slides/19-logistic-compare.html#log-likelihood",
    "href": "slides/19-logistic-compare.html#log-likelihood",
    "title": "Logistic Regression: Model comparison",
    "section": "Log likelihood",
    "text": "Log likelihood\n\\[\n\\log L = \\sum\\limits_{i=1}^n[y_i \\log(\\hat{\\pi}_i) + (1 - y_i)\\log(1 - \\hat{\\pi}_i)]\n\\]\n\nMeasure of how well the model fits the data\nHigher values of \\(\\log L\\) are better\nDeviance = \\(-2 \\log L\\)\n\n\\(-2 \\log L\\) follows a \\(\\chi^2\\) distribution with \\(n - p - 1\\) degrees of freedom"
  },
  {
    "objectID": "slides/19-logistic-compare.html#comparing-nested-models",
    "href": "slides/19-logistic-compare.html#comparing-nested-models",
    "title": "Logistic Regression: Model comparison",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced Model includes predictors \\(x_1, \\ldots, x_q\\)\nFull Model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the hypotheses\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least one }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\nTo do so, we will use the Drop-in-deviance test, also known as the Nested Likelihood Ratio test"
  },
  {
    "objectID": "slides/19-logistic-compare.html#drop-in-deviance-test",
    "href": "slides/19-logistic-compare.html#drop-in-deviance-test",
    "title": "Logistic Regression: Model comparison",
    "section": "Drop-in-deviance test",
    "text": "Drop-in-deviance test\nHypotheses:\n\\[\n\\begin{aligned}\nH_0&: \\beta_{q+1} = \\dots = \\beta_p = 0 \\\\\nH_A&: \\text{ at least 1 }\\beta_j \\text{ is not } 0\n\\end{aligned}\n\\]\n\nTest Statistic: \\[G = (-2 \\log L_{reduced}) - (-2 \\log L_{full})\\]\n\n\nP-value: \\(P(\\chi^2 &gt; G)\\), calculated using a \\(\\chi^2\\) distribution with degrees of freedom equal to the difference in the number of parameters in the full and reduced models"
  },
  {
    "objectID": "slides/19-logistic-compare.html#chi2-distribution",
    "href": "slides/19-logistic-compare.html#chi2-distribution",
    "title": "Logistic Regression: Model comparison",
    "section": "\\(\\chi^2\\) distribution",
    "text": "\\(\\chi^2\\) distribution"
  },
  {
    "objectID": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model",
    "href": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model",
    "title": "Logistic Regression: Model comparison",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nFirst model, reduced:\n\nrisk_fit_reduced &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")\n\n\n\nSecond model, full:\n\nrisk_fit_full &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education + currentSmoker, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model-1",
    "href": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model-1",
    "title": "Logistic Regression: Model comparison",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate deviance for each model:\n\n(dev_reduced &lt;- glance(risk_fit_reduced)$deviance)\n\n[1] 3244.187\n\n(dev_full &lt;- glance(risk_fit_full)$deviance)\n\n[1] 3221.901\n\n\n\nDrop-in-deviance test statistic:\n\n(test_stat &lt;- dev_reduced - dev_full)\n\n[1] 22.2863"
  },
  {
    "objectID": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model-2",
    "href": "slides/19-logistic-compare.html#should-we-add-currentsmoker-to-the-model-2",
    "title": "Logistic Regression: Model comparison",
    "section": "Should we add currentSmoker to the model?",
    "text": "Should we add currentSmoker to the model?\nCalculate the p-value using a pchisq(), with degrees of freedom equal to the number of new model terms in the second model:\n\npchisq(test_stat, 1, lower.tail = FALSE) \n\n[1] 2.348761e-06\n\n\n\nConclusion: The p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that the coefficient of currentSmoker is not equal to 0. Therefore, we should add it to the model."
  },
  {
    "objectID": "slides/19-logistic-compare.html#drop-in-deviance-test-in-r",
    "href": "slides/19-logistic-compare.html#drop-in-deviance-test-in-r",
    "title": "Logistic Regression: Model comparison",
    "section": "Drop-in-Deviance test in R",
    "text": "Drop-in-Deviance test in R\n\nWe can use the anova function to conduct this test\nAdd test = \"Chisq\" to conduct the drop-in-deviance test\n\n\n\nanova(risk_fit_reduced$fit, risk_fit_full$fit, test = \"Chisq\") |&gt;\n  tidy() |&gt; kable(digits = 3)\n\n\n\n\nterm\ndf.residual\nresidual.deviance\ndf\ndeviance\np.value\n\n\n\n\nhigh_risk ~ age + education\n4081\n3244.187\nNA\nNA\nNA\n\n\nhigh_risk ~ age + education + currentSmoker\n4080\n3221.901\n1\n22.286\n0"
  },
  {
    "objectID": "slides/19-logistic-compare.html#model-selection",
    "href": "slides/19-logistic-compare.html#model-selection",
    "title": "Logistic Regression: Model comparison",
    "section": "Model selection",
    "text": "Model selection\nUse AIC or BIC for model selection\n\\[\n\\begin{align}\n&AIC = - 2 * \\log L - {\\color{purple}n\\log(n)}+ 2(p +1)\\\\[5pt]\n&BIC =- 2 * \\log L - {\\color{purple}n\\log(n)} + log(n)\\times(p+1)\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/19-logistic-compare.html#aic-from-the-glance-function",
    "href": "slides/19-logistic-compare.html#aic-from-the-glance-function",
    "title": "Logistic Regression: Model comparison",
    "section": "AIC from the glance() function",
    "text": "AIC from the glance() function\nLet’s look at the AIC for the model that includes age, education, and currentSmoker\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\nCalculating AIC\n\n- 2 * glance(risk_fit_full)$logLik + 2 * (5 + 1)\n\n[1] 3233.901"
  },
  {
    "objectID": "slides/19-logistic-compare.html#comparing-the-models-using-aic",
    "href": "slides/19-logistic-compare.html#comparing-the-models-using-aic",
    "title": "Logistic Regression: Model comparison",
    "section": "Comparing the models using AIC",
    "text": "Comparing the models using AIC\nLet’s compare the full and reduced models using AIC.\n\nglance(risk_fit_reduced)$AIC\n\n[1] 3254.187\n\nglance(risk_fit_full)$AIC\n\n[1] 3233.901\n\n\n\n\nBased on AIC, which model would you choose?"
  },
  {
    "objectID": "slides/19-logistic-compare.html#comparing-the-models-using-bic",
    "href": "slides/19-logistic-compare.html#comparing-the-models-using-bic",
    "title": "Logistic Regression: Model comparison",
    "section": "Comparing the models using BIC",
    "text": "Comparing the models using BIC\nLet’s compare the full and reduced models using BIC\n\nglance(risk_fit_reduced)$BIC\n\n[1] 3285.764\n\nglance(risk_fit_full)$BIC\n\n[1] 3271.793\n\n\n\n\nBased on BIC, which model would you choose?"
  },
  {
    "objectID": "slides/19-logistic-compare.html#application-exercise",
    "href": "slides/19-logistic-compare.html#application-exercise",
    "title": "Logistic Regression: Model comparison",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 AE 14: Comparing logistic regression models\n\n\nSit with your lab groups.\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/17-odds-ratios.html#announcements",
    "href": "slides/17-odds-ratios.html#announcements",
    "title": "Odds ratios",
    "section": "Announcements",
    "text": "Announcements\n\nHW 03 TODAY at 11:59pm\nLab 06 due\n\nFriday, November 03 at 11:59pm (Tuesday labs)\nSunday, November 05 at 11:59pm (Thursday labs)"
  },
  {
    "objectID": "slides/17-odds-ratios.html#application-exercise",
    "href": "slides/17-odds-ratios.html#application-exercise",
    "title": "Odds ratios",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 AE 13: Logistic regression (continuation of AE 12)"
  },
  {
    "objectID": "slides/17-odds-ratios.html#topics",
    "href": "slides/17-odds-ratios.html#topics",
    "title": "Odds ratios",
    "section": "Topics",
    "text": "Topics\n\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors"
  },
  {
    "objectID": "slides/17-odds-ratios.html#computational-setup",
    "href": "slides/17-odds-ratios.html#computational-setup",
    "title": "Odds ratios",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/17-odds-ratios.html#risk-of-coronary-heart-disease",
    "href": "slides/17-odds-ratios.html#risk-of-coronary-heart-disease",
    "title": "Odds ratios",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\n\n\nRows: 4240 Columns: 16\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl (16): male, age, education, currentSmoker, cigsPerDay, BPMeds, prevalent...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "slides/17-odds-ratios.html#high-risk-vs.-education",
    "href": "slides/17-odds-ratios.html#high-risk-vs.-education",
    "title": "Odds ratios",
    "section": "High risk vs. education",
    "text": "High risk vs. education\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#compare-the-odds-for-two-groups",
    "href": "slides/17-odds-ratios.html#compare-the-odds-for-two-groups",
    "title": "Odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\n\nWe want to compare the risk of heart disease for those with a High School diploma/GED and those with a college degree.\nWe’ll use the odds to compare the two groups\n\n\\[\n\\text{odds} = \\frac{P(\\text{success})}{P(\\text{failure})} = \\frac{\\text{# of successes}}{\\text{# of failures}}\n\\]"
  },
  {
    "objectID": "slides/17-odds-ratios.html#compare-the-odds-for-two-groups-1",
    "href": "slides/17-odds-ratios.html#compare-the-odds-for-two-groups-1",
    "title": "Odds ratios",
    "section": "Compare the odds for two groups",
    "text": "Compare the odds for two groups\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\nOdds of being high risk for the High school or GED group: \\(\\frac{147}{1106} = 0.133\\)\nOdds of being high risk for the College group: \\(\\frac{70}{403} = 0.174\\)\nBased on this, we see those with a college degree had higher odds of being high risk for heart disease than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/17-odds-ratios.html#odds-ratio-or",
    "href": "slides/17-odds-ratios.html#odds-ratio-or",
    "title": "Odds ratios",
    "section": "Odds ratio (OR)",
    "text": "Odds ratio (OR)\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\nLet’s summarize the relationship between the two groups. To do so, we’ll use the odds ratio (OR).\n\\[\nOR = \\frac{\\text{odds}_1}{\\text{odds}_2} = \\frac{\\omega_1}{\\omega_2}\n\\]"
  },
  {
    "objectID": "slides/17-odds-ratios.html#or-college-vs.-high-school-or-ged",
    "href": "slides/17-odds-ratios.html#or-college-vs.-high-school-or-ged",
    "title": "Odds ratios",
    "section": "OR: College vs. High school or GED",
    "text": "OR: College vs. High school or GED\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{HS}} = \\frac{0.174}{0.133} = \\mathbf{1.308}\\]\n\nThe odds of being high risk for heart disease are 1.30 times higher for those with a college degree than those with a high school diploma or GED."
  },
  {
    "objectID": "slides/17-odds-ratios.html#or-college-vs.-some-high-school",
    "href": "slides/17-odds-ratios.html#or-college-vs.-some-high-school",
    "title": "Odds ratios",
    "section": "OR: College vs. Some high school",
    "text": "OR: College vs. Some high school\n\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403\n\n\n\n\n\n\\[OR = \\frac{\\text{odds}_{College}}{\\text{odds}_{Some HS}} = \\frac{70/403}{323/1397} = 0.751\\]\n\nThe odds of being high risk for having heart disease for those with a college degree are 0.751 times the odds of being high risk for heart disease for those with some high school."
  },
  {
    "objectID": "slides/17-odds-ratios.html#more-natural-interpretation",
    "href": "slides/17-odds-ratios.html#more-natural-interpretation",
    "title": "Odds ratios",
    "section": "More natural interpretation",
    "text": "More natural interpretation\n\nIt’s more natural to interpret the odds ratio with a statement with the odds ratio greater than 1.\nThe odds of being high risk for heart disease are 1.33 times higher for those with some high school than those with a college degree."
  },
  {
    "objectID": "slides/17-odds-ratios.html#making-the-table-1",
    "href": "slides/17-odds-ratios.html#making-the-table-1",
    "title": "Odds ratios",
    "section": "Making the table 1",
    "text": "Making the table 1\nFirst, rename the levels of the categorical variables:\n\nheart_disease &lt;- heart_disease |&gt;\n  mutate(\n    high_risk_names = if_else(high_risk == \"1\", \"High risk\", \"Not high risk\"),\n    education_names = case_when(\n      education == \"1\" ~ \"Some high school\",\n      education == \"2\" ~ \"High school or GED\",\n      education == \"3\" ~ \"Some college or vocational school\",\n      education == \"4\" ~ \"College\"\n    ),\n    education_names = fct_relevel(education_names, \"Some high school\", \"High school or GED\", \"Some college or vocational school\", \"College\")\n  )"
  },
  {
    "objectID": "slides/17-odds-ratios.html#making-the-table-2",
    "href": "slides/17-odds-ratios.html#making-the-table-2",
    "title": "Odds ratios",
    "section": "Making the table 2",
    "text": "Making the table 2\nThen, make the table:\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n) |&gt;\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))"
  },
  {
    "objectID": "slides/17-odds-ratios.html#deeper-look-into-the-code",
    "href": "slides/17-odds-ratios.html#deeper-look-into-the-code",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names)\n\n# A tibble: 8 × 3\n  education_names                   high_risk_names     n\n  &lt;fct&gt;                             &lt;chr&gt;           &lt;int&gt;\n1 Some high school                  High risk         323\n2 Some high school                  Not high risk    1397\n3 High school or GED                High risk         147\n4 High school or GED                Not high risk    1106\n5 Some college or vocational school High risk          88\n6 Some college or vocational school Not high risk     601\n7 College                           High risk          70\n8 College                           Not high risk     403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#deeper-look-into-the-code-1",
    "href": "slides/17-odds-ratios.html#deeper-look-into-the-code-1",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n)\n\n# A tibble: 4 × 3\n  education_names                   `High risk` `Not high risk`\n  &lt;fct&gt;                                   &lt;int&gt;           &lt;int&gt;\n1 Some high school                          323            1397\n2 High school or GED                        147            1106\n3 Some college or vocational school          88             601\n4 College                                    70             403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#deeper-look-into-the-code-2",
    "href": "slides/17-odds-ratios.html#deeper-look-into-the-code-2",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n) |&gt;\n  kable()\n\n\n\n\neducation_names\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#deeper-look-into-the-code-3",
    "href": "slides/17-odds-ratios.html#deeper-look-into-the-code-3",
    "title": "Odds ratios",
    "section": "Deeper look into the code",
    "text": "Deeper look into the code\n\nheart_disease |&gt;\n  count(education_names, high_risk_names) |&gt;\n  pivot_wider(names_from = high_risk_names, values_from = n) |&gt;\n  kable(col.names = c(\"Education\", \"High risk\", \"Not high risk\"))\n\n\n\n\nEducation\nHigh risk\nNot high risk\n\n\n\n\nSome high school\n323\n1397\n\n\nHigh school or GED\n147\n1106\n\n\nSome college or vocational school\n88\n601\n\n\nCollege\n70\n403"
  },
  {
    "objectID": "slides/17-odds-ratios.html#categorical-predictor",
    "href": "slides/17-odds-ratios.html#categorical-predictor",
    "title": "Odds ratios",
    "section": "Categorical predictor",
    "text": "Categorical predictor\nRecall: Education - 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\n\nheart_edu_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ education, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046"
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpreting-education4-log-odds",
    "href": "slides/17-odds-ratios.html#interpreting-education4-log-odds",
    "title": "Odds ratios",
    "section": "Interpreting education4: log-odds",
    "text": "Interpreting education4: log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe log-odds of being high risk for heart disease are expected to be 0.286 less for those with a college degree compared to those with some high school (the baseline group).\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpreting-education4-odds",
    "href": "slides/17-odds-ratios.html#interpreting-education4-odds",
    "title": "Odds ratios",
    "section": "Interpreting education4: odds",
    "text": "Interpreting education4: odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-1.464\n0.062\n-23.719\n0.000\n\n\neducation2\n-0.554\n0.107\n-5.159\n0.000\n\n\neducation3\n-0.457\n0.130\n-3.520\n0.000\n\n\neducation4\n-0.286\n0.143\n-1.994\n0.046\n\n\n\n\n\n\nThe odds of being high risk for heart disease for those with a college degree are expected to be 0.751 (exp{-0.286}) times the odds for those with some high school."
  },
  {
    "objectID": "slides/17-odds-ratios.html#coefficients-odds-ratios",
    "href": "slides/17-odds-ratios.html#coefficients-odds-ratios",
    "title": "Odds ratios",
    "section": "Coefficients + odds ratios",
    "text": "Coefficients + odds ratios\nThe model coefficient, -0.286, is the expected change in the log-odds when going from the Some high school group to the College group.\n\nTherefore, \\(e^{-0.286}\\) = 0.751 is the expected change in the odds when going from the Some high school group to the College group.\n\n\n\\[\nOR  = e^{\\hat{\\beta}_j} = \\exp\\{\\hat{\\beta}_j\\}\n\\]"
  },
  {
    "objectID": "slides/17-odds-ratios.html#quantitative-predictor",
    "href": "slides/17-odds-ratios.html#quantitative-predictor",
    "title": "Odds ratios",
    "section": "Quantitative predictor",
    "text": "Quantitative predictor\n\nheart_age_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_age_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0"
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpreting-age-log-odds",
    "href": "slides/17-odds-ratios.html#interpreting-age-log-odds",
    "title": "Odds ratios",
    "section": "Interpreting age: log-odds",
    "text": "Interpreting age: log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\nFor each additional year in age, the log-odds of being high risk for heart disease are expected to increase by 0.076.\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpreting-age-odds",
    "href": "slides/17-odds-ratios.html#interpreting-age-odds",
    "title": "Odds ratios",
    "section": "Interpreting age: odds",
    "text": "Interpreting age: odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.619\n0.288\n-19.498\n0\n\n\nage\n0.076\n0.005\n14.174\n0\n\n\n\n\n\n\n\nFor each additional year in age, the odds of being high risk for heart disease are expected to multiply by a factor of 1.08 (exp{0.076}).\nAlternate interpretation: For each additional year in age, the odds of being high risk for heart disease are expected to increase by 8%."
  },
  {
    "objectID": "slides/17-odds-ratios.html#multiple-predictors",
    "href": "slides/17-odds-ratios.html#multiple-predictors",
    "title": "Odds ratios",
    "section": "Multiple predictors",
    "text": "Multiple predictors\n\nheart_edu_age_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ education + age, data  = heart_disease, family = \"binomial\")\n\n\ntidy(heart_edu_age_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000"
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpretation-in-terms-of-log-odds",
    "href": "slides/17-odds-ratios.html#interpretation-in-terms-of-log-odds",
    "title": "Odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The log-odds of being high risk for heart disease are expected to be 0.020 less for those with a college degree compared to those with some high school, holding age constant.\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpretation-in-terms-of-log-odds-1",
    "href": "slides/17-odds-ratios.html#interpretation-in-terms-of-log-odds-1",
    "title": "Odds ratios",
    "section": "Interpretation in terms of log-odds",
    "text": "Interpretation in terms of log-odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the log-odds of being high risk for heart disease are expected to increase by 0.073, holding education level constant.\n\n\n\n\n\n\n\nWarning\n\n\nWe would not use the interpretation in terms of log-odds in practice."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpretation-in-terms-of-odds",
    "href": "slides/17-odds-ratios.html#interpretation-in-terms-of-odds",
    "title": "Odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\neducation4: The odds of being high risk for heart disease for those with a college degree are expected to be 0.98 (exp{-0.020}) times the odds for those with some high school, holding age constant.\n\n\n\n\n\n\nNote\n\n\nIn logistic regression with 2+ predictors, \\(exp\\{\\hat{\\beta}_j\\}\\) is often called the adjusted odds ratio (AOR)."
  },
  {
    "objectID": "slides/17-odds-ratios.html#interpretation-in-terms-of-odds-1",
    "href": "slides/17-odds-ratios.html#interpretation-in-terms-of-odds-1",
    "title": "Odds ratios",
    "section": "Interpretation in terms of odds",
    "text": "Interpretation in terms of odds\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.385\n0.308\n-17.507\n0.000\n\n\neducation2\n-0.242\n0.112\n-2.162\n0.031\n\n\neducation3\n-0.235\n0.134\n-1.761\n0.078\n\n\neducation4\n-0.020\n0.148\n-0.136\n0.892\n\n\nage\n0.073\n0.005\n13.385\n0.000\n\n\n\n\n\n\nage: For each additional year in age, the odds being high risk for heart disease are expected to multiply by a factor of 1.08 (exp(0.073)), holding education level constant.\n\n\n\n\n\n\nNote\n\n\nIn logistic regression with 2+ predictors, \\(exp\\{\\hat{\\beta}_j\\}\\) is often called the adjusted odds ratio (AOR)."
  },
  {
    "objectID": "slides/17-odds-ratios.html#recap",
    "href": "slides/17-odds-ratios.html#recap",
    "title": "Odds ratios",
    "section": "Recap",
    "text": "Recap\n\nUse the odds ratio to compare the odds of two groups\nInterpret the coefficients of a logistic regression model with\n\na single categorical predictor\na single quantitative predictor\nmultiple predictors\n\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#announcements",
    "href": "slides/14-mlr-inference-old.html#announcements",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Announcements",
    "text": "Announcements\n\nProject propsal due\n\nFriday, October 27 (Tuesday labs)\nSunday, October 29 (Thursday labs)\n\nHW 03 due Wednesday, November 1\n\nreleased after Section 002 lecture"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#topics",
    "href": "slides/14-mlr-inference-old.html#topics",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Topics",
    "text": "Topics\n\n\nInference for multiple linear regression\nChecking model conditions\nVariable transformations"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#computational-setup",
    "href": "slides/14-mlr-inference-old.html#computational-setup",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.2.1     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(countdown)\nlibrary(rms)\n\nLoading required package: Hmisc\n\nAttaching package: 'Hmisc'\n\nThe following object is masked from 'package:parsnip':\n\n    translate\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#modeling-workflow",
    "href": "slides/14-mlr-inference-old.html#modeling-workflow",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\nSplit data into training and test sets.\nUse cross validation on the training set to fit, evaluate, and compare candidate models. Choose a final model based on summary of cross validation results.\nRefit the model using the entire training set and do “final” evaluation on the test set (make sure you have not overfit the model).\n\nAdjust as needed if there is evidence of overfit.\n\nUse model fit on training set for inference and prediction."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#data-rail_trail",
    "href": "slides/14-mlr-inference-old.html#data-rail_trail",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n\nRows: 90 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): season, day_type\ndbl (5): volume, hightemp, avgtemp, cloudcover, precip\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 90 × 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ℹ 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#variables",
    "href": "slides/14-mlr-inference-old.html#variables",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\n\nPredictors\n\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of “Fall”, “Spring”, or “Summer”\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of “weekday” or “weekend”"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#review-simple-linear-regression-slr",
    "href": "slides/14-mlr-inference-old.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\nggplot(rail_trail, aes(x = hightemp, y = volume)) + \n  geom_point(alpha = 0.5) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"High temp (F)\", y = \"Number of riders\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#slr-model-summary",
    "href": "slides/14-mlr-inference-old.html#slr-model-summary",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nrt_slr_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.08\n59.40\n-0.29\n0.77\n\n\nhightemp\n5.70\n0.85\n6.72\n0.00"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#slr-hypothesis-test",
    "href": "slides/14-mlr-inference-old.html#slr-hypothesis-test",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.08\n59.40\n-0.29\n0.77\n\n\nhightemp\n5.70\n0.85\n6.72\n0.00\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t= 6.72\\) . The p-value is calculated using a \\(t\\) distribution with 88 degrees of freedom. The p-value is \\(\\approx 0\\) .\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders, i.e. there is a linear relationship between high temperature and number of daily riders."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#multiple-linear-regression",
    "href": "slides/14-mlr-inference-old.html#multiple-linear-regression",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nrt_mlr_main_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#multiple-linear-regression-1",
    "href": "slides/14-mlr-inference-old.html#multiple-linear-regression-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes \\[Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)\\]\n\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)\\), we can rewrite the previous statement as\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)\\]"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#estimating-sigma_epsilon",
    "href": "slides/14-mlr-inference-old.html#estimating-sigma_epsilon",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Estimating \\(\\sigma_\\epsilon\\)",
    "text": "Estimating \\(\\sigma_\\epsilon\\)\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)\\) the residual is \\[e_i = y_{i} - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\\]\n\nThe estimated value of the regression standard error , \\(\\sigma_{\\epsilon}\\), is\n\\[\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}\\]\n\n\nAs with SLR, we use \\(\\hat{\\sigma}_{\\epsilon}\\) to calculate \\(SE_{\\hat{\\beta}_j}\\), the standard error of each coefficient. See Matrix Form of Linear Regression for more detail."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#mlr-hypothesis-test-hightemp",
    "href": "slides/14-mlr-inference-old.html#mlr-hypothesis-test-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "MLR hypothesis test: hightemp",
    "text": "MLR hypothesis test: hightemp\n\nSet hypotheses: \\(H_0: \\beta_{hightemp} = 0\\) vs. \\(H_A: \\beta_{hightemp} \\ne 0\\), given season is in the model\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.43\\). The p-value is calculated using a \\(t\\) distribution with 86 \\((n - p - 1)\\) degrees of freedom. The p-value is \\(\\approx 0\\).\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature for the day is a useful predictor in a model that already contains the season as a predictor for number of daily riders."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#the-model-for-season-spring",
    "href": "slides/14-mlr-inference-old.html#the-model-for-season-spring",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Spring",
    "text": "The model for season = Spring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 1 - 76.84 \\times 0 \\\\\n&= -120.10 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#the-model-for-season-summer",
    "href": "slides/14-mlr-inference-old.html#the-model-for-season-summer",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Summer",
    "text": "The model for season = Summer\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 1 \\\\\n&= -202.07 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#the-model-for-season-fall",
    "href": "slides/14-mlr-inference-old.html#the-model-for-season-fall",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Fall",
    "text": "The model for season = Fall\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 0 \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#the-models",
    "href": "slides/14-mlr-inference-old.html#the-models",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The models",
    "text": "The models\nSame slope, different intercepts\n\nseason = Spring: \\(-120.10 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Summer: \\(-202.07 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Fall: \\(-125.23 + 7.54 \\times \\texttt{hightemp}\\)"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#interaction-terms",
    "href": "slides/14-mlr-inference-old.html#interaction-terms",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Interaction terms",
    "text": "Interaction terms\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.53\n166.80\n-0.06\n0.95\n\n\nhightemp\n5.48\n2.95\n1.86\n0.07\n\n\nseasonSpring\n-293.95\n190.33\n-1.54\n0.13\n\n\nseasonSummer\n354.18\n255.08\n1.39\n0.17\n\n\nhightemp:seasonSpring\n4.88\n3.26\n1.50\n0.14\n\n\nhightemp:seasonSummer\n-4.54\n3.75\n-1.21\n0.23\n\n\n\n\n\n\nDo the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#confidence-interval-for-beta_j-1",
    "href": "slides/14-mlr-inference-old.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C\\%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically: We are \\(C\\%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#confidence-interval-for-beta_j-2",
    "href": "slides/14-mlr-inference-old.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\ntidy(rt_mlr_main_fit, conf.int = TRUE) |&gt;\n  kable(digits= 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#ci-for-hightemp",
    "href": "slides/14-mlr-inference-old.html#ci-for-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "CI for hightemp",
    "text": "CI for hightemp\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\nWe are 95% confident that for every degree Fahrenheit the day is warmer, the number of riders increases by 5.21 to 9.87, on average, holding season constant."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#ci-for-seasonspring",
    "href": "slides/14-mlr-inference-old.html#ci-for-seasonspring",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "CI for seasonSpring",
    "text": "CI for seasonSpring\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\nWe are 95% confident that the number of riders on a Spring day is lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant.\n\n\nIs season a significant predictor of the number of riders, after accounting for high temperature?"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#large-sample-sizes",
    "href": "slides/14-mlr-inference-old.html#large-sample-sizes",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#small-sample-sizes",
    "href": "slides/14-mlr-inference-old.html#small-sample-sizes",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON’T immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#full-model",
    "href": "slides/14-mlr-inference-old.html#full-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Full model",
    "text": "Full model\nIncluding all available predictors\nFit:\n\nrt_full_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ ., data = rail_trail)\n\n\nSummarize:\n\ntidy(rt_full_fit)\n\n# A tibble: 8 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\n\n\nAugment:\n\nrt_full_aug &lt;- augment(rt_full_fit$fit)"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#model-conditions",
    "href": "slides/14-mlr-inference-old.html#model-conditions",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#checking-linearity",
    "href": "slides/14-mlr-inference-old.html#checking-linearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking Linearity",
    "text": "Checking Linearity\n\nLook at a plot of the residuals vs. predicted values\nLook at a plot of the residuals vs. each predictor\nLinearity is met if there is no discernible pattern in each of these plots"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#residuals-vs.-predicted-values",
    "href": "slides/14-mlr-inference-old.html#residuals-vs.-predicted-values",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. predicted values",
    "text": "Residuals vs. predicted values\n\nggplot(data = rt_full_aug, aes(x = .fitted, y = .resid)) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Predicted values\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#residuals-vs.-each-predictor",
    "href": "slides/14-mlr-inference-old.html#residuals-vs.-each-predictor",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. each predictor",
    "text": "Residuals vs. each predictor"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#checking-linearity-1",
    "href": "slides/14-mlr-inference-old.html#checking-linearity-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plot of the residuals vs. predicted values looked OK\nThe plots of residuals vs. hightemp and avgtemp appear to have a parabolic pattern.\nThe linearity condition does not appear to be satisfied given these plots.\n\n\n\nGiven this conclusion, what might be a next step in the analysis?"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#checking-constant-variance",
    "href": "slides/14-mlr-inference-old.html#checking-constant-variance",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#checking-constant-variance-1",
    "href": "slides/14-mlr-inference-old.html#checking-constant-variance-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied.\n\n\n\nWe will talk about to address this later in the notes."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#checking-normality",
    "href": "slides/14-mlr-inference-old.html#checking-normality",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking normality",
    "text": "Checking normality\n\nThe distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size 90 is sufficiently large to relax this condition if it was not satisfied."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#checking-independence",
    "href": "slides/14-mlr-inference-old.html#checking-independence",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#checking-independence-1",
    "href": "slides/14-mlr-inference-old.html#checking-independence-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs. order of data collection:\n\nggplot(rt_full_aug, aes(y = .resid, x = 1:nrow(rt_full_aug))) +\n  geom_line() +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(x = \"Order of data collection\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#checking-independence-2",
    "href": "slides/14-mlr-inference-old.html#checking-independence-2",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\n\nNo clear pattern in the residuals vs. order of data collection plot.\nIndependence condition appears to be satisfied, as far as we can evaluate it."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#what-is-multicollinearity",
    "href": "slides/14-mlr-inference-old.html#what-is-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "What is multicollinearity",
    "text": "What is multicollinearity\nMulticollinearity is the case when two or more predictor variables are strongly correlated with one another"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#example",
    "href": "slides/14-mlr-inference-old.html#example",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\nLet’s assume the true population regression equation is \\(y = 3 + 4x\\)\n\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#example-1",
    "href": "slides/14-mlr-inference-old.html#example-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the “best” combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#why-multicollinearity-is-a-problem",
    "href": "slides/14-mlr-inference-old.html#why-multicollinearity-is-a-problem",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have perfect collinearities, we are unable to get estimates for the coefficients\n\nWhen we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference\nIt is also difficult to interpret the model coefficients"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#detecting-multicollinearity",
    "href": "slides/14-mlr-inference-old.html#detecting-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when…\n\nThere are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\n\n\n\nOne (or more) predictor variables is an almost perfect linear combination of the others\nThere is a quadratic term in the model without mean-centering the variable first\nThere are interactions between two or more continuous variables\n\nCan reduce this by mean-centering the variables first\n\nThere is a categorical predictor with very few observations in the baseline level"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/14-mlr-inference-old.html#detecting-multicollinearity-in-the-eda",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship\n\nLook at the distribution of categorical predictors and avoid setting the baseline to a category with very few observations"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#detecting-multicollinearity-vif",
    "href": "slides/14-mlr-inference-old.html#detecting-multicollinearity-vif",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#detecting-multicollinearity-vif-1",
    "href": "slides/14-mlr-inference-old.html#detecting-multicollinearity-vif-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\n\nTypically \\(VIF &gt; 10\\) indicates concerning multicollinearity\nVariables with similar values of VIF are typically the ones correlated with each other\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#vif-for-rail-trail-model",
    "href": "slides/14-mlr-inference-old.html#vif-for-rail-trail-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "VIF for rail trail model",
    "text": "VIF for rail trail model\n\nvif(rt_full_fit$fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\n\nhightemp and avgtemp are correlated. We need to remove one of these variables and refit the model."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#model-without-hightemp",
    "href": "slides/14-mlr-inference-old.html#model-without-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ . - hightemp, data = rail_trail)\n  \nm1 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#model-without-avgtemp",
    "href": "slides/14-mlr-inference-old.html#model-without-avgtemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#choosing-a-model",
    "href": "slides/14-mlr-inference-old.html#choosing-a-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel without hightemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel without avgtemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\n\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model without avgtemp is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#selected-model-for-now",
    "href": "slides/14-mlr-inference-old.html#selected-model-for-now",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Selected model (for now)",
    "text": "Selected model (for now)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#topics-1",
    "href": "slides/14-mlr-inference-old.html#topics-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Topics",
    "text": "Topics\n\nLog transformation on the response variable\nLog transformation on the predictor variable"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#residuals-vs.-fitted-for-the-selected-model",
    "href": "slides/14-mlr-inference-old.html#residuals-vs.-fitted-for-the-selected-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. fitted for the selected model",
    "text": "Residuals vs. fitted for the selected model\n\nThe constant variance condition is not satisfied. We can transform the response variable to address the violation in this condition."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#identifying-a-need-to-transform-y",
    "href": "slides/14-mlr-inference-old.html#identifying-a-need-to-transform-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Identifying a need to transform \\(Y\\)",
    "text": "Identifying a need to transform \\(Y\\)\n\nTypically, a “fan-shaped” residual plot indicates the need for a transformation of the response variable \\(Y\\)\n\nThere are multiple ways to transform a variable, e.g., \\(\\sqrt{Y}\\), \\(1/Y\\), \\(\\log(Y)\\)\n\\(\\log(Y)\\) the most straightforward to interpret, so we use that transformation when possible\n\n\n\n\nWhen building a model:\n\nChoose a transformation and build the model on the transformed data\nReassess the residual plots\nIf the residuals plots did not sufficiently improve, try a new transformation!"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#log-transformation-on-y",
    "href": "slides/14-mlr-inference-old.html#log-transformation-on-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\n\nIf we apply a log transformation to the response variable, we want to estimate the parameters for the statistical model\n\n\\[\n\\log(Y) = \\beta_0+ \\beta_1 X_1 + \\dots +\\beta_pX_p + \\epsilon, \\hspace{10mm} \\epsilon \\sim N(0,\\sigma^2_\\epsilon)\n\\]\n\nThe regression equation is\n\n\\[\\widehat{\\log(Y)} = \\hat{\\beta}_0+ \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_pX_p\\]"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#log-transformation-on-y-1",
    "href": "slides/14-mlr-inference-old.html#log-transformation-on-y-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\nWe want to interpret the model in terms of the original variable \\(Y\\), not \\(\\log(Y)\\), so we need to write the regression equation in terms of \\(Y\\)\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\nThe predicted value \\(\\hat{Y}\\) is the predicted median of \\(Y\\). Note, when the distribution of \\(Y|X_1, \\ldots, X_p\\) is symmetric, then the median equals the mean. See the slides in the appendix for more detail."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#model-interpretation",
    "href": "slides/14-mlr-inference-old.html#model-interpretation",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n\n\nIntercept: When \\(X_1 = \\dots = X_p =0\\), \\(Y\\) is expected to be \\(\\exp\\{\\hat{\\beta}_0\\}\\)\nSlope: For every one unit increase in \\(X_j\\), the \\(Y\\) is expected to multiply by a factor of \\(\\exp\\{\\hat{\\beta}_j\\}\\), holding all else constant\n\n\nWhy is the interpretation in terms of a multiplicative change?"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#model-for-logvolume",
    "href": "slides/14-mlr-inference-old.html#model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model for \\(log(volume)\\)",
    "text": "Model for \\(log(volume)\\)\n\n#fit model\nlog_rt_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(log(volume) ~ hightemp + season + cloudcover + precip + day_type, data = rail_trail)\n\ntidy(log_rt_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#interpretation-of-model-for-logvolume",
    "href": "slides/14-mlr-inference-old.html#interpretation-of-model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Interpretation of model for \\(\\log(volume)\\)",
    "text": "Interpretation of model for \\(\\log(volume)\\)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327\n\n\n\n\n\n\n\nInterpret the intercept in terms of (1) log(volume) and (2) volume.\nInterpret the coefficient of hightemp in terms of (1) log(volume) and (2) volume."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#residuals-for-model-with-logvolume",
    "href": "slides/14-mlr-inference-old.html#residuals-for-model-with-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals for model with \\(\\log(volume)\\)",
    "text": "Residuals for model with \\(\\log(volume)\\)"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#compare-residual-plots",
    "href": "slides/14-mlr-inference-old.html#compare-residual-plots",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Compare residual plots",
    "text": "Compare residual plots"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#log-transformation-on-x",
    "href": "slides/14-mlr-inference-old.html#log-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log Transformation on \\(X\\)",
    "text": "Log Transformation on \\(X\\)\n\nTry a transformation on \\(X\\) if the scatterplot shows some curvature but the variance is constant for all values of \\(X\\)"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#respiratory-rate-vs.-age",
    "href": "slides/14-mlr-inference-old.html#respiratory-rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Respiratory Rate vs. Age",
    "text": "Respiratory Rate vs. Age\n\nA high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a “high” rate, we first want to understand the relationship between a child’s age and their respiratory rate.\nThe data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the Sleuth3 R package and is originally form a 1994 publication “Reference Values for Respiratory Rate in the First 3 Years of Life”.\nVariables:\n\nAge: age in months\nRate: respiratory rate (breaths per minute)"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#rate-vs.-age",
    "href": "slides/14-mlr-inference-old.html#rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Rate vs. Age",
    "text": "Rate vs. Age\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#model-with-transformation-on-x",
    "href": "slides/14-mlr-inference-old.html#model-with-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model with Transformation on \\(X\\)",
    "text": "Model with Transformation on \\(X\\)\nSuppose we have the following regression equation:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\log(X)\\]\n\n\nIntercept: When \\(X = 1\\) \\((\\log(X) = 0)\\), \\(Y\\) is expected to be \\(\\hat{\\beta}_0\\) (i.e. the mean of \\(Y\\) is \\(\\hat{\\beta}_0\\))\nSlope: When \\(X\\) is multiplied by a factor of \\(\\mathbf{C}\\), the mean of \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(C)}\\) units\n\nExample: when \\(X\\) is multiplied by a factor of 2, \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(2)}\\) units"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#model-interpretation-1",
    "href": "slides/14-mlr-inference-old.html#model-interpretation-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n50.135\n0.632\n79.330\n0\n\n\nlog(Age)\n-5.982\n0.263\n-22.781\n0\n\n\n\n\n\n\\[\\hat{\\text{Rate}} = 50.135 - 5.982 \\times \\log\\text{(Age)}\\]\n\n\nInterpret the intercept in the context of the data.\nInterpret the slope in terms of age multiplying by 2 in the context of the data."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#learn-more",
    "href": "slides/14-mlr-inference-old.html#learn-more",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Learn more",
    "text": "Learn more\nSee Log Transformations in Linear Regression for more details about interpreting regression models with log-transformed variables."
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#why-medianyx-instead-of-mu_yx",
    "href": "slides/14-mlr-inference-old.html#why-medianyx-instead-of-mu_yx",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\nSuppose we have a set of values\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet’s calculate \\(\\overline{\\log(x)}\\)\n\nlog_x &lt;- log(x)\nmean(log_x)\n\n[1] 2.066476\n\n\n\nLet’s calculate \\(\\log(\\bar{x})\\)\n\nxbar &lt;- mean(x)\nlog(xbar)\n\n[1] 2.228477\n\n\n\n\n\n\n\nNote: \\(\\overline{\\log(x)} \\neq \\log(\\bar{x})\\)"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#why-medianyx-instead-of-mu_yx-1",
    "href": "slides/14-mlr-inference-old.html#why-medianyx-instead-of-mu_yx-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet’s calculate \\(\\text{Median}(\\log(x))\\)\n\nlog_x &lt;- log(x)\nmedian(log_x)\n\n[1] 2.079442\n\n\n\nLet’s calculate \\(\\log(\\text{Median}(x))\\)\n\nmedian_x &lt;- median(x)\nlog(median_x)\n\n[1] 2.079442\n\n\n\n\n\n\n\nNote: \\(\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))\\)"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#mean-median-and-log",
    "href": "slides/14-mlr-inference-old.html#mean-median-and-log",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean, Median, and log",
    "text": "Mean, Median, and log\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\\[\\overline{\\log(x)} \\neq \\log(\\bar{x})\\]\n\nmean(log_x) == log(xbar)\n\n[1] FALSE\n\n\n\n\\[\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))\\]\n\nmedian(log_x) == log(median_x)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#mean-and-median-of-logy",
    "href": "slides/14-mlr-inference-old.html#mean-and-median-of-logy",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(Y)\\)",
    "text": "Mean and median of \\(\\log(Y)\\)\n\nRecall that \\(Y = \\beta_0 + \\beta_1 X\\) is the mean value of the response at the given value of the predictor \\(X\\). This doesn’t hold when we log-transform the response variable.\nMathematically, the mean of the logged values is not necessarily equal to the log of the mean value. Therefore at a given value of \\(X\\)\n\n\n\\[\n\\begin{aligned}\\exp\\{\\text{Mean}(\\log(Y|X))\\} \\neq \\text{Mean}(Y|X) \\\\[5pt]\n\\Rightarrow \\exp\\{\\beta_0 + \\beta_1 X\\} \\neq \\text{Mean}(Y|X) \\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/14-mlr-inference-old.html#mean-and-median-of-logy-1",
    "href": "slides/14-mlr-inference-old.html#mean-and-median-of-logy-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(y)\\)",
    "text": "Mean and median of \\(\\log(y)\\)\n\nHowever, the median of the logged values is equal to the log of the median value. Therefore,\n\n\\[\\exp\\{\\text{Median}(\\log(Y|X))\\} = \\text{Median}(Y|X)\\]\n\n\nIf the distribution of \\(\\log(Y)\\) is symmetric about the regression line, for a given value \\(X\\), we can expect \\(Mean(Y)\\) and \\(Median(Y)\\) to be approximately equal.\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/13-comparison-cv.html#announcements",
    "href": "slides/13-comparison-cv.html#announcements",
    "title": "Model comparison + cross validation",
    "section": "Announcements",
    "text": "Announcements\n\nSee Ed Discussion for upcoming events and internship opportunities\nStatistics Experience due Mon, Nov 20 at 11:59pm\nPlease submit mid-semester feedback by Friday\nProf. Tackett office hours Fridays 1:30 - 3:30pm for the rest of the semester"
  },
  {
    "objectID": "slides/13-comparison-cv.html#topics",
    "href": "slides/13-comparison-cv.html#topics",
    "title": "Model comparison + cross validation",
    "section": "Topics",
    "text": "Topics\n\n\nANOVA for multiple linear regression and sum of squares\nComparing models with \\(R^2\\) vs. \\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam’s razor and parsimony\nCross validation"
  },
  {
    "objectID": "slides/13-comparison-cv.html#computational-setup",
    "href": "slides/13-comparison-cv.html#computational-setup",
    "title": "Model comparison + cross validation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/13-comparison-cv.html#data-restaurant-tips",
    "href": "slides/13-comparison-cv.html#data-restaurant-tips",
    "title": "Model comparison + cross validation",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 × 4\n     Tip Party Meal   Age   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# ℹ 159 more rows"
  },
  {
    "objectID": "slides/13-comparison-cv.html#variables",
    "href": "slides/13-comparison-cv.html#variables",
    "title": "Model comparison + cross validation",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\n\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/13-comparison-cv.html#outcome-tip",
    "href": "slides/13-comparison-cv.html#outcome-tip",
    "title": "Model comparison + cross validation",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/13-comparison-cv.html#predictors",
    "href": "slides/13-comparison-cv.html#predictors",
    "title": "Model comparison + cross validation",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/13-comparison-cv.html#relevel-categorical-predictors",
    "href": "slides/13-comparison-cv.html#relevel-categorical-predictors",
    "title": "Model comparison + cross validation",
    "section": "Relevel categorical predictors",
    "text": "Relevel categorical predictors\n\ntips &lt;- tips |&gt;\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "slides/13-comparison-cv.html#predictors-again",
    "href": "slides/13-comparison-cv.html#predictors-again",
    "title": "Model comparison + cross validation",
    "section": "Predictors, again",
    "text": "Predictors, again"
  },
  {
    "objectID": "slides/13-comparison-cv.html#outcome-vs.-predictors",
    "href": "slides/13-comparison-cv.html#outcome-vs.-predictors",
    "title": "Model comparison + cross validation",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/13-comparison-cv.html#fit-and-summarize-model",
    "href": "slides/13-comparison-cv.html#fit-and-summarize-model",
    "title": "Model comparison + cross validation",
    "section": "Fit and summarize model",
    "text": "Fit and summarize model\n\ntip_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.170\n0.366\n-0.465\n0.643\n\n\nParty\n1.837\n0.124\n14.758\n0.000\n\n\nAgeMiddle\n1.009\n0.408\n2.475\n0.014\n\n\nAgeSenCit\n1.388\n0.485\n2.862\n0.005\n\n\n\n\n\n\n\n\nIs this the best model to explain variation in tips?"
  },
  {
    "objectID": "slides/13-comparison-cv.html#another-model-summary",
    "href": "slides/13-comparison-cv.html#another-model-summary",
    "title": "Model comparison + cross validation",
    "section": "Another model summary",
    "text": "Another model summary\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/13-comparison-cv.html#analysis-of-variance-anova-1",
    "href": "slides/13-comparison-cv.html#analysis-of-variance-anova-1",
    "title": "Model comparison + cross validation",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)"
  },
  {
    "objectID": "slides/13-comparison-cv.html#anova",
    "href": "slides/13-comparison-cv.html#anova",
    "title": "Model comparison + cross validation",
    "section": "ANOVA",
    "text": "ANOVA\n\nMain Idea: Decompose the total variation on the outcome into:\n\nthe variation that can be explained by the each of the variables in the model\nthe variation that can’t be explained by the model (left in the residuals)\n\nIf the variation that can be explained by the variables in the model is greater than the variation in the residuals, this signals that the model might be “valuable” (at least one of the \\(\\beta\\)s not equal to 0)"
  },
  {
    "objectID": "slides/13-comparison-cv.html#anova-output-in-r1",
    "href": "slides/13-comparison-cv.html#anova-output-in-r1",
    "title": "Model comparison + cross validation",
    "section": "ANOVA output in R1",
    "text": "ANOVA output in R1\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA\n\n\n\n\n\nClick here for explanation about the way R calculates sum of squares for each variable."
  },
  {
    "objectID": "slides/13-comparison-cv.html#anova-output-with-totals",
    "href": "slides/13-comparison-cv.html#anova-output-with-totals",
    "title": "Model comparison + cross validation",
    "section": "ANOVA output, with totals",
    "text": "ANOVA output, with totals\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\n\n\n\n\nTotal\n168\n1913.11"
  },
  {
    "objectID": "slides/13-comparison-cv.html#sum-of-squares",
    "href": "slides/13-comparison-cv.html#sum-of-squares",
    "title": "Model comparison + cross validation",
    "section": "Sum of squares",
    "text": "Sum of squares\n\n\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model"
  },
  {
    "objectID": "slides/13-comparison-cv.html#sum-of-squares-ss_total",
    "href": "slides/13-comparison-cv.html#sum-of-squares-ss_total",
    "title": "Model comparison + cross validation",
    "section": "Sum of squares: \\(SS_{Total}\\)",
    "text": "Sum of squares: \\(SS_{Total}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome\n\n\\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\) = 1913.11"
  },
  {
    "objectID": "slides/13-comparison-cv.html#sum-of-squares-ss_error",
    "href": "slides/13-comparison-cv.html#sum-of-squares-ss_error",
    "title": "Model comparison + cross validation",
    "section": "Sum of squares: \\(SS_{Error}\\)",
    "text": "Sum of squares: \\(SS_{Error}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals\n\n\\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\) = 686.44"
  },
  {
    "objectID": "slides/13-comparison-cv.html#sum-of-squares-ss_model",
    "href": "slides/13-comparison-cv.html#sum-of-squares-ss_model",
    "title": "Model comparison + cross validation",
    "section": "Sum of squares: \\(SS_{Model}\\)",
    "text": "Sum of squares: \\(SS_{Model}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Model}\\): Variability explained by the model\n\n\\(SS_{Total} - SS_{Error}\\) = 1226.67"
  },
  {
    "objectID": "slides/13-comparison-cv.html#r-squared-r2",
    "href": "slides/13-comparison-cv.html#r-squared-r2",
    "title": "Model comparison + cross validation",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\nRecall: \\(R^2\\) is the proportion of the variation in the response variable explained by the regression model.\n\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}} = 1 - \\frac{686.44}{1913.11} = 0.641\n\\]\n\n\n\nglance(tip_fit)$r.squared\n\n[1] 0.6411891"
  },
  {
    "objectID": "slides/13-comparison-cv.html#r-squared-r2-1",
    "href": "slides/13-comparison-cv.html#r-squared-r2-1",
    "title": "Model comparison + cross validation",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\n\\(R^2\\) will always increase as we add more variables to the model + If we add enough variables, we can always achieve \\(R^2=100\\%\\)\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables"
  },
  {
    "objectID": "slides/13-comparison-cv.html#adjusted-r2",
    "href": "slides/13-comparison-cv.html#adjusted-r2",
    "title": "Model comparison + cross validation",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model\nDiffers from \\(R^2\\) by using the mean squares (sumsq/df) rather than sums of squares and therefore adjusting for the number of predictor variables"
  },
  {
    "objectID": "slides/13-comparison-cv.html#r2-and-adjusted-r2",
    "href": "slides/13-comparison-cv.html#r2-and-adjusted-r2",
    "title": "Model comparison + cross validation",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/13-comparison-cv.html#using-r2-and-adjusted-r2",
    "href": "slides/13-comparison-cv.html#using-r2-and-adjusted-r2",
    "title": "Model comparison + cross validation",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables"
  },
  {
    "objectID": "slides/13-comparison-cv.html#comparing-models-with-r2_adj",
    "href": "slides/13-comparison-cv.html#comparing-models-with-r2_adj",
    "title": "Model comparison + cross validation",
    "section": "Comparing models with \\(R^2_{adj}\\)",
    "text": "Comparing models with \\(R^2_{adj}\\)\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n    data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.674         0.664\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.683         0.662\n\n\n\n\n\n\nWhy did we not use the full recipe() workflow to fit Model 1 or Model 2?\nWhich model would we choose based on \\(R^2\\)?\nWhich model would we choose based on Adjusted \\(R^2\\)?\nWhich statistic should we use to choose the final model - \\(R^2\\) or Adjusted \\(R^2\\)? Why?\n\nVote on Ed Discussion [10:05am lecture][1:25pm lecture]"
  },
  {
    "objectID": "slides/13-comparison-cv.html#aic-bic",
    "href": "slides/13-comparison-cv.html#aic-bic",
    "title": "Model comparison + cross validation",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nEstimators of prediction error and relative quality of models:\n\nAkaike’s Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \n\n\nSchwarz’s Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]"
  },
  {
    "objectID": "slides/13-comparison-cv.html#aic-bic-1",
    "href": "slides/13-comparison-cv.html#aic-bic-1",
    "title": "Model comparison + cross validation",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\n\nFirst Term: Decreases as p increases"
  },
  {
    "objectID": "slides/13-comparison-cv.html#aic-bic-2",
    "href": "slides/13-comparison-cv.html#aic-bic-2",
    "title": "Model comparison + cross validation",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\nSecond Term: Fixed for a given sample size n"
  },
  {
    "objectID": "slides/13-comparison-cv.html#aic-bic-3",
    "href": "slides/13-comparison-cv.html#aic-bic-3",
    "title": "Model comparison + cross validation",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nThird Term: Increases as p increases"
  },
  {
    "objectID": "slides/13-comparison-cv.html#using-aic-bic",
    "href": "slides/13-comparison-cv.html#using-aic-bic",
    "title": "Model comparison + cross validation",
    "section": "Using AIC & BIC",
    "text": "Using AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nChoose model with the smaller value of AIC or BIC\nIf \\(n \\geq 8\\), the penalty for BIC is larger than that of AIC, so BIC tends to favor more parsimonious models (i.e. models with fewer terms)"
  },
  {
    "objectID": "slides/13-comparison-cv.html#comparing-models-with-aic-and-bic",
    "href": "slides/13-comparison-cv.html#comparing-models-with-aic-and-bic",
    "title": "Model comparison + cross validation",
    "section": "Comparing models with AIC and BIC",
    "text": "Comparing models with AIC and BIC\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  714.  736.\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  720.  757.\n\n\n\n\n\n\nWhich model would we choose based on AIC?\nWhich model would we choose based on BIC?"
  },
  {
    "objectID": "slides/13-comparison-cv.html#commonalities-between-criteria",
    "href": "slides/13-comparison-cv.html#commonalities-between-criteria",
    "title": "Model comparison + cross validation",
    "section": "Commonalities between criteria",
    "text": "Commonalities between criteria\n\n\\(R^2_{adj}\\), AIC, and BIC all apply a penalty for more predictors\nThe penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\nGoal: Parsimony"
  },
  {
    "objectID": "slides/13-comparison-cv.html#parsimony-and-occams-razor",
    "href": "slides/13-comparison-cv.html#parsimony-and-occams-razor",
    "title": "Model comparison + cross validation",
    "section": "Parsimony and Occam’s razor",
    "text": "Parsimony and Occam’s razor\n\nThe principle of parsimony is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation1\nCalled Occam’s razor because he “shaved” his explanations down to the bare minimum\nParsimony in modeling:\n\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments relying on few assumptions should be preferred to those relying on many\nmodels should be pared down until they are minimal adequate\nsimple explanations should be preferred to complex explanations\n\n\n\nSource: The R Book by Michael J. Crawley."
  },
  {
    "objectID": "slides/13-comparison-cv.html#in-pursuit-of-occams-razor",
    "href": "slides/13-comparison-cv.html#in-pursuit-of-occams-razor",
    "title": "Model comparison + cross validation",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\nOccam’s razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\nModel selection follows this principle\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\nIn other words, we prefer the simplest best model, i.e. parsimonious model"
  },
  {
    "objectID": "slides/13-comparison-cv.html#alternate-views",
    "href": "slides/13-comparison-cv.html#alternate-views",
    "title": "Model comparison + cross validation",
    "section": "Alternate views",
    "text": "Alternate views\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nRadford Neal - Bayesian Learning for Neural Networks1\n\nSuggested blog post: Occam by Andrew Gelman"
  },
  {
    "objectID": "slides/13-comparison-cv.html#other-concerns-with-our-approach",
    "href": "slides/13-comparison-cv.html#other-concerns-with-our-approach",
    "title": "Model comparison + cross validation",
    "section": "Other concerns with our approach",
    "text": "Other concerns with our approach\n\nAll criteria we considered for model comparison require making predictions for our data and then uses the prediction error (\\(SS_{Error}\\)) somewhere in the formula\nBut we’re making prediction for the data we used to build the model (estimate the coefficients), which can lead to overfitting\nInstead we should\n\nsplit our data into testing and training sets\n“train” the model on the training data and pick a few models we’re genuinely considering as potentially good models\ntest those models on the testing set\n…and repeat this process multiple times"
  },
  {
    "objectID": "slides/13-comparison-cv.html#spending-our-data",
    "href": "slides/13-comparison-cv.html#spending-our-data",
    "title": "Model comparison + cross validation",
    "section": "Spending our data",
    "text": "Spending our data\n\nWe have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.\nHowever, we usually need to understand the effectiveness of the model before using the test set.\nTypically we can’t decide on which final model to take to the test set without making model assessments.\nRemedy: Resampling to make model assessments on training data in a way that can generalize to new data."
  },
  {
    "objectID": "slides/13-comparison-cv.html#resampling-for-model-assessment",
    "href": "slides/13-comparison-cv.html#resampling-for-model-assessment",
    "title": "Model comparison + cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\nResampling is only conducted on the training set. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\nThe model is fit with the analysis set. Model fit statistics such as \\(R^2_{Adj}\\), AIC, and BIC are calculated based on this fit.\nThe model is evaluated with the assessment set."
  },
  {
    "objectID": "slides/13-comparison-cv.html#resampling-for-model-assessment-1",
    "href": "slides/13-comparison-cv.html#resampling-for-model-assessment-1",
    "title": "Model comparison + cross validation",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\n\n\nImage source: Kuhn and Silge. Tidy modeling with R."
  },
  {
    "objectID": "slides/13-comparison-cv.html#analysis-and-assessment-sets",
    "href": "slides/13-comparison-cv.html#analysis-and-assessment-sets",
    "title": "Model comparison + cross validation",
    "section": "Analysis and assessment sets",
    "text": "Analysis and assessment sets\n\nAnalysis set is analogous to training set.\nAssessment set is analogous to test set.\nThe terms analysis and assessment avoids confusion with initial split of the data.\nThese data sets are mutually exclusive."
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-1",
    "href": "slides/13-comparison-cv.html#cross-validation-1",
    "title": "Model comparison + cross validation",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, v-fold cross validation – commonly used resampling technique:\n\nRandomly split your training data into v partitions\nUse v-1 partitions for analysis, and the remaining 1 partition for analysis (model fit + model fit statistics)\nRepeat v times, updating which partition is used for assessment each time\n\n\nLet’s give an example where v = 3…"
  },
  {
    "objectID": "slides/13-comparison-cv.html#to-get-started",
    "href": "slides/13-comparison-cv.html#to-get-started",
    "title": "Model comparison + cross validation",
    "section": "To get started…",
    "text": "To get started…\nSplit data into training and test sets\n\nset.seed(345)\n\ntips_split &lt;- initial_split(tips)\ntips_train &lt;- training(tips_split)\ntips_test &lt;- testing(tips_split)"
  },
  {
    "objectID": "slides/13-comparison-cv.html#to-get-started-1",
    "href": "slides/13-comparison-cv.html#to-get-started-1",
    "title": "Model comparison + cross validation",
    "section": "To get started…",
    "text": "To get started…\nCreate recipe\n\ntips_rec1 &lt;- recipe(Tip ~ Party + Age + Meal, \n                    data = tips_train)\n\ntips_rec1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3"
  },
  {
    "objectID": "slides/13-comparison-cv.html#to-get-started-2",
    "href": "slides/13-comparison-cv.html#to-get-started-2",
    "title": "Model comparison + cross validation",
    "section": "To get started…",
    "text": "To get started…\nSpecify model\n\ntips_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\nCreate workflow\n\ntips_wflow1 &lt;- workflow() |&gt;\n  add_model(tips_spec) |&gt;\n  add_recipe(tips_rec1)\n\ntips_wflow1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-step-1",
    "href": "slides/13-comparison-cv.html#cross-validation-step-1",
    "title": "Model comparison + cross validation",
    "section": "Cross validation, step 1",
    "text": "Cross validation, step 1\nRandomly split your training data into 3 partitions:"
  },
  {
    "objectID": "slides/13-comparison-cv.html#tips-split-training-data",
    "href": "slides/13-comparison-cv.html#tips-split-training-data",
    "title": "Model comparison + cross validation",
    "section": "Tips: Split training data",
    "text": "Tips: Split training data\n\nfolds &lt;- vfold_cv(tips_train, v = 3)\nfolds\n\n#  3-fold cross-validation \n# A tibble: 3 × 2\n  splits          id   \n  &lt;list&gt;          &lt;chr&gt;\n1 &lt;split [84/42]&gt; Fold1\n2 &lt;split [84/42]&gt; Fold2\n3 &lt;split [84/42]&gt; Fold3"
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-steps-2-and-3",
    "href": "slides/13-comparison-cv.html#cross-validation-steps-2-and-3",
    "title": "Model comparison + cross validation",
    "section": "Cross validation, steps 2 and 3",
    "text": "Cross validation, steps 2 and 3\n\n\nUse v-1 partitions for analysis, and the remaining 1 partition for assessment\nRepeat v times, updating which partition is used for assessment each time"
  },
  {
    "objectID": "slides/13-comparison-cv.html#tips-fit-resamples",
    "href": "slides/13-comparison-cv.html#tips-fit-resamples",
    "title": "Model comparison + cross validation",
    "section": "Tips: Fit resamples",
    "text": "Tips: Fit resamples\n\ntips_fit_rs1 &lt;- tips_wflow1 |&gt;\n  fit_resamples(folds)\n\ntips_fit_rs1 \n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 × 4\n  splits          id    .metrics         .notes          \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [84/42]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [84/42]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [84/42]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-now-what",
    "href": "slides/13-comparison-cv.html#cross-validation-now-what",
    "title": "Model comparison + cross validation",
    "section": "Cross validation, now what?",
    "text": "Cross validation, now what?\n\nWe’ve fit a bunch of models\nNow it’s time to use them to collect metrics (e.g., $R^2$, AIC, RMSE, etc. ) on each model and use them to evaluate model fit and how it varies across folds"
  },
  {
    "objectID": "slides/13-comparison-cv.html#collect-r2-and-rmse-from-cv",
    "href": "slides/13-comparison-cv.html#collect-r2-and-rmse-from-cv",
    "title": "Model comparison + cross validation",
    "section": "Collect \\(R^2\\) and RMSE from CV",
    "text": "Collect \\(R^2\\) and RMSE from CV\n\n# Produces summary across all CV\ncollect_metrics(tips_fit_rs1)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.10      3  0.243  Preprocessor1_Model1\n2 rsq     standard   0.591     3  0.0728 Preprocessor1_Model1\n\n\n\nNote: These are calculated using the assessment data"
  },
  {
    "objectID": "slides/13-comparison-cv.html#deeper-look-into-r2-and-rmse",
    "href": "slides/13-comparison-cv.html#deeper-look-into-r2-and-rmse",
    "title": "Model comparison + cross validation",
    "section": "Deeper look into \\(R^2\\) and RMSE",
    "text": "Deeper look into \\(R^2\\) and RMSE\n\ncv_metrics1 &lt;- collect_metrics(tips_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n\n# A tibble: 6 × 5\n  id    .metric .estimator .estimate .config             \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Fold1 rmse    standard       2.04  Preprocessor1_Model1\n2 Fold1 rsq     standard       0.736 Preprocessor1_Model1\n3 Fold2 rmse    standard       1.71  Preprocessor1_Model1\n4 Fold2 rsq     standard       0.509 Preprocessor1_Model1\n5 Fold3 rmse    standard       2.54  Preprocessor1_Model1\n6 Fold3 rsq     standard       0.528 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/13-comparison-cv.html#better-tabulation-of-r2-and-rmse-from-cv",
    "href": "slides/13-comparison-cv.html#better-tabulation-of-r2-and-rmse-from-cv",
    "title": "Model comparison + cross validation",
    "section": "Better tabulation of \\(R^2\\) and RMSE from CV",
    "text": "Better tabulation of \\(R^2\\) and RMSE from CV\n\ncv_metrics1 |&gt;\n  mutate(.estimate = round(.estimate, 3)) |&gt;\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) |&gt;\n  kable(col.names = c(\"Fold\", \"RMSE\", \"R-squared\"))\n\n\n\n\nFold\nRMSE\nR-squared\n\n\n\n\nFold1\n2.036\n0.736\n\n\nFold2\n1.709\n0.509\n\n\nFold3\n2.545\n0.528"
  },
  {
    "objectID": "slides/13-comparison-cv.html#how-does-rmse-compare-to-y",
    "href": "slides/13-comparison-cv.html#how-does-rmse-compare-to-y",
    "title": "Model comparison + cross validation",
    "section": "How does RMSE compare to y?",
    "text": "How does RMSE compare to y?\n\n\nCross validation RMSE stats:\n\ncv_metrics1 |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )\n\n# A tibble: 1 × 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.71  2.54  2.10 0.421\n\n\n\nTraining data tips:\n\ntips_train |&gt;\n  summarise(\n    min = min(Tip),\n    max = max(Tip),\n    mean = mean(Tip),\n    sd = sd(Tip)\n  )\n\n# A tibble: 1 × 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  19.5  4.87  3.37"
  },
  {
    "objectID": "slides/13-comparison-cv.html#calculate-r2_adj-aic-and-bic-for-each-fold",
    "href": "slides/13-comparison-cv.html#calculate-r2_adj-aic-and-bic-for-each-fold",
    "title": "Model comparison + cross validation",
    "section": "Calculate \\(R^2_{Adj}\\), AIC, and BIC for each fold",
    "text": "Calculate \\(R^2_{Adj}\\), AIC, and BIC for each fold\n\n# Function to get Adj R-sq, AIC, BIC\ncalc_model_stats &lt;- function(x) {\n  glance(extract_fit_parsnip(x)) |&gt;\n    select(adj.r.squared, AIC, BIC)\n}\n\n# Fit model and calculate statistics for each fold\ntips_fit_rs1 &lt;-tips_wflow1 |&gt;\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))\n\ntips_fit_rs1\n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 × 5\n  splits          id    .metrics         .notes           .extracts       \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [84/42]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n2 &lt;split [84/42]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n3 &lt;split [84/42]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;"
  },
  {
    "objectID": "slides/13-comparison-cv.html#collect-r2_adj-aic-bic-from-cv",
    "href": "slides/13-comparison-cv.html#collect-r2_adj-aic-bic-from-cv",
    "title": "Model comparison + cross validation",
    "section": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV",
    "text": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV\n\nmap_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) |&gt;\n  bind_cols(Fold = tips_fit_rs1$id)\n\n# A tibble: 3 × 4\n  adj.r.squared   AIC   BIC Fold \n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1         0.550  369.  386. Fold1\n2         0.659  377.  394. Fold2\n3         0.718  337.  354. Fold3\n\n\n\nNote: These are based on the model fit from the analysis data"
  },
  {
    "objectID": "slides/13-comparison-cv.html#cross-validation-in-practice",
    "href": "slides/13-comparison-cv.html#cross-validation-in-practice",
    "title": "Model comparison + cross validation",
    "section": "Cross validation in practice",
    "text": "Cross validation in practice\n\n\nTo illustrate how CV works, we used v = 3:\n\n\nAnalysis sets are 2/3 of the training set\nEach assessment set is a distinct 1/3\nThe final resampling estimate of performance averages each of the 3 replicates\n\n\nThis was useful for illustrative purposes, but v = 3 is a poor choice in practice\nValues of v are most often 5 or 10; we generally prefer 10-fold cross-validation as a default"
  },
  {
    "objectID": "slides/13-comparison-cv.html#recap",
    "href": "slides/13-comparison-cv.html#recap",
    "title": "Model comparison + cross validation",
    "section": "Recap",
    "text": "Recap\n\nANOVA for multiple linear regression and sum of squares\nComparing models with\n\n\\(R^2\\) vs. \\(R^2_{Adj}\\)\nAIC and BIC\n\nOccam’s razor and parsimony\n\n\n\nCross validation for\n\nmodel evaluation\nmodel comparison\n\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/11-feature-engineering.html#announcements",
    "href": "slides/11-feature-engineering.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\nGroup labs resume this week\nProf. Tackett will not have office hours on Friday\n\nEmail to schedule an appointment if you need to meet\nAll other office hours on regular schedule"
  },
  {
    "objectID": "slides/11-feature-engineering.html#statistician-of-the-day-rafael-irizarry",
    "href": "slides/11-feature-engineering.html#statistician-of-the-day-rafael-irizarry",
    "title": "Feature engineering",
    "section": "Statistician of the day: Rafael Irizarry",
    "text": "Statistician of the day: Rafael Irizarry\n\n\nDr. Rafael Irizarry is a Professor of Biostatistics at the Harvard T.H. Chan School of Public Health and Professor of Biostatistics and Computational Biology at the Dana-Farber Cancer Institute. He earned a Bachelor of Science degree in Mathematics from the University of Puerto Rico at Rio Piedras and a PhD from the University of California, Berkeley in Statistics. Dr. Irizarry’s work is highly cited, and he has been given many prestigious awards including the Presidents’ Award given by the Committee of Presidents of Statistical Societies.\n\n\n\n\n\n\n\n\nSource: hardin47.github.io/CURV/scholars/irizarry"
  },
  {
    "objectID": "slides/11-feature-engineering.html#work-on-impacts-of-hurricane-maria",
    "href": "slides/11-feature-engineering.html#work-on-impacts-of-hurricane-maria",
    "title": "Feature engineering",
    "section": "Work on impacts of Hurricane Maria",
    "text": "Work on impacts of Hurricane Maria\n\nPart of a team that used stratified sampling to survey residents in Puerto Rico about the impacts of the 2017 Hurricane Maria\nEstimated percent of population who lost access to services, such as electricity and water, and the association with remoteness\nUsed confidence intervals to estimate deaths that were directly and indirectly attributable to the hurricane\n\nTheir estimate was more than 70 times the official count\n\n\nArticle: Kishore, N., Marqués, D., Mahmud, A., Kiang, M. V., Rodriguez, I., Fuller, A., ... & Buckee, C. O. (2018). Mortality in Puerto Rico after Hurricane Maria. New England journal of medicine, 379(2), 162-170.\nGitHub repo: github.com/c2-d2/pr_mort_official"
  },
  {
    "objectID": "slides/11-feature-engineering.html#topics",
    "href": "slides/11-feature-engineering.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\nUnderstanding categorical predictors and interaction terms\nFeature engineering"
  },
  {
    "objectID": "slides/11-feature-engineering.html#computational-setup",
    "href": "slides/11-feature-engineering.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(colorblindr)\nlibrary(gghighlight)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-peer-to-peer-lender",
    "href": "slides/11-feature-engineering.html#data-peer-to-peer-lender",
    "title": "Feature engineering",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering.html#variables",
    "href": "slides/11-feature-engineering.html#variables",
    "title": "Feature engineering",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nResponse: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/11-feature-engineering.html#response-interest_rate",
    "href": "slides/11-feature-engineering.html#response-interest_rate",
    "title": "Feature engineering",
    "section": "Response: interest_rate",
    "text": "Response: interest_rate\n\n\n\n\n\n\nmin\nmedian\nmax\niqr\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/11-feature-engineering.html#predictors",
    "href": "slides/11-feature-engineering.html#predictors",
    "title": "Feature engineering",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-1-rescale-income",
    "href": "slides/11-feature-engineering.html#data-manipulation-1-rescale-income",
    "title": "Feature engineering",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/11-feature-engineering.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "Feature engineering",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/11-feature-engineering.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "Feature engineering",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )"
  },
  {
    "objectID": "slides/11-feature-engineering.html#interest-rate-vs.-annual-income",
    "href": "slides/11-feature-engineering.html#interest-rate-vs.-annual-income",
    "title": "Feature engineering",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/11-feature-engineering.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/11-feature-engineering.html#data-manipulation-4-create-interaction-variables",
    "title": "Feature engineering",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          &lt;dbl&gt; 1, 1, 1, 1, 1, …\n$ debt_inc_cent                                          &lt;dbl&gt; -0.16511719, 0.…\n$ annual_income_th_cent                                  &lt;dbl&gt; -27.17, -26.17,…\n$ `verified_incomeNot Verified`                          &lt;dbl&gt; 1, 1, 0, 1, 1, …\n$ `verified_incomeSource Verified`                       &lt;dbl&gt; 0, 0, 0, 0, 0, …\n$ verified_incomeVerified                                &lt;dbl&gt; 0, 0, 1, 0, 0, …\n$ `annual_income_th_cent:verified_incomeNot Verified`    &lt;dbl&gt; -27.17, -26.17,…\n$ `annual_income_th_cent:verified_incomeSource Verified` &lt;dbl&gt; 0.00, 0.00, 0.0…\n$ `annual_income_th_cent:verified_incomeVerified`        &lt;dbl&gt; 0.00, 0.00, -11…"
  },
  {
    "objectID": "slides/11-feature-engineering.html#interaction-term-in-the-model",
    "href": "slides/11-feature-engineering.html#interaction-term-in-the-model",
    "title": "Feature engineering",
    "section": "Interaction term in the model",
    "text": "Interaction term in the model\n\nint_cent_int_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(interest_rate ~ debt_inc_cent  +  verified_income + \n        annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.484\n0.989\n9.586\n0.000\n\n\ndebt_inc_cent\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n2.157\n1.418\n1.522\n0.135\n\n\nverified_incomeVerified\n7.181\n1.870\n3.840\n0.000\n\n\nannual_income_th_cent\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th_cent\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th_cent\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/11-feature-engineering.html#interpreting-interaction-terms",
    "href": "slides/11-feature-engineering.html#interpreting-interaction-terms",
    "title": "Feature engineering",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/11-feature-engineering.html#understanding-the-model",
    "href": "slides/11-feature-engineering.html#understanding-the-model",
    "title": "Feature engineering",
    "section": "Understanding the model",
    "text": "Understanding the model\n\\[\n\\begin{aligned}\n\\hat{interest\\_rate} &= 9.484 + 0.691 \\times debt\\_inc\\_cent\\\\ &- 0.007 \\times annual\\_income\\_th\\_cent \\\\ &+ 2.157 \\times SourceVerified + 7.181 \\times Verified \\\\ &- 0.016 \\times annual\\_inc\\_th\\_cent \\times SourceVerified\\\\ &- 0.032 \\times annual\\_inc\\_th\\_cent \\times Verified\n\\end{aligned}\n\\]\n\n\nWhat is \\(p\\), the number of predictor terms in the model?\nWrite the equation of the model to predict interest rate for applicants with Not Verified income.\nWrite the equation of the model to predict interest rate for applicants with Verified income."
  },
  {
    "objectID": "slides/11-feature-engineering.html#the-office",
    "href": "slides/11-feature-engineering.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/11-feature-engineering.html#data",
    "href": "slides/11-feature-engineering.html#data",
    "title": "Feature engineering",
    "section": "Data",
    "text": "Data\nThe data come from data.world, by way of TidyTuesday\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\noffice_ratings\n\n# A tibble: 188 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ℹ 178 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings",
    "href": "slides/11-feature-engineering.html#imdb-ratings",
    "title": "Feature engineering",
    "section": "IMDB ratings",
    "text": "IMDB ratings"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings-vs.-number-of-votes",
    "href": "slides/11-feature-engineering.html#imdb-ratings-vs.-number-of-votes",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. number of votes",
    "text": "IMDB ratings vs. number of votes"
  },
  {
    "objectID": "slides/11-feature-engineering.html#outliers",
    "href": "slides/11-feature-engineering.html#outliers",
    "title": "Feature engineering",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings-vs.-air-date",
    "href": "slides/11-feature-engineering.html#imdb-ratings-vs.-air-date",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. air date",
    "text": "IMDB ratings vs. air date"
  },
  {
    "objectID": "slides/11-feature-engineering.html#imdb-ratings-vs.-seasons",
    "href": "slides/11-feature-engineering.html#imdb-ratings-vs.-seasons",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. seasons",
    "text": "IMDB ratings vs. seasons"
  },
  {
    "objectID": "slides/11-feature-engineering.html#spending-our-data",
    "href": "slides/11-feature-engineering.html#spending-our-data",
    "title": "Feature engineering",
    "section": "Spending our data",
    "text": "Spending our data\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we’ve done so far)"
  },
  {
    "objectID": "slides/11-feature-engineering.html#splitting-the-data",
    "href": "slides/11-feature-engineering.html#splitting-the-data",
    "title": "Feature engineering",
    "section": "Splitting the data",
    "text": "Splitting the data\n\nTake a random sample of X% of the data and set aside (testing data)\n\nTypically 10 - 20%\n\nFit a model on the remaining Y% of the data (training data)\n\nTypically 80 - 90%\n\nUse the coefficients from the model fit on training data to make predictions and evaluate performance on the testing data"
  },
  {
    "objectID": "slides/11-feature-engineering.html#train-test",
    "href": "slides/11-feature-engineering.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings, prop = 0.75) # prop = 0.75 by default\n\n\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\n\n\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/11-feature-engineering.html#training-data",
    "href": "slides/11-feature-engineering.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 × 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ℹ 131 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering.html#feature-engineering-1",
    "href": "slides/11-feature-engineering.html#feature-engineering-1",
    "title": "Feature engineering",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple (parsimonious) models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance and improve interpretability)"
  },
  {
    "objectID": "slides/11-feature-engineering.html#feature-engineering-with-dplyr",
    "href": "slides/11-feature-engineering.html#feature-engineering-with-dplyr",
    "title": "Feature engineering",
    "section": "Feature engineering with dplyr",
    "text": "Feature engineering with dplyr\n\noffice_train |&gt;\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )\n\n# A tibble: 141 × 8\n  season episode title            imdb_rating total_votes air_date   month  wday\n  &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 8           18 Last Day in Flo…         7.8        1429 2012-03-08     3     5\n2 9           14 Vandalism                7.6        1402 2013-01-31     1     5\n3 2            8 Performance Rev…         8.2        2416 2005-11-15    11     3\n4 9            5 Here Comes Treb…         7.1        1515 2012-10-25    10     5\n5 3           22 Beach Games              9.1        2783 2007-05-10     5     5\n6 7            1 Nepotism                 8.4        1897 2010-09-23     9     5\n# ℹ 135 more rows\n\n\n\n\nCan you identify any potential problems with this approach?"
  },
  {
    "objectID": "slides/11-feature-engineering.html#modeling-workflow",
    "href": "slides/11-feature-engineering.html#modeling-workflow",
    "title": "Feature engineering",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\n\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/11-feature-engineering.html#initiate-a-recipe",
    "href": "slides/11-feature-engineering.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloging names and types of variables\n  )\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-1-alter-roles",
    "href": "slides/11-feature-engineering.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn’t a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec |&gt;\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-2-add-features",
    "href": "slides/11-feature-engineering.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec |&gt;\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-3-add-more-features",
    "href": "slides/11-feature-engineering.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec |&gt;\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-4-convert-numbers-to-factors",
    "href": "slides/11-feature-engineering.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec |&gt;\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-5-make-dummy-variables",
    "href": "slides/11-feature-engineering.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec |&gt;\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering.html#step-6-remove-zero-variance-predictors",
    "href": "slides/11-feature-engineering.html#step-6-remove-zero-variance-predictors",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance predictors",
    "text": "Step 6: Remove zero variance predictors\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec |&gt;\n  step_zv(all_predictors())\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering.html#putting-it-all-together",
    "href": "slides/11-feature-engineering.html#putting-it-all-together",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  # make title's role ID\n  update_role(title, new_role = \"ID\") |&gt;\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) |&gt;\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) |&gt;\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) |&gt;\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/11-feature-engineering.html#putting-it-all-together-1",
    "href": "slides/11-feature-engineering.html#putting-it-all-together-1",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering.html#next-step",
    "href": "slides/11-feature-engineering.html#next-step",
    "title": "Feature engineering",
    "section": "Next step…",
    "text": "Next step…\nWe will complete the workflow to fit a model predicting IMDB ratings that includes the following predictors:\n\nepisode\ntotal_votes\nindicator variables for season\nindicator variables for day of week aired (created using air_date)\nindicator variables for month aired (created using air_date)\n\n\n\nWhat feature will not end up in the final model? Why is it not included?"
  },
  {
    "objectID": "slides/11-feature-engineering.html#working-with-recipes",
    "href": "slides/11-feature-engineering.html#working-with-recipes",
    "title": "Feature engineering",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don’t get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n\n\n\n\n\n\nNote\n\n\nThis is optional, we’ll show the results for demonstrative purposes. It doesn’t need to be part of your modeling pipeline, but it can be assuring to see the effects of the recipe steps as you build the recipe."
  },
  {
    "objectID": "slides/11-feature-engineering.html#recap",
    "href": "slides/11-feature-engineering.html#recap",
    "title": "Feature engineering",
    "section": "Recap",
    "text": "Recap\n\n\nReview: Training and testing splits\nFeature engineering with recipes\n\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/10-exam-01-review.html#announcements",
    "href": "slides/10-exam-01-review.html#announcements",
    "title": "Exam 01 review",
    "section": "Announcements",
    "text": "Announcements\n\nHW 02 due TODAY at 11:59pm.\nAll lecture recordings available until Wed, Oct 4 at 9am.\n\nClick here for link to videos. You can also find the link in the navigation bar of the course website.\n\nNo labs this week\nNo office hours Wednesday - Friday\nEd Discussion archived (no new posts) Wednesday - Friday"
  },
  {
    "objectID": "slides/10-exam-01-review.html#exam-01",
    "href": "slides/10-exam-01-review.html#exam-01",
    "title": "Exam 01 review",
    "section": "Exam 01",
    "text": "Exam 01\n\n15% of final course grade\n50 points total\n\nin-class: 40 points\ntake-home: 10 points\n\nIn-class: 75 minutes during Wed, Oct 4 lecture\nTake-home:\n\nReleased: Wed, Oct 4 ~ 3pm\nDue: Fri, Oct 6 at 11:59pm\nSimilar in format to a lab/ HW\n\nIf you miss any part of the exam for an excused absence (with academic dean’s note), then Exam 02 score will be counted twice"
  },
  {
    "objectID": "slides/10-exam-01-review.html#content-weeks-1---5",
    "href": "slides/10-exam-01-review.html#content-weeks-1---5",
    "title": "Exam 01 review",
    "section": "Content: Weeks 1 - 5",
    "text": "Content: Weeks 1 - 5\n\n\n\nExploratory data analysis\nFitting and interpreting simple linear regression models\nStatistical models and regression equations\nSimulation-based inference\nMathematical models for inference\n\n\n\nPrediction\nModel conditions\nModel evaluation\nBasics of multiple linear regression"
  },
  {
    "objectID": "slides/10-exam-01-review.html#outline-of-in-class-portion",
    "href": "slides/10-exam-01-review.html#outline-of-in-class-portion",
    "title": "Exam 01 review",
    "section": "Outline of in-class portion",
    "text": "Outline of in-class portion\n\nClosed-book, closed-note.\n10 questions, some with multiple parts\nQuestion types:\n\nMultiple choice\nShort answer (no more than 3 sentences)\nTrue/ False.\n\nIf false, write 1 - 2 sentence justification about why it is false.\n\nEvaluate a response.\n\nLast page (front and back) will have analysis output.\nJust need a pencil. No calculator permitted on exam."
  },
  {
    "objectID": "slides/09-mlr-old.html#announcements",
    "href": "slides/09-mlr-old.html#announcements",
    "title": "Multiple linear regression (MLR)",
    "section": "Announcements",
    "text": "Announcements\n\nHW 02 due Mon, Oct 2 at 11:59pm.\nAll lecture recordings available until Wed, Oct 4 at 9am.\n\nClick here for link to videos. You can also find the link in the navigation bar of the course website.\n\nLab groups start this week. You will get your assigned group when you go to lab.\nSubmit questions about SLR by Thu, Sep 28. These questions will be used to make the Exam Review. Click here for more info.\nExam 01: Wed, Oct 4 (in-class + take-home)\nExam 01 review: Mon, Oct 2"
  },
  {
    "objectID": "slides/09-mlr-old.html#computational-setup",
    "href": "slides/09-mlr-old.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(colorblindr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/09-mlr-old.html#data-peer-to-peer-lender",
    "href": "slides/09-mlr-old.html#data-peer-to-peer-lender",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/09-mlr-old.html#variables",
    "href": "slides/09-mlr-old.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/09-mlr-old.html#outcome-interest_rate",
    "href": "slides/09-mlr-old.html#outcome-interest_rate",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n\n\nmin\nmedian\nmax\niqr\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/09-mlr-old.html#predictors",
    "href": "slides/09-mlr-old.html#predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/09-mlr-old.html#data-manipulation-1-rescale-income",
    "href": "slides/09-mlr-old.html#data-manipulation-1-rescale-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/09-mlr-old.html#outcome-vs.-predictors",
    "href": "slides/09-mlr-old.html#outcome-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/09-mlr-old.html#single-vs.-multiple-predictors",
    "href": "slides/09-mlr-old.html#single-vs.-multiple-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Single vs. multiple predictors",
    "text": "Single vs. multiple predictors\nSo far we’ve used a single predictor variable to understand variation in a quantitative response variable\n\nNow we want to use multiple predictor variables to understand variation in a quantitative response variable"
  },
  {
    "objectID": "slides/09-mlr-old.html#multiple-linear-regression-mlr",
    "href": "slides/09-mlr-old.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{interest_rate}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{debt_to_income} \\\\ & + \\hat{\\beta}_2 \\text{verified_income} \\\\ &+ \\hat{\\beta}_3 \\text{annual_income_th}\n\\end{aligned}\n\\]\nSimilar to simple linear regression, this model assumes that at each combination of the predictor variables, the values interest_rate follow a Normal distribution."
  },
  {
    "objectID": "slides/09-mlr-old.html#multiple-linear-regression-1",
    "href": "slides/09-mlr-old.html#multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nRecall: The simple linear regression model assumes\n\\[\nY|X\\sim N(\\beta_0 + \\beta_1 X, \\sigma_{\\epsilon}^2)\n\\]\n\nSimilarly: The multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/09-mlr-old.html#multiple-linear-regression-2",
    "href": "slides/09-mlr-old.html#multiple-linear-regression-2",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/09-mlr-old.html#model-fit",
    "href": "slides/09-mlr-old.html#model-fit",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit",
    "text": "Model fit\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078"
  },
  {
    "objectID": "slides/09-mlr-old.html#model-equation",
    "href": "slides/09-mlr-old.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{interest_rate}} =  10.726 &+0.671 \\times \\text{debt_to_income}\\\\\n&+ 2.211 \\times \\text{source_verified}\\\\  \n&+ 6.880 \\times \\text{verified}\\\\\n& -0.021 \\times \\text{annual_income_th}\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\nWe will talk about why there are two terms in the model for verified_income shortly!"
  },
  {
    "objectID": "slides/09-mlr-old.html#interpreting-hatbeta_j",
    "href": "slides/09-mlr-old.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(y\\) when \\(x_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nExample: The estimated coefficient for debt_to_income is 0.671. This means for each point in an borrower’s debt to income ratio, the interest rate on the loan is expected to be greater by 0.671%, holding annual income and income verification constant."
  },
  {
    "objectID": "slides/09-mlr-old.html#prediction",
    "href": "slides/09-mlr-old.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted interest rate for an borrower with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000?\n\n\n\n10.726 + 0.671 * 0.558 + 2.211 * 0 + 6.880 * 0 - 0.021 * 59\n\n[1] 9.861418\n\n\n\nThe predicted interest rate for an borrower with with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000 is 9.86%."
  },
  {
    "objectID": "slides/09-mlr-old.html#prediction-revisited",
    "href": "slides/09-mlr-old.html#prediction-revisited",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction, revisited",
    "text": "Prediction, revisited\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_borrower &lt;- tibble(\n  debt_to_income  = 0.558, \n  verified_income = \"Not Verified\", \n  annual_income_th = 59\n)\n\npredict(int_fit, new_borrower)\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1  9.89\n\n\n\n\n\n\n\n\nNote\n\n\nDifference in predicted value due to rounding the coefficients on the previous slide."
  },
  {
    "objectID": "slides/09-mlr-old.html#confidence-interval-for-hatmu_y",
    "href": "slides/09-mlr-old.html#confidence-interval-for-hatmu_y",
    "title": "Multiple linear regression (MLR)",
    "section": "Confidence interval for \\(\\hat{\\mu}_y\\)",
    "text": "Confidence interval for \\(\\hat{\\mu}_y\\)\n\nCalculate a 90% confidence interval for the estimated mean interest rate for borrowers with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000.\n\n\n\npredict(int_fit, new_borrower, type = \"conf_int\", level = 0.90)\n\n# A tibble: 1 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1        8.16        11.6"
  },
  {
    "objectID": "slides/09-mlr-old.html#prediction-interval-for-haty",
    "href": "slides/09-mlr-old.html#prediction-interval-for-haty",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 90% confidence interval for the predicted interest rate for an individual appllicant with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000.\n\n\n\npredict(int_fit, new_borrower, type = \"pred_int\", level = 0.90)\n\n# A tibble: 1 × 2\n  .pred_lower .pred_upper\n        &lt;dbl&gt;       &lt;dbl&gt;\n1        2.18        17.6"
  },
  {
    "objectID": "slides/09-mlr-old.html#cautions",
    "href": "slides/09-mlr-old.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/09-mlr-old.html#interpreting-results",
    "href": "slides/09-mlr-old.html#interpreting-results",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/09-mlr-old.html#mean-centering",
    "href": "slides/09-mlr-old.html#mean-centering",
    "title": "Multiple linear regression (MLR)",
    "section": "Mean-centering",
    "text": "Mean-centering\nIf we are interested in interpreting the intercept, we can mean-center the quantitative predictors in the model.\nWe can mean-center a quantitative predictor \\(X_j\\) using the following:\n\\[X_{j_{Cent}} = X_{j}- \\bar{X}_{j}\\]\n\nIf we mean-center all quantitative variables, then the intercept is interpreted as the expected value of the response variable when all quantitative variables are at their mean value."
  },
  {
    "objectID": "slides/09-mlr-old.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/09-mlr-old.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/09-mlr-old.html#visualize-mean-centered-predictors",
    "href": "slides/09-mlr-old.html#visualize-mean-centered-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualize mean-centered predictors",
    "text": "Visualize mean-centered predictors"
  },
  {
    "objectID": "slides/09-mlr-old.html#using-mean-centered-variables-in-the-model",
    "href": "slides/09-mlr-old.html#using-mean-centered-variables-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Using mean-centered variables in the model",
    "text": "Using mean-centered variables in the model\n\nHow do you expect the model to change if we use the debt_inc_cent and annual_income_cent in the model?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.444\n0.977\n9.663\n0.000\n7.476\n11.413\n\n\ndebt_inc_cent\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th_cent\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002"
  },
  {
    "objectID": "slides/09-mlr-old.html#original-vs.-mean-centered-model",
    "href": "slides/09-mlr-old.html#original-vs.-mean-centered-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Original vs. mean-centered model",
    "text": "Original vs. mean-centered model\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n10.726\n\n\ndebt_to_income\n0.671\n\n\nverified_incomeSource Verified\n2.211\n\n\nverified_incomeVerified\n6.880\n\n\nannual_income_th\n-0.021\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\n\n\n\n\n(Intercept)\n9.444\n\n\ndebt_inc_cent\n0.671\n\n\nverified_incomeSource Verified\n2.211\n\n\nverified_incomeVerified\n6.880\n\n\nannual_income_th_cent\n-0.021"
  },
  {
    "objectID": "slides/09-mlr-old.html#indicator-variables-1",
    "href": "slides/09-mlr-old.html#indicator-variables-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category"
  },
  {
    "objectID": "slides/09-mlr-old.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/09-mlr-old.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n\n\n\n# A tibble: 3 × 4\n  verified_income not_verified source_verified verified\n  &lt;fct&gt;                  &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified               1               0        0\n2 Verified                   0               0        1\n3 Source Verified            0               1        0"
  },
  {
    "objectID": "slides/09-mlr-old.html#indicators-in-the-model",
    "href": "slides/09-mlr-old.html#indicators-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe baseline is the category that doesn’t have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding.\n\n\n\nloan50 |&gt;\n  select(verified_income, source_verified, verified) |&gt;\n  slice(1, 3, 6)\n\n# A tibble: 3 × 3\n  verified_income source_verified verified\n  &lt;fct&gt;                     &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified                  0        0\n2 Verified                      0        1\n3 Source Verified               1        0"
  },
  {
    "objectID": "slides/09-mlr-old.html#interpreting-verified_income",
    "href": "slides/09-mlr-old.html#interpreting-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n9.444\n0.977\n9.663\n0.000\n7.476\n11.413\n\n\ndebt_inc_cent\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_th_cent\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\n\n\nThe baseline category is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\n\n\n\n\n\nInterpret the coefficient of Verified in the context of the data."
  },
  {
    "objectID": "slides/09-mlr-old.html#interaction-terms-1",
    "href": "slides/09-mlr-old.html#interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/09-mlr-old.html#interest-rate-vs.-annual-income",
    "href": "slides/09-mlr-old.html#interest-rate-vs.-annual-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification.\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/09-mlr-old.html#interaction-term-in-model",
    "href": "slides/09-mlr-old.html#interaction-term-in-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_cent_int_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(interest_rate ~ debt_inc_cent + verified_income + annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.484\n0.989\n9.586\n0.000\n\n\ndebt_inc_cent\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n2.157\n1.418\n1.522\n0.135\n\n\nverified_incomeVerified\n7.181\n1.870\n3.840\n0.000\n\n\nannual_income_th_cent\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th_cent\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th_cent\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/09-mlr-old.html#interpreting-interaction-terms",
    "href": "slides/09-mlr-old.html#interpreting-interaction-terms",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/09-mlr-old.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/09-mlr-old.html#data-manipulation-4-create-interaction-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          &lt;dbl&gt; 1, 1, 1, 1, 1, …\n$ debt_inc_cent                                          &lt;dbl&gt; -0.16511719, 0.…\n$ annual_income_th_cent                                  &lt;dbl&gt; -27.17, -26.17,…\n$ `verified_incomeNot Verified`                          &lt;dbl&gt; 1, 1, 0, 1, 1, …\n$ `verified_incomeSource Verified`                       &lt;dbl&gt; 0, 0, 0, 0, 0, …\n$ verified_incomeVerified                                &lt;dbl&gt; 0, 0, 1, 0, 0, …\n$ `annual_income_th_cent:verified_incomeNot Verified`    &lt;dbl&gt; -27.17, -26.17,…\n$ `annual_income_th_cent:verified_incomeSource Verified` &lt;dbl&gt; 0.00, 0.00, 0.0…\n$ `annual_income_th_cent:verified_incomeVerified`        &lt;dbl&gt; 0.00, 0.00, -11…"
  },
  {
    "objectID": "slides/09-mlr-old.html#recap",
    "href": "slides/09-mlr-old.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nIntroduced multiple linear regression\nInterpreted coefficients in the multiple linear regression model\nCalculated predictions and associated intervals for multiple linear regression models\nMean-centered quantitative predictors\nUsed indicator variables for categorical predictors\nUsed interaction terms"
  },
  {
    "objectID": "slides/09-mlr-old.html#looking-backward",
    "href": "slides/09-mlr-old.html#looking-backward",
    "title": "Multiple linear regression (MLR)",
    "section": "Looking backward",
    "text": "Looking backward\nData manipulation, with dplyr (from tidyverse):\n\nloan50 |&gt;\n  select(interest_rate, annual_income, debt_to_income, verified_income) |&gt;\n  mutate(\n    # 1. rescale income\n    annual_income_th = annual_income / 1000,\n    # 2. mean-center quantitative predictors\n    debt_inc_cent = debt_to_income - mean(debt_to_income),\n    annual_income_th_cent = annual_income_th - mean(annual_income_th),\n    # 3. create dummy variables for verified_income\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0),\n    # 4. create interaction variables\n    `annual_income_th_cent:verified_incomeSource Verified` = annual_income_th_cent * source_verified,\n    `annual_income_th_cent:verified_incomeVerified` = annual_income_th_cent * verified\n  )"
  },
  {
    "objectID": "slides/09-mlr-old.html#looking-forward-after-exam-01",
    "href": "slides/09-mlr-old.html#looking-forward-after-exam-01",
    "title": "Multiple linear regression (MLR)",
    "section": "Looking forward (after Exam 01)",
    "text": "Looking forward (after Exam 01)\nFeature engineering, with recipes (from tidymodels):\n\nloan_rec &lt;- recipe( ~ ., data = loan50) |&gt;\n  # 1. rescale income\n  step_mutate(annual_income_th = annual_income / 1000) |&gt;\n  # 2. mean-center quantitative predictors\n  step_center(all_numeric_predictors()) |&gt;\n  # 3. create dummy variables for verified_income\n  step_dummy(verified_income) |&gt;\n  # 4. create interaction variables\n  step_interact(terms = ~ annual_income_th:verified_income)"
  },
  {
    "objectID": "slides/09-mlr-old.html#recipe",
    "href": "slides/09-mlr-old.html#recipe",
    "title": "Multiple linear regression (MLR)",
    "section": "Recipe",
    "text": "Recipe\n\nloan_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\npredictor: 24\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: annual_income / 1000\n\n\n• Centering for: all_numeric_predictors()\n\n\n• Dummy variables from: verified_income\n\n\n• Interactions with: annual_income_th:verified_income\n\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#topics",
    "href": "slides/07-slr-sim-testing.html#topics",
    "title": "SLR: Randomization test for the slope",
    "section": "Topics",
    "text": "Topics\n\nEvaluate a claim about the slope using hypothesis testing\nDefine mathematical models to conduct inference for slope"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#computational-setup",
    "href": "slides/07-slr-sim-testing.html#computational-setup",
    "title": "SLR: Randomization test for the slope",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for nearly formatting model output\nlibrary(ggformula)   # for plotting\nlibrary(infer)       # for simulation based inference\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for neatly formatted tables\n\n# load my data\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#data-san-antonio-income-organic-food-access",
    "href": "slides/07-slr-sim-testing.html#data-san-antonio-income-organic-food-access",
    "title": "SLR: Randomization test for the slope",
    "section": "Data: San Antonio Income & Organic Food Access",
    "text": "Data: San Antonio Income & Organic Food Access"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#the-regression-model",
    "href": "slides/07-slr-sim-testing.html#the-regression-model",
    "title": "SLR: Randomization test for the slope",
    "section": "The regression model",
    "text": "The regression model\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\nSlope: For each additional $1,000 in average household income, we expect the number of organic options available at nearby HEBs to increase by 0.96, on average."
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#inference-for-simple-linear-regression",
    "href": "slides/07-slr-sim-testing.html#inference-for-simple-linear-regression",
    "title": "SLR: Randomization test for the slope",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope, \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#sampling-is-natural",
    "href": "slides/07-slr-sim-testing.html#sampling-is-natural",
    "title": "SLR: Randomization test for the slope",
    "section": "Sampling is natural",
    "text": "Sampling is natural\n\n\nWhen you taste a spoonful of soup and decide the spoonful you tasted isn’t salty enough, that’s exploratory analysis\nIf you generalize and conclude that your entire soup needs salt, that’s an inference\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population)"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#confidence-interval-via-bootstrapping",
    "href": "slides/07-slr-sim-testing.html#confidence-interval-via-bootstrapping",
    "title": "SLR: Randomization test for the slope",
    "section": "Confidence interval via bootstrapping",
    "text": "Confidence interval via bootstrapping\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#bootstrapping-pipeline-i",
    "href": "slides/07-slr-sim-testing.html#bootstrapping-pipeline-i",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline I",
    "text": "Bootstrapping pipeline I\n\nset.seed(212)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K)\n\nResponse: Number_Organic (numeric)\nExplanatory: Avg_Income_K (numeric)\n# A tibble: 37 × 2\n   Number_Organic Avg_Income_K\n            &lt;dbl&gt;        &lt;dbl&gt;\n 1             36         71.2\n 2              4         34.2\n 3             28         71.2\n 4             31         48.8\n 5             78         78.1\n 6             14         40.5\n 7             12         38.2\n 8             18         50.4\n 9             38         49.4\n10             84         66.1\n# ℹ 27 more rows"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#bootstrapping-pipeline-ii",
    "href": "slides/07-slr-sim-testing.html#bootstrapping-pipeline-ii",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline II",
    "text": "Bootstrapping pipeline II\n\nset.seed(212)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  generate(reps = 1000, type = \"bootstrap\")\n\nResponse: Number_Organic (numeric)\nExplanatory: Avg_Income_K (numeric)\n# A tibble: 37,000 × 3\n# Groups:   replicate [1,000]\n   replicate Number_Organic Avg_Income_K\n       &lt;int&gt;          &lt;dbl&gt;        &lt;dbl&gt;\n 1         1             44         50.3\n 2         1             18         50.4\n 3         1             29         56.3\n 4         1             85         78.2\n 5         1             86         84.2\n 6         1             12         38.2\n 7         1             15         41.3\n 8         1             82         92.9\n 9         1             28         72.1\n10         1             85         78.2\n# ℹ 36,990 more rows"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#bootstrapping-pipeline-iii",
    "href": "slides/07-slr-sim-testing.html#bootstrapping-pipeline-iii",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline III",
    "text": "Bootstrapping pipeline III\n\nset.seed(212)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()\n\n# A tibble: 2,000 × 3\n# Groups:   replicate [1,000]\n   replicate term         estimate\n       &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1         1 intercept      -8.22 \n 2         1 Avg_Income_K    0.869\n 3         2 intercept      -8.52 \n 4         2 Avg_Income_K    0.878\n 5         3 intercept     -16.4  \n 6         3 Avg_Income_K    0.948\n 7         4 intercept     -38.2  \n 8         4 Avg_Income_K    1.37 \n 9         5 intercept      -5.62 \n10         5 Avg_Income_K    0.836\n# ℹ 1,990 more rows"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#bootstrapping-pipeline-iv",
    "href": "slides/07-slr-sim-testing.html#bootstrapping-pipeline-iv",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline IV",
    "text": "Bootstrapping pipeline IV\n\nset.seed(212)\n\nboot_dist &lt;- heb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#visualize-the-bootstrap-distribution",
    "href": "slides/07-slr-sim-testing.html#visualize-the-bootstrap-distribution",
    "title": "SLR: Randomization test for the slope",
    "section": "Visualize the bootstrap distribution",
    "text": "Visualize the bootstrap distribution\n\nboot_dist |&gt;\n  filter(term == \"Avg_Income_K\") |&gt;\n  gf_histogram(~estimate, bins = 15)"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#compute-the-ci",
    "href": "slides/07-slr-sim-testing.html#compute-the-ci",
    "title": "SLR: Randomization test for the slope",
    "section": "Compute the CI",
    "text": "Compute the CI"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#but-first",
    "href": "slides/07-slr-sim-testing.html#but-first",
    "title": "SLR: Randomization test for the slope",
    "section": "But first…",
    "text": "But first…\n\nobs_fit &lt;- heb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  fit()\n\nobs_fit\n\n# A tibble: 2 × 2\n  term         estimate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 intercept     -14.7  \n2 Avg_Income_K    0.959"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#compute-95-confidence-interval",
    "href": "slides/07-slr-sim-testing.html#compute-95-confidence-interval",
    "title": "SLR: Randomization test for the slope",
    "section": "Compute 95% confidence interval",
    "text": "Compute 95% confidence interval\n\nboot_dist |&gt;\n  get_confidence_interval(\n    point_estimate = obs_fit,\n    level = 0.95,\n    type = \"percentile\"\n  )\n\n# A tibble: 2 × 3\n  term         lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K    0.792     1.24\n2 intercept     -31.9      -1.27"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#research-question-and-hypotheses",
    "href": "slides/07-slr-sim-testing.html#research-question-and-hypotheses",
    "title": "SLR: Randomization test for the slope",
    "section": "Research question and hypotheses",
    "text": "Research question and hypotheses\n“Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”\n\nNull hypothesis: there is no linear relationship between Number_Organic and Avg_Income_K\n\\[\nH_0: \\beta_1 = 0\n\\]\n\n\nAlternative hypothesis: there is a linear relationship between Number_Organic and Avg_Income_K\n\\[\nH_A: \\beta_1 \\ne 0\n\\]"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#hypothesis-testing-as-a-court-trial",
    "href": "slides/07-slr-sim-testing.html#hypothesis-testing-as-a-court-trial",
    "title": "SLR: Randomization test for the slope",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?”\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)\n\nNot guilty \\(\\neq\\) innocent \\(\\implies\\) why we say “fail to reject the null” rather than “accept the null”"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#hypothesis-testing-framework",
    "href": "slides/07-slr-sim-testing.html#hypothesis-testing-framework",
    "title": "SLR: Randomization test for the slope",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e. claim we’re testing\nUnder the assumption that the null hypothesis is true, calculate a p-value (probability of getting outcome or outcome even more favorable to the alternative)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#quantify-the-variability-of-the-slope",
    "href": "slides/07-slr-sim-testing.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Randomization test for the slope",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor testing\n\n\nTwo approaches:\n\nVia simulation\nVia mathematical models\n\nUse Randomization to quantify the variability of the slope for the purpose of testing, under the assumption that the null hypothesis is true:\n\nSimulate new samples from the original sample via permutation\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the permuted slopes to conduct a hypothesis test"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#permutation-described",
    "href": "slides/07-slr-sim-testing.html#permutation-described",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation, described",
    "text": "Permutation, described\n\n\n\nUse permuting to simulate data under the assumption the null hypothesis is true and measure the natural variability in the data due to sampling, not due to variables being correlated\n\nPermute reponse variable to eliminate any existing relationship with explanatory variable\n\nEach Number_Organic value is randomly assigned to the Avg_Household_K, i.e. Number_Organic and Avg_Household_K are no longer matched for a given store\n\n\n\n\n# A tibble: 37 × 3\n   Number_Organic_Original Number_Organic_Permuted Avg_Income_K\n                     &lt;dbl&gt;                   &lt;dbl&gt;        &lt;dbl&gt;\n 1                      36                      73         71.2\n 2                       4                      29         34.2\n 3                      28                      35         71.2\n 4                      31                      38         48.8\n 5                      78                      78         78.1\n 6                      14                      14         40.5\n 7                      12                      82         38.2\n 8                      18                      31         50.4\n 9                      38                       4         49.4\n10                      84                      12         66.1\n# ℹ 27 more rows"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#permutation-visualized",
    "href": "slides/07-slr-sim-testing.html#permutation-visualized",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation, visualized",
    "text": "Permutation, visualized\n\n\n\nEach of the observed values for area (and for price) exist in both the observed data plot as well as the permuted price plot\nPermuting removes the relationship between area and price"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#permutation-repeated",
    "href": "slides/07-slr-sim-testing.html#permutation-repeated",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation, repeated",
    "text": "Permutation, repeated\nRepeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true)"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#concluding-the-hypothesis-test",
    "href": "slides/07-slr-sim-testing.html#concluding-the-hypothesis-test",
    "title": "SLR: Randomization test for the slope",
    "section": "Concluding the hypothesis test",
    "text": "Concluding the hypothesis test\n\nIs the observed slope of \\(\\hat{\\beta_1} = 0.96\\) (or an even more extreme slope) a likely outcome under the null hypothesis that \\(\\beta = 0\\)? What does this mean for our original question: “Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#announcements",
    "href": "slides/07-slr-conditions-eval-old.html#announcements",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Announcements",
    "text": "Announcements\n\nHW 01: due TODAY at 11:59pm\nLab 03:\n\ndue Fri at 11:59pm (Tue labs)\ndue Sun at 11:59pm (Thu labs)\n\nLooking ahead: Exam 01:\n\nClosed note in-class: Wed, Oct 4\nOpen note take-home: Wed, Oct 4 - Fri, Oct 6\n\nReleased after Section 002\n\nMore about the exam next week"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#computational-set-up",
    "href": "slides/07-slr-conditions-eval-old.html#computational-set-up",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#regression-model-revisited",
    "href": "slides/07-slr-conditions-eval-old.html#regression-model-revisited",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#mathematical-representation-visualized",
    "href": "slides/07-slr-conditions-eval-old.html#mathematical-representation-visualized",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#model-conditions-1",
    "href": "slides/07-slr-conditions-eval-old.html#model-conditions-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#linearity",
    "href": "slides/07-slr-conditions-eval-old.html#linearity",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Linearity",
    "text": "Linearity\n\nIf the linear model, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\) adequately describes the relationship between \\(X\\) and \\(Y\\), then the residuals should reflect random (chance) error\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nLinearity satisfied if there is no distinguishable pattern in the residuals plot, i.e. the residuals should be randomly scattered\nA non-random pattern (e.g. a parabola) suggests a linear model does not adequately describe the relationship between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#linearity-1",
    "href": "slides/07-slr-conditions-eval-old.html#linearity-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Linearity",
    "text": "Linearity\n✅ The residuals vs. fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#residuals-vs.-fitted-values-code",
    "href": "slides/07-slr-conditions-eval-old.html#residuals-vs.-fitted-values-code",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Residuals vs. fitted values (code)",
    "text": "Residuals vs. fitted values (code)\n\ndf_aug &lt;- augment(df_fit$fit)\n\nggplot(df_aug, aes(x = .fitted, y = .resid)) +\n  geom_point() +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  ylim(-1000000, 1000000) +\n  labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#non-linear-relationships",
    "href": "slides/07-slr-conditions-eval-old.html#non-linear-relationships",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#constant-variance",
    "href": "slides/07-slr-conditions-eval-old.html#constant-variance",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n\nIf the spread of the distribution of \\(Y\\) is equal for all values of \\(X\\)then the spread of the residuals should be approximately equal for each value of \\(X\\)\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nConstant variance satisfied if the vertical spread of the residuals is approximately equal as you move from left to right (i.e. there is no “fan” pattern)\nA fan pattern suggests the constant variance assumption is not satisfied and transformation or some other remedy is required (more on this later in the semester)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#constant-variance-1",
    "href": "slides/07-slr-conditions-eval-old.html#constant-variance-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n✅ The vertical spread of the residuals is relatively constant across the plot"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#non-constant-variance",
    "href": "slides/07-slr-conditions-eval-old.html#non-constant-variance",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Non-constant variance",
    "text": "Non-constant variance\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#normality",
    "href": "slides/07-slr-conditions-eval-old.html#normality",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality\n\nThe linear model assumes that the distribution of \\(Y\\) is Normal for every value of \\(X\\)\nThis is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied\nNormality satisfied if a histogram of the residuals is approximately normal\n\nCan also check that the points on a normal QQ-plot falls along a diagonal line\n\nMost inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#normality-1",
    "href": "slides/07-slr-conditions-eval-old.html#normality-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#check-normality-using-a-qq-plot",
    "href": "slides/07-slr-conditions-eval-old.html#check-normality-using-a-qq-plot",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Check normality using a QQ-plot",
    "text": "Check normality using a QQ-plot\n\n\n\n\nCode\nggplot(df_aug, aes(x = .resid)) +\n  geom_histogram(binwidth = 50000, color = \"white\")  +\n  labs(\n    x = \"Residual\",\n    y = \"Count\",\n    title = \"Histogram of residuals\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot(df_aug, aes(sample = .resid)) +\n  stat_qq()+\n  stat_qq_line() + \n  labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")\n\n\n\n\n\n\n\n\n\n\n\n\nAssess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot).\nIf so, the residuals are normally distributed."
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#normality-2",
    "href": "slides/07-slr-conditions-eval-old.html#normality-2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality\n\n❌ The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied.\n✅ The sample size \\(n = 98 &gt; 30\\), so the sample size is large enough to relax this condition and proceed with inference."
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#independence",
    "href": "slides/07-slr-conditions-eval-old.html#independence",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nTwo common violations of the independence assumption:\n\nSerial Effect: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation)\nCluster Effect: If there are subgroups represented in the data that are not accounted for in the model (e.g., type of house), you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#independence-1",
    "href": "slides/07-slr-conditions-eval-old.html#independence-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Independence",
    "text": "Independence\nRecall the description of the data:\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\n\n\n✅ Based on the information we have, we can reasonably treat this as a random sample of Duke Forest Houses and assume the error for one house does not tell us anything about the error for another house."
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#recap",
    "href": "slides/07-slr-conditions-eval-old.html#recap",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\n\nLinearity\nConstant variance\n\n\n\n\n\nNormality\nIndependence\n\n\n\n\n\n\nWhich of these conditions are required for fitting a SLR? Which for simulation-based inference for the slope for an SLR? Which for inference with mathematical models?\n\nEd Discussion [Section 001][Section 002]\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#comparing-inferential-methods",
    "href": "slides/07-slr-conditions-eval-old.html#comparing-inferential-methods",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Comparing inferential methods",
    "text": "Comparing inferential methods\n\n\nWhat are the advantages of using simulation-based inference methods? What are the advantages of using inference methods based on mathematical models?\nUnder what scenario(s) would you prefer to use simulation-based methods? Under what scenario(s) would you prefer to use methods based on mathematical models?\n\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#two-statistics",
    "href": "slides/07-slr-conditions-eval-old.html#two-statistics",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#r2",
    "href": "slides/07-slr-conditions-eval-old.html#r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() using the augmented data:\n\n\nrsq(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.445"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#interpreting-r2",
    "href": "slides/07-slr-conditions-eval-old.html#interpreting-r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\n🗳️ Discussion\n\n\nThe \\(R^2\\) of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n\n\nArea correctly predicts 44.5% of price for houses in Duke Forest.\n44.5% of the variability in price for houses in Duke Forest can be explained by area.\n44.5% of the variability in area for houses in Duke Forest can be explained by price\n44.5% of the time price for houses in Duke Forest can be predicted by area."
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#alternative-approach-for-r2",
    "href": "slides/07-slr-conditions-eval-old.html#alternative-approach-for-r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Alternative approach for \\(R^2\\)",
    "text": "Alternative approach for \\(R^2\\)\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(df_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared   sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.445         0.439 168798.      77.0 6.29e-14     1 -1318. 2641. 2649.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nglance(df_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#rmse",
    "href": "slides/07-slr-conditions-eval-old.html#rmse",
    "title": "SLR: Conditions + Model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rmse() using the augmented data:\n\nrmse(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     167067.\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval-old.html#obtaining-r2-and-rmse",
    "href": "slides/07-slr-conditions-eval-old.html#obtaining-r2-and-rmse",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\n\nUse rsq() and rmse(), respectively\n\nrsq(df_aug, truth = price, estimate = .fitted)\nrmse(df_aug, truth = price, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#announcements",
    "href": "slides/06-slr-math-models-old.html#announcements",
    "title": "SLR: Mathematical models for inference",
    "section": "Announcements",
    "text": "Announcements\n\nHW 01: due Wed, Sep 20 at 11:59pm\n\nQuestions?\n\nStatistics experience - due Mon, Nov 20 at 11:59pm"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#course-policy-reminders",
    "href": "slides/06-slr-math-models-old.html#course-policy-reminders",
    "title": "SLR: Mathematical models for inference",
    "section": "Course policy reminders",
    "text": "Course policy reminders\n\nLate work\n\nHW and labs accepted up to 2 days late.\n5% deduction for each 24-hour period the assignment is late.\n\nOne time late waiver\n\nCan use on HW and individual labs\n\nLowest HW and lowest lab grade dropped at the end of the semester."
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#course-policy-reminders-1",
    "href": "slides/06-slr-math-models-old.html#course-policy-reminders-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Course policy reminders",
    "text": "Course policy reminders\n\nRegrade requests\n\nOpened 1 day after assignment is returned and due within 1 week\nOnly submit regrade if there is an error in the grading not to dispute points\nProf. Tackett or Sam the Head TA will regrade the entire exercise being disputed, which could result in a lower grade\n\n\n\n\n\n\n\nTip\n\n\nSee the posted solutions and ask during office hours before submitting a regrade request"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#statistician-of-the-day-regina-nuzzo",
    "href": "slides/06-slr-math-models-old.html#statistician-of-the-day-regina-nuzzo",
    "title": "SLR: Mathematical models for inference",
    "section": "Statistician of the day: Regina Nuzzo",
    "text": "Statistician of the day: Regina Nuzzo\n\n\n\n\n\n\n\n\nDr. Nuzzo received her PhD in Statistics from Stanford University and is now Professor of Science, Technology, & Mathematics at Gallaudet University. Dr. Nuzzo teaches statistics using American Sign Language.\nHer work has appeared in Nature, Los Angeles Times, New York Times, Reader’s Digest, New Scientist, and Scientific American. Primarily, she works to help lay-audiences understand science and statistics in particular. She earned the American Statistical Association’s 2014 Excellence in Statistical Reporting Award for her article on p-values in Nature. Her work led to the ASA’s statement on p-values.\nSource: hardin47.github.io/CURV/scholars/nuzzo"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#regina-nuzzo",
    "href": "slides/06-slr-math-models-old.html#regina-nuzzo",
    "title": "SLR: Mathematical models for inference",
    "section": "Regina Nuzzo",
    "text": "Regina Nuzzo\nFrom “Tips for Communicating Statistical Significance.”\n“That we use p-values to alert us to surprising data results, not to give a final answer on anything. (Or at least that’s what we should be doing). And that results can get flagged as”statistically surprising” with a small p-value for a number of reasons”\n\nThere was a fluke.\nSomething was violated.\nThere was a real but tiny relationship.\nThere was a relationship that is worth more study.\n\nOr any combination of the above."
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#topics",
    "href": "slides/06-slr-math-models-old.html#topics",
    "title": "SLR: Mathematical models for inference",
    "section": "Topics",
    "text": "Topics\n\nDefine mathematical models to conduct inference for the slope\nUse mathematical models to\n\ncalculate confidence interval for the slope\nconduct a hypothesis test for the slope"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#computational-setup",
    "href": "slides/06-slr-math-models-old.html#computational-setup",
    "title": "SLR: Mathematical models for inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#the-regression-model-revisited",
    "href": "slides/06-slr-math-models-old.html#the-regression-model-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#inference-revisited",
    "href": "slides/06-slr-math-models-old.html#inference-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "Inference, revisited",
    "text": "Inference, revisited\n\n\nEarlier we computed a confidence interval and conducted a hypothesis test via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we’ll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the confidence interval and the p-value for the hypothesis test"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#mathematical-representation-of-the-model",
    "href": "slides/06-slr-math-models-old.html#mathematical-representation-of-the-model",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\n\nindependent: Knowing the error term for one observation doesn’t tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#mathematical-representation-visualized",
    "href": "slides/06-slr-math-models-old.html#mathematical-representation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#regression-standard-error",
    "href": "slides/06-slr-math-models-old.html#regression-standard-error",
    "title": "SLR: Mathematical models for inference",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error, the average distance between the observed values and the regression line\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\nWhy divide by \\(n - 2\\)?\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#standard-error-of-hatbeta_1",
    "href": "slides/06-slr-math-models-old.html#standard-error-of-hatbeta_1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\nThe standard error of \\(\\hat{\\beta}_1\\) quantifies the sampling variability in the estimated slopes\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#hypothesis-test-for-the-slope",
    "href": "slides/06-slr-math-models-old.html#hypothesis-test-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test for the slope",
    "text": "Hypothesis test for the slope\nHypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\nT = \\frac{\\text{Estimate - Null}}{\\text{Standard error}} \\\\\n\\]\n\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\np-value = P(|t| &gt; |\\text{test statistic}|),\n\\]\ncalculated from a \\(t\\) distribution with \\(n - 2\\) degrees of freedom"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#hypothesis-test-test-statistic",
    "href": "slides/06-slr-math-models-old.html#hypothesis-test-test-statistic",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Test statistic",
    "text": "Hypothesis test: Test statistic\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\\[\nT = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}} = \\frac{159.48 - 0}{18.17} = 8.78\n\\]\n\nSelect the best interpretation of the test statistic on Ed Discussion.\n[10:05am lecture] [1:25pm lecture]"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#hypothesis-test-p-value",
    "href": "slides/06-slr-math-models-old.html#hypothesis-test-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#hypothesis-test-p-value-1",
    "href": "slides/06-slr-math-models-old.html#hypothesis-test-p-value-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\nA more exact p-value\n\n2 * pt(q = 8.78, df = 96, lower.tail = FALSE)\n\n[1] 6.19602e-14\n\n\n\nSelect the meaning of the p-value on Ed Discussion.\n[10:05am lecture][1:25pm lecture]"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#understanding-the-p-value",
    "href": "slides/06-slr-math-models-old.html#understanding-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#hypothesis-test-conclusion-in-context",
    "href": "slides/06-slr-math-models-old.html#hypothesis-test-conclusion-in-context",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Conclusion, in context",
    "text": "Hypothesis test: Conclusion, in context\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\nThe data provide convincing evidence that the population slope \\(\\beta_1\\) is different from 0.\nThe data provide convincing evidence of a linear relationship between area and price of houses in Duke Forest."
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#confidence-interval-for-the-slope",
    "href": "slides/06-slr-math-models-old.html#confidence-interval-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#confidence-interval-critical-value",
    "href": "slides/06-slr-math-models-old.html#confidence-interval-critical-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(duke_forest) - 2)\n\n[1] 1.984984\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(duke_forest) - 2)\n\n[1] 1.660881\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(duke_forest) - 2)\n\n[1] 2.628016"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#ci-for-the-slope-calculation",
    "href": "slides/06-slr-math-models-old.html#ci-for-the-slope-calculation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\\[\\hat{\\beta}_1 = 159.48 \\hspace{15mm} t^* = 1.98 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 18.17\\]\n\n\\[\n159.48 \\pm 1.98 \\times 18.17 = (123.50, 195.46)\n\\]"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#ci-for-the-slope-computation",
    "href": "slides/06-slr-math-models-old.html#ci-for-the-slope-computation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(df_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n10847.77\n222456.88\n\n\narea\n159.48\n18.17\n8.78\n0.00\n123.41\n195.55"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#intervals-for-predictions-1",
    "href": "slides/06-slr-math-models-old.html#intervals-for-predictions-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Intervals for predictions",
    "text": "Intervals for predictions\n\nSuppose we want to answer the question “What is the predicted sale price of a Duke Forest house that is 2,800 square feet?”\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#two-types-of-predictions",
    "href": "slides/06-slr-math-models-old.html#two-types-of-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: “What is the average predicted sale price of Duke Forest houses that are 2,800 square feet?”\nPrediction for an individual observation: “What is the predicted sale price of a Duke Forest house that is 2,800 square feet?”\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#uncertainty-in-predictions",
    "href": "slides/06-slr-math-models-old.html#uncertainty-in-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}}\\]\n\nPrediction interval for an individual observation: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}}\\]"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#standard-errors",
    "href": "slides/06-slr-math-models-old.html#standard-errors",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#standard-errors-1",
    "href": "slides/06-slr-math-models-old.html#standard-errors-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#confidence-interval",
    "href": "slides/06-slr-math-models-old.html#confidence-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcome:\n\nnew_house &lt;- tibble(area = 2800)\n\npredict(df_fit, new_data = new_house, type = \"conf_int\", level = 0.95) |&gt;\n  kable()\n\n\n\n\n.pred_lower\n.pred_upper\n\n\n\n\n529351\n597060.1\n\n\n\n\n\n\nWe are 95% confident that mean sale price of Duke Forest houses that are 2,800 square feet is between $529,351 and $597,060."
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#prediction-interval",
    "href": "slides/06-slr-math-models-old.html#prediction-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction interval for an individual outcome:\n\npredict(df_fit, new_data = new_house, type = \"pred_int\", level = 0.95) |&gt;\n  kable()\n\n\n\n\n.pred_lower\n.pred_upper\n\n\n\n\n226438.3\n899972.7\n\n\n\n\n\n\nWe are 95% confident that predicted sale price of a Duke Forest house that is 2,800 square feet is between $226,438 and $899,973."
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#comparing-intervals",
    "href": "slides/06-slr-math-models-old.html#comparing-intervals",
    "title": "SLR: Mathematical models for inference",
    "section": "Comparing intervals",
    "text": "Comparing intervals\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\nWarning: Removed 1 row containing non-finite outside the scale range\n(`stat_smooth()`).\n\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_point()`).\n\n\nWarning: Removed 9 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_line()`)."
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#extrapolation",
    "href": "slides/06-slr-math-models-old.html#extrapolation",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\nUsing the model to predict for values outside the range of the original data is extrapolation.\n\n\n\n\nCalculate the prediction interval for the sale price of a “tiny house” in Duke Forest that is 225 square feet.\n\n\n\n\n\n\n\n\n\n\n\nNo, thanks!"
  },
  {
    "objectID": "slides/06-slr-math-models-old.html#extrapolation-1",
    "href": "slides/06-slr-math-models-old.html#extrapolation-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nWhy do we want to avoid extrapolation?\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#announcements",
    "href": "slides/05-slr-sim-testing-old.html#announcements",
    "title": "SLR: Randomization test for the slope",
    "section": "Announcements",
    "text": "Announcements\n\nLab 02 due\n\nFriday, Sep 15 at 11:59pm (Tuesday labs)\nSunday, Sep 17 at 11:59pm (Thursday labs)\n\nHW 01\n\nReleased later today (will get email when HW is available)\ndue Wed, Sep 20 at 11:59pm\n\nStatistics experience - due Mon, Nov 20 at 11:59pm"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#topics",
    "href": "slides/05-slr-sim-testing-old.html#topics",
    "title": "SLR: Randomization test for the slope",
    "section": "Topics",
    "text": "Topics\n\nEvaluate a claim about the slope using hypothesis testing\nDefine mathematical models to conduct inference for slope"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#computational-setup",
    "href": "slides/05-slr-sim-testing-old.html#computational-setup",
    "title": "SLR: Randomization test for the slope",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(openintro)   # for Duke Forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for neatly formatted tables\nlibrary(kableExtra)  # also for neatly formatted tablesf\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#data-duke-forest-houses",
    "href": "slides/05-slr-sim-testing-old.html#data-duke-forest-houses",
    "title": "SLR: Randomization test for the slope",
    "section": "Data: Duke Forest houses",
    "text": "Data: Duke Forest houses"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#the-regression-model",
    "href": "slides/05-slr-sim-testing-old.html#the-regression-model",
    "title": "SLR: Randomization test for the slope",
    "section": "The regression model",
    "text": "The regression model\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\nSlope: For each additional square foot, we expect the sale price of Duke Forest houses to be higher by $159, on average."
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#inference-for-simple-linear-regression",
    "href": "slides/05-slr-sim-testing-old.html#inference-for-simple-linear-regression",
    "title": "SLR: Randomization test for the slope",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope, \\(\\beta_1\\)"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#sampling-is-natural",
    "href": "slides/05-slr-sim-testing-old.html#sampling-is-natural",
    "title": "SLR: Randomization test for the slope",
    "section": "Sampling is natural",
    "text": "Sampling is natural\n\n\nWhen you taste a spoonful of soup and decide the spoonful you tasted isn’t salty enough, that’s exploratory analysis\nIf you generalize and conclude that your entire soup needs salt, that’s an inference\nFor your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population)"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#confidence-interval-via-bootstrapping",
    "href": "slides/05-slr-sim-testing-old.html#confidence-interval-via-bootstrapping",
    "title": "SLR: Randomization test for the slope",
    "section": "Confidence interval via bootstrapping",
    "text": "Confidence interval via bootstrapping\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#bootstrapping-pipeline-i",
    "href": "slides/05-slr-sim-testing-old.html#bootstrapping-pipeline-i",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline I",
    "text": "Bootstrapping pipeline I\n\nset.seed(210)\n\nduke_forest |&gt;\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 × 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ℹ 88 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#bootstrapping-pipeline-ii",
    "href": "slides/05-slr-sim-testing-old.html#bootstrapping-pipeline-ii",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline II",
    "text": "Bootstrapping pipeline II\n\nset.seed(210)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98,000 × 3\n# Groups:   replicate [1,000]\n   replicate   price  area\n       &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;\n 1         1  290000  2414\n 2         1  285000  2108\n 3         1  265000  1300\n 4         1  416000  2949\n 5         1  541000  2740\n 6         1  525000  2256\n 7         1 1270000  3909\n 8         1  265000  1300\n 9         1  815000  3904\n10         1  535000  2937\n# ℹ 97,990 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#bootstrapping-pipeline-iii",
    "href": "slides/05-slr-sim-testing-old.html#bootstrapping-pipeline-iii",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline III",
    "text": "Bootstrapping pipeline III\n\nset.seed(210)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()\n\n# A tibble: 2,000 × 3\n# Groups:   replicate [1,000]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept   80699.\n 2         1 area          168.\n 3         2 intercept  -18821.\n 4         2 area          205.\n 5         3 intercept  234297.\n 6         3 area          117.\n 7         4 intercept  134481.\n 8         4 area          150.\n 9         5 intercept   23861.\n10         5 area          190.\n# ℹ 1,990 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#bootstrapping-pipeline-iv",
    "href": "slides/05-slr-sim-testing-old.html#bootstrapping-pipeline-iv",
    "title": "SLR: Randomization test for the slope",
    "section": "Bootstrapping pipeline IV",
    "text": "Bootstrapping pipeline IV\n\nset.seed(210)\n\nboot_dist &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#visualize-the-bootstrap-distribution",
    "href": "slides/05-slr-sim-testing-old.html#visualize-the-bootstrap-distribution",
    "title": "SLR: Randomization test for the slope",
    "section": "Visualize the bootstrap distribution",
    "text": "Visualize the bootstrap distribution\n\nboot_dist |&gt;\n  filter(term == \"area\") |&gt;\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10)"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#compute-the-ci",
    "href": "slides/05-slr-sim-testing-old.html#compute-the-ci",
    "title": "SLR: Randomization test for the slope",
    "section": "Compute the CI",
    "text": "Compute the CI"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#but-first",
    "href": "slides/05-slr-sim-testing-old.html#but-first",
    "title": "SLR: Randomization test for the slope",
    "section": "But first…",
    "text": "But first…\n\nobs_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\nobs_fit\n\n# A tibble: 2 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#compute-95-confidence-interval",
    "href": "slides/05-slr-sim-testing-old.html#compute-95-confidence-interval",
    "title": "SLR: Randomization test for the slope",
    "section": "Compute 95% confidence interval",
    "text": "Compute 95% confidence interval\n\nboot_dist |&gt;\n  get_confidence_interval(\n    point_estimate = obs_fit,\n    level = 0.95,\n    type = \"percentile\"\n  )\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          91.7     211.\n2 intercept -18290.   287711."
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#research-question-and-hypotheses",
    "href": "slides/05-slr-sim-testing-old.html#research-question-and-hypotheses",
    "title": "SLR: Randomization test for the slope",
    "section": "Research question and hypotheses",
    "text": "Research question and hypotheses\n“Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”\n\nNull hypothesis: there is no linear relationship between area and price\n\\[\nH_0: \\beta_1 = 0\n\\]\n\n\nAlternative hypothesis: there is a linear relationship between area and price\n\\[\nH_A: \\beta_1 \\ne 0\n\\]"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#hypothesis-testing-as-a-court-trial",
    "href": "slides/05-slr-sim-testing-old.html#hypothesis-testing-as-a-court-trial",
    "title": "SLR: Randomization test for the slope",
    "section": "Hypothesis testing as a court trial",
    "text": "Hypothesis testing as a court trial\n\n\nNull hypothesis, \\(H_0\\): Defendant is innocent\nAlternative hypothesis, \\(H_A\\): Defendant is guilty\nPresent the evidence: Collect data\nJudge the evidence: “Could these data plausibly have happened by chance if the null hypothesis were true?”\n\nYes: Fail to reject \\(H_0\\)\nNo: Reject \\(H_0\\)"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#hypothesis-testing-framework",
    "href": "slides/05-slr-sim-testing-old.html#hypothesis-testing-framework",
    "title": "SLR: Randomization test for the slope",
    "section": "Hypothesis testing framework",
    "text": "Hypothesis testing framework\n\n\nStart with a null hypothesis, \\(H_0\\) that represents the status quo\nSet an alternative hypothesis, \\(H_A\\) that represents the research question, i.e. claim we’re testing\nConduct a hypothesis test under the assumption that the null hypothesis is true and calculate a p-value (probability of getting the observed or a more extreme outcome given that the null hypothesis is true)\n\nif the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis\nif they do, then reject the null hypothesis in favor of the alternative"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#quantify-the-variability-of-the-slope",
    "href": "slides/05-slr-sim-testing-old.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Randomization test for the slope",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor testing\n\n\nTwo approaches:\n\nVia simulation\nVia mathematical models\n\nUse Randomization to quantify the variability of the slope for the purpose of testing, under the assumption that the null hypothesis is true:\n\nSimulate new samples from the original sample via permutation\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the permuted slopes to conduct a hypothesis test"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#permutation-described",
    "href": "slides/05-slr-sim-testing-old.html#permutation-described",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation, described",
    "text": "Permutation, described\n\n\n\nUse permuting to simulate data under the assumption the null hypothesis is true and measure the natural variability in the data due to sampling, not due to variables being correlated\n\nPermute one variable to eliminate any existing relationship between the variables\n\nEach price value is randomly assigned to the area of a given house, i.e. area and price are no longer matched for a given house\n\n\n\n\n# A tibble: 98 × 3\n   price_Observed price_Permuted  area\n            &lt;dbl&gt;          &lt;dbl&gt; &lt;dbl&gt;\n 1        1520000         342500  6040\n 2        1030000         750000  4475\n 3         420000         645000  1745\n 4         680000         697500  2091\n 5         428500         428500  1772\n 6         456000         481000  1950\n 7        1270000         610000  3909\n 8         557450         680000  2841\n 9         697500         485000  3924\n10         650000         105000  2173\n# ℹ 88 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#permutation-visualized",
    "href": "slides/05-slr-sim-testing-old.html#permutation-visualized",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation, visualized",
    "text": "Permutation, visualized\n\n\n\nEach of the observed values for area (and for price) exist in both the observed data plot as well as the permuted price plot\nThe permutation removes the relationship between area and price\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#permutation-repeated",
    "href": "slides/05-slr-sim-testing-old.html#permutation-repeated",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation, repeated",
    "text": "Permutation, repeated\nRepeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true)\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#concluding-the-hypothesis-test",
    "href": "slides/05-slr-sim-testing-old.html#concluding-the-hypothesis-test",
    "title": "SLR: Randomization test for the slope",
    "section": "Concluding the hypothesis test",
    "text": "Concluding the hypothesis test\n\nIs the observed slope of \\(\\hat{\\beta_1} = 159\\) (or an even more extreme slope) a likely outcome under the null hypothesis that \\(\\beta = 0\\)? What does this mean for our original question: “Do the data provide sufficient evidence that \\(\\beta_1\\) (the true slope for the population) is different from 0?”\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`)."
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#permutation-pipeline-i",
    "href": "slides/05-slr-sim-testing-old.html#permutation-pipeline-i",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline I",
    "text": "Permutation pipeline I\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area)\n\nResponse: price (numeric)\nExplanatory: area (numeric)\n# A tibble: 98 × 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ℹ 88 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#permutation-pipeline-ii",
    "href": "slides/05-slr-sim-testing-old.html#permutation-pipeline-ii",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline II",
    "text": "Permutation pipeline II\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98 × 2\n     price  area\n     &lt;dbl&gt; &lt;dbl&gt;\n 1 1520000  6040\n 2 1030000  4475\n 3  420000  1745\n 4  680000  2091\n 5  428500  1772\n 6  456000  1950\n 7 1270000  3909\n 8  557450  2841\n 9  697500  3924\n10  650000  2173\n# ℹ 88 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#permutation-pipeline-iii",
    "href": "slides/05-slr-sim-testing-old.html#permutation-pipeline-iii",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline III",
    "text": "Permutation pipeline III\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\")\n\nResponse: price (numeric)\nExplanatory: area (numeric)\nNull Hypothesis: independence\n# A tibble: 98,000 × 3\n# Groups:   replicate [1,000]\n     price  area replicate\n     &lt;dbl&gt; &lt;dbl&gt;     &lt;int&gt;\n 1  465000  6040         1\n 2  481000  4475         1\n 3 1020000  1745         1\n 4  520000  2091         1\n 5  592000  1772         1\n 6  650000  1950         1\n 7  473000  3909         1\n 8  705000  2841         1\n 9  785000  3924         1\n10  671500  2173         1\n# ℹ 97,990 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#permutation-pipeline-iv",
    "href": "slides/05-slr-sim-testing-old.html#permutation-pipeline-iv",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline IV",
    "text": "Permutation pipeline IV\n\nset.seed(1125)\n\nduke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()\n\n# A tibble: 2,000 × 3\n# Groups:   replicate [1,000]\n   replicate term       estimate\n       &lt;int&gt; &lt;chr&gt;         &lt;dbl&gt;\n 1         1 intercept 553355.  \n 2         1 area           2.35\n 3         2 intercept 635824.  \n 4         2 area         -27.3 \n 5         3 intercept 536072.  \n 6         3 area           8.57\n 7         4 intercept 598649.  \n 8         4 area         -13.9 \n 9         5 intercept 556202.  \n10         5 area           1.33\n# ℹ 1,990 more rows"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#permutation-pipeline-v",
    "href": "slides/05-slr-sim-testing-old.html#permutation-pipeline-v",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline V",
    "text": "Permutation pipeline V\n\nset.seed(1125)\n\nnull_dist &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#visualize-the-null-distribution",
    "href": "slides/05-slr-sim-testing-old.html#visualize-the-null-distribution",
    "title": "SLR: Randomization test for the slope",
    "section": "Visualize the null distribution",
    "text": "Visualize the null distribution\n\nnull_dist |&gt;\n  filter(term == \"area\") |&gt;\n  ggplot(aes(x = estimate)) +\n  geom_histogram(binwidth = 10, color = \"white\")"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#reason-around-the-p-value",
    "href": "slides/05-slr-sim-testing-old.html#reason-around-the-p-value",
    "title": "SLR: Randomization test for the slope",
    "section": "Reason around the p-value",
    "text": "Reason around the p-value\n\nIn a world where the there is no relationship between the area of a Duke Forest house and in its price (\\(\\beta_1 = 0\\)), what is the probability that we observe a sample of 98 houses where the slope fo the model predicting price from area is 159 or even more extreme?\n\n\n\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_bar()`)."
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#compute-the-p-value",
    "href": "slides/05-slr-sim-testing-old.html#compute-the-p-value",
    "title": "SLR: Randomization test for the slope",
    "section": "Compute the p-value",
    "text": "Compute the p-value\n\nWhat does this warning mean?\n\n\nget_p_value(\n  null_dist,\n  obs_stat = obs_fit,\n  direction = \"two-sided\"\n)\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\nPlease be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n# A tibble: 2 × 2\n  term      p_value\n  &lt;chr&gt;       &lt;dbl&gt;\n1 area            0\n2 intercept       0"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#the-regression-model-revisited",
    "href": "slides/05-slr-sim-testing-old.html#the-regression-model-revisited",
    "title": "SLR: Randomization test for the slope",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\ndf_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.325\n53302.463\n2.188\n0.031\n\n\narea\n159.483\n18.171\n8.777\n0.000"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#inference-revisited",
    "href": "slides/05-slr-sim-testing-old.html#inference-revisited",
    "title": "SLR: Randomization test for the slope",
    "section": "Inference, revisited",
    "text": "Inference, revisited\n\n\nEarlier we computed a confidence interval and conducted a hypothesis test via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we’ll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the confidence interval and the p-value for the hypothesis test"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#mathematical-representation-of-the-model",
    "href": "slides/05-slr-sim-testing-old.html#mathematical-representation-of-the-model",
    "title": "SLR: Randomization test for the slope",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\n\nindependent: Knowing the error term for one observation doesn’t tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#mathematical-representation-visualized",
    "href": "slides/05-slr-sim-testing-old.html#mathematical-representation-visualized",
    "title": "SLR: Randomization test for the slope",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\nGraph reproduced from Beyond Multiple Linear Regression.\n\n\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#regression-standard-error",
    "href": "slides/05-slr-sim-testing-old.html#regression-standard-error",
    "title": "SLR: Randomization test for the slope",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error (the spread of the distribution of the response, for a given value of the predictor variable):\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\n\nWhy divide by \\(n - 2\\)?\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/05-slr-sim-testing-old.html#standard-error-of-hatbeta_1",
    "href": "slides/05-slr-sim-testing-old.html#standard-error-of-hatbeta_1",
    "title": "SLR: Randomization test for the slope",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\nor…\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/04-slr-intro.html#topics",
    "href": "slides/04-slr-intro.html#topics",
    "title": "Simple Linear Regression",
    "section": "Topics",
    "text": "Topics\n\nUse simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nEstimate the slope and intercept of the regression line using the least squares method.\nInterpret the slope and intercept of the regression line."
  },
  {
    "objectID": "slides/04-slr-intro.html#computation-set-up",
    "href": "slides/04-slr-intro.html#computation-set-up",
    "title": "Simple Linear Regression",
    "section": "Computation set up",
    "text": "Computation set up\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(ggformula)       # for plotting\nlibrary(fivethirtyeight) # for the fandango dataset\nlibrary(broom)           # for formatting model output\nlibrary(knitr)           # for formatting tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/04-slr-intro.html#movie-scores",
    "href": "slides/04-slr-intro.html#movie-scores",
    "title": "Simple Linear Regression",
    "section": "Movie scores",
    "text": "Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/04-slr-intro.html#data-prep",
    "href": "slides/04-slr-intro.html#data-prep",
    "title": "Simple Linear Regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/04-slr-intro.html#data-overview",
    "href": "slides/04-slr-intro.html#data-overview",
    "title": "Simple Linear Regression",
    "section": "Data overview",
    "text": "Data overview\n\nglimpse(movie_scores)\n\nRows: 146\nColumns: 23\n$ film                       &lt;chr&gt; \"Avengers: Age of Ultron\", \"Cinderella\", \"A…\n$ year                       &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ critics                    &lt;int&gt; 74, 85, 80, 18, 14, 63, 42, 86, 99, 89, 84,…\n$ audience                   &lt;int&gt; 86, 80, 90, 84, 28, 62, 53, 64, 82, 87, 77,…\n$ metacritic                 &lt;int&gt; 66, 67, 64, 22, 29, 50, 53, 81, 81, 80, 71,…\n$ metacritic_user            &lt;dbl&gt; 7.1, 7.5, 8.1, 4.7, 3.4, 6.8, 7.6, 6.8, 8.8…\n$ imdb                       &lt;dbl&gt; 7.8, 7.1, 7.8, 5.4, 5.1, 7.2, 6.9, 6.5, 7.4…\n$ fandango_stars             &lt;dbl&gt; 5.0, 5.0, 5.0, 5.0, 3.5, 4.5, 4.0, 4.0, 4.5…\n$ fandango_ratingvalue       &lt;dbl&gt; 4.5, 4.5, 4.5, 4.5, 3.0, 4.0, 3.5, 3.5, 4.0…\n$ rt_norm                    &lt;dbl&gt; 3.70, 4.25, 4.00, 0.90, 0.70, 3.15, 2.10, 4…\n$ rt_user_norm               &lt;dbl&gt; 4.30, 4.00, 4.50, 4.20, 1.40, 3.10, 2.65, 3…\n$ metacritic_norm            &lt;dbl&gt; 3.30, 3.35, 3.20, 1.10, 1.45, 2.50, 2.65, 4…\n$ metacritic_user_nom        &lt;dbl&gt; 3.55, 3.75, 4.05, 2.35, 1.70, 3.40, 3.80, 3…\n$ imdb_norm                  &lt;dbl&gt; 3.90, 3.55, 3.90, 2.70, 2.55, 3.60, 3.45, 3…\n$ rt_norm_round              &lt;dbl&gt; 3.5, 4.5, 4.0, 1.0, 0.5, 3.0, 2.0, 4.5, 5.0…\n$ rt_user_norm_round         &lt;dbl&gt; 4.5, 4.0, 4.5, 4.0, 1.5, 3.0, 2.5, 3.0, 4.0…\n$ metacritic_norm_round      &lt;dbl&gt; 3.5, 3.5, 3.0, 1.0, 1.5, 2.5, 2.5, 4.0, 4.0…\n$ metacritic_user_norm_round &lt;dbl&gt; 3.5, 4.0, 4.0, 2.5, 1.5, 3.5, 4.0, 3.5, 4.5…\n$ imdb_norm_round            &lt;dbl&gt; 4.0, 3.5, 4.0, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5…\n$ metacritic_user_vote_count &lt;int&gt; 1330, 249, 627, 31, 88, 34, 17, 124, 62, 54…\n$ imdb_user_vote_count       &lt;int&gt; 271107, 65709, 103660, 3136, 19560, 39373, …\n$ fandango_votes             &lt;int&gt; 14846, 12640, 12055, 1793, 1021, 397, 252, …\n$ fandango_difference        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…"
  },
  {
    "objectID": "slides/04-slr-intro.html#movie-scores-data",
    "href": "slides/04-slr-intro.html#movie-scores-data",
    "title": "Simple Linear Regression",
    "section": "Movie scores data",
    "text": "Movie scores data\nThe data set contains the “Tomatometer” score (critics) and audience score (audience) for 146 movies rated on rottentomatoes.com.\n\n\nCode\nmovie_scores |&gt; \ngf_point(audience ~ critics, alpha = 0.5) |&gt;  \n  gf_labs(x = \"Critics Score\" , \n       y = \"Audience Score\")"
  },
  {
    "objectID": "slides/04-slr-intro.html#movie-ratings-data",
    "href": "slides/04-slr-intro.html#movie-ratings-data",
    "title": "Simple Linear Regression",
    "section": "Movie ratings data",
    "text": "Movie ratings data\nGoal: Fit a line to describe the relationship between the critics score and audience score."
  },
  {
    "objectID": "slides/04-slr-intro.html#why-fit-a-line",
    "href": "slides/04-slr-intro.html#why-fit-a-line",
    "title": "Simple Linear Regression",
    "section": "Why fit a line?",
    "text": "Why fit a line?\nWe fit a line to accomplish one or both of the following:\n\n\nPrediction\n\nWhat is the audience score expected to be for an upcoming movie that received 35% from the critics?\n\n\n\n\nInference\n\nIs the critics score a useful predictor of the audience score? By how much is the audience score expected to change for each additional point in the critics score?"
  },
  {
    "objectID": "slides/04-slr-intro.html#terminology",
    "href": "slides/04-slr-intro.html#terminology",
    "title": "Simple Linear Regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nResponse, Y: variable describing the outcome of interest\nPredictor, X: variable we use to help understand the variability in the response"
  },
  {
    "objectID": "slides/04-slr-intro.html#regression-model",
    "href": "slides/04-slr-intro.html#regression-model",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\nRegression model: a function that describes the relationship between a quantitive response, \\(Y\\), and the predictor, \\(X\\) (or many predictors).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/04-slr-intro.html#regression-model-1",
    "href": "slides/04-slr-intro.html#regression-model-1",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu_{Y|X}\\) is the mean value of \\(Y\\) given a particular value of \\(X\\)."
  },
  {
    "objectID": "slides/04-slr-intro.html#regression-model-2",
    "href": "slides/04-slr-intro.html#regression-model-2",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[5pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/04-slr-intro.html#slr-statistical-model",
    "href": "slides/04-slr-intro.html#slr-statistical-model",
    "title": "Simple Linear Regression",
    "section": "SLR: Statistical model",
    "text": "SLR: Statistical model\n\nSimple linear regression: model to describe the relationship between \\(Y\\) and \\(X\\) where:\n\n\\(Y\\) is a quantitative/numerical reponse\n\\(X\\) is a single quantitative predictor\n\\[\\Large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}\\]\n\n\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "slides/04-slr-intro.html#slr-regression-equation",
    "href": "slides/04-slr-intro.html#slr-regression-equation",
    "title": "Simple Linear Regression",
    "section": "SLR: Regression equation",
    "text": "SLR: Regression equation\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{Y}\\): Predicted value of \\(Y\\) for a given \\(X\\)\nNo error term!"
  },
  {
    "objectID": "slides/04-slr-intro.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/04-slr-intro.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple Linear Regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)"
  },
  {
    "objectID": "slides/04-slr-intro.html#residuals",
    "href": "slides/04-slr-intro.html#residuals",
    "title": "Simple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]"
  },
  {
    "objectID": "slides/04-slr-intro.html#least-squares-line",
    "href": "slides/04-slr-intro.html#least-squares-line",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nResidual for the \\(i^{th}\\) observation:\n\n\\[e_i = \\text{observed} - \\text{predicted}\n= y_i - \\hat{y}_i\\]\n\nSum of squared residuals:\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nLeast squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/04-slr-intro.html#properties-of-least-squares-regression",
    "href": "slides/04-slr-intro.html#properties-of-least-squares-regression",
    "title": "Simple Linear Regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\n\nPasses through center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nSlope has same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\n\n\\(r\\): correlation coefficient\n\\(s_Y, s_X\\): sample standard deviations of \\(X\\) and \\(Y\\)\n\nSum of the residuals is zero: \\(\\sum_{i = 1}^n e_i \\approx 0\\)\n\nIntuition: Residuals are “balanced”\n\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/04-slr-intro.html#estimating-the-slope",
    "href": "slides/04-slr-intro.html#estimating-the-slope",
    "title": "Simple Linear Regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n\n\\[\\begin{aligned}\ns_X &= 30.1688 \\\\\ns_Y &=  20.0244 \\\\\nr &= 0.7814\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\n\\hat{\\beta}_1 &= 0.7814 \\times \\frac{20.0244}{30.1688} \\\\\n&= 0.5187\\end{aligned}\\]\n\n\n\n\nClickhere for details on deriving the equations for slope and intercept which is easy if you know multivariate calculus."
  },
  {
    "objectID": "slides/04-slr-intro.html#estimating-the-intercept",
    "href": "slides/04-slr-intro.html#estimating-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n\n\\[\\begin{aligned}\n&\\bar{x} = 60.8493 \\\\\n&\\bar{y} = 63.8767 \\\\\n&\\hat{\\beta}_1 = 0.5187\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\\hat{\\beta}_0 &= 63.8767 - 0.5187 \\times 60.8493 \\\\\n&= 32.3142\n\\end{aligned}\\]\n\n\n\n\nClick here for details on deriving the equations for slope and intercept."
  },
  {
    "objectID": "slides/04-slr-intro.html#interpretation",
    "href": "slides/04-slr-intro.html#interpretation",
    "title": "Simple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\nSlope: For each additional unit of \\(X\\) we expect the \\(Y\\) to increase by \\(\\hat{\\beta}_1\\), on average.\nIntercept: If \\(X\\) were 0, we predict \\(Y\\) to be \\(\\hat{\\beta}_0\\)\n\n\n\nFor our example, we have \\(\\hat{\\beta}_1 = 0.5187\\), and \\(\\hat{\\beta}_0 = 32.3142\\). How would you interpret these values in context?\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/04-slr-intro.html#does-it-make-sense-to-interpret-the-intercept",
    "href": "slides/04-slr-intro.html#does-it-make-sense-to-interpret-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Does it make sense to interpret the intercept?",
    "text": "Does it make sense to interpret the intercept?\n\n✅ The intercept is meaningful in the context of the data if\n\nthe predictor can feasibly take values equal to or near zero, or\nthere are values near zero in the observed data.\n\n\n\n🛑 Otherwise, the intercept may not be meaningful!"
  },
  {
    "objectID": "slides/04-slr-intro.html#group-work-roles",
    "href": "slides/04-slr-intro.html#group-work-roles",
    "title": "Simple Linear Regression",
    "section": "Group Work Roles",
    "text": "Group Work Roles\nAny time we are working on AEs, I will randomly assign you to groups of two/three. Each person will have a role:\n\nDriver: Writes code and fills out Quarto template. Responsible for submitting final work.\nReporter: Share thoughts from group during class discussions\nGopher: Responsible for looking stuff up (e.g. code, cheatsheets, etc.)\n\nYou’ll stay in your groups for three exercise and then we’ll switch (you must rotate your role every class.)\nDriver \\(\\to\\) Reporter \\(\\to\\) Gopher \\(\\to\\) Driver"
  },
  {
    "objectID": "slides/04-slr-intro.html#application-exercise",
    "href": "slides/04-slr-intro.html#application-exercise",
    "title": "Simple Linear Regression",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 AE 01 - Movie Budgets and Revenues"
  },
  {
    "objectID": "slides/04-slr-intro.html#recap",
    "href": "slides/04-slr-intro.html#recap",
    "title": "Simple Linear Regression",
    "section": "Recap",
    "text": "Recap\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nUsed the least squares method to estimate the slope and intercept.\nInterpreted the slope and intercept.\n\nSlope: For every one unit increase in \\(x\\), we expect y to change by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#statistics-experience-assignment",
    "href": "slides/04-slr-bootstrap-old.html#statistics-experience-assignment",
    "title": "SLR: Simulation-based inference",
    "section": "Statistics experience assignment",
    "text": "Statistics experience assignment\nGoal: Engage with statistics / data science outside the classroom and connect your experience with what you’re learning in the course.\nWhat: Have a statistics experience + create a slide reflecting on the experience. Counts as a homework grade.\nWhen: Must do the activity this semester. Reflection due Monday, November 20 at 11:59pm\nFor more info: sta210-fa23.netlify.app/stats-experience"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#statistician-of-the-day",
    "href": "slides/04-slr-bootstrap-old.html#statistician-of-the-day",
    "title": "SLR: Simulation-based inference",
    "section": "Statistician of the day",
    "text": "Statistician of the day\nWhat: Each week, we will highlight a statisticians, data scientists, or other scholars from groups who have been historically marginalized in the field and whose work has made a significant impact.\nGoal: Learn about scholars you may not see in traditional textbooks and see the breadth of past and current work in the field.\nWho: Scholars are from the CURV (connecting, uplifting, and recognizing voices) database started by Dr. Jo Hardin at Pomona College\nParticipate: Present a Statistician of the Day or contribute to the CURV data base as your statistics experience"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#w.e.b.-du-bois",
    "href": "slides/04-slr-bootstrap-old.html#w.e.b.-du-bois",
    "title": "SLR: Simulation-based inference",
    "section": "W.E.B. Du Bois",
    "text": "W.E.B. Du Bois\n\n\n\n\nDu Bois (1868 - 1963) was a sociologist who contributed to the field of data visualization through infographics related to the African American in the early twentieth century.\nIn 1900 Du Bois contributed approximately 60 data visualizations to an exhibit at the Exposition Universelle in Paris, an exhibit designed to illustrate the progress made by African Americans since the end of slavery (only 37 years prior, in 1863).\n\nSource: hardin47.github.io/CURV/scholars/dubois.html"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#w.e.b.-du-bois-1",
    "href": "slides/04-slr-bootstrap-old.html#w.e.b.-du-bois-1",
    "title": "SLR: Simulation-based inference",
    "section": "W.E.B. Du Bois",
    "text": "W.E.B. Du Bois\n\n\nThe set of visualizations demonstrate how powerfully a picture can tell 1000 words, as the information Du Bois used was primarily available from public records (e.g., census and other government reports).\nSource: hardin47.github.io/CURV/scholars/dubois.html\n\n\n\n\nVisualization by W.E.B. Du Bois"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#ae-03-follow-up-1",
    "href": "slides/04-slr-bootstrap-old.html#ae-03-follow-up-1",
    "title": "SLR: Simulation-based inference",
    "section": "AE 03 Follow-up",
    "text": "AE 03 Follow-up\nGoal: Use simple linear regression to model the relationship between temperature and daily bike rentals in the winter season"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#ae-03-follow-up-2",
    "href": "slides/04-slr-bootstrap-old.html#ae-03-follow-up-2",
    "title": "SLR: Simulation-based inference",
    "section": "AE 03 Follow-up",
    "text": "AE 03 Follow-up\nStatistical Model:\n\\[count = \\beta_0 +\\beta_1 ~ temp\\_orig + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)\\]\n\n\nwinter_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(count ~ temp_orig, data = winter)\n\ntidy(winter_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-111.038\n238.312\n-0.466\n0.642\n\n\ntemp_orig\n222.416\n18.459\n12.049\n0.000"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#ae-03-follow-up-3",
    "href": "slides/04-slr-bootstrap-old.html#ae-03-follow-up-3",
    "title": "SLR: Simulation-based inference",
    "section": "AE 03 Follow-up",
    "text": "AE 03 Follow-up\nUse the output to write out the estimated regression equation.\n\\[\n\\hat{count} =  -111.038 + 222.416 ~temp\\_orig\n\\]\n\n\n\n\n\n\n\nYour turn!\n\n\n\nInterpret the slope in the context of the data.\nWhy is there no error term in the regression equation?\n\n\n\n\n\nLaTex:\n\\$\\$\\hat{count} = -111.038 + 222.416 ~ temp\\_orig\\$\\$"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#topics",
    "href": "slides/04-slr-bootstrap-old.html#topics",
    "title": "SLR: Simulation-based inference",
    "section": "Topics",
    "text": "Topics\n\nFind range of plausible values for the slope using bootstrap confidence intervals"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#computational-setup",
    "href": "slides/04-slr-bootstrap-old.html#computational-setup",
    "title": "SLR: Simulation-based inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for Duke Forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\nlibrary(knitr)       # for neatly formatted tables\nlibrary(kableExtra)  # also for neatly formatted tablesf\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#data-houses-in-duke-forest",
    "href": "slides/04-slr-bootstrap-old.html#data-houses-in-duke-forest",
    "title": "SLR: Simulation-based inference",
    "section": "Data: Houses in Duke Forest",
    "text": "Data: Houses in Duke Forest\n\n\n\nData on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020\nScraped from Zillow\nSource: openintro::duke_forest\n\n\n\n\n\nGoal: Use the area (in square feet) to understand variability in the price of houses in Duke Forest."
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#exploratory-data-analysis",
    "href": "slides/04-slr-bootstrap-old.html#exploratory-data-analysis",
    "title": "SLR: Simulation-based inference",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\nCode\nggplot(duke_forest, aes(x = area, y = price)) +\n  geom_point(alpha = 0.7) +\n  labs(\n    x = \"Area (square feet)\",\n    y = \"Sale price (USD)\",\n    title = \"Price and area of houses in Duke Forest\"\n  ) +\n  scale_y_continuous(labels = label_dollar())"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#modeling",
    "href": "slides/04-slr-bootstrap-old.html#modeling",
    "title": "SLR: Simulation-based inference",
    "section": "Modeling",
    "text": "Modeling\n\ndf_fit &lt;- linear_reg() |&gt;\n  #set_engine(\"lm\") |&gt;\n  fit(price ~ area, data = duke_forest)\n\ntidy(df_fit) |&gt;\n  kable(digits = 2) #neatly format table to 2 digits\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n116652.33\n53302.46\n2.19\n0.03\n\n\narea\n159.48\n18.17\n8.78\n0.00\n\n\n\n\n\n\n\n\nIntercept: Duke Forest houses that are 0 square feet are expected to sell, for $116,652, on average.\n\nIs this interpretation useful?\n\nSlope: For each additional square foot, we expect the sale price of Duke Forest houses to be higher by $159, on average."
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#from-sample-to-population",
    "href": "slides/04-slr-bootstrap-old.html#from-sample-to-population",
    "title": "SLR: Simulation-based inference",
    "section": "From sample to population",
    "text": "From sample to population\n\nFor each additional square foot, we expect the sale price of Duke Forest houses to be higher by $159, on average.\n\n\n\nThis estimate is valid for the single sample of 98 houses.\nBut what if we’re not interested quantifying the relationship between the size and price of a house in this single sample?\nWhat if we want to say something about the relationship between these variables for all houses in Duke Forest?"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#statistical-inference",
    "href": "slides/04-slr-bootstrap-old.html#statistical-inference",
    "title": "SLR: Simulation-based inference",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nStatistical inference provide methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be random and representative of the population we’re interested in"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#inference-for-simple-linear-regression",
    "href": "slides/04-slr-bootstrap-old.html#inference-for-simple-linear-regression",
    "title": "SLR: Simulation-based inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope,\\(\\beta_1\\)"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#confidence-interval",
    "href": "slides/04-slr-bootstrap-old.html#confidence-interval",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\n\nA plausible range of values for a population parameter is called a confidence interval\nUsing only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nSimilarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#confidence-interval-for-the-slope-1",
    "href": "slides/04-slr-bootstrap-old.html#confidence-interval-for-the-slope-1",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nA confidence interval will allow us to make a statement like “For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus X dollars.”\n\n\nShould X be $10? $100? $1000?\nIf we were to take another sample of 98 would we expect the slope calculated based on that sample to be exactly $159? Off by $10? $100? $1000?\nThe answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is\nWe need a way to quantify the variability of the sample statistic"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#quantify-the-variability-of-the-slope",
    "href": "slides/04-slr-bootstrap-old.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Simulation-based inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor estimation\n\n\nTwo approaches:\n\nVia simulation (what we’ll do today)\nVia mathematical models (what we’ll do in the next class)\n\nBootstrapping to quantify the variability of the slope for the purpose of estimation:\n\nBootstrap new samples from the original sample\nFit models to each of the samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#bootstrap-sample-1",
    "href": "slides/04-slr-bootstrap-old.html#bootstrap-sample-1",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 1",
    "text": "Bootstrap sample 1\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#bootstrap-sample-2",
    "href": "slides/04-slr-bootstrap-old.html#bootstrap-sample-2",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 2",
    "text": "Bootstrap sample 2\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#bootstrap-sample-3",
    "href": "slides/04-slr-bootstrap-old.html#bootstrap-sample-3",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 3",
    "text": "Bootstrap sample 3\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#bootstrap-sample-4",
    "href": "slides/04-slr-bootstrap-old.html#bootstrap-sample-4",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 4",
    "text": "Bootstrap sample 4\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#bootstrap-sample-5",
    "href": "slides/04-slr-bootstrap-old.html#bootstrap-sample-5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 5",
    "text": "Bootstrap sample 5\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\nso on and so forth…"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#bootstrap-samples-1---5",
    "href": "slides/04-slr-bootstrap-old.html#bootstrap-samples-1---5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 5",
    "text": "Bootstrap samples 1 - 5\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#bootstrap-samples-1---100",
    "href": "slides/04-slr-bootstrap-old.html#bootstrap-samples-1---100",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 100",
    "text": "Bootstrap samples 1 - 100\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#slopes-of-bootstrap-samples",
    "href": "slides/04-slr-bootstrap-old.html#slopes-of-bootstrap-samples",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus ___ dollars.\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#slopes-of-bootstrap-samples-1",
    "href": "slides/04-slr-bootstrap-old.html#slopes-of-bootstrap-samples-1",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional square foot, we expect the sale price of Duke Forest houses to be higher, on average, by $159, plus or minus ___ dollars.\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#confidence-level",
    "href": "slides/04-slr-bootstrap-old.html#confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence level",
    "text": "Confidence level\n\nHow confident are you that the true slope is between $0 and $250? How about $150 and $170? How about $90 and $210?"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#confidence-interval-1",
    "href": "slides/04-slr-bootstrap-old.html#confidence-interval-1",
    "title": "SLR: Simulation-based inference",
    "section": "95% confidence interval",
    "text": "95% confidence interval\n\n\n\nA 95% confidence interval is bounded by the middle 95% of the bootstrap distribution\nWe are 95% confident that for each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by $90.43 to $205.77."
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#computing-the-ci-for-the-slope-i",
    "href": "slides/04-slr-bootstrap-old.html#computing-the-ci-for-the-slope-i",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope I",
    "text": "Computing the CI for the slope I\nCalculate the observed slope:\n\nobserved_fit &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  fit()\n\nobserved_fit\n\n# A tibble: 2 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept  116652.\n2 area          159."
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#computing-the-ci-for-the-slope-ii",
    "href": "slides/04-slr-bootstrap-old.html#computing-the-ci-for-the-slope-ii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope II",
    "text": "Computing the CI for the slope II\nTake 100 bootstrap samples and fit models to each one:\n\nset.seed(1120)\n\nboot_fits &lt;- duke_forest |&gt;\n  specify(price ~ area) |&gt;\n  generate(reps = 100, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_fits\n\n# A tibble: 200 × 3\n# Groups:   replicate [100]\n   replicate term      estimate\n       &lt;int&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1         1 intercept   47819.\n 2         1 area          191.\n 3         2 intercept  144645.\n 4         2 area          134.\n 5         3 intercept  114008.\n 6         3 area          161.\n 7         4 intercept  100639.\n 8         4 area          166.\n 9         5 intercept  215264.\n10         5 area          125.\n# ℹ 190 more rows"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#computing-the-ci-for-the-slope-iii",
    "href": "slides/04-slr-bootstrap-old.html#computing-the-ci-for-the-slope-iii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope III",
    "text": "Computing the CI for the slope III\nPercentile method: Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\" #default method\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#precision-vs.-accuracy",
    "href": "slides/04-slr-bootstrap-old.html#precision-vs.-accuracy",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#precision-vs.-accuracy-1",
    "href": "slides/04-slr-bootstrap-old.html#precision-vs.-accuracy-1",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nHow can we get best of both worlds – high precision and high accuracy?"
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#changing-confidence-level",
    "href": "slides/04-slr-bootstrap-old.html#changing-confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nHow would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?\n\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          92.1     223.\n2 intercept -36765.   296528."
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#changing-confidence-level-1",
    "href": "slides/04-slr-bootstrap-old.html#changing-confidence-level-1",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          104.     212.\n2 intercept  -24380.  256730.\n\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term      lower_ci upper_ci\n  &lt;chr&gt;        &lt;dbl&gt;    &lt;dbl&gt;\n1 area          56.3     226.\n2 intercept -61950.   370395."
  },
  {
    "objectID": "slides/04-slr-bootstrap-old.html#recap",
    "href": "slides/04-slr-bootstrap-old.html#recap",
    "title": "SLR: Simulation-based inference",
    "section": "Recap",
    "text": "Recap\n\nPopulation: Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = \\(N\\))\nSample: Subset of the population, ideally random and representative (sample size = \\(n\\))\nSample statistic \\(\\ne\\) population parameter, but if the sample is good, it can be a good estimate\nStatistical inference: Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\nWe report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\nSince we can’t continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#topics",
    "href": "slides/03-slr-intro-old.html#topics",
    "title": "Simple Linear Regression",
    "section": "Topics",
    "text": "Topics\n\n\nUse simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nEstimate the slope and intercept of the regression line using the least squares method.\nInterpret the slope and intercept of the regression line.\nPredict the response given a value of the predictor variable.\nUse tidymodels to fit and summarize regression models in R."
  },
  {
    "objectID": "slides/03-slr-intro-old.html#computation-set-up",
    "href": "slides/03-slr-intro-old.html#computation-set-up",
    "title": "Simple Linear Regression",
    "section": "Computation set up",
    "text": "Computation set up\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\nlibrary(knitr)          # for formatting tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#movie-scores",
    "href": "slides/03-slr-intro-old.html#movie-scores",
    "title": "Simple Linear Regression",
    "section": "Movie scores",
    "text": "Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#data-prep",
    "href": "slides/03-slr-intro-old.html#data-prep",
    "title": "Simple Linear Regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#data-overview",
    "href": "slides/03-slr-intro-old.html#data-overview",
    "title": "Simple Linear Regression",
    "section": "Data overview",
    "text": "Data overview\n\nglimpse(movie_scores)\n\nRows: 146\nColumns: 23\n$ film                       &lt;chr&gt; \"Avengers: Age of Ultron\", \"Cinderella\", \"A…\n$ year                       &lt;dbl&gt; 2015, 2015, 2015, 2015, 2015, 2015, 2015, 2…\n$ critics                    &lt;int&gt; 74, 85, 80, 18, 14, 63, 42, 86, 99, 89, 84,…\n$ audience                   &lt;int&gt; 86, 80, 90, 84, 28, 62, 53, 64, 82, 87, 77,…\n$ metacritic                 &lt;int&gt; 66, 67, 64, 22, 29, 50, 53, 81, 81, 80, 71,…\n$ metacritic_user            &lt;dbl&gt; 7.1, 7.5, 8.1, 4.7, 3.4, 6.8, 7.6, 6.8, 8.8…\n$ imdb                       &lt;dbl&gt; 7.8, 7.1, 7.8, 5.4, 5.1, 7.2, 6.9, 6.5, 7.4…\n$ fandango_stars             &lt;dbl&gt; 5.0, 5.0, 5.0, 5.0, 3.5, 4.5, 4.0, 4.0, 4.5…\n$ fandango_ratingvalue       &lt;dbl&gt; 4.5, 4.5, 4.5, 4.5, 3.0, 4.0, 3.5, 3.5, 4.0…\n$ rt_norm                    &lt;dbl&gt; 3.70, 4.25, 4.00, 0.90, 0.70, 3.15, 2.10, 4…\n$ rt_user_norm               &lt;dbl&gt; 4.30, 4.00, 4.50, 4.20, 1.40, 3.10, 2.65, 3…\n$ metacritic_norm            &lt;dbl&gt; 3.30, 3.35, 3.20, 1.10, 1.45, 2.50, 2.65, 4…\n$ metacritic_user_nom        &lt;dbl&gt; 3.55, 3.75, 4.05, 2.35, 1.70, 3.40, 3.80, 3…\n$ imdb_norm                  &lt;dbl&gt; 3.90, 3.55, 3.90, 2.70, 2.55, 3.60, 3.45, 3…\n$ rt_norm_round              &lt;dbl&gt; 3.5, 4.5, 4.0, 1.0, 0.5, 3.0, 2.0, 4.5, 5.0…\n$ rt_user_norm_round         &lt;dbl&gt; 4.5, 4.0, 4.5, 4.0, 1.5, 3.0, 2.5, 3.0, 4.0…\n$ metacritic_norm_round      &lt;dbl&gt; 3.5, 3.5, 3.0, 1.0, 1.5, 2.5, 2.5, 4.0, 4.0…\n$ metacritic_user_norm_round &lt;dbl&gt; 3.5, 4.0, 4.0, 2.5, 1.5, 3.5, 4.0, 3.5, 4.5…\n$ imdb_norm_round            &lt;dbl&gt; 4.0, 3.5, 4.0, 2.5, 2.5, 3.5, 3.5, 3.5, 3.5…\n$ metacritic_user_vote_count &lt;int&gt; 1330, 249, 627, 31, 88, 34, 17, 124, 62, 54…\n$ imdb_user_vote_count       &lt;int&gt; 271107, 65709, 103660, 3136, 19560, 39373, …\n$ fandango_votes             &lt;int&gt; 14846, 12640, 12055, 1793, 1021, 397, 252, …\n$ fandango_difference        &lt;dbl&gt; 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5…"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#movie-scores-data",
    "href": "slides/03-slr-intro-old.html#movie-scores-data",
    "title": "Simple Linear Regression",
    "section": "Movie scores data",
    "text": "Movie scores data\nThe data set contains the “Tomatometer” score (critics) and audience score (audience) for 146 movies rated on rottentomatoes.com."
  },
  {
    "objectID": "slides/03-slr-intro-old.html#movie-ratings-data",
    "href": "slides/03-slr-intro-old.html#movie-ratings-data",
    "title": "Simple Linear Regression",
    "section": "Movie ratings data",
    "text": "Movie ratings data\nGoal: Fit a line to describe the relationship between the critics score and audience score.\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#why-fit-a-line",
    "href": "slides/03-slr-intro-old.html#why-fit-a-line",
    "title": "Simple Linear Regression",
    "section": "Why fit a line?",
    "text": "Why fit a line?\nWe fit a line to accomplish one or both of the following:\n\n\nPrediction\n\nWhat is the audience score expected to be for an upcoming movie that received 35% from the critics?\n\n\n\n\nInference\n\nIs the critics score a useful predictor of the audience score? By how much is the audience score expected to change for each additional point in the critics score?"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#terminology",
    "href": "slides/03-slr-intro-old.html#terminology",
    "title": "Simple Linear Regression",
    "section": "Terminology",
    "text": "Terminology\n\n\n\nResponse, Y: variable describing the outcome of interest\nPredictor, X: variable we use to help understand the variability in the response\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#regression-model",
    "href": "slides/03-slr-intro-old.html#regression-model",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\nA regression model is a function that describes the relationship between the response, \\(Y\\), and the predictor, \\(X\\).\n\\[\\begin{aligned} Y &= \\color{black}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{black}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{black}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#regression-model-1",
    "href": "slides/03-slr-intro-old.html#regression-model-1",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\text{Error} \\\\[8pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\epsilon \\\\[8pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\epsilon \\end{aligned}\\]\n\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\n\\(\\mu_{Y|X}\\) is the mean value of \\(Y\\) given a particular value of \\(X\\)."
  },
  {
    "objectID": "slides/03-slr-intro-old.html#regression-model-2",
    "href": "slides/03-slr-intro-old.html#regression-model-2",
    "title": "Simple Linear Regression",
    "section": "Regression model",
    "text": "Regression model\n\n\n\\[\n\\begin{aligned} Y &= \\color{purple}{\\textbf{Model}} + \\color{blue}{\\textbf{Error}} \\\\[5pt]\n&= \\color{purple}{\\mathbf{f(X)}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n&= \\color{purple}{\\boldsymbol{\\mu_{Y|X}}} + \\color{blue}{\\boldsymbol{\\epsilon}} \\\\[5pt]\n\\end{aligned}\n\\]\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#slr-statistical-model",
    "href": "slides/03-slr-intro-old.html#slr-statistical-model",
    "title": "Simple Linear Regression",
    "section": "SLR: Statistical model",
    "text": "SLR: Statistical model\nWhen we have a quantitative response, \\(Y\\), and a single quantitative predictor, \\(X\\), we can use a simple linear regression model to describe the relationship between \\(Y\\) and \\(X\\). \\[\\Large{Y = \\mathbf{\\beta_0 + \\beta_1 X} + \\epsilon}\\]\n\n\n\\(\\beta_1\\): True slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\beta_0\\): True intercept of the relationship between \\(X\\) and \\(Y\\)\n\\(\\epsilon\\): Error"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#slr-regression-equation",
    "href": "slides/03-slr-intro-old.html#slr-regression-equation",
    "title": "Simple Linear Regression",
    "section": "SLR: Regression equation",
    "text": "SLR: Regression equation\n\\[\\Large{\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X}\\]\n\n\\(\\hat{\\beta}_1\\): Estimated slope of the relationship between \\(X\\) and \\(Y\\)\n\\(\\hat{\\beta}_0\\): Estimated intercept of the relationship between \\(X\\) and \\(Y\\)\nNo error term!"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "href": "slides/03-slr-intro-old.html#choosing-values-for-hatbeta_1-and-hatbeta_0",
    "title": "Simple Linear Regression",
    "section": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)",
    "text": "Choosing values for \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_0\\)\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "slides/03-slr-intro-old.html#residuals",
    "href": "slides/03-slr-intro-old.html#residuals",
    "title": "Simple Linear Regression",
    "section": "Residuals",
    "text": "Residuals\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\\[\\text{residual} = \\text{observed} - \\text{predicted} = y_i - \\hat{y}_i\\]"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#least-squares-line",
    "href": "slides/03-slr-intro-old.html#least-squares-line",
    "title": "Simple Linear Regression",
    "section": "Least squares line",
    "text": "Least squares line\n\nThe residual for the \\(i^{th}\\) observation is\n\n\\[e_i = \\text{observed} - \\text{predicted}\n= y_i - \\hat{y}_i\\]\n\nThe sum of squared residuals is\n\n\\[e^2_1 + e^2_2 + \\dots + e^2_n\\]\n\nThe least squares line is the one that minimizes the sum of squared residuals"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#properties-of-least-squares-regression",
    "href": "slides/03-slr-intro-old.html#properties-of-least-squares-regression",
    "title": "Simple Linear Regression",
    "section": "Properties of least squares regression",
    "text": "Properties of least squares regression\n\n\nThe regression line goes through the center of mass point, the coordinates corresponding to average \\(X\\) and average \\(Y\\): \\(\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}\\)\nThe slope has the same sign as the correlation coefficient: \\(\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}\\)\nThe sum of the residuals is approximately zero: \\(\\sum_{i = 1}^n e_i \\approx 0\\)\nThe residuals and \\(X\\) values are uncorrelated"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#estimating-the-slope",
    "href": "slides/03-slr-intro-old.html#estimating-the-slope",
    "title": "Simple Linear Regression",
    "section": "Estimating the slope",
    "text": "Estimating the slope\n\\[\\large{\\hat{\\beta}_1 = r \\frac{s_Y}{s_X}}\\]\n\n\n\\[\\begin{aligned}\ns_X &= 30.1688 \\\\\ns_Y &=  20.0244 \\\\\nr &= 0.7814\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\n\\hat{\\beta}_1 &= 0.7814 \\times \\frac{20.0244}{30.1688} \\\\\n&= 0.5187\\end{aligned}\\]\n\n\n\n\nClickhere for details on deriving the equations for slope and intercept."
  },
  {
    "objectID": "slides/03-slr-intro-old.html#estimating-the-intercept",
    "href": "slides/03-slr-intro-old.html#estimating-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Estimating the intercept",
    "text": "Estimating the intercept\n\\[\\large{\\hat{\\beta}_0 = \\bar{Y} - \\hat{\\beta}_1\\bar{X}}\\]\n\n\n\\[\\begin{aligned}\n&\\bar{x} = 60.8493 \\\\\n&\\bar{y} = 63.8767 \\\\\n&\\hat{\\beta}_1 = 0.5187\n\\end{aligned}\\]\n\n\\[\\begin{aligned}\\hat{\\beta}_0 &= 63.8767 - 0.5187 \\times 60.8493 \\\\\n&= 32.3142\n\\end{aligned}\\]\n\n\n\n\nClick here for details on deriving the equations for slope and intercept."
  },
  {
    "objectID": "slides/03-slr-intro-old.html#interpretation",
    "href": "slides/03-slr-intro-old.html#interpretation",
    "title": "Simple Linear Regression",
    "section": "Interpretation",
    "text": "Interpretation\n\n\nPost your answers to the following questions on Ed Discussion:\n\nThe slope of the model for predicting audience score from critics score is 0.5187 . Which of the following is the best interpretation of this value?\n32.3142 is the predicted mean audience score for what type of movies?\n\n\n\n\nLink for Section 001 (10:05am lecture)\nLink for Section 002 (1:25pm lecture)\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#does-it-make-sense-to-interpret-the-intercept",
    "href": "slides/03-slr-intro-old.html#does-it-make-sense-to-interpret-the-intercept",
    "title": "Simple Linear Regression",
    "section": "Does it make sense to interpret the intercept?",
    "text": "Does it make sense to interpret the intercept?\n\n✅ The intercept is meaningful in the context of the data if\n\nthe predictor can feasibly take values equal to or near zero, or\nthere are values near zero in the observed data.\n\n\n\n🛑 Otherwise, the intercept may not be meaningful!"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#making-a-prediction",
    "href": "slides/03-slr-intro-old.html#making-a-prediction",
    "title": "Simple Linear Regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that a movie has a critics score of 70. According to this model, what is the movie’s predicted audience score?\n\\[\\begin{aligned}\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics} \\\\\n&= 32.3142 + 0.5187 \\times 70 \\\\\n&= 68.6232\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#tidymodels",
    "href": "slides/03-slr-intro-old.html#tidymodels",
    "title": "Simple Linear Regression",
    "section": "tidymodels",
    "text": "tidymodels\n\nThe tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.2.1     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#why-tidymodels",
    "href": "slides/03-slr-intro-old.html#why-tidymodels",
    "title": "Simple Linear Regression",
    "section": "Why tidymodels?",
    "text": "Why tidymodels?\n\nConsistent syntax for different model types (linear, logistic, random forest, Bayesian, etc.)\nStreamline modeling workflow\n\nSplit data into train and test sets\nTransform and create new variables\nAssess model performance\nUse model for prediction and inference"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#step-1-specify-model",
    "href": "slides/03-slr-intro-old.html#step-1-specify-model",
    "title": "Simple Linear Regression",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#step-2-set-model-fitting-engine",
    "href": "slides/03-slr-intro-old.html#step-2-set-model-fitting-engine",
    "title": "Simple Linear Regression",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#a-closer-look-at-the-regression-output",
    "href": "slides/03-slr-intro-old.html#a-closer-look-at-the-regression-output",
    "title": "Simple Linear Regression",
    "section": "A closer look at the regression output",
    "text": "A closer look at the regression output\n\nmovie_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores)\n\nmovie_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187  \n\n\n\\[\\widehat{\\text{audience}} = 32.3155 + 0.5187 \\times \\text{critics}\\]\n\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is just due to rounding in the hand calculation."
  },
  {
    "objectID": "slides/03-slr-intro-old.html#the-regression-output",
    "href": "slides/03-slr-intro-old.html#the-regression-output",
    "title": "Simple Linear Regression",
    "section": "The regression output",
    "text": "The regression output\nWe’ll focus on the first column for now…\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores) |&gt;\n  tidy() \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#format-output-with-kable",
    "href": "slides/03-slr-intro-old.html#format-output-with-kable",
    "title": "Simple Linear Regression",
    "section": "Format output with kable",
    "text": "Format output with kable\nUse the kable function from the knitr package to produce a table and specify number of significant digits\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores) |&gt;\n  tidy() |&gt;\n  kable(digits = 4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n32.3155\n2.3425\n13.7953\n0\n\n\ncritics\n0.5187\n0.0345\n15.0281\n0"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#prediction-1",
    "href": "slides/03-slr-intro-old.html#prediction-1",
    "title": "Simple Linear Regression",
    "section": "Prediction",
    "text": "Prediction\n\n# create a data frame for a new movie\nnew_movie &lt;- tibble(critics = 70)\n\n# predict the outcome for a new movie\npredict(movie_fit, new_movie)\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1  68.6"
  },
  {
    "objectID": "slides/03-slr-intro-old.html#recap",
    "href": "slides/03-slr-intro-old.html#recap",
    "title": "Simple Linear Regression",
    "section": "Recap",
    "text": "Recap\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nUsed the least squares method to estimate the slope and intercept.\nInterpreted the slope and intercept.\n\nSlope: For every one unit increase in \\(x\\), we expect y to change by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units\n\nPredicted the response given a value of the predictor variable.\nUsed tidymodels to fit and summarize regression models in R.\n\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/02-big-picture.html#topics",
    "href": "slides/02-big-picture.html#topics",
    "title": "The big picture",
    "section": "Topics",
    "text": "Topics\n\nData analysis life cycle\nReproducible data analysis\n\n\n\n\n\nSource: R for Data Science with additions from The Art of Statistics: How to Learn from Data.\n\n\n\n\n\n\nSource:R for Data Science"
  },
  {
    "objectID": "slides/02-big-picture.html#reproducibility-checklist",
    "href": "slides/02-big-picture.html#reproducibility-checklist",
    "title": "The big picture",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n. . .\nNear term goals:\n✔️ Can the tables and figures be exactly reproduced from the code and data?\n✔️ Does the code actually do what you think it does?\n✔️ In addition to what was done, is it clear why it was done?\n. . .\nLong term goals:\n✔️ Can the code be used for other data?\n✔️ Can you extend the code to do other things?"
  },
  {
    "objectID": "slides/02-big-picture.html#why-is-reproducibility-important",
    "href": "slides/02-big-picture.html#why-is-reproducibility-important",
    "title": "The big picture",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)"
  },
  {
    "objectID": "slides/02-big-picture.html#when-things-go-wrong",
    "href": "slides/02-big-picture.html#when-things-go-wrong",
    "title": "The big picture",
    "section": "When things go wrong",
    "text": "When things go wrong\n\n\n\nReproducibility error\nConsequence\nSource(s)\n\n\n\n\nLimitations in Excel data formats\nLoss of 16,000 COVID case records in the UK\n(Kelion 2020)\n\n\nAutomatic formatting in Excel\nImportant genes disregarded in scientific studies\n(Ziemann, Eren, and El-Osta 2016)\n\n\nDeletion of a cell caused rows to shift\nMix-up of which patient group received the treatment\n(Wallensteen et al. 2018)\n\n\nUsing binary instead of explanatory labels\nMix-up of the intervention with the control group\n(Aboumatar and Wise 2019)\n\n\nUsing the same notation for missing data and zero values\nPaper retraction\n(Whitehouse et al. 2021)\n\n\nIncorrectly copying data in a spreadsheet\nDelay in the opening of a hospital\n(Picken 2020)\n\n\n\nSource: Ostblom and Timbers (2022)"
  },
  {
    "objectID": "slides/02-big-picture.html#toolkit",
    "href": "slides/02-big-picture.html#toolkit",
    "title": "The big picture",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub (Beyond the scope of this course)"
  },
  {
    "objectID": "slides/02-big-picture.html#r-and-rstudio",
    "href": "slides/02-big-picture.html#r-and-rstudio",
    "title": "The big picture",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\n\n\nSource: Statistical Inference via Data Science"
  },
  {
    "objectID": "slides/02-big-picture.html#rstudio-ide",
    "href": "slides/02-big-picture.html#rstudio-ide",
    "title": "The big picture",
    "section": "RStudio IDE",
    "text": "RStudio IDE"
  },
  {
    "objectID": "slides/02-big-picture.html#quarto",
    "href": "slides/02-big-picture.html#quarto",
    "title": "The big picture",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports – the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)"
  },
  {
    "objectID": "slides/02-big-picture.html#quarto-1",
    "href": "slides/02-big-picture.html#quarto-1",
    "title": "The big picture",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "slides/02-big-picture.html#how-will-we-use-quarto",
    "href": "slides/02-big-picture.html#how-will-we-use-quarto",
    "title": "The big picture",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou’ll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester"
  },
  {
    "objectID": "slides/02-big-picture.html#rest-of-class",
    "href": "slides/02-big-picture.html#rest-of-class",
    "title": "The big picture",
    "section": "Rest of class",
    "text": "Rest of class\n\nWork on HW 0!"
  },
  {
    "objectID": "slides/02-big-picture.html#references",
    "href": "slides/02-big-picture.html#references",
    "title": "The big picture",
    "section": "References",
    "text": "References\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule\n\n\n\n\nAlexander, Rohan. 2023. “Telling Stories with Data,” June. https://doi.org/10.1201/9781003229407.\n\n\nOstblom, Joel, and Tiffany Timbers. 2022. “Opinionated Practices for Teaching Reproducibility: Motivation, Guided Instruction and Practice.” Journal of Statistics and Data Science Education 30 (3): 241–50. https://doi.org/10.1080/26939169.2022.2074922."
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-friedlander",
    "href": "slides/01-welcome.html#meet-prof.-friedlander",
    "title": "Welcome to MAT 212!",
    "section": "",
    "text": "Education and career journey\n\nGrew up outside New York City\nBS in Math & Statistics from Rice University (Houston, TX)\nBusiness Analyst at Capital One (Plano, TX)\nMS and PhD in Statistics & Operations Research from UNC-Chapel Hill\nPostdoc in Population Genetics at University of Chicago\nAssistant Professor of Math at St. Norbert College (Green Bay, WI)\n\nWork focuses on statistics education, queueing theory, and population genetics\nBig sports fan: NY Knicks, Giants, Rangers, Yankess, UNC Tarheels\nDad of three cute dogs: Allie, Miriam, Tony"
  },
  {
    "objectID": "slides/01-welcome.html#meet-prof.-friedlander-1",
    "href": "slides/01-welcome.html#meet-prof.-friedlander-1",
    "title": "Welcome to MAT 212!",
    "section": "Meet Prof. Friedlander!",
    "text": "Meet Prof. Friedlander!"
  },
  {
    "objectID": "slides/01-welcome.html#tell-me-about-yourself",
    "href": "slides/01-welcome.html#tell-me-about-yourself",
    "title": "Welcome to MAT 212!",
    "section": "",
    "text": "Navigate to Poll Everywhere and answer the survey questions. Note that your answers will not be visible by anyone but myself."
  },
  {
    "objectID": "slides/01-welcome.html#what-is-a-model",
    "href": "slides/01-welcome.html#what-is-a-model",
    "title": "Welcome to MAT 212!",
    "section": "What is a model?",
    "text": "What is a model?\n\n\nDATA = MODEL + ERROR\nDATA = PATTERN + DEPARTURES FROM PATTERN\n\nHow do we identify the actual pattern?\n\nGOAL: Find a model for a relationship between a response/outcome/target variable \\(Y\\) and one (or more) explanatory/predictor variables (\\(X_1,\\ldots,X_k\\))\nModels are a simplified but tractable version of reality\n\n\n\nWhat are response and explanatory variables?"
  },
  {
    "objectID": "slides/01-welcome.html#geoge-e.-p.-box",
    "href": "slides/01-welcome.html#geoge-e.-p.-box",
    "title": "Welcome to MAT 212!",
    "section": "Geoge E. P. Box",
    "text": "Geoge E. P. Box\n\n\n\nFrom wikipedia: British statistician, who worked in the areas of quality control, time-series analysis, design of experiments, and Bayesian inference. He has been called “one of the great statistical minds of the 20th century”.\n“all models are wrong, but some are useful”"
  },
  {
    "objectID": "slides/01-welcome.html#why-build-a-model",
    "href": "slides/01-welcome.html#why-build-a-model",
    "title": "Welcome to MAT 212!",
    "section": "Why build a model?",
    "text": "Why build a model?\n\nMaking predictions\nUnderstanding relationships\nAssessing differences"
  },
  {
    "objectID": "slides/01-welcome.html#what-is-regression-analysis",
    "href": "slides/01-welcome.html#what-is-regression-analysis",
    "title": "Welcome to MAT 212!",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\n\n“In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or ‘predictors’). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‘criterion variable’) changes when any one of the independent variables is varied, while the other independent variables are held fixed.”\n\nSource: Wikipedia (previous definition)\n\nNote: I don’t really like the terms “independent” and “dependent” variables"
  },
  {
    "objectID": "slides/01-welcome.html#examples-of-regression-in-practice",
    "href": "slides/01-welcome.html#examples-of-regression-in-practice",
    "title": "Welcome to MAT 212!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight’s 2020 Presidential Forecast Works — And What’s Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it’s so freaking hard to make a good COVID-19 model (from March 2020)"
  },
  {
    "objectID": "slides/01-welcome.html#course-faq",
    "href": "slides/01-welcome.html#course-faq",
    "title": "Welcome to MAT 212!",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What background is assumed for the course?\nA - Introductory statistics or previous experience with mathematics at a level that would allow you to learn intro stats concepts relatively easily\n. . .\nQ - Will we be doing computing?\nA - Yes. We will use the computing language R for analysis and Quarto for writing up results.\n. . .\nQ - Am I expected to have experience using any of these tools?\nA - No. I do not expect you to have any exposure to R and certainly not Quarto.\n. . .\nQ - Will we learn the mathematical theory of regression?\nA - Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression."
  },
  {
    "objectID": "slides/01-welcome.html#course-learning-objectives",
    "href": "slides/01-welcome.html#course-learning-objectives",
    "title": "Welcome to MAT 212!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to…\n\nanalyze real-world data to answer questions about multivariable relationships.\nuse R to fit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate statistical results through writing and oral presentations."
  },
  {
    "objectID": "slides/01-welcome.html#course-topics",
    "href": "slides/01-welcome.html#course-topics",
    "title": "Welcome to MAT 212!",
    "section": "Course Topics",
    "text": "Course Topics\n\nWhat is a quantitative and what is a categorical variable?\n\n. . .\n\n\n\nChapter\nResponse\nPredictor/Explanatory\n\n\n\n\n1-2\nQuantitative\nSingle Quantitative\n\n\n3-4\nQuantitative\nMultiple Quantitative\n\n\n5\nQuantitative\nSingle Categorical\n\n\n6-8\nQuantitative\nMultiple Categorical\n\n\n9\nCategorical\nSingle Quant/Cat\n\n\n10\nCategorical\nMultiple Quant/Cat\n\n\n11\nBoth\nBoth"
  },
  {
    "objectID": "slides/01-welcome.html#course-toolkit",
    "href": "slides/01-welcome.html#course-toolkit",
    "title": "Welcome to MAT 212!",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nCourse website: mat212fa24.netlify.app\n\nCentral hub for the course!\nTour of the website\n\nCanvas: cofi.instructure.com\n\nGradebook\nAnnouncements"
  },
  {
    "objectID": "slides/01-welcome.html#computing-toolkit",
    "href": "slides/01-welcome.html#computing-toolkit",
    "title": "Welcome to MAT 212!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nWrite reproducible reports in Quarto\nAccess RStudio through College of Idaho posit Workbench\nUse your College of Idaho email and password"
  },
  {
    "objectID": "slides/01-welcome.html#prepare-participate-practice-perform",
    "href": "slides/01-welcome.html#prepare-participate-practice-perform",
    "title": "Welcome to MAT 212!",
    "section": "Prepare, Participate, Practice, Perform",
    "text": "Prepare, Participate, Practice, Perform\n\n\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you’ve learned to analyze real-world data\n\nHomework assignments (individual)\nTwo exams\nOral R quiz\nFinal group project"
  },
  {
    "objectID": "slides/01-welcome.html#grading",
    "href": "slides/01-welcome.html#grading",
    "title": "Welcome to MAT 212!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n25%\n\n\nFinal project\n30%\n\n\nExam 01\n15%\n\n\nExam 02\n20%\n\n\nApplication Exercises\n5%\n\n\nOral R Quiz\n5%\n\n\n\nSee the syllabus for details on how the final letter grade will be calculated."
  },
  {
    "objectID": "slides/01-welcome.html#support",
    "href": "slides/01-welcome.html#support",
    "title": "Welcome to MAT 212!",
    "section": "Support",
    "text": "Support\n\nAttend office hours to meet with Prof. Friedlander (Boone 126B)\n\nM: 2:30-3:30pm\nT: 9:45-10:45am\nW: 1:30-2:30pm\nTH: 1-2pm\n\nAsk and answer questions on course discussion forum\nUse email for questions regarding personal matters and/or grades\nSee the Course Support page for more details"
  },
  {
    "objectID": "slides/01-welcome.html#late-homework",
    "href": "slides/01-welcome.html#late-homework",
    "title": "Welcome to MAT 212!",
    "section": "Late Homework",
    "text": "Late Homework\n\nThere will be a 5% deduction for each 24-hour period the assignment is late for the first two days. After 2 days, students will receive a 30% reduction. No homework will be accepted after it is returned to the class"
  },
  {
    "objectID": "slides/01-welcome.html#late-application-exercises",
    "href": "slides/01-welcome.html#late-application-exercises",
    "title": "Welcome to MAT 212!",
    "section": "Late Application Exercises",
    "text": "Late Application Exercises\n\nAEs are due three days after the class they are assigned. No late work is accepted for application exercises, since these are designed as in-class activities to help you prepare for homework."
  },
  {
    "objectID": "slides/01-welcome.html#late-exams-and-final-project",
    "href": "slides/01-welcome.html#late-exams-and-final-project",
    "title": "Welcome to MAT 212!",
    "section": "Late Exams and Final Project",
    "text": "Late Exams and Final Project\n\nIf an application exercise, exam, or final project must be missed due to a school-sponsored event, you must let me know at least a week ahead of time so that we can schedule a time for you to make up the work before you leave. If you must miss a exam or the final project due to illness, you must let me know before class that day so that we can schedule a time for you to take a make-up quiz or exam. Failure to adhere to this policy will result in a 35% penalty the corresponding assignment."
  },
  {
    "objectID": "slides/01-welcome.html#academic-integrity",
    "href": "slides/01-welcome.html#academic-integrity",
    "title": "Welcome to MAT 212!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nThe College of Idaho maintains that academic honesty and integrity are essential values in the educational process. Operating under an Honor Code philosophy, the College expects conduct rooted in honesty, integrity, and understanding, allowing members of a diverse student body to live together and interact and learn from one another in ways that protect both personal freedom and community standards. Violations of academic honesty are addressed primarily by the instructor and may be referred to the Student Judicial Board.\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the College of Idaho Honor Code."
  },
  {
    "objectID": "slides/01-welcome.html#collaboration-sharing-code",
    "href": "slides/01-welcome.html#collaboration-sharing-code",
    "title": "Welcome to MAT 212!",
    "section": "Collaboration & sharing code",
    "text": "Collaboration & sharing code\n\nI have policies!\nLet’s read about them in the Academic honesty section of the syllabus"
  },
  {
    "objectID": "slides/01-welcome.html#use-of-artificial-intelligence-ai",
    "href": "slides/01-welcome.html#use-of-artificial-intelligence-ai",
    "title": "Welcome to MAT 212!",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nYou should treat AI tools, such as ChatGPT, the same as other online resources.\nThere are two guiding principles that govern how you can use AI in this course:1\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity."
  },
  {
    "objectID": "slides/01-welcome.html#use-of-artificial-intelligence-ai-1",
    "href": "slides/01-welcome.html#use-of-artificial-intelligence-ai-1",
    "title": "Welcome to MAT 212!",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code.\n❌ No AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments.\n\n\n\n\n\n\nImportant\n\n\n\nIn general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content."
  },
  {
    "objectID": "slides/01-welcome.html#five-tips-for-success",
    "href": "slides/01-welcome.html#five-tips-for-success",
    "title": "Welcome to MAT 212!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions.\nDo the homework; get started early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Canvas and sent via email."
  },
  {
    "objectID": "slides/01-welcome.html#for-this-week",
    "href": "slides/01-welcome.html#for-this-week",
    "title": "Welcome to MAT 212!",
    "section": "For this week…",
    "text": "For this week…\n\nRead the syllabus\nHomework 0:\n\nStation 1\nStation 2\nStation 3\n\nFriday’s lecture: The Big Picture"
  },
  {
    "objectID": "slides/01-welcome.html#application-exercise",
    "href": "slides/01-welcome.html#application-exercise",
    "title": "Welcome to MAT 212!",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 AE 01 - Movie Budgets and Revenues"
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "This page contains an outline of the topics, content, and assignments for the semester. Note that this schedule will be updated as the semester progresses, with all changes documented here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nweek\ndow\ndate\ntopic\nprepare\nslides\nae\nhw\nproject\nnotes\n\n\n\n\n1\nW\nAug 21\nWelcome to MAT 212\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nAug 23\nThe big picture\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2\nM\nAug 26\nMultivariate Relationships\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW\nAug 28\nSimple linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 00 Due/ HW 01 Assigned\n\n\n\nF\nAug 30\nSLR - Prediction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3\nM\nSep 2\nNO CLASS: LABOR DAY\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW\nSep 4\nSLR: Bootstrap confidence intervals\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 02 Assigned\n\n\n\nF\nSep 6\nSLR: Finishing Bootstrap, starting Randomization test for the slope\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 01 Due\n\n\n4\nM\nSep 9\nSLR: Mathematical models for inference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW\nSep 11\nSLR: Mathematical models for inference continued\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nSep 13\nSLR: Model conditions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 02 Due/ HW 03 Assigned\n\n\n5\nM\nSep 16\nSLR: Model conditions + Evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW\nSep 18\nSLR: Model Evaluation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nSep 20\nSLR: Transformations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 03 Due/ HW 04 Assigned\n\n\n6\nM\nSep 23\nProject Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject Stage I Assigned\n\n\n\nW\nSep 25\nSLR: Evaluations + Transformation Activities\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nSep 27\nSLR: Outliers\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 04 Due/HW 05 Assigned\n\n\n7\nM\nSep 30\nMultiple linear regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW\nOct 2\nMLR: Interaction Terms\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nOct 4\nInference for MLR\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHW 05 Due/ HW 06 Assigned\n\n\n8\nM\nOct 7\nCategorical Predictors\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW\nOct 9\nExam 01 Review\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nOct 11\nExam 01\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject proposal due (accepted without penalty until Monday the 14th)\n\n\n9\nM\nOct 14\nNO CLASS: FALL BREAK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProject accepted until today\n\n\n\nW\nOct 16\nNO CLASS: FALL BREAK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nOct 18\nNO CLASS: FALL BREAK\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n10\nM\nOct 21\nMLR: Conditions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nW\nOct 23\nMLR: Model Comparison\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nF\nOct 25\nMLR: Model Comparison Continued",
    "crumbs": [
      "Schedule"
    ]
  },
  {
    "objectID": "project-rubric.html",
    "href": "project-rubric.html",
    "title": "Final paper rubric",
    "section": "",
    "text": "Applied Stats Individual Project Evaluation\nPossible\nScore\n\n\n\n\nIntroduction and motivation for the project – 8 pts\n\n\n\n\nIs the overarching question motivating the analysis clearly stated and explained? Are the specific questions this analysis will address clearly stated and explained?\n4\n\n\n\nIs relevant literature described and properly cited? Is the connection between the literature and this work made clear?\n4\n\n\n\nMethods – 33 pts\n\n\n\n\nIs the data source described and referenced?\n3\n\n\n\nIs each variable used in the analysis defined, including: - descriptions of the levels for categorical variables - units for numerical variables\n6\n\n\n\nAre data preparation and cleaning procedures clearly described so that the study could be replicated by another researcher? Is the handling of missing data/unusual observations/outliers described and is there sound rationale for it? Is the initial and final number of observations in the analysis reported and are those that were removed accounted for?\n4\n\n\n\nIf necessary, were interactions and data transformations explored? Were these included in the model when appropriate? Were the conditions of the model checked, and were they met?\n10\n\n\n\nWas thoughtful variable selection employed to decide upon the final model? Was the final model compared with simpler options, and the form of the final model justified?\n10\n\n\n\nResults – 20 pts\n\n\n\n\nAre descriptive statistics provided for key variables? Are they presented in a tabular form that is concise and informative?\n5\n\n\n\nAre results of modeling reported in appropriate tabular form with parameter estimates, standard errors, p-values, and confidence intervals?\n5\n\n\n\nAre parameter estimates and CIs correctly interpreted in the context of the research question?\n5\n\n\n\nHow well are graphics/figures/equations presented? (axes labeled, captions, placement, readable, useful, add to content)\n5\n\n\n\nDiscussion – 21 pts\n\n\n\n\nDoes the discussion begin with an accurate summary statement of the major findings of the analysis? Are \\(R^2\\) or misclassification rate reported (either here or in the Results)?\n4\n\n\n\nAre the possible implications of the results discussed in the context of the research question?\n3\n\n\n\nIs there a thoughtful discussion of generalizability of the results? Are limitations of the study identified and is the potential impact on the conclusions stated?\n4\n\n\n\nAre strengths and weaknesses of the analysis identified, including a discussion of model conditions?\n4\n\n\n\nAre potential confounding variables discussed?\n3\n\n\n\nAre suggestions for future research identified and useful?\n3\n\n\n\nOverall – Report – 18 pts\n\n\n\n\nWere research question(s) complex and insightful, demonstrating understanding of the research topic?\n6\n\n\n\nHow well does the author demonstrate an understanding of the statistics? Are statements and conclusions accurate?\n6\n\n\n\nHow well-written is the report (e.g., correct grammar, spelling, etc.)?\n6\n\n\n\n(Tentative) Total\n100\n\n\n\nSupporting Materials (points to be deleted)\n\n\n\n\nBibliography included; citations are stylistically consistent.\n-3\n\n\n\nTitle and author appropriately identified Paper follows formatting guidelines (1.5 spacing, 8 page max, etc.)\n-5\n\n\n\nGrant Total",
    "crumbs": [
      "Project",
      "Rubric"
    ]
  },
  {
    "objectID": "prepare/week-02.html",
    "href": "prepare/week-02.html",
    "title": "August 28 lecture",
    "section": "",
    "text": "Stat2 Textbook: Chapter 1.1 The Simple Linear Regression Model"
  },
  {
    "objectID": "prepare/week-02.html#assigned-reading",
    "href": "prepare/week-02.html#assigned-reading",
    "title": "August 28 lecture",
    "section": "",
    "text": "Stat2 Textbook: Chapter 1.1 The Simple Linear Regression Model"
  },
  {
    "objectID": "prepare/week-02.html#additional-resources-optional",
    "href": "prepare/week-02.html#additional-resources-optional",
    "title": "August 28 lecture",
    "section": "Additional resources (optional)",
    "text": "Additional resources (optional)\n📖 Introduction to Regression Analysis: A Data Science Approach: Ch 4: Simple linear regression\n📖 Introduction to Modern Statistics (IMS) Ch 7: Linear regression with a single predictor"
  },
  {
    "objectID": "prepare/sep-13.html",
    "href": "prepare/sep-13.html",
    "title": "September 13 lecture",
    "section": "",
    "text": "📖 IMS, Sec 24.2: Randomization test for the slope\n📖 IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "prepare/sep-13.html#assigned-reading",
    "href": "prepare/sep-13.html#assigned-reading",
    "title": "September 13 lecture",
    "section": "",
    "text": "📖 IMS, Sec 24.2: Randomization test for the slope\n📖 IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "prepare/oct-09.html",
    "href": "prepare/oct-09.html",
    "title": "October 09 & 11 lectures",
    "section": "",
    "text": "📖 Tidy Modeling withr R, Ch 8:Feature engineering with recipes"
  },
  {
    "objectID": "prepare/oct-09.html#assigned-reading",
    "href": "prepare/oct-09.html#assigned-reading",
    "title": "October 09 & 11 lectures",
    "section": "",
    "text": "📖 Tidy Modeling withr R, Ch 8:Feature engineering with recipes"
  },
  {
    "objectID": "prepare/bootstrap.html",
    "href": "prepare/bootstrap.html",
    "title": "September 4 lecture",
    "section": "",
    "text": "Stat2: Topic 4.6 Randomizatoin Test for Relationship\nStat2: Topic 4.7 Bootstrap for Regression"
  },
  {
    "objectID": "prepare/bootstrap.html#assigned-reading",
    "href": "prepare/bootstrap.html#assigned-reading",
    "title": "September 4 lecture",
    "section": "",
    "text": "Stat2: Topic 4.6 Randomizatoin Test for Relationship\nStat2: Topic 4.7 Bootstrap for Regression"
  },
  {
    "objectID": "prepare/bootstrap.html#recommended-reading-optional",
    "href": "prepare/bootstrap.html#recommended-reading-optional",
    "title": "September 4 lecture",
    "section": "Recommended reading (optional)",
    "text": "Recommended reading (optional)\n📖 IMS, Sec 24.2: Randomization test for the slope\n📖 IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "labs/lab-06.html",
    "href": "labs/lab-06.html",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, November 03 , 11:59pm (Tuesday labs)\nSunday, November 05, 11:59pm (Thursday labs)"
  },
  {
    "objectID": "labs/lab-06.html#introduction",
    "href": "labs/lab-06.html#introduction",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Introduction",
    "text": "Introduction\nIn this assignment, you’ll get to put into practice the logistic regression skills you’ve developed to analyze data about Palmer Penguins. You will also practice handling merge conflicts."
  },
  {
    "objectID": "labs/lab-06.html#learning-goals",
    "href": "labs/lab-06.html#learning-goals",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will be able to\n\nconduct exploratory data analysis for logistic regression\nfit logistic regression models and write the regression equation\nuse the model to calculate predicted probabilities\ncontinue developing a collaborative workflow with your teammates\ndevelop strategies for dealing with merge conflicts"
  },
  {
    "objectID": "labs/lab-06.html#merge-conflicts-uh-oh",
    "href": "labs/lab-06.html#merge-conflicts-uh-oh",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Merge Conflicts (uh oh)",
    "text": "Merge Conflicts (uh oh)\n\n\n\nCollaboration with no merge conflict\n\n\n\n\n\nCollaboration with a merge conflict\n\n\nYou may have seen something like the image above through the course of your collaboration. When two collaborators make changes to a file and push the file to their repository, git merges these two files.\nIf these two files have conflicting content on the same line, git will produce a merge conflict. Merge conflicts need to be resolved manually, as they require a human intervention:\n\n\n\nMerge conflict identifiers\n\n\nTo resolve the merge conflict, decide if you want to keep only your text, the text on GitHub, or incorporate changes from both texts. Delete the conflict markers &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt; and make the changes you want in the final merge.\nAssign numbers 1, 2, 3, and 4 to each of your team members (if only 3 team members, just number 1 through 3). Go through the following steps in detail, which simulate a merge conflict. Completing this exercise will be part of the lab grade.\n\nResolving a merge conflict\nStep 1: Everyone clone the merge-conflict assignment repo in RStudio and open file merge-conflict.Rmd.\nMember 4 should look at the group’s repo on GitHub.com to ensure that the other members’ files are pushed to GitHub after every step.\nStep 2: Member 1 Change the team name to your team name. Render, commit, and push.\nStep 3: Member 2 Change the team name to something different (i.e., not your team name). Render, commit, and push.\nYou should get an error.\nPull and review the document with the merge conflict. Read the error to your teammates. You can also show them the error by sharing your screen. A merge conflict occurred because you edited the same part of the document as Member 1. Resolve the conflict with whichever name you want to keep, then render, commit and push again.\nStep 4: Member 3 Write some narrative in the space provided. You should get an error.\nThis time, no merge conflicts should occur, since you edited a different part of the document from Members 1 and 2. Read the error to your teammates. You can also show them the error by sharing your screen.\nClick to pull. Then render, commit, and push.\nPlease ask your TA if you have any questions about merge conflicts and collaborating in GitHub."
  },
  {
    "objectID": "labs/lab-06.html#getting-started",
    "href": "labs/lab-06.html#getting-started",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-fa23 organization on GitHub. Click on the repo with the prefix lab-05. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo.\nDo not make any changes to the .qmd file until the instructions tell you do to so."
  },
  {
    "objectID": "labs/lab-06.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-06.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nThere are no Team Member markers in this lab; however, you should use a similar workflow as in Lab 04. Only one person should type in the group’s .qmd file at a time. Once that person has finished typing the group’s responses, they should render, commit, and push the changes to GitHub. All other teammates can pull to see the updates in RStudio.\nEvery teammate must have at least one commit in the lab. Everyone is expected to contribute to discussion even when they are not typing."
  },
  {
    "objectID": "labs/lab-06.html#packages",
    "href": "labs/lab-06.html#packages",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(palmerpenguins)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "labs/lab-06.html#data-palmer-penguins",
    "href": "labs/lab-06.html#data-palmer-penguins",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Data: Palmer Penguins",
    "text": "Data: Palmer Penguins\nWe will go back to the Palmer penguins data used in HW 02.\nData were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. (Gorman, Williams, and Fraser 2014)\n\nThese data can be found in the palmerpenguins package. We’re going to be working with the penguins dataset from this package. The dataset contains data for 344 penguins. There are 3 different species of penguins in this dataset, collected from 3 islands in the Palmer Archipelago, Antarctica. Click here to see the codebook.\nWe will focus on the following variables:\n\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\nspecies\ninteger\nPenguin species (Adelie, Gentoo, Chinstrap)\n\n\nbody_mass_g\ninteger\nBody mass in g\n\n\nflipper_length_mm\ninteger\nFlipper length in mm"
  },
  {
    "objectID": "labs/lab-06.html#exercises",
    "href": "labs/lab-06.html#exercises",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Exercises",
    "text": "Exercises\nThe goal of this analysis is to use logistic regression to understand the relationship between flipper length, island, and whether a penguin is considered large. Let’s assume a penguin is considered large if it has an above average body mass. First, remove missing values and create a new response variable to identify whether a penguin is large.\n\npenguins &lt;- penguins |&gt;\n  drop_na() |&gt;\n  mutate(large = factor(if_else(body_mass_g &gt; mean(body_mass_g, na.rm = TRUE), 1, 0)))\n\nAnd let’s make sure the new variable looks how we would expect before we continue with the analysis. Note the mean body mass for the penguins in the data is 4207.057 grams.\n\npenguins |&gt; \n  group_by(large) |&gt;\n  summarise(min = min(body_mass_g), mean = mean(body_mass_g), max = max(body_mass_g))\n\n# A tibble: 2 × 4\n  large   min  mean   max\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt;\n1 0      2700 3605.  4200\n2 1      4250 4988.  6300\n\n\n\nExercise 1\nLet’s start by examining the relationship between large and species.\nVisualize the relationship between large and species. Write something you observe about the relationship between these two variables based on the plot.\n\n\nExercise 2\n\nCalculate the probability a randomly selected penguin is large, given that it is from the Adelie species.\nCalculate the odds a randomly selected penguin is large, given it is from the Adelie speices.\n\n\n\nExercise 3\nSplit the data into a 75% training set, penguins_train, and 25% test set, penguins_test. Use a seed of 29. We will be using the training set throughout the assignment for modeling and the test set to assess the performance.\n\n\nExercise 4\nYou want to fit a model using species to predict the odds a penguins is large. Let \\(\\pi\\) be the probability a penguin is large. The model has the form shown below.\n\\[\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1~ Chinstrap + \\beta_2 ~ Gentoo\n\\]\n\nFit the model using the training data and neatly display the model output using three digits.\nWhat are the predicted odds of a penguin being large if it is Gentoo?\nWhat are the predicted odds of a penguin being large if it is Adelie?\n\n\n\nExercise 5\nNext, we’d like to add flipper length to the model. We’ll start by examining the relationship between these two variables.\n\nVisualize the relationship between flipper_length_mm and large. What is something you observe about the relationship between these two variables from the plot?\nAdd flipper length to the model from Exercise 4 so that there are two predictors, species and flipper_length_mm. Neatly display the model output using three digits.\nWrite the estimated regression equation.\n\n\n\nExercise 6\nUse the model from Exercise 5.\n\nHow do you expect the log-odds of being a large penguin to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins are of the Gentoo species.\nHow do you expect the odds of being a large penguin to change when going from a penguin with flipper length 185 mm to a penguin with flipper length 200 mm? Assume both penguins are of the Gentoo species.\n\n\n\nExercise 7\nUse the model from Exercise 5.\n\nHow do you expect the log-odds of being a large penguin to change when going from a penguin with flipper length 195 mm recorded as Chinstrap to a penguin with flipper length 210 mm recorded as Adelie?\nHow do you expect the odds of being a large penguin to change when going from a penguin with flipper length 195 mm recorded as Chinstrap to a penguin with flipper length 210 mm recorded as Adelie?\n\n\n\nExercise 8\nNow let’s assess the performance of both models on the test set.\n\nUse predict() function to get the predicted class for the observations in the penguins_test based on the model from Exercise 4. Calculate the percentage of observations that were mislcassified.\nUse the predict() function to get the predicted class for the observations in penguins_test based on the model from Exercise 5. Calculate the percentage of observations that were mislcassified.\nBased on this analysis, which model would you select? Briefly explain."
  },
  {
    "objectID": "labs/lab-06.html#submission",
    "href": "labs/lab-06.html#submission",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nOne team member submit the assignment:\n\nGo to http://www.gradescope.com and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nSelect all team members’ names, so they receive credit on the assignment. Click here for video on adding team members to assignment on Gradescope.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-06.html#grading",
    "href": "labs/lab-06.html#grading",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1 - 8\n40\n\n\nWorkflow & formatting\n51\n\n\nComplete merge conflict activity\n5"
  },
  {
    "objectID": "labs/lab-06.html#footnotes",
    "href": "labs/lab-06.html#footnotes",
    "title": "Lab 06: Large Palmer Penguins",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and collaboration. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.↩︎"
  },
  {
    "objectID": "labs/lab-04.html",
    "href": "labs/lab-04.html",
    "title": "Lab 04: Alumni jobs",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, September 29, 11:59pm (Tuesday labs)\nSunday, October 1, 11:59pm (Thursday labs)\nIn today’s lab, you’ll use simple linear regression to analyze the relationship between the median early career salary and percent of alumni who perceive their job as making the world a better place for colleges and universities in the United States. You will also start working with your lab teams, write a team agreement, and be introduced to using GitHub for collaboration."
  },
  {
    "objectID": "labs/lab-04.html#exercise-1",
    "href": "labs/lab-04.html#exercise-1",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a linear model that can be used to predict the typical (median) early career pay based on the percent of STEM degrees. Display the model output using 3 digits for numerical values."
  },
  {
    "objectID": "labs/lab-04.html#exercise-2",
    "href": "labs/lab-04.html#exercise-2",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 2",
    "text": "Exercise 2\nCalculate the predicted values and residuals from your model and save these results in a data frame. Print the first five rows of the new data frame.\n\nTeam Member 1: Knit, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercises 1- 2.\nTeam Member 2: It’s your turn! Type the team’s response to exercise 3."
  },
  {
    "objectID": "labs/lab-04.html#exercise-3",
    "href": "labs/lab-04.html#exercise-3",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 3",
    "text": "Exercise 3\nBefore using the model for prediction, let’s check the model conditions. To figure out how to do so, we type the following prompt into ChatGPT (“an AI powered language model”) “How do you check the model conditions for simple linear regression?”\nThe response from ChatGPT shown below would receive partial credit. For each condition, state whether it is correct or incorrect/ incomplete. If it incorrect/incomplete, state what is wrong or missing, then write the correct response.\n\nIn simple linear regression, you’re trying to establish a linear relationship between two variables: a dependent variable (Y) and an independent variable (X). The model conditions or assumptions for simple linear regression are important to check to ensure that your model is valid and reliable. Here are the key assumptions and how to check them:\n1. Linearity Assumption:\n\nAssumption: The relationship between the independent variable (X) and the dependent variable (Y) is linear.\nHow to Check: Create a scatterplot of Y against X. If the points roughly form a straight line, the linearity assumption is likely met.\n\n2. Independence Assumption:\n\nAssumption: The residuals (the differences between observed Y and predicted Y) are independent of each other.\nHow to Check: Plot the residuals against the independent variable (X) or the predicted values. Look for patterns or trends in the residuals. There should be no clear structure or autocorrelation in the residuals.\n\n3. Homoscedasticity [Constant variance] Assumption:\n\nAssumption: The variance of the residuals is constant across all levels of the independent variable.\nHow to Check: Plot the residuals against the predicted values or the independent variable (X). The spread of the residuals should remain roughly constant across the range of X.\n\n4. Normality Assumption:\n\nAssumption: The residuals follow a normal distribution.\nHow to Check: Create a histogram or a normal probability plot (Q-Q plot) of the residuals. If the residuals are approximately normally distributed, they should follow a bell-shaped curve in the histogram and lie along a straight line in the Q-Q plot.\n\nIt’s important to note that these assumptions may not always hold perfectly in real-world data, and minor violations may not necessarily invalidate your model. However, if you detect significant violations of these assumptions, you may need to consider alternative modeling techniques or transformations of your data to address the issues.\n\n\n✅ ⬆️ Team Member 2: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercise 3.\nTeam Member 3: It’s your turn! Type the team’s response to exercises 4 - 5.\n\n\n\n\n\n\n\nImportant\n\n\n\nYou can proceed assuming all the model conditions are satisfied."
  },
  {
    "objectID": "labs/lab-04.html#exercise-4",
    "href": "labs/lab-04.html#exercise-4",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 4",
    "text": "Exercise 4\n\n31 percent of the alumni at Johns Hopkins University received a STEM degree. Calculate the predicted early career pay at this university and the appropriate 95% interval. Interpret the interval in the context of the data.\nThe actual early career pay at this university is $67,200. Is this what you expected based on your analysis? Briefly explain your reasoning."
  },
  {
    "objectID": "labs/lab-04.html#exercise-5",
    "href": "labs/lab-04.html#exercise-5",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 5",
    "text": "Exercise 5\nNext, let’s consider how well the model fits the relationship between the early career pay and percent with a STEM degree.\n\nCalculate \\(R^2\\) and interpret it in the context of the data.\nCalculate RMSE and interpret it in the context of the data.\n\n\nTeam Member 3: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercise 4 - 5.\nTeam Member 4: It’s your turn! Type the team’s response to exercise 6."
  },
  {
    "objectID": "labs/lab-04.html#exercise-6",
    "href": "labs/lab-04.html#exercise-6",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 6",
    "text": "Exercise 6\nDo you think the model is useful for understanding and predicting the typical early career pay for alumni at a university? Briefly explain your reasoning.\n\nTeam Member 4: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercise 6.\nTeam Member 1: It’s your turn! Type the team’s response to exercise 7."
  },
  {
    "objectID": "labs/lab-04.html#exercise-7",
    "href": "labs/lab-04.html#exercise-7",
    "title": "Lab 04: Alumni jobs",
    "section": "Exercise 7",
    "text": "Exercise 7\nFit another model which predicts the typical early career pay using only the percentage of alumni that believe their career will make the world a better place.\n\nNeatly display this model with 3 digits.\nWhich model do you think is better - this model or the model fit in Exercise 1? Explain your response showing any analysis used to make the decision.\n\n\nTeam Member 1: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and the rest of the team can see the completed lab.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the team’s completed lab!"
  },
  {
    "objectID": "labs/lab-04.html#footnotes",
    "href": "labs/lab-04.html#footnotes",
    "title": "Lab 04: Alumni jobs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDon’t trust yourself to keep your hands off the keyboard? Put them in your pocket or cross your arms. No matter how silly it might feel, resist the urge to touch your keyboard until otherwise instructed!↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and collaboration. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.↩︎↩︎"
  },
  {
    "objectID": "labs/lab-02.html",
    "href": "labs/lab-02.html",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nWednesday, September"
  },
  {
    "objectID": "labs/lab-02.html#learning-goals",
    "href": "labs/lab-02.html#learning-goals",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will…\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use simulation-based inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "labs/lab-02.html#exercise-10",
    "href": "labs/lab-02.html#exercise-10",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nThere is a statistically significant linear relationship between average air temperature and average ice duration on Lake Monona \\((\\beta_1 \\neq 0)\\).\n\nDoes the confidence interval you calculated in the previous exercise support or refute this claim? Briefly explain.\n\nRender, commit, and push your final changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/lab-02.html#footnotes",
    "href": "labs/lab-02.html#footnotes",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://lter.github.io/lterdatasampler/index.html↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "labs/lab-00.html",
    "href": "labs/lab-00.html",
    "title": "Lab 00: Getting started",
    "section": "",
    "text": "Important\n\n\n\nPlease complete all today’s lab tasks before leaving lab today.\n\n\n\nReserve a STA 210 container\n\nMake sure to reserve the container titled “STA210” not “RStudio”\nStart the container and confirm you see the RStudio interface\n\nSubmit your GitHub username and confirm you have access to the 210 container\n\nIf you do not have a GitHub username, click here for information on registering for a GitHub account and choosing a username\n\nComplete the STA 210 Student Survey"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MAT 212: Multiple Regression Analysis",
    "section": "",
    "text": "An introduction to statistical modeling and methods. Topics may include model construction and analysis using multiple linear regression, analysis of variance, and logistic regression. The course makes substantial use of statistical software. Students who have taken more than one calculus course should generally take MAT-212 rather than MAT-125.\n\n\n\nWe will learn approaches for analyzing multivariate data sets, emphasizing analysis of variance, linear regression, and logistic regression. You will learn techniques for checking the appropriateness of proposed models, such as residual analyses and case influence diagnostics, and techniques for selecting models. Gain experience dealing with the challenges that arise in practice through assignments that utilize real-world data. This class emphasizes data analysis over mathematical theory."
  },
  {
    "objectID": "index.html#catalog-description",
    "href": "index.html#catalog-description",
    "title": "MAT 212: Multiple Regression Analysis",
    "section": "",
    "text": "An introduction to statistical modeling and methods. Topics may include model construction and analysis using multiple linear regression, analysis of variance, and logistic regression. The course makes substantial use of statistical software. Students who have taken more than one calculus course should generally take MAT-212 rather than MAT-125."
  },
  {
    "objectID": "index.html#erics-addition",
    "href": "index.html#erics-addition",
    "title": "MAT 212: Multiple Regression Analysis",
    "section": "",
    "text": "We will learn approaches for analyzing multivariate data sets, emphasizing analysis of variance, linear regression, and logistic regression. You will learn techniques for checking the appropriateness of proposed models, such as residual analyses and case influence diagnostics, and techniques for selecting models. Gain experience dealing with the challenges that arise in practice through assignments that utilize real-world data. This class emphasizes data analysis over mathematical theory."
  },
  {
    "objectID": "index.html#instructor",
    "href": "index.html#instructor",
    "title": "MAT 212: Multiple Regression Analysis",
    "section": "Instructor",
    "text": "Instructor\nEric is an Assistant Professor in the Department of Mathematics and Physical Sciences at the College of Idaho. His expertise lies in the areas of probability, statistics, and statistics education. He joined the College of Idaho faculty in 2024 after spending three years as an Assistant Professor at St. Norbert College in De Pere, Wisconsin. He received his bachelor’s degree in mathematics and statistics from Rice University in 2011. After earning his degree, he worked for Capital One for two years in their home loans division before enrolling in graduate school. In 2018, Eric received his Ph.D. in statistics and operations research from the University of North Carolina at Chapel Hill studying under professor Amarjit Budhiraja. His dissertation work focused on modeling and analyzing large systems which arise in industrial engineering (e.g. large server and communication networks). Following his Ph.D., Eric did a postdoc in the Department of Ecology & Evolution at the University of Chicago under the direction of professor Matthias Steinrücken where he used stochastic processes to study natural selection and population genetics.\nOutside of school, Eric is an avid fan of the New York Giants, New York Knicks, and North Carolina Tarheels. In addition, he enjoys comic books, the Fast and the Furious franchise, and spending time with his lovely wife Maria and lovable dogs Allie, Tony, and Miriam.\n\n\n\nOffice hours (Boone 126B)\n\n\n\n\n\nMonday\n2:30-3:30pm\n\n\nTuesday\n9:45-10:45am\n\n\nWednesday\n1:30-2:30pm\n\n\nThursday\n1-2pm\n\n\nor by appointment."
  },
  {
    "objectID": "hw/hw-04.html",
    "href": "hw/hw-04.html",
    "title": "HW 04: Alumni jobs",
    "section": "",
    "text": "Important\n\n\n\nDue: Friday, September 27, 11:59pm\nIn this homework, you’ll use simple linear regression to analyze the relationship between the median early career salary and percent of alumni who perceive their job as making the world a better place for colleges and universities in the United States."
  },
  {
    "objectID": "hw/hw-04.html#learning-goals",
    "href": "hw/hw-04.html#learning-goals",
    "title": "HW 04: Logistic regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the assignment you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nConduct exploratory data analysis for logistic regression\nInterpret coefficients of logistic regression model\nUse statistics to help choose the best fit model\nUse the logistic regression model for prediction and classification"
  },
  {
    "objectID": "hw/hw-04.html#getting-started",
    "href": "hw/hw-04.html#getting-started",
    "title": "HW 04: Logistic regression",
    "section": "Getting started",
    "text": "Getting started\nThe repo for this assignment is available on GitHub at github.com/sta210-fa23 and starts with the prefix hw-04. See Lab 01 for more detailed instructions on getting started."
  },
  {
    "objectID": "hw/hw-04.html#packages",
    "href": "hw/hw-04.html#packages",
    "title": "HW 04: Logistic regression",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this assignment.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-04.html#exercise-1",
    "href": "hw/hw-04.html#exercise-1",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a linear model that can be used to predict the typical (median) early career pay based on the percent of STEM degrees. Display the model output using 3 digits for numerical values."
  },
  {
    "objectID": "hw/hw-04.html#exercise-2",
    "href": "hw/hw-04.html#exercise-2",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 2",
    "text": "Exercise 2\nCalculate the predicted values and residuals from your model and save these results in a data frame. Print the first five rows of the new data frame."
  },
  {
    "objectID": "hw/hw-04.html#exercise-3",
    "href": "hw/hw-04.html#exercise-3",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 3",
    "text": "Exercise 3\nBefore using the model for prediction, let’s check the model conditions. To figure out how to do so, we type the following prompt into ChatGPT (“an AI powered language model”) “How do you check the model conditions for simple linear regression?”\nThe response from ChatGPT shown below would receive partial credit. For each condition, state whether it is correct or incorrect/incomplete. If it incorrect/incomplete, state what is wrong or missing, then write the correct response. Please put all answers after the ChatGPT text.\n\nIn simple linear regression, you’re trying to establish a linear relationship between two variables: a dependent variable (Y) and an independent variable (X). The model conditions or assumptions for simple linear regression are important to check to ensure that your model is valid and reliable. Here are the key assumptions and how to check them:\n1. Linearity Assumption:\n\nAssumption: The relationship between the independent variable (X) and the dependent variable (Y) is linear.\nHow to Check: Create a scatterplot of Y against X. If the points roughly form a straight line, the linearity assumption is likely met.\n\n2. Independence Assumption:\n\nAssumption: The residuals (the differences between observed Y and predicted Y) are independent of each other.\nHow to Check: Plot the residuals against the independent variable (X) or the predicted values. Look for patterns or trends in the residuals. There should be no clear structure or autocorrelation in the residuals.\n\n3. Homoscedasticity [Constant variance] Assumption:\n\nAssumption: The variance of the residuals is constant across all levels of the independent variable.\nHow to Check: Plot the residuals against the predicted values or the independent variable (X). The spread of the residuals should remain roughly constant across the range of X.\n\n4. Normality Assumption:\n\nAssumption: The residuals follow a normal distribution.\nHow to Check: Create a histogram or a normal probability plot (Q-Q plot) of the residuals. If the residuals are approximately normally distributed, they should follow a bell-shaped curve in the histogram and lie along a straight line in the Q-Q plot.\n\nIt’s important to note that these assumptions may not always hold perfectly in real-world data, and minor violations may not necessarily invalidate your model. However, if you detect significant violations of these assumptions, you may need to consider alternative modeling techniques or transformations of your data to address the issues."
  },
  {
    "objectID": "hw/hw-04.html#exercise-4",
    "href": "hw/hw-04.html#exercise-4",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 4",
    "text": "Exercise 4\nDetermine whether the model conditions hold.\n\n\n\n\n\n\nImportant\n\n\n\nRegardless of your previous answer, you can proceed assuming all the model conditions are satisfied."
  },
  {
    "objectID": "hw/hw-04.html#exercise-5",
    "href": "hw/hw-04.html#exercise-5",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n31 percent of the alumni at Johns Hopkins University received a STEM degree. Calculate the predicted early career pay at this university and the appropriate 95% interval. Interpret the interval in the context of the data.\nThe actual early career pay at this university is $67,200. Is this what you expected based on your analysis? Briefly explain your reasoning."
  },
  {
    "objectID": "hw/hw-04.html#exercise-6",
    "href": "hw/hw-04.html#exercise-6",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 6",
    "text": "Exercise 6\nNext, let’s consider how well the model fits the relationship between the early career pay and percent with a STEM degree.\n\nCalculate \\(R^2\\) and interpret it in the context of the data.\nCalculate RMSE and interpret it in the context of the data."
  },
  {
    "objectID": "hw/hw-04.html#exercise-7",
    "href": "hw/hw-04.html#exercise-7",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 7",
    "text": "Exercise 7\nDo you think the model is useful for understanding and predicting the typical early career pay for alumni at a university? Briefly explain your reasoning."
  },
  {
    "objectID": "hw/hw-04.html#exercise-8",
    "href": "hw/hw-04.html#exercise-8",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit another model which predicts the typical early career pay using only the percentage of alumni that believe their career will make the world a better place.\n\nNeatly display this model with 3 digits.\nWhich model do you think is better - this model or the model fit in Exercise 1? Explain your response showing any analysis used to make the decision."
  },
  {
    "objectID": "hw/hw-04.html#exercise-9",
    "href": "hw/hw-04.html#exercise-9",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nUse the testing data to produce the ROC curve and calculate the area under curve (AUC) for the model selected in Exercise 6. Write 1 - 2 sentences describing how well the model fits the data."
  },
  {
    "objectID": "hw/hw-04.html#exercise-10",
    "href": "hw/hw-04.html#exercise-10",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nYou have been tasked by a local political organization to identify adults in the community who are frequent voters. These adults will receive targeted political mailings that will be different from the mailings sent to adults who are not frequent voters. You will use the model selected in Exercise 6 to identify the frequent voters.\nMake a confusion matrix based on the cut-off probability of 0.25. Use the confusion matrix to calculate the following:\n\nSensitivity\nSpecificity\nFalse negative rate\nFalse positive rate"
  },
  {
    "objectID": "hw/hw-04.html#footnotes",
    "href": "hw/hw-04.html#footnotes",
    "title": "HW 04: Alumni jobs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflowamd overall readability.↩︎"
  },
  {
    "objectID": "hw/hw-02.html",
    "href": "hw/hw-02.html",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, September 13th"
  },
  {
    "objectID": "hw/hw-02.html#learning-goals",
    "href": "hw/hw-02.html#learning-goals",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the hw you will…\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use simulation-based inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-02.html#exercise-10",
    "href": "hw/hw-02.html#exercise-10",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nThere is a statistically significant linear relationship between average air temperature and average ice duration on Lake Monona \\((\\beta_1 \\neq 0)\\).\n\nDoes the confidence interval you calculated in the previous exercise support or refute this claim? Briefly explain.\nWritten answer here.\n\n\n\n\n\n\nTip\n\n\n\nRender your document here."
  },
  {
    "objectID": "hw/hw-02.html#footnotes",
    "href": "hw/hw-02.html#footnotes",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://lter.github.io/lterdatasampler/index.html↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-01.html",
    "href": "hw/hw-01.html",
    "title": "HW 01: Park access",
    "section": "",
    "text": "Important\n\n\n\nDue\n\nWednesday, September 4th"
  },
  {
    "objectID": "hw/hw-01.html#learning-goals",
    "href": "hw/hw-01.html#learning-goals",
    "title": "HW 01: Park access",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the , you will…\n\nBe familiar with the workflow using RStudio\nGain practice writing a reproducible report using Quarto\nBe able to create data visualizations using ggformula and use those visualizations to describe distributions\nBe able to fit, interpret, and evaluate simple linear regression models"
  },
  {
    "objectID": "hw/hw-01.html#r-and-r-studio",
    "href": "hw/hw-01.html#r-and-r-studio",
    "title": "HW 01: Park access",
    "section": "R and R Studio",
    "text": "R and R Studio\nHere are the components of the RStudio IDE and here are the components of an Quarto (.qmd) file.\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for ““Yet Another Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document. You can now work off of that file."
  },
  {
    "objectID": "hw/hw-01.html#footnotes",
    "href": "hw/hw-01.html#footnotes",
    "title": "HW 01: Park access",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-00.html",
    "href": "hw/hw-00.html",
    "title": "Homework 0 - The Stations",
    "section": "",
    "text": "The purpose of this homework is to get you familiar and up to speed with R and RStudio. It comes in three parts. For each of the three you should upload a .qmd and .html file to Canvas. All three .qmd file templates are available on Canvas.\n\nStation 1: Getting Started with R and RStudio\nStation 2: Exploratory Data Analysis (EDA) with R\nStation 3: Inference with R"
  },
  {
    "objectID": "hw/hw-0-Station-2.html",
    "href": "hw/hw-0-Station-2.html",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "",
    "text": "In this document you will be introduced to the functions we will use most frequently in this class. If you are moderate to advanced R user, feel free to use whatever functions you’d like to accomplish the tasks. The first part of this document contains explanations for beginning R users, but all students should work through the entire document.\n\n\nIn general, I suggest “rendering as you go”: rendering every few chunks, to make sure things are rendering correctly, rather than waiting until the end of a document to render the whole thing (and potentially encountering lots of errors that you have to unpack)! I also ALWAYS suggest “saving as you go”: it’s a good idea to save every couple of minutes. (This is good practice for all your files.)\n\nTASK–Run the code below to set options for the document and load packages. Note that #| warning: FALSE causes the code to run without displaying any associated warnings when you render.\n\n\n# Clear Workspace\nrm(list = ls()) \n\n# load packages we typically use for this class.\nlibrary(mosaic) \nlibrary(ggformula)\nlibrary(Stat2Data)\nlibrary(tidyverse)"
  },
  {
    "objectID": "hw/hw-0-Station-2.html#a.-introduction",
    "href": "hw/hw-0-Station-2.html#a.-introduction",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "",
    "text": "In this document you will be introduced to the functions we will use most frequently in this class. If you are moderate to advanced R user, feel free to use whatever functions you’d like to accomplish the tasks. The first part of this document contains explanations for beginning R users, but all students should work through the entire document.\n\n\nIn general, I suggest “rendering as you go”: rendering every few chunks, to make sure things are rendering correctly, rather than waiting until the end of a document to render the whole thing (and potentially encountering lots of errors that you have to unpack)! I also ALWAYS suggest “saving as you go”: it’s a good idea to save every couple of minutes. (This is good practice for all your files.)\n\nTASK–Run the code below to set options for the document and load packages. Note that #| warning: FALSE causes the code to run without displaying any associated warnings when you render.\n\n\n# Clear Workspace\nrm(list = ls()) \n\n# load packages we typically use for this class.\nlibrary(mosaic) \nlibrary(ggformula)\nlibrary(Stat2Data)\nlibrary(tidyverse)"
  },
  {
    "objectID": "hw/hw-0-Station-2.html#b.-data",
    "href": "hw/hw-0-Station-2.html#b.-data",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "B. Data",
    "text": "B. Data"
  },
  {
    "objectID": "hw/hw-0-Station-2.html#load-data",
    "href": "hw/hw-0-Station-2.html#load-data",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "Load data",
    "text": "Load data\nDoing statistics requires data. R comes with many built-in datasets, and the packages you loaded above (specifically Stat2Data) give you access to even more. To load a dataset from within R you can use the data command:\n\nTASK–Use the code below to load the dataset diamonds, which is part of the ggplot2 package and included in the tidyverse.\n\n\ndata(\"diamonds\")\n\nThis is a great chance to remind you that R cares about letter case. This means that data(\"diamonds\") and data(\"Diamonds\") actually load different datasets! In this case, please make sure to load the lowercase letter diamonds dataset.\n\nInspecting the data source\nNow you’re ready to learn a little bit about the diamonds data set.\n\n# Inspecting the data source\nglimpse(diamonds)\n\nRows: 53,940\nColumns: 10\n$ carat   &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.…\n$ cut     &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver…\n$ color   &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,…\n$ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, …\n$ depth   &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64…\n$ table   &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58…\n$ price   &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34…\n$ x       &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.…\n$ y       &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.…\n$ z       &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.…\n\nhead(diamonds)\n\n# A tibble: 6 × 10\n  carat cut       color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n\nnames(diamonds)\n\n [1] \"carat\"   \"cut\"     \"color\"   \"clarity\" \"depth\"   \"table\"   \"price\"  \n [8] \"x\"       \"y\"       \"z\"      \n\n\n\nTASK: Edit the bullet list below to add a short description in your own words describing what each function does. Hint: if it isn’t clear based on the output you can use ? before the function name in the console to bring up the documentation on that function.\n\n\nglimpse() : this function…\nhead() : this function…\nnames() : this function…\n\n\n\nSome Data Prep\nThe following is a little bit of data wrangling to get the source data in shape for our purposes. You can ignore this part for now, and we can talk about it another time.\n\n# Recode & filter (no edits needed - just run this chunk)\nrecoded &lt;-  # make a new dataset called recoded\n  diamonds |&gt;  # by starting with the diamonds data\n  filter(color==\"D\" | color==\"J\") |&gt; # and filtering observations to keep only colors D and J\n  mutate(col = as.character(color)) # tell R some specifics about how to record the variable color.\n\nBasically, we’re going to do a bit of exploration of variables that impact cost of diamonds. Even if you haven’t used R before, you might be able to tell from the code that we started with the diamonds data, filtered (i.e. restricted) our data set to only include the diamonds that are either color D or J, created a new categorical variable called col, and stored the whole thing in a new data set called recoded.\nStatisticians should always know something about the data domain in order to be useful. If you don’t know anything about the subject area you need to at least learn some basics. Wikipedia is usually a good place to start: https://en.wikipedia.org/wiki/Diamond_color."
  },
  {
    "objectID": "hw/hw-0-Station-2.html#c.-exploratory-data-analysis",
    "href": "hw/hw-0-Station-2.html#c.-exploratory-data-analysis",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "C. Exploratory Data Analysis",
    "text": "C. Exploratory Data Analysis\nFor the purposes of our class, it’s useful to learn a model-centric approach to R. The psuedo-code below is going to be our foundation for the rest of the class:\nfunction( Y ~ X, data = DataSetName )\nHere’s a short description of each part in the pseudo-code above:\n\nfunction is an R function that dictates something you want to do with your data; for example,\n\nmean calculates the mean\nt.test performs a t-test\nlm fits a linear regression model\n\nY is the outcome of interest (response variable)\nX is some explanatory variable; you can use 1 as a placeholder if there is no explanatory variable\nDataSetName is the name of a data set loaded into the R environment\n\nAlways start with clear research questions. Our question for this exercise:\n\nHow do diamond prices compare for D and J colored diamonds?\n\nThe purpose of the exploratory data analysis (EDA) is to learn as much as you can about your research question before doing any statistical modeling. We basically want to try and answer the research question with EDA if possible…or at least have a guess as to what the answer “should” be. Then we use statistical models to formally accommodate variability in the data and calculate the uncertainty of our conclusions.\n\nMean price by color\nUse the R code chunk below to calculate the mean price by color. Summarize your observations below the code chunk (under “TASK”). Don’t forget, we did some data wrangling above and made a new data set called recoded. Use the recoded data for the rest of this analysis.\n\nmean(price ~ col, data = recoded)\n\n       D        J \n3169.954 5323.818 \n\n\n\nTASK–Share your observations:\n\n\n\nOther summary statistics by color\nOf course, there are lots of other ways to summarize a numerical variable besides the mean. Use the R code chunk to calculate the other summary statistics for the price of each diamond color using favstats(). Summarize your observations below the code chunk. How do the prices compare between D diamonds and J diamonds?\n\nTASK–Produce the required code:\n\n(Hint: Do you not know how favstats works? Well, it’s a function just like any other: it follows the syntax described at the top of this section! Also, you can always find details and examples by searching the Help menu in the lower-right quadrant.)\n\nTASK–Summarize your observations:\n\n\n\n2-3 Basic plots of the data\nMake side-by-side boxplots along with a scatterplot using the R code shown below.\n\nTASK–Run the code to make a boxplot; then modify the 2nd bit of code to make the desired scatterplot:\n\n\n# make a boxplot of price by color\ngf_boxplot(price ~ col, data = recoded) \n\n\n\n\n\n\n\n# make a scatter plot of price versus carat\n#gf_point(response ~ explanatory, data = recoded)\n\n\nTASK–Share your observations:"
  },
  {
    "objectID": "hw/hw-0-Station-2.html#multivariable-relationships",
    "href": "hw/hw-0-Station-2.html#multivariable-relationships",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "Multivariable relationships",
    "text": "Multivariable relationships\nThe world is often too complicated to be understood by studying one or two variables at a time. Color is certainly not the only variable that impacts the value of a diamond. You may have noticed in the Wikipedia article that color is only one of the “4 C’s” that influence the value of a diamond.\n\nAdjusting for other variables\nAlter the pseudo-code below to create a scatter plot of price vs carat, which each plotted point colored according the color of the diamond it represents. Write a few sentences below the code chunk to explain what you’ve observed from this plot.\n\nTASK–modify the code to produce the scatterplot:\n\n\n#gf_point(response ~ explanatory1, color = ~ explanatory2, data = recoded)\n\n\nTASK–Share your observations:"
  },
  {
    "objectID": "hw/hw-0-Station-2.html#d.-us-states-data-analysis",
    "href": "hw/hw-0-Station-2.html#d.-us-states-data-analysis",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "D. US States Data Analysis",
    "text": "D. US States Data Analysis"
  },
  {
    "objectID": "hw/hw-0-Station-2.html#use-data-from-an-outside-source",
    "href": "hw/hw-0-Station-2.html#use-data-from-an-outside-source",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "Use data from an outside source",
    "text": "Use data from an outside source\nIt’s important to be able to download a file from an outside source and then load it in R for use. There are multiple ways to do this, but one is described in Station_1. If you don’t know how to do this, see Station_1 for instructions, or ask your partner.\n\nTASK–Download the file US_States_fall2021.csv from Canvas and load the data into your environment\n\nOnce the data is loaded into the Environment, that means it’s loaded in the Console, NOT this Quarto document! Make sure you copy the code that loaded it into the Console into the R chunk below. Call the data set state_data.\n\nTASK–Use head() and glimpse() to explore the format of the data a bit:\n\nThis data has one row for each of the 50 U.S. states, and each variable is recorded for the entire state. For example, that variable HeavyDrinkers gives the percentage of residents in a state that are heavy drinkers. According to these data 5.9% of Pennsylvania residents are heavy drinkers.\n\nTASK–Refer to the US States Merged Data codebook (also on Canvas) and select some variables of interest:\n\nYou can choose whatever variables you want. You will explore these variables (and relationships between them) in Stations 2 and 3.\n\nQuantitative variable 1: …\nQuantitative variable 2: …\nCategorical variable 1: …\nCategorical variable 2: …"
  },
  {
    "objectID": "hw/hw-0-Station-2.html#eda-for-state-data",
    "href": "hw/hw-0-Station-2.html#eda-for-state-data",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "EDA for State data",
    "text": "EDA for State data\nFor all the tasks below, you will have to alter the code given for your chosen variables!\n\nTASK–Determine how many states are in each category of a categorical variable:\n\n\n#tally( ~ CatVariable1, data = state_data)\n\n\nTASK–create a barchart for one categorical variable:\n\n\n#gf_bar( ~ CatVariable1, data = state_data)\n\n\nTASK–Determine how many states are in combinations of categories for two categorical variables:\n\n\n#tally(CatVariable1 ~ CatVariable2, data = state_data)\n\n\nTASK–create a barchart for two categorical variables. Try changing \"dodge\" to \"stacked\" and \"fill\". Choose one and briefly explain why you chose that barchart:\n\n\n#gf_bar( ~ CatVariable1, fill = ~ CatVariable2, data = state_data, position = \"dodge\")\n\n\nTASK–create a histogram for one of your quantitative variables. Adjust the number of bins until the noise in your data does not overwhelm any patterns:\n\n\n#gf_histogram( ~ QuantVariable1, data = state_data, bins = 30)\n\n\nTASK–create side-by-side boxplots for one quantitative and one categorical variable:\n\n\n#gf_boxplot(QuantVariable1 ~ CatVariable1, data = state_data)\n\n\nTASK–create a scatterplot for two quantitative variables:\n\n\n#gf_point(QuantVariable2 ~ QuantVariable1, data = state_data)\n\n\nTASK– for two quantitative variables create both scatterplots, using a categorical variable for color in the first plot. What is the difference between the two plots? What do you think the tilde ~ does:\n\n\n#gf_point(QuantVariable2 ~ QuantVariable1, color = ~ CatVariable1, data = state_data)\n#gf_point(QuantVariable2 ~ QuantVariable1, color = \"blue\", data = state_data)"
  },
  {
    "objectID": "hw/hw-0-Station-2.html#summarizing-what-you-learned",
    "href": "hw/hw-0-Station-2.html#summarizing-what-you-learned",
    "title": "Station 2: Exploratory Data Analysis (EDA) with R",
    "section": "Summarizing what you learned",
    "text": "Summarizing what you learned\nWhenever you summarize the results of an analysis, it should always be in context. “In context” means that you talk about variables, not codes or shorthands like “CatVariable1”. A good question to ask yourself is: “If I walked up to a person on the street and told them this fact about my data, would they understand what was talking about?”\nExample: BAD\nYou: “Hi, did you know that there is a positive relationship between QuantVariable1 and QuantVariable2? It looks like the r is pretty strong.”\nPerson on street: “WTF?”\nExample: GOOD\nYou: “Hi, did you know that states with higher 8th grade math test scores also have higher average IQ’s? The relationship between these two variables is quite strong!”\nPerson on street: “How interesting, although maybe not surprising? Let’s have a discussion about the difficulties of measuring ‘intelligence’ with either standardized testing or IQ tests. What is ‘intelligence’, anyway?”\n\nTASK–share three interesting observations you learned from these figures (in Section D). Anything you learned must be in context!\n\n\nNote: Make sure you save your work in this Station."
  },
  {
    "objectID": "computing-inferencesummary.html",
    "href": "computing-inferencesummary.html",
    "title": "Inference Summary Sheet",
    "section": "",
    "text": "Variable type\nParameter of interest\nNull hypothesis\nTest name\nFunction for test\n\n\n\n\nOne quantitative variable\n\\(\\mu\\)\n\\(\\mu = \\mu_0\\)\nOne-sample t-test\nt.test(~variable1, mu = mu0, data = data_name)\n\n\nOne binary variable\n\\(p\\) or \\(\\pi\\)\n\\(p = p_0\\)\nZ-test for one proportion\nprop.test(~variable1, p = p0, data = data_name)\n\n\nOne quantitative and one binary variable with independent samples\n\\(\\mu_1-\\mu_2\\)\n\\(\\mu_1 = \\mu_2\\)\nTwo-sample t-test\nt.test(variable1~variable2, data = data_name)\n\n\nOne quantitative and one binary variable with paired samples\n\\(\\mu_d\\)\n\\(\\mu_d = 0\\)\nPaired t-test\nt.test(variable1~variable2, paried = TRUE, data = data_name)\n\n\nTwo binary variables\n\\(p_1-p_2\\) or \\(\\pi_1-\\pi_2\\)\n\\(p_1=p_2\\)\nZ-test for a difference in proportions\nprop.test(Variable1~variable2, data = data_name)\n\n\nTwo quantitative variables\ncorrelation, \\(\\rho\\) or slope \\(\\beta\\)\n\\(\\rho = 0\\) or \\(\\beta = 0\\)\nCorrelation or Regression\ncor.test(variable1~variable2, data = data_name) or lm(variable1 ~ variable2, data = data_name)\n\n\nTwo categorical variables\nNA\n2 variables are independent\nChi-squared test\nchisq.test(variable1~variable2, data = data_name)\n\n\n\nFor all tests, specify direction of \\(H_a\\) with the argument: alternative = \"greater\", \"less\", or \"two.sided\".\nTo construct a Confidence Interval: remove the null hypothesis, and specify conf.level=number_here",
    "crumbs": [
      "Computing",
      "Inference Summary Cheatsheet"
    ]
  },
  {
    "objectID": "computing-inferencesummary.html#useful-code-for-calculating-summary-statistics-requires-mosaic",
    "href": "computing-inferencesummary.html#useful-code-for-calculating-summary-statistics-requires-mosaic",
    "title": "Inference Summary Sheet",
    "section": "Useful code for calculating summary statistics: (requires mosaic)",
    "text": "Useful code for calculating summary statistics: (requires mosaic)\n\nMeans: mean( ~ variable, data = data_name) or mean(variable1 ~ variable2, data = data_name)\nCounts/Tables: tally( ~ variable, data = data_name) or tally(variable1 ~ variable2, data = data_name)\n5-number-summary etc: favstats( ~ variable, data = data_name) or favstats(variable1 ~ variable2, data = data_name)",
    "crumbs": [
      "Computing",
      "Inference Summary Cheatsheet"
    ]
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html",
    "href": "ae/ae-17-exam-02-review.html",
    "title": "AE 17: Exam 02 Review",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-17 repo to get started.Render, commit, and push your responses to GitHub by the end of class.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Saturday, December 2 at 11:59pm.\nNote: This in-class review is not exhaustive. Use lecture notes notes, application exercises, labs, and homework for a comprehensive exam review."
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#packages",
    "href": "ae/ae-17-exam-02-review.html#packages",
    "title": "AE 17: Exam 02 Review",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#data",
    "href": "ae/ae-17-exam-02-review.html#data",
    "title": "AE 17: Exam 02 Review",
    "section": "Data",
    "text": "Data\nThe data for this analysis is about credit card customers. It can be found in the file credit.csv. The following variables are in the data set:\n\nincome: Income in $1,000’s\nlimit: Credit limit\nrating: Credit rating\ncards: Number of credit cards\nage: Age in years\neducation: Number of years of education\nown: A factor with levels No and Yes indicating whether the individual owns their home\nstudent: A factor with levels No and Yes indicating whether the individual was a student\nmarried: A factor with levels No and Yes indicating whether the individual was married\nregion: A factor with levels South, East, and West indicating the region of the US the individual is from\nbalance: Average credit card balance in $.\n\nThe objective of this analysis is to predict whether a person has maxed out their credit card, i.e., had $0 average card balance.\n\ncredit &lt;- read_csv(\"data/credit.csv\") |&gt;\n  mutate(maxed = factor(if_else(balance == 0, 1, 0)))"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-1",
    "href": "ae/ae-17-exam-02-review.html#exercise-1",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nWhy is logistic regression the best modeling approach for this analysis?\n\n\n\nFill in the blanks. In logistic regression, we\n\nuse log-odds to …\nuse odds to …\nuse probabilities to …"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-2",
    "href": "ae/ae-17-exam-02-review.html#exercise-2",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 2",
    "text": "Exercise 2\nSplit the data into 80% training and 20% test sets. Use seed 210.\n\nset.seed(210)\ncredit_split &lt;- initial_split(credit, prop = 0.8)\ncredit_train &lt;- training(credit_split)\ncredit_test &lt;- testing(credit_split)\n\nSpecify the logistic regression model. Call it credit_spec.\n\n#add code here"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-3",
    "href": "ae/ae-17-exam-02-review.html#exercise-3",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 3",
    "text": "Exercise 3\nCreate a recipe called credit_rec that does the following:\n\nPredict maxed using income, age, and student.\nMean center the quantitative predictors.\nCreate dummy variables where needed and drop any zero variance variables.\n\n\ncredit_rec &lt;- recipe(maxed ~ income + age + student, \n                     data = credit_train)\n\n# add recipe steps"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-4",
    "href": "ae/ae-17-exam-02-review.html#exercise-4",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 4",
    "text": "Exercise 4\nCreate the workflow that brings together the model specification and recipe. Call it credit_wflow.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-5",
    "href": "ae/ae-17-exam-02-review.html#exercise-5",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 5",
    "text": "Exercise 5\nConduct 5-fold cross validation. Use seed 210 to create the folds.\n\n# add code here\n\nThen, summarize the metrics from your CV resamples.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-6",
    "href": "ae/ae-17-exam-02-review.html#exercise-6",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 6",
    "text": "Exercise 6\nCreate a workflow for another model with a new recipe that includes the variable region along with all the variables and recipe steps from Exercise 3. Conduct cross validation, then select a model between the two.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-7",
    "href": "ae/ae-17-exam-02-review.html#exercise-7",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 7",
    "text": "Exercise 7\nFit the model you selected in the previous exercise to the entire training set.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-8",
    "href": "ae/ae-17-exam-02-review.html#exercise-8",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let’s evaluate the performance of the selected model using the testing data.\nCreate a confusion matrix using a cutoff probability of 0.5.\n\n# add code here\n\n\nWhat is the sensitivity? What does it mean in the context of the data ?\nWhat is the specificity? What does it mean in the context of the data?\nWhat is the false positive rate? What does it mean in the context of the data?\nWhat is the false negative rate? What does it mean in the context of the data?"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-9",
    "href": "ae/ae-17-exam-02-review.html#exercise-9",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 9",
    "text": "Exercise 9\nProduce the ROC curve.\n\n# add code here\n\n\nDescribe how you can use this curve to select a cutoff probability (rather than just going with 0.5)."
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#exercise-10",
    "href": "ae/ae-17-exam-02-review.html#exercise-10",
    "title": "AE 17: Exam 02 Review",
    "section": "Exercise 10",
    "text": "Exercise 10\nQuestions about checking conditions for logistic regression:\n\nDo we assess conditions on the training or testing set?\nWhy do we not consider categorical predictors when checking linearity?\nWhy do we not need to check constant variance for logistic regression?"
  },
  {
    "objectID": "ae/ae-17-exam-02-review.html#submission",
    "href": "ae/ae-17-exam-02-review.html#submission",
    "title": "AE 17: Exam 02 Review",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-17 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-15-logistic-inference.html",
    "href": "ae/ae-15-logistic-inference.html",
    "title": "AE 15: Inference for logistic regression",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-15 repo to get started.Render, commit, and push your responses to GitHub by the end of class.\n\nThe responses are due in your GitHub repo no later than Thursday, November 16 at 11:59pm."
  },
  {
    "objectID": "ae/ae-15-logistic-inference.html#packages",
    "href": "ae/ae-15-logistic-inference.html#packages",
    "title": "AE 15: Inference for logistic regression",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-15-logistic-inference.html#response-to-leukemia-treatment",
    "href": "ae/ae-15-logistic-inference.html#response-to-leukemia-treatment",
    "title": "AE 15: Inference for logistic regression",
    "section": "Response to Leukemia treatment",
    "text": "Response to Leukemia treatment\nToday’s data is from a study where 51 untreated adult patients with Acute Myeloid Leukemia who were given a course of treatment, and they were assessed as to their response to the treatment.1\nThe goal of today’s analysis is to use pre-treatment factors to predict how likely it is a patient will respond to the treatment.\nWe will use the following variables:\n\nAge: Age at diagnosis (in years)\nSmear: Differential percentage of blasts\nInfil: Percentage of absolute marrow leukemia infiltrate\nIndex: Percentage labeling index of the bone marrow leukemia cells\nBlasts: Absolute number of blasts, in thousands\nTemp: Highest temperature of the patient prior to treatment, in degrees Fahrenheit\nResp: 1 = responded to treatment or 0 = failed to respond\n\n\nleukemia &lt;- read_csv(\"data/leukemia.csv\") |&gt;\n  mutate(Resp = factor(Resp))"
  },
  {
    "objectID": "ae/ae-15-logistic-inference.html#estimating-coefficients-for-logistic-regression",
    "href": "ae/ae-15-logistic-inference.html#estimating-coefficients-for-logistic-regression",
    "title": "AE 15: Inference for logistic regression",
    "section": "Estimating coefficients for logistic regression",
    "text": "Estimating coefficients for logistic regression\n\nIllustrating maximum likelihood estimation (MLE)\nSuppose we want to use the data to understand the probability a randomly selected adult diagnosed with leukemia will respond to the treatment.\nHere are the results from our data:\n\nleukemia %&gt;%\n  count(Resp) %&gt;%\n  mutate(prob = n /sum(n))\n\n# A tibble: 2 × 3\n  Resp      n  prob\n  &lt;fct&gt; &lt;int&gt; &lt;dbl&gt;\n1 0        27 0.529\n2 1        24 0.471\n\n\nLet’s set up some notation:\n\n\\(Y\\): Bernoulli random variable indicating whether an adult diagnosed with leukemia responds to the treatment\n\\(y_i\\): outcome for an individual adult (1: responded to treatment or 0: failed to respond)\n\\(p\\): probability a randomly selected adult diagnosed with leukemia responds to the treatment\n\nOur goal is to find the most likely value of \\(p\\) given our data.\nThe likelihood function of the outcome for a single randomly selected adult diagnosed with leukemia is \\[P(Y = y_i) = p^{y_i}(1-p)^{1-y_i}\\]\nThe likelihood function, i.e. the joint probability, for the outcomes of 51 adults is\n\\[L = P(y_1, y_2, \\ldots, y_{51}) = \\prod_{i = 1}^{51} p^{y_i} (1 - p)^{1 - y_i}\\]\nWe often use the log of the likelihood function, so we can work with sums rather than products.\n\\[\\begin{aligned}\\log L = \\log[P(y_1, y_2, \\ldots, y_{51})] &= \\log\\Big[\\prod_{i = 1}^{51} p^{y_i} (1 - p)^{1 - y_i}\\Big] \\\\\n&= \\sum_{i=1}^{51} y_i \\log(p) + (1 - y_i)\\log(1 - p)\\end{aligned}\\]\nWe can rewrite the equation and plug in the results from our data:\n\\[\\begin{aligned} \\log L &= \\sum_{i=1}^{51} y_i \\log(p) + (1 - y_i)\\log(1 - p) \\\\\n& = \\log(p)\\sum_{i=1}^{51}y_i + \\log(1 - p)\\sum_{i=1}^{51}(1 - y_{i}) \\\\\n& = \\log(p)\\times 24 + \\log(1 - p)\\times 27 \\end{aligned}\\]\nLet’s plug in different values of \\(p\\) and find the value that maximizes this equation.\n\np &lt;- seq(0.01,.99,0.005)\nlogL &lt;- log(p)*24+ log(1-p)*27 #log likelihood\nL &lt;- exp(logL) #likelihood\nresults &lt;- tibble(p, logL, L)\n\n#identify the the mle\nresults &lt;- results %&gt;%\n  mutate(mle = if_else(logL == max(results$logL), \"1\", \"0\"))\n\n\n# plot log likelihood\nggplot(data = results, aes(x = p, y = logL)) +\n  geom_point(aes(color = mle)) + \n  geom_line() +\n  scale_color_manual(values = c(\"gray\", \"red\")) + \n  scale_x_continuous(breaks = seq(0,1,.1)) + \n  labs(title = \"Maximizing Log Liklehood\") \n\n\n\n\n\n\n\n\n\n#plot likelihood\nggplot(data = results, aes(x = p, y = L)) +\n  geom_point(aes(color = mle)) + \n  geom_line() +\n  scale_color_manual(values = c(\"gray\", \"red\")) + \n  scale_x_continuous(breaks = seq(0,1,.1)) + \n  labs(title = \"Maximizing Likelihood\") \n\n\n\n\n\n\n\n\nThe MLE, the value that maximizes the (log) likelihood function is\n\nresults %&gt;%\n  filter(mle == 1)\n\n# A tibble: 1 × 4\n      p  logL        L mle  \n  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;\n1  0.47 -35.3 4.85e-16 1    \n\n\nAs we might expect, this is the sample proportion \\(\\hat{p}\\) from our data.\n\n\nExtending to logistic regression\nThe parameters for logistic regression are estimated using maximum likelihood estimation. Suppose we have the model\n\\[\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1 x\\]\nThe estimates \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) are the ones maximize the log likelihood function\n\\[L = P(y_1, y_2, \\ldots, y_n) = \\prod_{i = 1}^{n} \\pi^{y_i}(1-\\pi)^{1-y_i}\\]\nwhere \\[\\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1x\\}}{1 + \\exp\\{\\beta_0 + \\beta_1x\\}}\\]\nThere isn’t a closed-form solution for the log-likelihood, so numerical methods such as Iterative Weighted Least Squares are used to estimate the coefficients (learn more about IWLS in STA 211 and STA 310). This is the method used to estimate the coefficients in R.\nLet’s fit a model using Age as a predictor and see some details of coefficient estimation.2 This is for demonstration purposes.\n\n# Function that allows us to see the coefficients at each iteration\ntrace(glm.fit, quote(cat &lt;- function(...) {\n  base::cat(...)\n  if (...length() &gt;= 3 && identical(..3, \" Iterations - \")) print(coefold)\n}))\n\n[1] \"glm.fit\"\n\n# Fit model to see details\nresp_fit_details &lt;- glm(Resp ~ Age, data = leukemia, \n                        family = \"binomial\", \n                        control = glm.control(trace = TRUE))\n\nTracing glm.fit(x = structure(c(1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,  .... on entry \nDeviance = 64.07455 Iterations - 1\nNULL\nDeviance = 64.0038 Iterations - 2\n[1]  2.44957126 -0.05199527\nDeviance = 64.00374 Iterations - 3\n[1]  2.18965178 -0.04660907\nDeviance = 64.00374 Iterations - 4\n[1]  2.196772 -0.046760\n\nuntrace(glm.fit)\n\nFinal model\n\nresp_fit &lt;- logistic_reg() |&gt;\n  fit(Resp ~ Age, data = leukemia, family = \"binomial\")\n\ntidy(resp_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   2.20      1.01        2.18  0.0289\n2 Age          -0.0468    0.0195     -2.39  0.0166"
  },
  {
    "objectID": "ae/ae-15-logistic-inference.html#inference-for-coefficients",
    "href": "ae/ae-15-logistic-inference.html#inference-for-coefficients",
    "title": "AE 15: Inference for logistic regression",
    "section": "Inference for coefficients",
    "text": "Inference for coefficients\n\nFit a model using Temp , Age, and Index to predict whether or not a patient responds to the treatment. Display the model and the 95% confidence interval for the model coefficients.\n\n\n# add code here\n\n\nInterpret the coefficient of Temp in the context of the data in terms of the odds an individual responds to the treatment.\nInterpret the 95% confidence interval for the coefficient of Temp in terms of the odds an individual responds to the treatment.\nWhat is the distribution of the test statistic associated with Temp?\nIs Temp a useful predictor of the likelihood an individual responds to the treatment? Briefly explain."
  },
  {
    "objectID": "ae/ae-15-logistic-inference.html#submission",
    "href": "ae/ae-15-logistic-inference.html#submission",
    "title": "AE 15: Inference for logistic regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-15 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-15-logistic-inference.html#footnotes",
    "href": "ae/ae-15-logistic-inference.html#footnotes",
    "title": "AE 15: Inference for logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data set is from the Stat2Data R package. This AE is adapted from exercises in Stat 2.↩︎\nTrace function for current version of R from StackOverflow.↩︎"
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html",
    "href": "ae/ae-12-logistic-intro.html",
    "title": "AE 12: Logistic regression introduction",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-12 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, November 2 at 11:59pm."
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#packages",
    "href": "ae/ae-12-logistic-intro.html#packages",
    "title": "AE 12: Logistic regression introduction",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\nheart_disease &lt;- read_csv(\"data/framingham.csv\") |&gt;\n  select(totChol, TenYearCHD) |&gt;\n  drop_na() |&gt;\n  mutate(high_risk = as.factor(TenYearCHD)) |&gt;\n  select(totChol, high_risk)\n\n\nData: Framingham study\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use the total cholesterol to predict if a randomly selected adult is high risk for heart disease in the next 10 years.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\ntotChol: total cholesterol (mg/dL)\n\n\n\nOutcome: high_risk\n\nggplot(data = heart_disease, aes(x = high_risk)) + \n  geom_bar() + \n  scale_x_discrete(labels = c(\"1\" = \"High risk\", \"0\" = \"Low risk\")) +\n  labs(\n    title = \"Distribution of 10-year risk of heart disease\", \n    x = NULL)\n\n\n\n\n\n\n\n\n\nheart_disease |&gt;\n  count(high_risk)\n\n# A tibble: 2 × 2\n  high_risk     n\n  &lt;fct&gt;     &lt;int&gt;\n1 0          3555\n2 1           635"
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#calculating-probability-and-odds",
    "href": "ae/ae-12-logistic-intro.html#calculating-probability-and-odds",
    "title": "AE 12: Logistic regression introduction",
    "section": "Calculating probability and odds",
    "text": "Calculating probability and odds\n\nWhat is the probability a randomly selected person in the study is not high risk for heart disease in the next 10 years?\nWhat are the odds a randomly selected person in the study is not high risk for heart disease in the next 10 years?"
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#logistic-regression-model",
    "href": "ae/ae-12-logistic-intro.html#logistic-regression-model",
    "title": "AE 12: Logistic regression introduction",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nFit a logistic regression model to understand the relationship between total cholesterol and risk for heart disease.\nLet \\(\\pi\\) be the probability an adult is high risk of heart disease. The statistical model is\n\\[\\log\\Big(\\frac{\\pi_i}{1-\\pi_i}\\Big) = \\beta_0 + \\beta_1 TotChol_i\\]\n\nheart_disease_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ totChol, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.894\n0.230\n-12.607\n0\n\n\ntotChol\n0.005\n0.001\n5.268\n0\n\n\n\n\n\n\nWrite the regression equation. Round to 3 digits."
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#calculating-log-odds-odds-and-probabilities",
    "href": "ae/ae-12-logistic-intro.html#calculating-log-odds-odds-and-probabilities",
    "title": "AE 12: Logistic regression introduction",
    "section": "Calculating log-odds, odds and probabilities",
    "text": "Calculating log-odds, odds and probabilities\nBased on the model, if a randomly selected person has a total cholesterol of 250 mg/dL,\n\nWhat are the log-odds they are high risk for heart disease in the next 10 years?\nWhat are the odds they are high risk for heart disease in the next 10 years?\nWhat is the probability they are high risk for heart disease in the next 10 years? Use the odds to calculate your answer."
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#comparing-observations",
    "href": "ae/ae-12-logistic-intro.html#comparing-observations",
    "title": "AE 12: Logistic regression introduction",
    "section": "Comparing observations",
    "text": "Comparing observations\nSuppose a person’s cholesterol changes from 250 mg/dL to 200 mg/dL.\n\nHow do you expect the log-odds that this person is high risk for heart disease in the next 10 years to change?\nHow do you expect the odds that this person is high risk for heart disease in the next 10 years to change?"
  },
  {
    "objectID": "ae/ae-12-logistic-intro.html#submission",
    "href": "ae/ae-12-logistic-intro.html#submission",
    "title": "AE 12: Logistic regression introduction",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-12 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-10-model-workflow.html",
    "href": "ae/ae-10-model-workflow.html",
    "title": "AE 10: Model workflow",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-10 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Saturday, October 14 at 11:59pm."
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#packages-data",
    "href": "ae/ae-10-model-workflow.html#packages-data",
    "title": "AE 10: Model workflow",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\nThe data for this AE is from the loan50 data set in the openintro R package. We will focus on the following variables:\nPredictors\n\nannual_income: Annual income (in US dollars)\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\nResponse\n\ninterest_rate: Interest rate for the loan (0- 100)"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#analysis-goal",
    "href": "ae/ae-10-model-workflow.html#analysis-goal",
    "title": "AE 10: Model workflow",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this analysis are to build a recipe to fit a linear regression model on the training data that has the following features:\n\nannual_income rescaled to thousands of dollars\n\nDo not include the original variable annual_income in the model\n\nMean-centered quantitative variables\nIndicator (dummy) variables for the categorical predictor\nInteraction term between rescaled annual_income and verified_income\n\nand (2) use prep() and bake() to check the recipe"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#relevel-verified_income",
    "href": "ae/ae-10-model-workflow.html#relevel-verified_income",
    "title": "AE 10: Model workflow",
    "section": "Relevel verified_income",
    "text": "Relevel verified_income\nMake Verified the baseline level for the model.\n\nloan50 &lt;- loan50 |&gt;\n  mutate(verified_income = factor(verified_income,\n                                  levels = c(\"Verified\", \"Not Verified\", \n                                             \"Source Verified\")))"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#testtrain-split",
    "href": "ae/ae-10-model-workflow.html#testtrain-split",
    "title": "AE 10: Model workflow",
    "section": "Test/train split",
    "text": "Test/train split\nSplit the data into 90% training, 10% testing.\n\nset.seed(123)\n\nloan_split &lt;- initial_split(loan50, prop = 0.9)\nloan_train &lt;- training(loan_split)\nloan_test  &lt;- testing(loan_split)"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#build-and-troubleshoot-recipe",
    "href": "ae/ae-10-model-workflow.html#build-and-troubleshoot-recipe",
    "title": "AE 10: Model workflow",
    "section": "Build (and troubleshoot) recipe",
    "text": "Build (and troubleshoot) recipe\n\nUse step_mutate() to create a new variable annual_income_th that is annual_income rescaled to thousands of dollars\nUse step_rm() to remove annual_income from the model\nUse step_center() to mean-center quantitative variables\nUse step_dummy() to create indicator variables for the categorical predictor\nUse step_interact() to create interaction between annual_income_th and verified_income\n\n\nloan_rec &lt;-  recipe(interest_rate ~ annual_income + debt_to_income + verified_income, \n                    data = loan_train) |&gt;\n  step_mutate(annual_income_th = annual_income / 1000) |&gt;\n  step_rm(annual_income) |&gt;\n  step_center(all_numeric_predictors()) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_interact(terms = ~ annual_income_th:verified_income) \n\n\nloan_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Variable mutation for: annual_income / 1000\n\n\n• Variables removed: annual_income\n\n\n• Centering for: all_numeric_predictors()\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Interactions with: annual_income_th:verified_income\n\n\n\nCheck recipe using prep() and bake()\nOnce you’ve corrected the code, remove #| eval: false before rendering the document.\n\nloan_rec |&gt;\n  prep() |&gt;\n  bake(loan_train) |&gt;\n  glimpse()\n\n\nIn which step do we have an error?\nClick here to access the recipes reference page. Find the reference page for the relevant step_ function.\nSee the examples at the bottom of the reference page. Which model most closely aligns the interaction we’re trying to create?\nUse example to help you fix the code. Then, use prep() and bake() to see the updated results."
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#workflows-and-model-fitting",
    "href": "ae/ae-10-model-workflow.html#workflows-and-model-fitting",
    "title": "AE 10: Model workflow",
    "section": "Workflows and model fitting",
    "text": "Workflows and model fitting\n\nSpecify model\n\nloan_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\nloan_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm \n\n\n\n\nBuild workflow\n\nloan_wflow &lt;- workflow() |&gt;\n  add_model(loan_spec) |&gt;\n  add_recipe(loan_rec)\n\nloan_wflow\n\n\n\nFit model to training data\nRemove #| eval: false before rendering the document.\n\nloan_fit &lt;- loan_wflow |&gt;\n  fit(data = loan_train)\n\ntidy(loan_fit) |&gt;\n  kable(digits = 3)"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#evaluate-model-on-training-data",
    "href": "ae/ae-10-model-workflow.html#evaluate-model-on-training-data",
    "title": "AE 10: Model workflow",
    "section": "Evaluate model on training data",
    "text": "Evaluate model on training data\n\nMake predictions\nFill in the code and remove #| eval: false before rendering the document.\n\nloan_train_pred &lt;- predict(loan_fit, ______) |&gt;\n  bind_cols(_____)\n\n\n\nCalculate \\(R^2\\)\nFill in the code and remove #| eval: false before rendering the document.\n\nrsq(loan_train_pred, truth = _____, estimate = _____)\n\n\n\nCalculate RMSE\nFill in the code and remove #| eval: false before rendering the document.\n\nrmse(______, ________, ________)\n\n\nIs this RMSE considered high or low? Hint: Consider the range of the response variable to answer this question.\n\nloan_train |&gt;\n  summarise(min = min(interest_rate), max = max(interest_rate))\n\n# A tibble: 1 × 2\n    min   max\n  &lt;dbl&gt; &lt;dbl&gt;\n1  5.31  26.3"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#evaluate-model-on-testing-data",
    "href": "ae/ae-10-model-workflow.html#evaluate-model-on-testing-data",
    "title": "AE 10: Model workflow",
    "section": "Evaluate model on testing data",
    "text": "Evaluate model on testing data\n\nMake predictions\n\n# fill in code to make predictions from testing data\n\n\n\nCalculate \\(R^2\\)\n\n# fill in code to calculate R-sq for testing data\n\n\n\nCalculate RMSE\n\n# fill in code to calculate RMSE for testing data"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#compare-training-and-testing-data-results",
    "href": "ae/ae-10-model-workflow.html#compare-training-and-testing-data-results",
    "title": "AE 10: Model workflow",
    "section": "Compare training and testing data results",
    "text": "Compare training and testing data results\n\nCompare the \\(R^2\\) for the training and testing data. Is this what you expected?\nCompare the RMSE for the training and testing data. Is this what you expected?"
  },
  {
    "objectID": "ae/ae-10-model-workflow.html#to-submit-the-ae",
    "href": "ae/ae-10-model-workflow.html#to-submit-the-ae",
    "title": "AE 10: Model workflow",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-10 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html",
    "href": "ae/ae-08-exam-01-review.html",
    "title": "Exam 01 Review/Practice",
    "section": "",
    "text": "Important\n\n\n\nNote that you are not required to turn anything in for this assignment and you should complete this on a sheet of paper."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#packages",
    "href": "ae/ae-08-exam-01-review.html#packages",
    "title": "Exam 01 Review/Practice",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(yardstick)\nlibrary(ggformula)\nlibrary(knitr)\nlibrary(infer)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#restaurant-tips",
    "href": "ae/ae-08-exam-01-review.html#restaurant-tips",
    "title": "Exam 01 Review/Practice",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nMeal: which meal this was (Lunch, Late Lunch, Dinner)\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips &lt;- read_csv(\"data/tip-data.csv\")\n\nglimpse(tips)\n\nRows: 169\nColumns: 13\n$ Day              &lt;chr&gt; \"Saturday\", \"Saturday\", \"Tuesday\", \"Tuesday\", \"Tuesda…\n$ Meal             &lt;chr&gt; \"Dinner\", \"Dinner\", \"Dinner\", \"Dinner\", \"Dinner\", \"Di…\n$ Payment          &lt;chr&gt; \"Credit\", \"Credit\", \"Credit\", \"Credit\", \"Cash\", \"Cred…\n$ Party            &lt;dbl&gt; 1, 1, 1, 3, 2, 2, 4, 3, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2,…\n$ Age              &lt;chr&gt; \"Yadult\", \"Yadult\", \"SenCit\", \"Middle\", \"SenCit\", \"Mi…\n$ GiftCard         &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Comps            &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\"…\n$ Alcohol          &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bday             &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bill             &lt;dbl&gt; 17.01, 14.23, 20.97, 20.87, 34.66, 25.15, 48.17, 22.6…\n$ `W/Tip`          &lt;dbl&gt; 20.00, 16.23, 25.97, 24.87, 45.00, 30.00, 53.17, 26.6…\n$ Tip              &lt;dbl&gt; 2.99, 2.00, 5.00, 4.00, 10.34, 4.85, 5.00, 4.00, 5.00…\n$ `Tip Percentage` &lt;dbl&gt; 0.1757790, 0.1405481, 0.2384359, 0.1916627, 0.2983266…"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exploratory-data-analysis",
    "href": "ae/ae-08-exam-01-review.html#exploratory-data-analysis",
    "title": "Exam 01 Review/Practice",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\np1 &lt;- tips |&gt; \n  gf_histogram(~Tip, color = \"white\", binwidth = 2) |&gt; \n  gf_labs(x = \"Tips ($)\",\n       title = \"Tips at local restaurant\")\n\np2 &lt;-  tips |&gt; \n  gf_histogram(~Tip, color = \"white\") |&gt; \n  gf_labs(x = \"Party\",\n       title = \"Number of diners in party\") |&gt; \n  gf_lims(x=c(0, 7))\n\np3 &lt;- tips |&gt;\n  gf_bar(~ Meal) |&gt; \n  gf_labs(x = \"Meal\",\n       title = \"Number of meal types\")\n\np4 &lt;- tips |&gt;\n  gf_jitter(Tip ~ Party) |&gt; \n  gf_labs(x = \"Number of diners in party\", \n       y = \"Tips ($)\",\n       title = \"Tips vs. Party\")\n\np5 &lt;- tips |&gt;\n  gf_jitter(Tip ~ Meal, height = 0, width = 0.25) |&gt; \n  gf_labs(x = \"Meal\", \n       y = \"Tips ($)\",\n       title = \"Tips vs. Meal\")\n\n(p1 + p2) / p3\n\n\n\n\n\n\n\n(p4 + p5)\n\n\n\n\n\n\n\n\nThe goal is to fit a model that uses the number of diners in the party and the meal to understand variability in the tips. For Exercise 2-8, assume we are only using Party and not Meal to predict Tip."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-1",
    "href": "ae/ae-08-exam-01-review.html#exercise-1",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nWhat is another type of plot we could use to look at the relationship between Meal and Tips?\nWhy might we want to use a jitter plot instead of a scatter plot to display the relationship between Party and Tips?"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-2",
    "href": "ae/ae-08-exam-01-review.html#exercise-2",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 2",
    "text": "Exercise 2\nWrite the statistical model that we will be trying to estimate. Use Greek letters and include an error term."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-3",
    "href": "ae/ae-08-exam-01-review.html#exercise-3",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe regression output with 90% confidence intervals for the coefficients is shown below.\n\ntips_fit &lt;- lm(Tip ~ Party, data = tips)\n\ntips_fit |&gt; \n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.383\n0.321\n1.195\n0.234\n\n\nParty\n1.957\n0.118\n16.553\n0.000\n\n\n\n\n\n\nWrite the regression equation and interpret the slope in the context of the data.\nWhat would your model predict is the Tip for a party of size 3?\nOne family for four walks into your restaurant. Consider the following two predictions and intervals for the Tip left by this family. Which one is appropriate for this scenario? If you chose the wider one, explain why it is larger. If you chose the narrower one, explain why it is smaller.\n\n\nfamily &lt;- tibble(Party = 4)\npredict(tips_fit, newdata = family, interval = \"confidence\") |&gt; kable(digits = 3)\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n8.21\n7.712\n8.709\n\n\n\n\npredict(tips_fit, newdata = family, interval = \"prediction\") |&gt; kable(digits = 3)\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n8.21\n4.068\n12.353"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-4",
    "href": "ae/ae-08-exam-01-review.html#exercise-4",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nglance(tips_fit) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.621\n0.619\n2.083\n273.996\n0\n1\n-362.794\n731.588\n740.978\n724.472\n167\n169\n\n\n\n\n\nWhat is the regression error for the model. Write the definition of this value in the context of the data."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-5",
    "href": "ae/ae-08-exam-01-review.html#exercise-5",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 5",
    "text": "Exercise 5\n\ntips_aug &lt;- augment(tips_fit)\n\nrsq(tips_aug, truth = Tip, estimate = .fitted) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.6213116\n\n\n\n\nrmse(tips_aug, truth = Tip, estimate = .fitted) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n2.070463\n\n\n\n\n\n\nDefine \\(R^2\\) and interpret it in the context of the data.\nDefine RMSE and interpret it in the context of the data."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-6",
    "href": "ae/ae-08-exam-01-review.html#exercise-6",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 6",
    "text": "Exercise 6\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we’ll focus primarily on the slope in our inference) for the coefficient of Party in our linear model. Use the plot below to (visually) construct a 90% confidence interval for the slope:\n\nDescribe why you chose the values you chose for your interval.\nInterpret the interval in the context of the data.\nHow would increasing the number of repetitions change the size of the confidence interval?\nHow would increasing the sample size change the size of the confidence interval?\nHow would increasing the confidence level change the size of the condidence interval?\n\n\nset.seed(1234)\n\nboot_dist &lt;- tips |&gt;\n  specify(Tip ~ Party) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_dist |&gt;\n  filter(term == \"Party\") |&gt; \n  gf_histogram(~estimate)"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-7",
    "href": "ae/ae-08-exam-01-review.html#exercise-7",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 7",
    "text": "Exercise 7\nSet-up a hypothesis test for the slope of Party. Make sure to include:\n\nBoth hypotheses in mathematical notation and words.\nThe test statistic.\nThe distribution of the test statistic.\nThe p-value.\nThe result of your test at a significance level of \\(\\alpha= 0.05\\).\n\nYou may want to refer to the output in Exercise 3."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-8",
    "href": "ae/ae-08-exam-01-review.html#exercise-8",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 8",
    "text": "Exercise 8\nList the conditions necessary for conducting inference. Include how you would test each one and how you would determine if they were satisfied.\nExtra credit: Based on the context of this problem, you should expect that the constant variance assumption is likely to be violated… why? Think about how people tip."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-9",
    "href": "ae/ae-08-exam-01-review.html#exercise-9",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 9",
    "text": "Exercise 9\nConsider two models:\nModel 1\n\nlm(Tip ~ Party + Meal, data = tips) |&gt;\n  tidy() |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.142\n0.341\n3.345\n0.001\n\n\nParty\n1.852\n0.116\n15.984\n0.000\n\n\nMealLate Night\n-1.842\n0.386\n-4.774\n0.000\n\n\nMealLunch\n-0.582\n0.402\n-1.446\n0.150\n\n\n\n\n\nModel 2\n\nlm(Tip ~ Party + Meal + Party*Meal, data = tips) |&gt;\n  tidy() |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.209\n0.453\n2.672\n0.008\n\n\nParty\n1.824\n0.169\n10.795\n0.000\n\n\nMealLate Night\n-2.136\n0.682\n-3.133\n0.002\n\n\nMealLunch\n-0.419\n0.867\n-0.484\n0.629\n\n\nParty:MealLate Night\n0.159\n0.283\n0.560\n0.576\n\n\nParty:MealLunch\n-0.052\n0.283\n-0.184\n0.854\n\n\n\n\n\n\nFor model 1, interpret the slope and p-value of Party in the context of the problem.\nFor model 1, what is the reference level of Meal.\nWhich model has the lower RMSE? How do you know that this is true without being able to see it?\nConsider the following two plots of the data. Roughly sketch model 1 on the left and model 2 on the right. You will receive full credit if (a) positive slopes are positive and negative slopes are negative, (b) steeper slopes are steeper, and (c) higher intercepts are higher.\n\n\np1 &lt;- tips |&gt; \n  gf_jitter(Tip ~ Party, color = ~Meal) |&gt; \n  gf_labs(title = \"Sketch model 1\")\n\np2 &lt;- tips |&gt; \n  gf_jitter(Tip ~ Party, color = ~Meal) |&gt; \n  gf_labs(title = \"Sketch model 2\")\n\np1 + p2 + plot_layout(guides = \"collect\")"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-10",
    "href": "ae/ae-08-exam-01-review.html#exercise-10",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 10",
    "text": "Exercise 10\n\n\n\n\n\n\n\nBased on your model, predict the tip for a party of 4.\nSuppose you’re asked to construct a 95% confidence and a 95% prediction interval for your finding in the previous exercise. Which one would you expect to be wider and why? In your answer clearly state the difference between these intervals.\nNow construct the intervals and comment on whether your guess is confirmed.\n\n\n\n\n\n# add your code here"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#to-submit-the-ae",
    "href": "ae/ae-08-exam-01-review.html#to-submit-the-ae",
    "title": "Exam 01 Review/Practice",
    "section": "To submit the AE:",
    "text": "To submit the AE:\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-08 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#footnotes",
    "href": "ae/ae-08-exam-01-review.html#footnotes",
    "title": "Exam 01 Review/Practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.↩︎"
  },
  {
    "objectID": "ae/ae-06-conditions.html",
    "href": "ae/ae-06-conditions.html",
    "title": "AE 06: Model Conditions",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-06”\nGo to the Canvas and locate your AE 06 assignment to get started.\nUpload the ae-06.qmd and spotify-popular.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas no later than Monday, September 16 at 11:59pm.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-06-conditions.html#data",
    "href": "ae/ae-06-conditions.html#data",
    "title": "AE 06: Model Conditions",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is a subset from the Spotify Songs Tidy Tuesday data set. The data were originally obtained from Spotify using the spotifyr R package.\nIt contains numerous characteristics for each song. You can see the full list of variables and definitions here. This analysis will focus specifically on the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\n\n\nspotify &lt;- read_csv(\"data/spotify-popular.csv\")\n\nAre high energy songs more positive? To answer this question, we’ll analyze data on some of the most popular songs on Spotify, i.e. those with track_popularity &gt;= 80. We’ll use linear regression to fit a model to predict a song’s positiveness (valence) based on its energy level (energy).\nBelow are plots as part of the exploratory data analysis.\n\np1 &lt;- gf_histogram(~valence, data = spotify) |&gt;  \n  gf_labs(title = \"Distribution of Valence\", \n       subtitle = \" for Popular songs on Spotify\", \n       x = \"Valence\")\n\np2 &lt;- gf_histogram(~energy, data = spotify) |&gt; \n  gf_labs(title = \"Distribution of Energy\", \n       subtitle = \"for Popular songs on Spotify\", \n       x = \"Energy\")\np1 + p2 # The patchwork package will arrange your plots for you\n\n\n\n\n\n\n\n\n\ngf_point(valence ~ energy, data = spotify) |&gt; \n  gf_labs(title = \"Valence vs. Energy\", \n       subtitle = \"Popular songs on Spotify\", \n       x = \"Energy\", \n       y = \"Valence\")"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-1",
    "href": "ae/ae-06-conditions.html#exercise-1",
    "title": "AE 06: Model Conditions",
    "section": "Exercise 1",
    "text": "Exercise 1\nLast time, we were too hasty in performing statistical inference. We need to make sure you data satifies the four conditions of inference. Let’s check those model conditions now. Fill in the code below to use the augment() function to create a new data frame containing the residuals and fitted values (among other information)/\n\n\n\n\n\n\nImportant\n\n\n\nNote: Remove #|eval: false from the code chunk after you have filled in the code.\n\n\n\nspotify_aug &lt;- augment(_____)"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-2",
    "href": "ae/ae-06-conditions.html#exercise-2",
    "title": "AE 06: Model Conditions",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet’s check the model conditions before doing any inference. Fill in the code below to use the augment() function to create a new data frame containing the residuals and fitted values (among other information)\n\n\n\n\n\n\nImportant\n\n\n\nNote: Remove #|eval: false from the code chunk after you have filled in the code.\n\n\n\nspotify_aug &lt;- augment(_____)"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-3",
    "href": "ae/ae-06-conditions.html#exercise-3",
    "title": "AE 06: Model Conditions",
    "section": "Exercise 3",
    "text": "Exercise 3\nMake a plot of the residual vs. fitted values.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-4",
    "href": "ae/ae-06-conditions.html#exercise-4",
    "title": "AE 06: Model Conditions",
    "section": "Exercise 4",
    "text": "Exercise 4\nFill in the code to make a histogram of the residuals and a normal QQ-plot.\n\nresid_hist &lt;- gf_histogram(~____, data = ____)) |&gt; \n  gf_labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_qq &lt;- gf_qq(~____, data = ____)  |&gt;\n  gf_qqline() |&gt; \n  gf_labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_hist + resid_qq"
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-5",
    "href": "ae/ae-06-conditions.html#exercise-5",
    "title": "AE 06: Model Conditions",
    "section": "Exercise 5",
    "text": "Exercise 5\nAssess the four model conditions. Use the plots from the previous exercises to help make the assessment.\n\nLinearity\nConstant variance\nNormality\nIndependence\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-6",
    "href": "ae/ae-06-conditions.html#exercise-6",
    "title": "AE 06: Model Conditions",
    "section": "Exercise 6",
    "text": "Exercise 6\nCalculate \\(R^2\\) and interpret this value in the context of the data."
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-7",
    "href": "ae/ae-06-conditions.html#exercise-7",
    "title": "AE 06: Model Conditions",
    "section": "Exercise 7",
    "text": "Exercise 7\nCalculate RMSE and interpret this value in the context of the data.\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-06 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-03-bootstrap.html",
    "href": "ae/ae-03-bootstrap.html",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-03”\nGo to the course Canvas page and locate your AE 03 assignment to get started.\nUpload the ae-03.qmd and dcbikeshare.csv files into the folder you just created.\n\nThe .qmd and .pdf files uploaded to Canvas no later than Monday, September 9 at 11:59pm.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(infer)\nlibrary(openintro)\nlibrary(kableExtra)"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#data",
    "href": "ae/ae-03-bootstrap.html#data",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012 filtered to only contain the winter months. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\n\n\nwinter &lt;- read_csv(\"../ae/data/dcbikeshare.csv\") |&gt;\n  mutate(season = case_when(\n    season == 1 ~ \"winter\",\n    season == 2 ~ \"spring\",\n    season == 3 ~ \"summer\",\n    season == 4 ~ \"fall\"\n  ),\n  season = factor(season)) |&gt;\n  filter(season == \"winter\")\n\nRows: 731 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (16): instant, season, yr, mnth, holiday, weekday, workingday, weathers...\ndate  (1): dteday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(winter)\n\nRows: 181\nColumns: 17\n$ instant    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dteday     &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-05…\n$ season     &lt;fct&gt; winter, winter, winter, winter, winter, winter, winter, win…\n$ yr         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mnth       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ holiday    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ weekday    &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,…\n$ workingday &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,…\n$ weathersit &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2,…\n$ temp       &lt;dbl&gt; 0.3441670, 0.3634780, 0.1963640, 0.2000000, 0.2269570, 0.20…\n$ atemp      &lt;dbl&gt; 0.3636250, 0.3537390, 0.1894050, 0.2121220, 0.2292700, 0.23…\n$ hum        &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261,…\n$ windspeed  &lt;dbl&gt; 0.1604460, 0.2485390, 0.2483090, 0.1602960, 0.1869000, 0.08…\n$ casual     &lt;dbl&gt; 331, 131, 120, 108, 82, 88, 148, 68, 54, 41, 43, 25, 38, 54…\n$ registered &lt;dbl&gt; 654, 670, 1229, 1454, 1518, 1518, 1362, 891, 768, 1280, 122…\n$ count      &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 126…\n$ temp_orig  &lt;dbl&gt; 14.110847, 14.902598, 8.050924, 8.200000, 9.305237, 8.37826…"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#exploratory-data-analysis",
    "href": "ae/ae-03-bootstrap.html#exploratory-data-analysis",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\ngf_point(count ~ temp_orig, data = winter, alpha = 0.7) |&gt; \n  gf_labs(\n    x = \"Temperature (C)\",\n    y = \"Number of Bike Rentals\",\n  )"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#model",
    "href": "ae/ae-03-bootstrap.html#model",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Model",
    "text": "Model\n\nmodel_fit &lt;- lm(count ~ temp_orig, data = winter)\n\ntidy(model_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-111.04\n238.31\n-0.47\n0.64\n\n\ntemp_orig\n222.42\n18.46\n12.05\n0.00"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#bootstrap-confidence-interval",
    "href": "ae/ae-03-bootstrap.html#bootstrap-confidence-interval",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Bootstrap confidence interval",
    "text": "Bootstrap confidence interval\n\n1. Calculate the observed fit (slope)\n\nobserved_fit &lt;- winter |&gt;\n  specify(count ~ temp_orig) |&gt;\n  fit()\n\nobserved_fit\n\n# A tibble: 2 × 2\n  term      estimate\n  &lt;chr&gt;        &lt;dbl&gt;\n1 intercept    -111.\n2 temp_orig     222.\n\n\n\n\n2. Take n bootstrap samples and fit models to each one.\nFill in the code, then set eval: true .\n\nn = 100\nset.seed(212)\n\nboot_fits &lt;- ______ |&gt;\n  specify(______) |&gt;\n  generate(reps = ____, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_fits\n\n\nWhy do we set a seed before taking the bootstrap samples?\nMake a histogram of the bootstrap samples to visualize the bootstrap distribution.\n\n# Code for histogram\n\n\n\n\n3. Compute the 95% confidence interval as the middle 95% of the bootstrap distribution\nFill in the code, then set eval: true .\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = _____, \n  level = ____,\n  type = \"percentile\"\n)"
  },
  {
    "objectID": "ae/ae-03-bootstrap.html#changing-confidence-level",
    "href": "ae/ae-03-bootstrap.html#changing-confidence-level",
    "title": "AE 03: Bootstrap confidence intervals",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nModify the code from Step 3 to create a 90% confidence interval.\n\n# Paste code for 90% confidence interval\n\n\n\nModify the code from Step 3 to create a 99% confidence interval.\n\n# Paste code for 90% confidence interval\n\n\nWhich confidence level produces the most accurate confidence interval (90%, 95%, 99%)? Explain\nWhich confidence level produces the most precise confidence interval (90%, 95%, 99%)? Explain\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF file with all of your work from today’s class.\nUpload your .qmd file and your .pdf file to Canvas. Note, please unzip the folder you download from the RStudio platform before uploading it."
  },
  {
    "objectID": "ae/ae-02-bikeshare.html",
    "href": "ae/ae-02-bikeshare.html",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "",
    "text": "Important\n\n\n\nFor this AE, you will discuss the questions in groups and submit answers on Poll Everywhere. This AE does not count towards the Application Exercise grade.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#data",
    "href": "ae/ae-02-bikeshare.html#data",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-02-bikeshare.html#daily-counts-and-temperature",
    "href": "ae/ae-02-bikeshare.html#daily-counts-and-temperature",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Daily counts and temperature",
    "text": "Daily counts and temperature\nLet first visualize the distribution of daily bike rentals and temperature as well as the relationship between these two variables.\n\np1 &lt;- gf_histogram(~count, data = bikeshare, binwidth = 250) |&gt;  \n  gf_labs(x = \"Daily bike rentals\")\n\np2 &lt;- gf_histogram(~temp_orig, data = bikeshare) |&gt; \n  gf_labs(x = \"Temperature (Celsius)\")\n\np3 &lt;- gf_point(count ~ temp_orig, data = bikeshare)  |&gt;  \n  gf_labs(x = \"Temperature (Celsius)\", \n       y = \"Daily bike rentals\")\n\n(p1 | p2) / p3\n\n\n\n\n\n\n\n\nSubmit your answers to the following questions to Poll Everywhere.\n\nExercise 1\nDescribe the relationship between daily bike rentals and temperature. Comment on how we expect the number of bike rentals to change as the temperature increases.\n\n\nExercise 2\nSuppose you want to fit a model so you can use the temperature to predict the number of bike rentals. Would a model of the form\n\\[\\text{count} = \\beta_0 + \\beta_1 ~ \\text{temp\\_orig} + \\epsilon\\]\nbe an appropriate fit for the data? Why or why not?"
  },
  {
    "objectID": "ae/ae-01-movies.html",
    "href": "ae/ae-01-movies.html",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "",
    "text": "Important\n\n\n\nFor this AE, you will discuss the questions in groups and submit answers on Canvas. This AE does not count towards the Application Exercise grade.\nWe will look at the relationship between budget and revenue for movies made in the United States in 1986 to 2020. The dataset is created based on data from the Internet Movie Database (IMDB).\nlibrary(tidyverse)\nlibrary(ggformula)"
  },
  {
    "objectID": "ae/ae-01-movies.html#data",
    "href": "ae/ae-01-movies.html#data",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Data",
    "text": "Data\nThe movies data set includes basic information about each movie including budget, genre, movie studio, director, etc. A full list of the variables may be found here.\n\nmovies &lt;- read_csv(\"https://raw.githubusercontent.com/danielgrijalva/movie-stats/master/movies.csv\")\n\nView the first 10 rows of data.\n\nmovies |&gt;\n  slice(1:10)\n\n# A tibble: 10 × 15\n   name   rating genre  year released score  votes director writer star  country\n   &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt;    &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt; &lt;chr&gt;  \n 1 The S… R      Drama  1980 June 13…   8.4 9.27e5 Stanley… Steph… Jack… United…\n 2 The B… R      Adve…  1980 July 2,…   5.8 6.5 e4 Randal … Henry… Broo… United…\n 3 Star … PG     Acti…  1980 June 20…   8.7 1.20e6 Irvin K… Leigh… Mark… United…\n 4 Airpl… PG     Come…  1980 July 2,…   7.7 2.21e5 Jim Abr… Jim A… Robe… United…\n 5 Caddy… R      Come…  1980 July 25…   7.3 1.08e5 Harold … Brian… Chev… United…\n 6 Frida… R      Horr…  1980 May 9, …   6.4 1.23e5 Sean S.… Victo… Bets… United…\n 7 The B… R      Acti…  1980 June 20…   7.9 1.88e5 John La… Dan A… John… United…\n 8 Ragin… R      Biog…  1980 Decembe…   8.2 3.30e5 Martin … Jake … Robe… United…\n 9 Super… PG     Acti…  1980 June 19…   6.8 1.01e5 Richard… Jerry… Gene… United…\n10 The L… R      Biog…  1980 May 16,…   7   1   e4 Walter … Bill … Davi… United…\n# ℹ 4 more variables: budget &lt;dbl&gt;, gross &lt;dbl&gt;, company &lt;chr&gt;, runtime &lt;dbl&gt;"
  },
  {
    "objectID": "ae/ae-01-movies.html#analysis",
    "href": "ae/ae-01-movies.html#analysis",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Analysis",
    "text": "Analysis\nWe begin by looking at how the average gross revenue (gross) has changed over time. Since we want to visualize the results, we will choose a few genres of interest for the analysis.\n\ngenre_list &lt;- c(\"Comedy\", \"Action\", \"Animation\", \"Horror\")\n\n\nmovies |&gt;\n  filter(genre %in% genre_list) |&gt; \n  group_by(genre,year) |&gt;\n  summarise(avg_gross = mean(gross)) |&gt;\n  gf_point(avg_gross ~ year, color = ~genre) |&gt;\n  gf_line() |&gt; \n  gf_labs(y = \"Average Gross Revenue (in US Dollars)\",\n    title = \"Gross Revenue Over Time\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat do you observe from the plot?\n\n\n\nNext, let’s see the relationship between a movie’s budget and its gross revenue.\n\nmovies |&gt;\n  filter(genre %in% genre_list, budget &gt; 0) |&gt; \n  gf_point(log(gross) ~ log(budget) | genre, color = ~genre) |&gt; \n  gf_lm(se=FALSE) |&gt;  \n  gf_labs(x = \"Log-transformed Budget\",\n  y = \"Log-transformed Gross Revenue\")"
  },
  {
    "objectID": "ae/ae-01-movies.html#exercises",
    "href": "ae/ae-01-movies.html#exercises",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Exercises",
    "text": "Exercises\nPlease enter your names and responses in Canvas.\n\nSuppose we fit a regression model for each genre that uses budget to predict gross revenue. What are the signs of the correlation between budget and gross and the slope in each regression equation?\nSuppose we fit the regression model from the previous question. Which genre would you expect to have the smallest residuals, on average (residual = observed revenue - predicted revenue)?\n[Time permitting] Discuss the following: Notice in the graph above that budget and gross are log-transformed. Why are the log-transformed values of the variables displayed rather than the original values (in U.S. dollars)?"
  },
  {
    "objectID": "ae/ae-01-movies.html#references",
    "href": "ae/ae-01-movies.html#references",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "References",
    "text": "References\n\ngithub.com/danielgrijalva/movie-stats\nInternet Movie Database"
  },
  {
    "objectID": "ae/ae-01-movies.html#appendix",
    "href": "ae/ae-01-movies.html#appendix",
    "title": "AE 01: Movie Budgets and Revenues",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of genres in the data set:\n\nmovies |&gt; \n  arrange(genre) |&gt; \n  select(genre) |&gt;\n  distinct()\n\n# A tibble: 19 × 1\n   genre    \n   &lt;chr&gt;    \n 1 Action   \n 2 Adventure\n 3 Animation\n 4 Biography\n 5 Comedy   \n 6 Crime    \n 7 Drama    \n 8 Family   \n 9 Fantasy  \n10 History  \n11 Horror   \n12 Music    \n13 Musical  \n14 Mystery  \n15 Romance  \n16 Sci-Fi   \n17 Sport    \n18 Thriller \n19 Western"
  },
  {
    "objectID": "ae/ae-02-slr.html",
    "href": "ae/ae-02-slr.html",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-02”\nGo to the Canvas and locate the AE-2 assignment. That link should take you directly there.\nUpload the ae-02.qmd and dcbikeshare.csv files into the folder you just created.\n\nThe .qmd and .html files form this assignment are due in Canvas no later than Tuesday, September 3 at 11:59pm.\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/ae-02-slr.html#data",
    "href": "ae/ae-02-slr.html#data",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Data",
    "text": "Data\nOur data set contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\nseason: 1 - winter, 2 - spring, 3 - summer, 4 - fall\n\nClick here for the full list of variables and definitions.\nNote that you may need to adjust the file path below:\n\nbikeshare &lt;- read_csv(\"data/dcbikeshare.csv\")"
  },
  {
    "objectID": "ae/ae-02-slr.html#exercises",
    "href": "ae/ae-02-slr.html#exercises",
    "title": "AE 02: Bike rentals in Washington, DC",
    "section": "Exercises",
    "text": "Exercises\n\nExercise 1\nBelow are visualizations of the distributions of daily bike rentals and temperature as well as the relationship between these two variables.\n\np1 &lt;- gf_histogram(~ count, data = bikeshare, binwidth = 250) |&gt; \n  gf_labs(x = \"Daily bike rentals\")\n\np2 &lt;- gf_histogram(~temp_orig, data = bikeshare) |&gt;  \n  gf_labs(x = \"Temperature (Celsius)\")\n\np3 &lt;- gf_point(count ~ temp_orig, data = bikeshare) |&gt; \n  gf_labs(x = \"Temperature (Celsius)\", \n       y = \"Daily bike rentals\")\n\n(p1 | p2) / p3\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThere appears to be one day with a very small number of bike rentals. What was the day? Why were the number of bike rentals so low on that day? Hint: You can Google the date to figure out what was going on that day.\n\n\n\n[Add your answer here]\n\n\nExercise 2\n\n\n\n\n\n\nIn the raw data, seasons are coded as 1, 2, 3, 4 as numerical values, corresponding to winter, spring, summer, and fall respectively. Recode the season variable to make it a categorical variable (a factor) with levels corresponding to season names, making sure that the levels appear in a reasonable order in the variable (i.e., not alphabetical).\n\n\n\n\n# add code developed during livecoding here\n\n\n\nExercise 3\n\n\n\n\n\n\nWe want to evaluate whether the relationship between temperature and daily bike rentals is the same for each season. To answer this question, first create a scatter plot of daily bike rentals vs. temperature faceted by season.\n\n\n\n\n# add code developed during livecoding here\n\n\n\nExercise 4\n\n\n\n\n\n\n\nWhich season appears to have the strongest relationship between temperature and daily bike rentals? Why do you think the relationship is strongest in this season?\nWhich season appears to have the weakest relationship between temperature and daily bike rentals? Why do you think the relationship is weakest in this season?\n\n\n\n\n[Add your answer here]\n\n\nExercise 5\n\n\n\n\n\n\nFilter your data for the season with the strongest apparent relationship between temperature and daily bike rentals.\n\n\n\n\n# add code developed during livecoding here\n\n\n\nExercise 6\n\n\n\n\n\n\nUsing the subset of the data from Exercise 5, fit a linear model for predicting daily bike rentals from temperature for this season.\n\n\n\n\n# add code developed during livecoding here\n\n\n\nExercise 7\n\n\n\n\n\n\nUse the output to write out the estimated regression equation.\n\n\n\n[Add your answer here]\n\n\nExercise 8\n\n\n\n\n\n\nInterpret the slope in the context of the data.\n\n\n\n[Add your answer here]\n\n\nExercise 9\n\n\n\n\n\n\nInterpret the intercept in the context of the data.\n\n\n\n[Add your answer here]\n\n\nExercise 10\n\n\n\n\n\n\nSuppose you work for a bike share company in Boise, ID, and they want to predict daily bike rentals in 2024. What is one reason you might recommend they use your analysis for this task? What is one reason you would recommend they not use your analysis for this task?\n\n\n\n[Add your answer here]\n\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the HTML with all of your work from today’s class.\nThe driver for your group should upload your .qmd and .html files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-04-sim-testing.html",
    "href": "ae/ae-04-sim-testing.html",
    "title": "AE 04: Randomization test for the slope",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-04”\nGo to the Canvas and locate your AE 04 assignment to get started.\nUpload the ae-04.qmd and dcbikeshare.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas no later than Thursday, September 12 at 11:59pm.\nlibrary(tidyverse)\nlibrary(infer)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(openintro)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-04-sim-testing.html#data",
    "href": "ae/ae-04-sim-testing.html#data",
    "title": "AE 04: Randomization test for the slope",
    "section": "Data",
    "text": "Data\nOur dataset contains daily rentals from the Capital Bikeshare in Washington, DC in 2011 and 2012 filtered to only contain the winter months. It was obtained from the dcbikeshare data set in the dsbox R package.\nWe will focus on the following variables in the analysis:\n\ncount: total bike rentals\ntemp_orig: Temperature in degrees Celsius\n\n\nwinter &lt;- read_csv(\"../ae/data/dcbikeshare.csv\") |&gt;\n  mutate(season = case_when(\n    season == 1 ~ \"winter\",\n    season == 2 ~ \"spring\",\n    season == 3 ~ \"summer\",\n    season == 4 ~ \"fall\"\n  ),\n  season = factor(season)) |&gt;\n  filter(season == \"winter\")\n\nRows: 731 Columns: 17\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (16): instant, season, yr, mnth, holiday, weekday, workingday, weathers...\ndate  (1): dteday\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(winter)\n\nRows: 181\nColumns: 17\n$ instant    &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ dteday     &lt;date&gt; 2011-01-01, 2011-01-02, 2011-01-03, 2011-01-04, 2011-01-05…\n$ season     &lt;fct&gt; winter, winter, winter, winter, winter, winter, winter, win…\n$ yr         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ mnth       &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ holiday    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…\n$ weekday    &lt;dbl&gt; 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4, 5, 6, 0, 1, 2, 3, 4,…\n$ workingday &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1,…\n$ weathersit &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 1, 2, 1, 2, 2, 2, 2,…\n$ temp       &lt;dbl&gt; 0.3441670, 0.3634780, 0.1963640, 0.2000000, 0.2269570, 0.20…\n$ atemp      &lt;dbl&gt; 0.3636250, 0.3537390, 0.1894050, 0.2121220, 0.2292700, 0.23…\n$ hum        &lt;dbl&gt; 0.805833, 0.696087, 0.437273, 0.590435, 0.436957, 0.518261,…\n$ windspeed  &lt;dbl&gt; 0.1604460, 0.2485390, 0.2483090, 0.1602960, 0.1869000, 0.08…\n$ casual     &lt;dbl&gt; 331, 131, 120, 108, 82, 88, 148, 68, 54, 41, 43, 25, 38, 54…\n$ registered &lt;dbl&gt; 654, 670, 1229, 1454, 1518, 1518, 1362, 891, 768, 1280, 122…\n$ count      &lt;dbl&gt; 985, 801, 1349, 1562, 1600, 1606, 1510, 959, 822, 1321, 126…\n$ temp_orig  &lt;dbl&gt; 14.110847, 14.902598, 8.050924, 8.200000, 9.305237, 8.37826…"
  },
  {
    "objectID": "ae/ae-04-sim-testing.html#exploratory-data-analysis",
    "href": "ae/ae-04-sim-testing.html#exploratory-data-analysis",
    "title": "AE 04: Randomization test for the slope",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\ngf_point(count ~ temp_orig, data = winter, alpha = 0.7) |&gt; \n  gf_labs(\n    x = \"Temperature (C)\",\n    y = \"Number of Bike Rentals\",\n  )"
  },
  {
    "objectID": "ae/ae-04-sim-testing.html#model",
    "href": "ae/ae-04-sim-testing.html#model",
    "title": "AE 04: Randomization test for the slope",
    "section": "Model",
    "text": "Model\n\nmodel_fit &lt;- lm(count ~ temp_orig, data = winter)\n\ntidy(model_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-111.04\n238.31\n-0.47\n0.64\n\n\ntemp_orig\n222.42\n18.46\n12.05\n0.00"
  },
  {
    "objectID": "ae/ae-04-sim-testing.html#hypothesis-test",
    "href": "ae/ae-04-sim-testing.html#hypothesis-test",
    "title": "AE 04: Randomization test for the slope",
    "section": "Hypothesis test",
    "text": "Hypothesis test\n\n\n\n\n\n\nTip\n\n\n\nFor code chunks with fill-in-the-blank code, change code chunk option to #| eval: true once you’ve filled in the code.\n\n\n\nState the null and alternative hypotheses\n[Add hypotheses in mathematical notation]\n\n\nGenerate null distribution using permutation\nFill in the code, then set eval: true .\n\nn = 100\nset.seed(212)\n\nnull_dist &lt;- _____ |&gt;\n  specify(______) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = _____, type = \"permute\") |&gt;\n  fit()\n\n\n\nVisualize distribution\n\n# Code for histogram of null distribution\n\n\n\nCalculate the p-value.\n\n# get observed fit \nobserved_fit &lt;- winter |&gt;\n  specify(count ~ temp_orig) |&gt;\n  fit()\n\n# calculate p-value\nget_p_value(\n  ____,\n  obs_stat = ____,\n  direction = \"two-sided\"\n)\n\n\nWhat does the warning message mean?\n\n\n\nState conclusion\n[Write your conclusion in the context of the data.]\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your PDf and .qmd files to Canvas."
  },
  {
    "objectID": "ae/ae-07-model-eval.html",
    "href": "ae/ae-07-model-eval.html",
    "title": "AE 07: Model evaluation",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-07”.\nGo to the Canvas and locate your AE 07 assignment to get started.\nUpload the ae-07.qmd and spotify-popular.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#data",
    "href": "ae/ae-07-model-eval.html#data",
    "title": "AE 07: Model evaluation",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is a subset from the Spotify Songs Tidy Tuesday data set. The data were originally obtained from Spotify using the spotifyr R package.\nIt contains numerous characteristics for each song. You can see the full list of variables and definitions here. This analysis will focus specifically on the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\n\n\nspotify &lt;- read_csv(\"data/spotify-popular.csv\")\n\nAre high energy songs more positive? To answer this question, we’ll analyze data on some of the most popular songs on Spotify, i.e. those with track_popularity &gt;= 80. We’ll use linear regression to fit a model to predict a song’s positiveness (valence) based on its energy level (energy).\nBelow are plots as part of the exploratory data analysis.\n\np1 &lt;- gf_histogram(~valence, data = spotify) |&gt;  \n  gf_labs(title = \"Distribution of Valence\", \n       subtitle = \" for Popular songs on Spotify\", \n       x = \"Valence\")\n\np2 &lt;- gf_histogram(~energy, data = spotify) |&gt; \n  gf_labs(title = \"Distribution of Energy\", \n       subtitle = \"for Popular songs on Spotify\", \n       x = \"Energy\")\np1 + p2\n\n\n\n\n\n\n\n\n\ngf_point(valence ~ energy, data = spotify) |&gt; \n  gf_labs(title = \"Valence vs. Energy\", \n       subtitle = \"Popular songs on Spotify\", \n       x = \"Energy\", \n       y = \"Valence\")"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#model-with-90-ci-for-coefficients",
    "href": "ae/ae-07-model-eval.html#model-with-90-ci-for-coefficients",
    "title": "AE 07: Model evaluation",
    "section": "Model with 90% CI for coefficients",
    "text": "Model with 90% CI for coefficients\n\nspotify_fit &lt;- lm(valence ~ energy, data = spotify)\n\ntidy(spotify_fit, conf.int = TRUE, conf.level = 0.9) |&gt; \n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n0.121\n0.035\n3.401\n0.001\n0.062\n0.179\n\n\nenergy\n0.614\n0.054\n11.321\n0.000\n0.525\n0.703"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#create-augmented-data-frame",
    "href": "ae/ae-07-model-eval.html#create-augmented-data-frame",
    "title": "AE 07: Model evaluation",
    "section": "Create augmented data frame",
    "text": "Create augmented data frame\n\nspotify_aug &lt;- augment(spotify_fit)"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#create-residual-plots",
    "href": "ae/ae-07-model-eval.html#create-residual-plots",
    "title": "AE 07: Model evaluation",
    "section": "Create residual plots",
    "text": "Create residual plots\n\ngf_point(.resid ~ .fitted, data = spotify_aug) |&gt;  \n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") |&gt; \n  gf_labs(x = \"Fitted values\", \n       y = \"Residuals\", \n       title = \"Residuals vs. fitted\")\n\n\n\n\n\n\n\n\n\nresid_hist &lt;- gf_histogram(~.resid, data = spotify_aug) |&gt;  \n  gf_labs(x = \"Residuals\", \n       title = \"Distribution of residuals\")\n\nresid_qq &lt;- gf_qq(~.resid, data = spotify_aug) |&gt; \n  gf_qqline()  |&gt; \n  gf_labs(x = \"Theoretical quantiles\",\n       y = \"Observed residuals\", \n       title = \"Normal QQ-plot\")\n\nresid_hist + resid_qq"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#exercise-1",
    "href": "ae/ae-07-model-eval.html#exercise-1",
    "title": "AE 07: Model evaluation",
    "section": "Exercise 1",
    "text": "Exercise 1\nCalculate \\(R^2\\) and interpret this value in the context of the data."
  },
  {
    "objectID": "ae/ae-07-model-eval.html#exercise-2",
    "href": "ae/ae-07-model-eval.html#exercise-2",
    "title": "AE 07: Model evaluation",
    "section": "Exercise 2",
    "text": "Exercise 2\nCalculate RMSE and interpret this value in the context of the data.\n    \\newcommand{\\E}{\\mathbb{E}}\n    \\newcommand{\\Var}{\\text{Var}}\n    \\newcommand{\\SD}{\\text{SD}}\n    \\newcommand{\\PP}{\\text{P}}\n    \\newcommand{\\bq}{\\begin{questions}}\n    \\newcommand{\\eq}{\\end{questions}}\n    \\newcommand{\\bp}{\\begin{parts}}\n    \\newcommand{\\ep}{\\end{parts}}\n    \\newcommand{\\bs}{\\begin{subparts}}\n    \\newcommand{\\es}{\\end{subparts}}"
  },
  {
    "objectID": "ae/ae-07-model-eval.html#exercise-3",
    "href": "ae/ae-07-model-eval.html#exercise-3",
    "title": "AE 07: Model evaluation",
    "section": "Exercise 3",
    "text": "Exercise 3\nChoose another quantitative variable in the data set. Check here for specific definitions of variables. After Dr. Friedlander approves your choice, fit a model predicting valence using that variable as a predictor. Make an argument, citing quantitative evidence, for which model is better, the energy model or your new model. Reporters, be ready to discuss with the class."
  },
  {
    "objectID": "ae/ae-07-model-eval.html#to-submit-the-ae",
    "href": "ae/ae-07-model-eval.html#to-submit-the-ae",
    "title": "AE 07: Model evaluation",
    "section": "To submit the AE:",
    "text": "To submit the AE:\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html",
    "href": "ae/ae-09-feature-engineering.html",
    "title": "AE 09: Feature engineering with recipes",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-09 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, October 12 at 11:59pm."
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#packages-data",
    "href": "ae/ae-09-feature-engineering.html#packages-data",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\nThe data for this AE is from the loan50 data set in the openintro R package. We will focus on the following variables:\nPredictors\n\nannual_income: Annual income (in US dollars)\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\nResponse\n\ninterest_rate: Interest rate for the loan (0- 100)"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#analysis-goal",
    "href": "ae/ae-09-feature-engineering.html#analysis-goal",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this analysis are to build a recipe to fit a linear regression model on the training data that has the following features:\n\nannual_income rescaled to thousands of dollars\nMean-centered quantitative variables\nIndicator (dummy) variables for the categorical predictor\nInteraction term between rescaled annual_income and verified_income\n\nand (2) use prep() and bake() to check the recipe"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#testtrain-split",
    "href": "ae/ae-09-feature-engineering.html#testtrain-split",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Test/train split",
    "text": "Test/train split\nFill in the code to split the data into 90% training, 10% testing.\n\n\n\n\n\n\nImportant\n\n\n\nRemove #| eval: false from the code chunk.\n\n\n\nset.seed(123)\n\nloans_split &lt;- initial_split(loan50, prop = _____)\nloan_train &lt;- training(_____)\nloan_test  &lt;- _____(loan_split)"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#build-a-recipe",
    "href": "ae/ae-09-feature-engineering.html#build-a-recipe",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Build a recipe",
    "text": "Build a recipe\n\nUse step_mutate() to create a new variable annual_income_th that is annual_income rescaled to thousands of dollars\nUse step_center() to mean-center quantitative variables\nUse step_dummy() to create indicator variables for the categorical predictor\nUse step_interact() to create interaction between annual_income_th and verified_income\n\n\n\n\n\n\n\nImportant\n\n\n\nRemove #| eval: false from the code chunk.\n\n\n\n# use original variables when specifying recipe\nloan_rec &lt;-  recipe(interest_rate ~ annaul_income + debt_to_income + verified_income, \n                    data = loan_train) |&gt;\n  # add recipe steps\n\n\nloan_rec"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#check-recipe-using-prep-and-bake",
    "href": "ae/ae-09-feature-engineering.html#check-recipe-using-prep-and-bake",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Check recipe using prep() and bake()",
    "text": "Check recipe using prep() and bake()\nRemove #| eval: false from the code chunk\n\n# determine required parameters to be estimated\nloan_rec_trained &lt;- prep(loan_rec)\n\n# apply recipe computations to data\nbake(loan_rec_trained, loan_train) |&gt;\n  glimpse()"
  },
  {
    "objectID": "ae/ae-09-feature-engineering.html#to-submit-the-ae",
    "href": "ae/ae-09-feature-engineering.html#to-submit-the-ae",
    "title": "AE 09: Feature engineering with recipes",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-09 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-11-cross-validation.html",
    "href": "ae/ae-11-cross-validation.html",
    "title": "AE 11: Cross validation",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-11 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, October 26 at 11:59pm."
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-statistics-function",
    "href": "ae/ae-11-cross-validation.html#model-statistics-function",
    "title": "AE 11: Cross validation",
    "section": "Model statistics function",
    "text": "Model statistics function\nYou will use this function to calculate \\(Adj. R^2\\), AIC, and BIC in the cross validation.\n\ncalc_model_stats &lt;- function(x) {\n  glance(extract_fit_parsnip(x)) |&gt;\n    select(adj.r.squared, AIC, BIC)\n}"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#packages",
    "href": "ae/ae-11-cross-validation.html#packages",
    "title": "AE 11: Cross validation",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#load-data-and-relevel-factors",
    "href": "ae/ae-11-cross-validation.html#load-data-and-relevel-factors",
    "title": "AE 11: Cross validation",
    "section": "Load data and relevel factors",
    "text": "Load data and relevel factors\n\ntips &lt;- read_csv(\"data/tip-data.csv\")\n\ntips &lt;- tips |&gt;\n  mutate(Age = factor(Age, levels = c(\"Yadult\", \"Middle\", \"SenCit\")), \n         Meal = factor(Meal, levels = c(\"Lunch\", \"Dinner\", \"Late Night\"))\n  )"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#split-data-into-training-and-testing",
    "href": "ae/ae-11-cross-validation.html#split-data-into-training-and-testing",
    "title": "AE 11: Cross validation",
    "section": "Split data into training and testing",
    "text": "Split data into training and testing\nSplit your data into testing and training sets.\n\nset.seed(10232023)\ntips_split &lt;- initial_split(tips)\ntips_train &lt;- training(tips_split)\ntips_test &lt;- testing(tips_split)"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#specify-model",
    "href": "ae/ae-11-cross-validation.html#specify-model",
    "title": "AE 11: Cross validation",
    "section": "Specify model",
    "text": "Specify model\nSpecify a linear regression model. Call it tips_spec.\n\ntips_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\ntips_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#create-recipe",
    "href": "ae/ae-11-cross-validation.html#create-recipe",
    "title": "AE 11: Cross validation",
    "section": "Create recipe",
    "text": "Create recipe\nCreate a recipe to use Party, Age, and Meal to predict Tip. Call it tips_rec1.\n\ntips_rec1 &lt;- recipe(Tip ~ Party + Age + Meal,\n                    data = tips_train) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())\n\ntips_rec1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3\n\n\n\n\n\n── Operations \n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#preview-recipe",
    "href": "ae/ae-11-cross-validation.html#preview-recipe",
    "title": "AE 11: Cross validation",
    "section": "Preview recipe",
    "text": "Preview recipe\n\nprep(tips_rec1) |&gt;\n  bake(tips_train) |&gt;\n  glimpse()\n\nRows: 126\nColumns: 6\n$ Party           &lt;dbl&gt; 3, 2, 2, 4, 2, 7, 4, 3, 2, 4, 1, 2, 2, 1, 2, 1, 2, 3, …\n$ Tip             &lt;dbl&gt; 4.00, 4.92, 5.09, 8.84, 3.09, 15.00, 8.00, 4.00, 5.00,…\n$ Age_Middle      &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, …\n$ Age_SenCit      &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ Meal_Dinner     &lt;dbl&gt; 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, …\n$ Meal_Late.Night &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 1, …"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#create-workflow",
    "href": "ae/ae-11-cross-validation.html#create-workflow",
    "title": "AE 11: Cross validation",
    "section": "Create workflow",
    "text": "Create workflow\nCreate the workflow that brings together the model specification and recipe. Call it tips_wflow1.\n\ntips_wflow1 &lt;- workflow() |&gt;\n  add_model(tips_spec) |&gt;\n  add_recipe(tips_rec1)\n\ntips_wflow1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n2 Recipe Steps\n\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#create-folds",
    "href": "ae/ae-11-cross-validation.html#create-folds",
    "title": "AE 11: Cross validation",
    "section": "Create folds",
    "text": "Create folds\nCreate 5 folds.\n\n# make 10 folds\nset.seed(10232023)\nfolds &lt;- vfold_cv(tips_train, v = 5)"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#conduct-cross-validation",
    "href": "ae/ae-11-cross-validation.html#conduct-cross-validation",
    "title": "AE 11: Cross validation",
    "section": "Conduct cross validation",
    "text": "Conduct cross validation\nConduct cross validation on the 5 folds.\n\n# Fit model and performance statistics for each iteration\ntips_fit_rs1 &lt;- tips_wflow1 |&gt;\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#take-a-look-at-tips_fit_rs1",
    "href": "ae/ae-11-cross-validation.html#take-a-look-at-tips_fit_rs1",
    "title": "AE 11: Cross validation",
    "section": "Take a look at tips_fit_rs1",
    "text": "Take a look at tips_fit_rs1\n\ntips_fit_rs1\n\n# Resampling results\n# 5-fold cross-validation \n# A tibble: 5 × 5\n  splits           id    .metrics         .notes           .extracts       \n  &lt;list&gt;           &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [100/26]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n2 &lt;split [101/25]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n3 &lt;split [101/25]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n4 &lt;split [101/25]&gt; Fold4 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n5 &lt;split [101/25]&gt; Fold5 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#summarize-assessment-cv-metrics",
    "href": "ae/ae-11-cross-validation.html#summarize-assessment-cv-metrics",
    "title": "AE 11: Cross validation",
    "section": "Summarize assessment CV metrics",
    "text": "Summarize assessment CV metrics\nSummarize assessment metrics from your CV iterations These statistics are calculated using the assessment set.\n\ncollect_metrics(tips_fit_rs1, summarize = TRUE)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.09      5  0.265  Preprocessor1_Model1\n2 rsq     standard   0.673     5  0.0519 Preprocessor1_Model1\n\n\n\n\n\n\n\n\nTip\n\n\n\nSet summarize = FALSE to see the individual \\(R^2\\) and RMSE for each iteration."
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#summarize-model-fit-cv-metrics",
    "href": "ae/ae-11-cross-validation.html#summarize-model-fit-cv-metrics",
    "title": "AE 11: Cross validation",
    "section": "Summarize model fit CV metrics",
    "text": "Summarize model fit CV metrics\nSummarize model fit statistics from your CV iterations These statistics are calculated using the analysis set.\n\nmap_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) |&gt;\n  summarise(mean_adj_rsq = mean(adj.r.squared), \n            mean_aic = mean(AIC), \n            mean_bic = mean(BIC))\n\n# A tibble: 1 × 3\n  mean_adj_rsq mean_aic mean_bic\n         &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1        0.670     434.     453.\n\n\n\n\n\n\n\n\nTip\n\n\n\nRun the first line of code map_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) to see the individual \\(Adj. R^2\\), AIC, and BIC for each iteration."
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-recipe",
    "href": "ae/ae-11-cross-validation.html#model-2-recipe",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Recipe",
    "text": "Model 2: Recipe\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-model-building-workflow",
    "href": "ae/ae-11-cross-validation.html#model-2-model-building-workflow",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Model building workflow",
    "text": "Model 2: Model building workflow\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-conduct-cv",
    "href": "ae/ae-11-cross-validation.html#model-2-conduct-cv",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Conduct CV",
    "text": "Model 2: Conduct CV\n\n\n\n\n\n\nNote\n\n\n\nWe will use the same folds as the ones used for Model 1. Why should we use the same folds to evaluate and compare both models?\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-summarize-assessment-cv-metrics",
    "href": "ae/ae-11-cross-validation.html#model-2-summarize-assessment-cv-metrics",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Summarize assessment CV metrics",
    "text": "Model 2: Summarize assessment CV metrics\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#model-2-summarize-model-fit-cv-metrics",
    "href": "ae/ae-11-cross-validation.html#model-2-summarize-model-fit-cv-metrics",
    "title": "AE 11: Cross validation",
    "section": "Model 2: Summarize model fit CV metrics",
    "text": "Model 2: Summarize model fit CV metrics\n\n# add code here"
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#fit-the-selected-model",
    "href": "ae/ae-11-cross-validation.html#fit-the-selected-model",
    "title": "AE 11: Cross validation",
    "section": "Fit the selected model",
    "text": "Fit the selected model\nFit the selected model using the entire training set.\n\n# add code here\n\n\n\n\n\n\n\nTip\n\n\n\nSee notes for example code."
  },
  {
    "objectID": "ae/ae-11-cross-validation.html#evaluate-the-performance-of-the-selected-model-on-the-testing-data",
    "href": "ae/ae-11-cross-validation.html#evaluate-the-performance-of-the-selected-model-on-the-testing-data",
    "title": "AE 11: Cross validation",
    "section": "Evaluate the performance of the selected model on the testing data",
    "text": "Evaluate the performance of the selected model on the testing data\n\nCalculate predicted values\n\n# add code here\n\n\n\nCalculate \\(RMSE\\)\n\n# add code here\n\n\n\n\n\n\n\nTip\n\n\n\nSee notes notes for example code.\n\n\n\nHow does the model performance on the testing data compare to its performance on the training data?\nIs this what you expected? Why or why not?"
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html",
    "href": "ae/ae-14-logistic-compare.html",
    "title": "AE 14: Comparing logistic regression models",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-14 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Saturday, November 11 at 11:59pm."
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#packages",
    "href": "ae/ae-14-logistic-compare.html#packages",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)"
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#response-to-leukemia-treatment",
    "href": "ae/ae-14-logistic-compare.html#response-to-leukemia-treatment",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Response to Leukemia treatment",
    "text": "Response to Leukemia treatment\nToday’s data is from a study where 51 untreated adult patients with Acute Myeloid Leukemia who were given a course of treatment, and they were assessed as to their response to the treatment.1\nThe goal of today’s analysis is to use pre-treatment factors to predict how likely it is a patient will respond to the treatment.\nWe will use the following variables:\n\nAge: Age at diagnosis (in years)\nSmear: Differential percentage of blasts\nInfil: Percentage of absolute marrow leukemia infiltrate\nIndex: Percentage labeling index of the bone marrow leukemia cells\nBlasts: Absolute number of blasts, in thousands\nTemp: Highest temperature of the patient prior to treatment, in degrees Fahrenheit\nResp: 1 = responded to treatment or 0 = failed to respond\n\n\nleukemia &lt;- read_csv(\"data/leukemia.csv\") |&gt;\n  mutate(Resp = factor(Resp))"
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#comparing-models",
    "href": "ae/ae-14-logistic-compare.html#comparing-models",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Comparing models",
    "text": "Comparing models\n\nConsider a model with all the pre-treatment variables: Age, Smear, Infil, Index, Blasts and Temp. Fit a model using these six variables to predict whether a patient responded to the treatment. Call the model full_model. Display the model.\n\n\n# add code\n\n\nBased on the model, which pre-treatment variables are statistically significant using a threshold of \\(\\alpha = 0.05\\)? (We will talk more about inference for logistic regression coefficients in an upcoming lecture.)\nFit a model that only includes the statistically significant predictors. Call the model reduced_model.\n\n\n# add code\n\n\nUse a drop-in-deviance test to compare a model that includes only the significant predictors to the full model. Which model do you choose based on the results of this test?\n\n\n# add code\n\n\nIs your choice based on AIC consistent with your choice from the previous exercise? What about a choice based on BIC?\n\n\n# add code"
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#submission",
    "href": "ae/ae-14-logistic-compare.html#submission",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-14 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-14-logistic-compare.html#footnotes",
    "href": "ae/ae-14-logistic-compare.html#footnotes",
    "title": "AE 14: Comparing logistic regression models",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe data set is from the Stat2Data R package. This AE is adapted from exercises in Stat 2.↩︎"
  },
  {
    "objectID": "ae/ae-16-logistic-conditions.html",
    "href": "ae/ae-16-logistic-conditions.html",
    "title": "AE 16: Conditions for logistic regression",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-16 repo to get started.Render, commit, and push your responses to GitHub by the end of class.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Saturday, November 18 at 11:59pm."
  },
  {
    "objectID": "ae/ae-16-logistic-conditions.html#packages",
    "href": "ae/ae-16-logistic-conditions.html#packages",
    "title": "AE 16: Conditions for logistic regression",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(NHANES)\n\nToday’s data is the nhanes_adult data set derived from the NHANES data set in the NHANES R package. It contains information for U.S. adults (age 18+) who participated in the 2009 - 2010 and 2011 - 2012 years of the National Health and Nutrition Examination Survey (NHANES). You can find details about how participants are selected on the CDC website.\n\nWe will use the following variables:\n\nHealthGen: Self-reported rating of participant’s health in general. Excellent, Vgood, Good, Fair, or Poor.\nAge: Age at time of screening (in years). Participants 80 or older were recorded as 80.\nPhysActive: Participant does moderate to vigorous-intensity sports, fitness or recreational activities.\n\n\nnhanes_adult &lt;- read_csv(\"data/nhanes-adult.csv\")\n\nThe goal of this AE is to check the conditions for a model that uses age and physical activity to understand the odds an adult in the United States rates their health as “excellent”.\n\n\n\n\n\n\nTip\n\n\n\nClick here to see notes on conditions for logistic regression.\n\n\n\nCreate a new variable called HealthExcellent that takes the value 1 if HealthGen = “Excellent” and 0 otherwise. Make a table showing the distribution of HealthExcellent.\n\n\n# add code\n\n\nCalculate the empirical logit of HealthExcellent == 1 for each level of PhysActive. Then describe what the empirical logit means in the context of the data.\n\n\n# add code\n\n\nCheck the model conditions for a logistic regression model that uses age and physical activity to understand the odds an adult in the United States rates their health as “excellent”.\n\nState whether each condition is satisfied and briefly explain your response.\n\nLinearity\nRandomness\nIndependence\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can make the plots to check linearity “manually” using ggplot or using the functions from the Stat2Data package.\n\nIf you use the Stat2Data package, you need to add library(Stat2Data) to the load-packages code chunk at the top of the document. You may also need to install the package by running the code below in the console.\ninstall.packages(\"Stat2Data\")"
  },
  {
    "objectID": "ae/ae-16-logistic-conditions.html#submission",
    "href": "ae/ae-16-logistic-conditions.html#submission",
    "title": "AE 16: Conditions for logistic regression",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-16 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "computing-access.html",
    "href": "computing-access.html",
    "title": "Computing access",
    "section": "",
    "text": "To access RStudio at the College of Idaho, go to the CofI Posit Workbench website and sign-in using the CofI email and password.",
    "crumbs": [
      "Computing",
      "Access"
    ]
  },
  {
    "objectID": "computing-r-resources.html",
    "href": "computing-r-resources.html",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#books-free-online",
    "href": "computing-r-resources.html#books-free-online",
    "title": "Resources for learning R",
    "section": "",
    "text": "R for Data Science by Hadley Wickham & Garrett Grolemund\nTidy Modeling with R by Max Kuhn & Julia Silge",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#rstudio-primers",
    "href": "computing-r-resources.html#rstudio-primers",
    "title": "Resources for learning R",
    "section": "RStudio Primers",
    "text": "RStudio Primers\n\nInteractive LearnR Tutorial\nR Primers for full list of tutorials. Note that we will be using a simplified version of ggplot2 in this course.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "computing-r-resources.html#additional-resources",
    "href": "computing-r-resources.html#additional-resources",
    "title": "Resources for learning R",
    "section": "Additional resources",
    "text": "Additional resources\n\nRStudio Cheatsheets\nR Fun workshops and videos by Duke Center for Data and Visualization Sciences.",
    "crumbs": [
      "Computing",
      "R resources"
    ]
  },
  {
    "objectID": "hw/hw-0-Station-1.html",
    "href": "hw/hw-0-Station-1.html",
    "title": "Station 1: Getting Started with R and RStudio",
    "section": "",
    "text": "You can access the College of Idaho’s RStudio server at rstudio.collegeofidaho.edu. Login using your CofI email and password. You can access your RStudio folder from any computer connected to the internet – no need to download anything, or transfer files between computers!\nYou may also download R and RStudio on your personal machine, although I don’t recommend doing this right now. To do this follow the direction here.\nAs you can see, the RStudio environment contains four quadrants: the upper-left contains the files (including data sets) you’re working on; the lower-left contains the Console, where you can run R code; the lower-right contains several tabs including Plots, Packages, and Help; and the upper-right contains your History, including a list of loaded data sets.\n\n\nI recommend creating a folder to keep all your DATA 230/231 activities and handouts in. You will have a LOT of them this semester, and it’s best to start an organizational system now. If you’re working with RStudio on your desktop, you know how to create folders/subfolders. If you’re working on CofI’s RStudio server, click on “Files” in the lower-right corner, then create a New Folder called “MAT212” and within that, a folder called “ae” and a folder called “hw”. Within the hw folder, create a subfolder called “hw0”. Save this document in that folder now. Call it “hw-0-Station1.qmd”."
  },
  {
    "objectID": "hw/hw-0-Station-1.html#a.-accessing-rstudio",
    "href": "hw/hw-0-Station-1.html#a.-accessing-rstudio",
    "title": "Station 1: Getting Started with R and RStudio",
    "section": "",
    "text": "You can access the College of Idaho’s RStudio server at rstudio.collegeofidaho.edu. Login using your CofI email and password. You can access your RStudio folder from any computer connected to the internet – no need to download anything, or transfer files between computers!\nYou may also download R and RStudio on your personal machine, although I don’t recommend doing this right now. To do this follow the direction here.\nAs you can see, the RStudio environment contains four quadrants: the upper-left contains the files (including data sets) you’re working on; the lower-left contains the Console, where you can run R code; the lower-right contains several tabs including Plots, Packages, and Help; and the upper-right contains your History, including a list of loaded data sets.\n\n\nI recommend creating a folder to keep all your DATA 230/231 activities and handouts in. You will have a LOT of them this semester, and it’s best to start an organizational system now. If you’re working with RStudio on your desktop, you know how to create folders/subfolders. If you’re working on CofI’s RStudio server, click on “Files” in the lower-right corner, then create a New Folder called “MAT212” and within that, a folder called “ae” and a folder called “hw”. Within the hw folder, create a subfolder called “hw0”. Save this document in that folder now. Call it “hw-0-Station1.qmd”."
  },
  {
    "objectID": "hw/hw-0-Station-1.html#b.-quarto",
    "href": "hw/hw-0-Station-1.html#b.-quarto",
    "title": "Station 1: Getting Started with R and RStudio",
    "section": "B. Quarto",
    "text": "B. Quarto\nThe thing you are reading right now is an Quarto document (very similar to R Markdown if you’ve ever used that). Quarto runs inside R Studio. Quarto is a simple formatting syntax for authoring web pages, word documents, pdfs, and many more file types. You can find the link to a useful Quarto guide here.\nWhen you click the Render button at the top of this window an html file will be generated that includes both content as well as the output of any embedded R code chunks within the document. Not only can you embed the R code, you can embed the output produced by the R code. In this way, your analysis is fully reproducible and updatable. All of your homework assignments must be prepared using Quarto.\nThe nice thing about Quarto is that you can write prose (as I am doing now), mathematical equations using LaTeX syntax (like \\(y_i = a + b x_i\\)), and R code/output/plots all in one synthesized document. This makes it approximately 10,000 times easier to use than doing the same thing in Word or LaTeX. As you go through this introduction, I recommend that you look also at the .qmd file and your output file side-by-side, to get an idea of how Quarto works. (Since you’re going to have to use Quarto this semester, you might as well start learning it now!)\nIn the future, I will typically give you a Quarto template to fill out. However, occasionally you will need to create your own Quarto document. When you want to start a new Quarto document, click “File &gt; New File &gt; Quarto Document…” Put yourself as author, and make sure to give it a descriptive title!\nThe goal of this station is to get you ready to read, visualize, and use data. You will also become comfortable navigating the RStudio interface and running code from within a document.\nFirst, read through this page. I suggest watching the first 6:35 of the embedded video as anything after that is not relevant for our class."
  },
  {
    "objectID": "hw/hw-0-Station-1.html#c.-using-r-as-a-calculator-and-running-code-within-a-quarto-document",
    "href": "hw/hw-0-Station-1.html#c.-using-r-as-a-calculator-and-running-code-within-a-quarto-document",
    "title": "Station 1: Getting Started with R and RStudio",
    "section": "C. Using R as a Calculator and Running Code within a Quarto Document",
    "text": "C. Using R as a Calculator and Running Code within a Quarto Document\nWith R by your side, you will never need your TI-84 again. Consider the simple arithmetic in the chunk below. You can run this code chunk all at once by clicking the Run button, the sideways green arrow in the top right of the chunk. Notice the interactive nature of the output. You can click through to see the different pieces of output.\nAs you work through this document, you should Run each chunk as you come to it.\n\n5 + 3\n\n[1] 8\n\n5.3 * 23.4\n\n[1] 124.02\n\nsqrt(16) # this is a comment.  R will not 'see' anything past a #, but you can use it to explain your code\n\n[1] 4\n\n\nLook closely at how Quarto denotes the R code and the output. Also note in the .qmd file how I include R code as separate from prose. These are called “chunks”. The easiest way to add a new chunk is to click on the green “c” icon with a plus in the corner above, then choose “R”. R code that is not inside of a chunk will not be run by Quarto!\nYou can also save values to named variables, to be used later:\n\nproduct &lt;- 15.3 * 23.4 #save result\n\nIf you save something like this, R will not show the output unless you expressly ask for it:\n\nproduct #show result\n\n[1] 358.02\n\n\nThe symbol “&lt;-” is the assignment operator. If you’ve ever programmed before, it’s essentially the same as “=” in this instance. When we get into more complicated work, there are cases where “&lt;-” works and “=” doesn’t, so it’s good to get into the habit of using “&lt;-” now.\nOnce variables are defined, they can be referenced with other operators and functions. Try executing each line of code individually by placing your cursor on the first line of the chunk below and pressing Ctrl+Enter (Cmd +Enter for Mac users); then do the same for the second line. (This is how you can run a single line within a larger chunk.)\n\n.5 * product # half of the product\n\n[1] 179.01\n\nproduct^(1/4) # fourth root of the product\n\n[1] 4.349875\n\n\nYou can also use in-line R code in Quarto, which can be useful when calling defined variables. Did you know that the natural log of 358.02 is 5.8805889?\nThe semi-colon can be used to place multiple commands on one line. One frequent use of this is to save and print a value all in one go, as below:\n\nproduct &lt;- 15.3 * 23.4 ; product  # save result and show it\n\n[1] 358.02\n\n\nOnly R code (and comments) should be inside chunks. Prose (interpretations/explanations/descriptions) should never be put inside a chunk; prose should be below or above the chunk, as I have done above (and continue to do throughout this document). You should also never cut-and-paste output or graphs into the chunks. The whole point of code chunks is that they contain the code and they’ll run the code (resulting in the output and/or graphs).\nIf you want to run something in R but don’t want it to appear in the Quarto document, simply run it in the Console in the lower-left quadrant of RStudio. Type the last line of R code above into the Console and see what happens."
  },
  {
    "objectID": "hw/hw-0-Station-1.html#d.-four-things-to-know-about-r-and-rstudio",
    "href": "hw/hw-0-Station-1.html#d.-four-things-to-know-about-r-and-rstudio",
    "title": "Station 1: Getting Started with R and RStudio",
    "section": "D. Four Things to Know About R and RStudio",
    "text": "D. Four Things to Know About R and RStudio\n\nR is case-sensitive. If you mis-capitalize something, it won’t do what you want.\nTAB completion and arrows can improve typing speed and accuracy. If you begin a command and hit the TAB key, R will show you a list of possible ways to complete the command. If you hit TAB after the opening parenthesis of a function, it will show you the list of arguments it expects. The up and down arrows can be used to retrieve past commands.\nIf you see a + prompt, it means R is waiting for more input. Often this means that you have forgotten a closing parenthesis or made some other syntax error. If you have completely messed up and just want to get back to the console, hit the ESC key and start the command fresh.\nThe Console/qmd file and the Quarto output document are separate entities. Suppose you’ve run something, saved something, opened a data set, or loaded a package in the Console. It exists only in the Console, not in the Quarto document. Think of the Console as a “practice area” – a place where you can play around and try stuff. Once you’ve settled on a piece of code and you want it to appear in the “official” document (that you’ll turn in to me), put it into the Quarto document (within a “chunk”, of course). Note that when you click the green run button, it runs the code in the console not the Quarto document. This can occasionally cause inconsistencies if you run your code chunks out of order while you’re writing your document. When you ultimately press Render, Quarto will re-run the WHOLE document in order from the beginning to generate the output file."
  },
  {
    "objectID": "hw/hw-0-Station-1.html#e.-r-packages",
    "href": "hw/hw-0-Station-1.html#e.-r-packages",
    "title": "Station 1: Getting Started with R and RStudio",
    "section": "E. R packages",
    "text": "E. R packages\nIn addition to its core features, R provides many more features through a (large) number of packages. To use a package, it must be installed (one time), and loaded (each session). You install the package by clicking the “Tools &gt; Install Packages…” in the toolbar, then typing the name of the package. If you are using CofI’s RStudio server, packages will be installed for you. To load the package in the Console, you can type library(package_name), or just click the button next to the package in the “Packages” tab. However, if you are going to use a package within an Quarto document, you must “call” the package within your particular Quarto document. (Why? Because the Console and the Quarto document are separate entities!)\nFor example: We will be using the tidyverse package a lot this term. (If you’re using a desktop version of RStudio you will need to install it if you haven’t used it before.) Load the package in the Console by typing library(tidyverse) in the Console. Now we still need to load it in this Quarto document.\n\nlibrary(tidyverse) #this loads the tidyverse package into this R Markown document\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nThe tidyverse package is now loaded. But it sure takes up a lot of space on the html file, doesn’t it? If we wanted to suppress the output from this particular chunk on the html file, we could use #| include: false at the beginning of the chunk, as below. (Note that the chunk still runs; but the code and the output is suppressed when it renders to html. There are other useful chunk headers that we will see as we continue learning Quarto…)\nRemember! The Quarto document (.qmd) and the Console are separate entities. Just because you’ve done something (run some code, loaded some data, etc.) in one place, it does not transfer to the other place."
  },
  {
    "objectID": "hw/hw-0-Station-1.html#f.-getting-help",
    "href": "hw/hw-0-Station-1.html#f.-getting-help",
    "title": "Station 1: Getting Started with R and RStudio",
    "section": "F. Getting Help",
    "text": "F. Getting Help\nIf something doesn’t go quite right, or if you can’t remember something, it’s good to know where to turn for help. In addition to asking your friends and neighbors, you can use the R help system.\n\nTo get help on a specific function or data set, simply precede its name with a ?. Type ?histogram into the Console and see what happens. Alternatively, you can type histogram into the search bar in the Help tab in the lower-right quadrant.\nIf you don’t know the exact name of a function, you can give part of the name to apropos and R will find all functions that match. Quotation marks inside the apropos() function (single or double) are mandatory here. Type apropos('hist') into the Console and see what happens. Alternatively, you can type hist into the search bar in the Help tab in the lower-right quadrant.\nIf that fails, you can do a broader search using ?? or help.search(), which will find matches not only in the names of functions and data sets, but also in the documentation for them. Quotations marks are optional here. Try typing ??histogram and/or help.search('histogram') into the Console.\nExamples and Demos. Many functions and data sets in R include example code demonstrating typical uses. For example, example(histogram) will generate a number of example plots (and provide you with the commands used to create them). Examples such as this are intended to help you learn how specific R functions work. These examples also appear at the end of the documentation for functions and data sets.\n\nSome packages include demos. Demos are bits of R code that can be executed using the demo() command with the name of the demo. Demos are intended to illustrate a concept, a method, or some such thing, and are independent of any particular function or data set. You can get a list of available demos using demo(), or demos from just the package of interest using demo(package='NAME OF PACKAGE').\n\nThe Internet. One of the great things about R is how well-documented it is online. Since it’s free and open-source and people across disciplines use it, there is an astounding amount of information available on the web. If you get stuck on something, Google it: I can almost guarantee you will find someone who has posted a solution to either your exact problem, or something close to it. You should treat AI tools, such as ChatGPT, the same as other online resources. There are two guiding principles that govern how you can use AI in this course: (1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning. (2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity. In general:\n\n\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines for citing AI-generated content.\n❌ AI tools for narrative: Unless instructed otherwise, you may not use generative AI to write narrative on assignments. In general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content\n\nNOTE: There are usually MANY different ways to do things in R. The ways I use, and the ways I show you, are (almost) never the only way, and are not necessarily the best or simplest way. Feel free to find different/better/easier ways to do things! I (almost) never care about the details of the code, as long as you’re getting the right result."
  },
  {
    "objectID": "hw/hw-0-Station-1.html#g.-loading-data",
    "href": "hw/hw-0-Station-1.html#g.-loading-data",
    "title": "Station 1: Getting Started with R and RStudio",
    "section": "G. Loading Data",
    "text": "G. Loading Data\nAs long as they are in the right format, data sets can be loaded from files saved on your computer, or straight from webpages. Data sets can be whitespace-delimited, comma-delimited, or tab-delimited. RStudio is smart enough that it can usually tell which kind of delimiter is being used.\nMany of the data sets that we will be using in class this semester are in the Stat2Data package. We loaded this package above. (As long as you ran the chunk above. Remember that you should be running every chunk as you work through this document!)\nThe Day1Survey data set lives within the Stat2Data package. To load this data set, type data(Day1Survey) in the chunk below. Note that as you type, R will start to auto-complete with the appropriate data set! Then run the chunk below, so that the data set has also been loaded in the Console. (Because the Quarto document and the Console are separate entities, right?)\nLet’s load another dataset from the Stat2Data package, and do a couple fun with things with it. Notice how, when you Render this document, the results (the means and side-by-side boxplots) show up just below the code!\n\n# this dataset is from an experiment studying rats and weight gain as a function of diet\ndata(\"FatRats\") \n\n#calculate means for each of three protein sources\nmean(Gain ~  Source, data = FatRats)\n\n  Beef Cereal   Pork \n  89.6   84.9   89.2 \n\n# next, create side-by-side boxplots by protein level (hi vs. lo) *and* source (beef vs cereal vs pork)\ngf_boxplot(Gain ~ Protein | Source, data = FatRats)\n\n\n\n\n\n\n\n\nData can also be loaded from files on your computer (.txt, .xls, .csv), or from the internet. If working on the College’s RStudio server, you often need to upload the data on the server first. Let’s try this now:\n\nDownload the USStates_fall2021.csv file from Canvas. It lives under the “Files &gt; Data sets” tab.\nIn the lower-right quadrant of RStudio, click on Files, then your new MAT212 folder that you created in Section A.\nClick “Upload &gt; Choose File”, and select the USStates_fall2021.csv file that you downloaded. Then click the Upload button.\n\nNow you can Import the data set into your Console and/or Quarto document.\n\nIn the top-right corner of RStudio, click “Environment &gt; Import Dataset &gt; From Text (base)”.\nNavigate to your MAT212 folder and select USStates_fall2021.csv and click “Open”.\nAs I said above, RStudio is smart enough that it can usually tell which kind of delimiter is being used. It is showing you a preview of the data set in the “Data Frame” window. Make sure it looks okay: columns and rows, with column names given by the Headings (not V1, V2, etc). Then hit “Import”.\nThe data set is now loaded in the Console, and the Console now has the code to load the data set. But of course, that doesn’t mean it’s loaded in the Quarto document! Copy that line (the read.csv line, not the View line) in the code chunk below.\n\nNow use the code in lines 151 and 154 as a guide to calculate mean household income by region and make side-by-side boxplots of household income by region."
  },
  {
    "objectID": "hw/hw-0-Station-1.html#finally",
    "href": "hw/hw-0-Station-1.html#finally",
    "title": "Station 1: Getting Started with R and RStudio",
    "section": "FINALLY",
    "text": "FINALLY\n…Render this entire document and make sure it renders correctly. In general, I suggest “rendering as you go”: rendering every few chunks, to make sure things are rendering correctly, rather than waiting until the end of a document to render the whole thing (and potentially encountering lots of errors that you have to unpack)! I also ALWAYS suggest “saving as you go”: it’s a good idea to save every couple of minutes. (This is good practice for all your files.)"
  },
  {
    "objectID": "hw/hw-0-Station-3.html",
    "href": "hw/hw-0-Station-3.html",
    "title": "Station 3: Inference with R",
    "section": "",
    "text": "You should remember the basics of hypothesis testing from your introductory statistics class, whether it was MAT 125, AP statistics, or a course through a different school or department. You can find the R code to perform all the necessary inference procedures on the “Inference Summary Sheet” page under the Computing tab on the course website.\nAt this station, you will pose and answer research questions from beginning to end, using the US States data (from Station 2). You will have the opportunity to ask and answer 3 different research questions: one using only one variable, and two exploring the relationship between two variables. Although not required, you may re-use your work from Station 2.\nPlease reference the data codebook (available on the Data sets tab on Canvas) to decide which variables you’d like to use when forming your research question."
  },
  {
    "objectID": "hw/hw-0-Station-3.html#introduction",
    "href": "hw/hw-0-Station-3.html#introduction",
    "title": "Station 3: Inference with R",
    "section": "",
    "text": "You should remember the basics of hypothesis testing from your introductory statistics class, whether it was MAT 125, AP statistics, or a course through a different school or department. You can find the R code to perform all the necessary inference procedures on the “Inference Summary Sheet” page under the Computing tab on the course website.\nAt this station, you will pose and answer research questions from beginning to end, using the US States data (from Station 2). You will have the opportunity to ask and answer 3 different research questions: one using only one variable, and two exploring the relationship between two variables. Although not required, you may re-use your work from Station 2.\nPlease reference the data codebook (available on the Data sets tab on Canvas) to decide which variables you’d like to use when forming your research question."
  },
  {
    "objectID": "hw/hw-0-Station-3.html#a.-univariate-quantitative",
    "href": "hw/hw-0-Station-3.html#a.-univariate-quantitative",
    "title": "Station 3: Inference with R",
    "section": "A. Univariate: Quantitative",
    "text": "A. Univariate: Quantitative\nTASK 1.1.1–Choose a quantitative variable from the US States dataset you’d like to explore. What is the variable? \n\nPose a research question\nTASK 1.1.2–What would be a reasonable research question to ask about this parameter? Your answer should make sense practically.\nTASK 1.1.3–Write the null and alternative hypotheses in terms of the population parameter. If your research question is better suited for a confidence interval, you do not need to state hypotheses.\n\n\nPerform EDA\nA good EDA should attempt to preliminarily answer the research question and also identify any issues that might impact eventual inference.\nTASK 1.1.4–Generate a relevant plot for your research question. \nTASK 1.1.5– Calculate the summary statistic that is most relevant to your CI/test.\nTASK 1.1.6–Write a few sentences summarizing what you learned from your EDA\n\n\nAnswer the question using inference\nTo answer your research question, you’ll need to test the hypotheses you defined earlier, or construct the relevant confidence interval.\nTASK 1.1.7–What is the name of the statistical test or interval you will perform/construct (e.g., 1-sample t-test for the mean, CI for the difference in means, 1-sample proportion test)?\nTASK 1.1.8–Did your EDA indicate any problems with performing this test/CI? That is, are the conditions for inference met?\nTASK 1.1.9–Use R to generate the test statistic and p-value, or build the confidence interval.\nTASK 1.1.10–Make a conclusion about the null and alternative hypotheses, being sure to answer your research question in context. OR Provide an interpretation of the confidence interval in context."
  },
  {
    "objectID": "hw/hw-0-Station-3.html#b.-relationship-between-two-variables-1-numericalbinary",
    "href": "hw/hw-0-Station-3.html#b.-relationship-between-two-variables-1-numericalbinary",
    "title": "Station 3: Inference with R",
    "section": "B. Relationship between two variables 1: Numerical/Binary",
    "text": "B. Relationship between two variables 1: Numerical/Binary\nChoose two variables from the Codebook: one numerical, one binary. In this section you will investigate the relationship between these two variables.\nTASK 2.1.1–What are the two variables? Your answer should include a brief description and be more than simply the variable name.\n\nPose a research question\nTASK 2.1.2–What would be a reasonable research question to ask about this parameter? Your answer should make sense practically.\nTASK 2.1.3–Write the null and alternative hypotheses in terms of the population parameter. In this case, I want you to perform a hypothesis test. Later, I’ll also have you build the CI.\n\n\nPerform EDA\nRemember, a good EDA should attempt to answer the research questions and also identify any issues that might impact eventual inference.\nTASK 2.1.4–Generate a relevant plot for your research question.\nTASK 2.1.5–Calculate the summary statistics that are most relevant to your test.\nTASK 2.1.6–Write a few sentences summarizing what you learned from your EDA\n\n\nAnswer the question using inference\nTo answer your research question, you’ll need to test the hypotheses you defined earlier.\nTASK 2.1.7–What is the name of the statistical test you will perform?\nTASK 2.1.8–Did your EDA indicate any problems with performing this test? That is, are the conditions for inference met?\nTASK 2.1.9–Use R to generate the test statistic and p-value.\nTASK 2.1.10–Make a conclusion about the null and alternative hypotheses, being sure to answer your research question in context.\nTASK 2.1.11–Now use R to calculate a 95% confidence interval for the parameter of interest, and provide an interpretation in context. First, you must ask yourself, “What is the parameter of interest?”"
  },
  {
    "objectID": "hw/hw-0-Station-3.html#c.-relationship-between-two-variables-2-binarybinary",
    "href": "hw/hw-0-Station-3.html#c.-relationship-between-two-variables-2-binarybinary",
    "title": "Station 3: Inference with R",
    "section": "C. Relationship between two variables 2: Binary/Binary",
    "text": "C. Relationship between two variables 2: Binary/Binary\nChoose two binary variables from the Codebook. In this section you will investigate the relationship between these two variables.\nTASK 2.2.1–What are the two variables? Your answer should include a brief description and be more than simply the variable name.\n\nPose a research question\nTASK 2.2.2–What would be a reasonable research question to ask about this parameter? Your answer should make sense practically.\nTASK 2.2.3–Write the null and alternative hypotheses in terms of the population parameter. In this case, I want you to perform a hypothesis test. Later, I’ll also have you build the CI.\n\n\nPerform EDA\nRemember, a good EDA should attempt to answer the research questions and also identify any issues that might impact eventual inference.\nTASK 2.2.4–Generate a relevant plot for your research question.\nTASK 2.2.5–Calculate the summary statistics that are most relevant to your hypothesis test.\nTASK 2.2.6–Write a few sentences summarizing what you learned from your EDA\n\n\nAnswer the question using inference\nTo answer your research question, you’ll need to test the hypotheses you defined earlier.\nTASK 2.2.7–What is the name of the statistical test will you perform to test the hypotheses?\nTASK 2.2.8–Did your EDA indicate any problems with performing this test? That is, are the conditions for inference met?\nTASK 2.2.9–Use R to generate the test statistic and p-value.\nTASK 2.2.10–Make a conclusion about the null and alternative hypotheses, being sure to answer your research question in context.\nTASK 2.2.11–Now use R to calculate a 95% confidence interval for the parameter of interest, and provide an interpretation in context. First, you must ask yourself, “What is the parameter of interest?”"
  },
  {
    "objectID": "hw/hw-01-old.html",
    "href": "hw/hw-01-old.html",
    "title": "HW 01: Education & median income in US Counties",
    "section": "",
    "text": "In this assignment, you will use simple linear regression to examine the association between between the percent of adults with a bachelor’s degree and the median household income for counties in the United States."
  },
  {
    "objectID": "hw/hw-01-old.html#part-1-exploratory-data-analysis",
    "href": "hw/hw-01-old.html#part-1-exploratory-data-analysis",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 1: Exploratory data analysis",
    "text": "Part 1: Exploratory data analysis\n\nExercise 1\nCreate a histogram of the distribution of the predictor variable bachelorsand calculate appropriate summary statistics. Use the visualization and summary statistics to describe the distribution. Include an informative title and axis labels on the plot.\n\n\nExercise 2\nLet’s view the data in another way. Use the code below to make a map of the United States with the color of the counties filled in based on the percent of residents 25 years old and older who have a Bachelor’s degree. Fill in title and axis labels.\nThen use the plot answer the following:\n\nWhat are 2 observations you have from the map?\nWhat is a feature that is apparent in the map that wasn’t as easily apparent from the histogram in the previous exercise? What is a feature that is apparent in the histogram that is not as easily apparent from the map?\n\n\ncounty_map_data &lt;- left_join(county_data_sample, map_data_sample)\n\nggplot(data = map_data_all) +\n  geom_polygon(aes(x = long, y = lat, group = group),\n    fill = \"lightgray\", color = \"white\"\n    ) +\n  geom_polygon(data = county_map_data, aes(x = long, y = lat, group = group,\n    fill = bachelors)\n    ) +\n  labs(\n    x = \"Longitude\",\n    y = \"Latitude\",\n    fill = \"_____\",\n    title = \"_____\"\n  ) +\n  scale_fill_viridis_c(labels = label_percent(scale = 1)) +\n  coord_quickmap()\n\n\n\nExercise 3\nCreate a visualization of the relationship between bachelors and median_household_income and calculate the correlation. Use the visualization and correlation to describe the relationship between the two variables.\n\n\n\n\n\n\nTip\n\n\n\nRecall the analysis objective stated at the beginning of the Exercises section.\n\n\n\nIf you haven’t yet done so, now is a good time to render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01-old.html#part-2-modeling",
    "href": "hw/hw-01-old.html#part-2-modeling",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 2: Modeling",
    "text": "Part 2: Modeling\n\nExercise 4\nWe will use a linear regression model to better quantify the relationship between bachelors and median_household_income.\nWrite the form of the statistical model we will use for this task using mathematical notation. Use variable names (bachelors and median_household_income) in the equation for your model1.\n\n\nExercise 5\n\nFit the regression line corresponding to the statistical model in the previous exercise. Neatly display the model output using 3 digits.\nWrite the equation of the fitted model using mathematical notation. Use variable names (bachelors and median_household_income) in the equation.\n\n\n\nExercise 6\n\nInterpret the slope. The interpretation should be written in a way that is meaningful in the context of the data.\nIs it useful to interpret the intercept for this data? If so, write the interpretation in the context of the data. Otherwise, briefly explain why not.\n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01-old.html#part-3-inference-for-the-u.s.",
    "href": "hw/hw-01-old.html#part-3-inference-for-the-u.s.",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Part 3: Inference for the U.S.",
    "text": "Part 3: Inference for the U.S.\nWe want to use the data from these 600 randomly selected counties to draw conclusions about the relationship between the percent of adults age 25 and older with a bachelor’s degree and median household income for the over 3,000 counties in the United States.\n\nExercise 7\n\nWhat is the population of interest? What is the sample?\nIs it reasonable to treat the sample in this analysis as representative of the population? Briefly explain why or why not.\n\n\n\nExercise 8\nConduct a hypothesis test for the slope to assess whether there is sufficient evidence of a linear relationship between the percent of adults age 25 and older with a bachelor’s degree and the median household income in a county. Use a randomization (permutation) test. In your response:\n\nState the null and alternative hypotheses in words and mathematical notation\nShow all relevant code and output used to conduct the test. Use set.seed(2023) and 1000 iterations to construct the appropriate distribution.\nState the conclusion in the context of the data.\n\n\n\nExercise 9\nNext, construct a 95% confidence interval for the slope using bootstrapping with set.seed(2023) and 1000 iterations.\n\nShow all relevant code and output used to calculate the interval.\nInterpret the confidence interval in the context of the data.\nIs the confidence interval consistent with the results of the test from the previous exercise? Briefly explain why or why not.\n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01-old.html#reproducibility",
    "href": "hw/hw-01-old.html#reproducibility",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Reproducibility",
    "text": "Reproducibility\n\nExercise 10\nYou are asked to use a reproducible workflow for all of your work in the class, and the goal of this question to is better understand potential real-world implications of doing (or not) doing so. Below are some real-life examples in which having a non-reproducible workflow resulted in errors that impacted research and public records.\n\nSource: Ostblom and Timbers (2022)\n\n\nReproducibility error\nConsequence\nSource(s)\n\n\n\n\nLimitations in Excel data formats\nLoss of 16,000 COVID case records in the UK\n(Kelion 2020)\n\n\nAutomatic formatting in Excel\nImportant genes disregarded in scientific studies\n(Ziemann, Eren, and El-Osta 2016)\n\n\nDeletion of a cell caused rows to shift\nMix-up of which patient group received the treatment\n(Wallensteen et al. 2018)\n\n\nUsing binary instead of explanatory labels\nMix-up of the intervention with the control group\n(Aboumatar and Wise 2019)\n\n\nUsing the same notation for missing data and zero values\nPaper retraction\n(Whitehouse et al. 2021)\n\n\nIncorrectly copying data in a spreadsheet\nDelay in the opening of a hospital\n(Picken 2020)\n\n\n\nChoose one of the scenarios from the table and read the linked article discussing what went wrong. Then,\n\nBriefly describe what went wrong, i.e., what part of the process of was not reproducible and what error or impact that had.\nThen, describe how the researchers could make the process reproducible.\n\n\nNow is a good time to render your document again if you haven’t done so recently and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-01-old.html#submission",
    "href": "hw/hw-01-old.html#submission",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nTo submit your assignment:\n\nGo to http://www.gradescope.com and click Log in in the top right corner.\nClick School Credentials ➡️ Duke NetID and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-01-old.html#grading-50-points",
    "href": "hw/hw-01-old.html#grading-50-points",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Grading (50 points)",
    "text": "Grading (50 points)\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n5\n\n\nEx 2\n5\n\n\nEx 3\n4\n\n\nEx 4\n3\n\n\nEx 5\n4\n\n\nEx 6\n4\n\n\nEx 7\n3\n\n\nEx 8\n7\n\n\nEx 9\n7\n\n\nEx 10\n5\n\n\nWorkflow & formatting\n32"
  },
  {
    "objectID": "hw/hw-01-old.html#footnotes",
    "href": "hw/hw-01-old.html#footnotes",
    "title": "HW 01: Education & median income in US Counties",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClick here for a guide on writing mathematical symbols using LaTex.↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-02-old.html",
    "href": "hw/hw-02-old.html",
    "title": "HW 02: Palmer penguins",
    "section": "",
    "text": "In this analysis you will use simple and multiple linear regression to analyze relationships between variables in three different scenarios."
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-1",
    "href": "hw/hw-02-old.html#exercise-1",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 1",
    "text": "Exercise 1\nOur eventual goal is to fit a model predicting body mass (which is more difficult to measure) from island, bill length, bill depth, flipper length, species, and sex.\nWe will start by preparing the data.\n\nUse the drop_na() function to remove any observations from the penguins data frame that has missing values. Your resulting data frame should have 333 observations.\n\n\n\n\n\n\n\nNote\n\n\n\nFor the purposes of this assignment, we will treat the missingness as random, i.e., this is still a representative sample even though we removed observations with missing values."
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-2",
    "href": "hw/hw-02-old.html#exercise-2",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nUse the data to fit a model predicting body mass (which can be difficult to measure) from bill length.\nNeatly display the model using 3 digits.\nWrite the estimated regression equation. Use the variable names in your equation.\nInterpret the slope in the context of the data.\nCalculate the \\(R^2\\) value."
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-3",
    "href": "hw/hw-02-old.html#exercise-3",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nPlot the fitted values vs. the residual values for the model in the previous exercise.\nMake another plot of the histogram of the residuals.\nIs inference for linear regression appropriate? Why or why not?"
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-4",
    "href": "hw/hw-02-old.html#exercise-4",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nThe p-value in Exercise 2 indicates statistical significance. Does this mean the model is a good fit for the data?"
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-5",
    "href": "hw/hw-02-old.html#exercise-5",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 5",
    "text": "Exercise 5\n\nNow, use the data to fit a model predicting body mass (which can be difficult to measure) from bill length, bill depth, flipper length, island, species, and sex. Only include main effects, i.e., no interaction terms, in this model.\nNeatly display the model using 3 digits.\nWrite the estimated regression equation. Use the variable names in your equation.\nCalculate the \\(R^2\\) value. Why is it larger than the previous one? Interpret this value in context of the data and the model.\n\n\n\n\n\n\n\nTip\n\n\n\nThe code for fitting multiple linear regression models is the same as simple linear regression. Use + to add multiple predictor variables to the model."
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-6",
    "href": "hw/hw-02-old.html#exercise-6",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 6",
    "text": "Exercise 6\n\nPlot the fitted values vs. the residual values for the model in the previous exercise.\nMake another plot of the histogram of the residuals.\nIs inference for linear regression appropriate? Why or why not?"
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-7",
    "href": "hw/hw-02-old.html#exercise-7",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nInterpret the intercept in the context of the data.\nInterpret the coefficient of sex in the context of the data.\nInterpret coefficient (slope) of bill length in the context of the data.\nIs the coefficient of bill length the same as in Exercise 2. Briefly explain why or why not?"
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-8",
    "href": "hw/hw-02-old.html#exercise-8",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 8",
    "text": "Exercise 8\n\nCalculate the residual for a female Gentoo penguin on the Biscoe island that weighs 4450 grams with the following body measurements: bill_length_mm = 43.2, bill_depth_mm = 14.5, flipper_length_mm = 208.\nDoes the model overpredict or underpredict this penguin’s mass?"
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-9",
    "href": "hw/hw-02-old.html#exercise-9",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 9",
    "text": "Exercise 9\nFill in the starter code below to write the function predict_boots which takes a bootstrap sample, calculates \\(R^2\\) terms for the two models listed below. Set the seed to 29.\n\nModel 1: The model from Exercise 5. Model 2: The model from Exercise 5, without the island term.\nThe last line of the code runs the function predict_boots for 1000 iterations and saves the output as the object r_squared_diffs. Create a new variable that calculates the difference between the \\(R^2\\) values for the two models.\nMake a histogram of the differences.\nWhat do you notice about the histogram?\nWhich model is best for predicting body mass of a penguin? Briefly explain.\n\n\nset.seed(29)\n\npredict_boots &lt;- function(i) {\n  # take a bootstrap sample\n  boot &lt;- penguins |&gt;\n    slice_sample(n = nrow(penguins), replace = TRUE) |&gt;\n    mutate(boot_samp = i)\n  \n  #fit the model with island to the bootstrap sample\n  fit_island &lt;- linear_reg() |&gt;\n    fit(_____, data = boot)\n  \n  # fit the model without island to the bootstrap sample\n  fit_no_island &lt;- linear_reg() |&gt;\n    fit(_______, data = boot)\n  \n  #save the rsq values for each model\n  tibble(\n    island_r2 = glance(fit_island)$r.squared,\n    no_island_r2 = glance(fit_no_island)$r.squared\n  )\n}\n\n# run the function 1000 times and save the differences between\nr_squared_diffs &lt;- map_df(1:1000, predict_boots)"
  },
  {
    "objectID": "hw/hw-02-old.html#exercise-10",
    "href": "hw/hw-02-old.html#exercise-10",
    "title": "HW 02: Palmer penguins",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nUse the model from the previous exercise without island, and set the seed to 210.\nFit the model to 1000 bootstrap samples.\nMake a scatterplot of the estimated coefficients for flipper length (y-axis) vs. bill length (x-axis).\nCalculate a 95% confidence interval for the difference between these coefficient values. Is there evidence that either bill length or flipper length has a larger influence on the model? Briefly explain.\n\n\n\n\n\n\n\nTip\n\n\n\nThe code for bootstrapping in multiple linear regression is the same as the code for simple linear regression."
  },
  {
    "objectID": "hw/hw-02-old.html#footnotes",
    "href": "hw/hw-02-old.html#footnotes",
    "title": "HW 02: Palmer penguins",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-03.html",
    "href": "hw/hw-03.html",
    "title": "Homework 03: Coffee grades",
    "section": "",
    "text": "Important\n\n\n\nDue: Friday, September 20, 11:59pm"
  },
  {
    "objectID": "hw/hw-03.html#exercise-1",
    "href": "hw/hw-03.html#exercise-1",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 1",
    "text": "Exercise 1\nIn this analysis, we dropped observations that have missing values for some of the relevant variables. What is a disadvantage of dropping observations that have missing values, instead of using a method to impute, i.e., fill in, the missing data? How might dropping these observations impact the generalizability of conclusions? What is the disadvantage of replacing the NAs in Minifigures to 0?"
  },
  {
    "objectID": "hw/hw-03.html#exercise-2",
    "href": "hw/hw-03.html#exercise-2",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 2",
    "text": "Exercise 2\nVisualize the distributions of the predictor variables Pieces, Size, Year, and Pages. Neatly arrange the plots using the patchwork package."
  },
  {
    "objectID": "hw/hw-03.html#exercise-3",
    "href": "hw/hw-03.html#exercise-3",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat are some feature engineering steps you might use to prepare the variables in the previous exercise for the model? Describe the steps and the function you would use. The list should incorporate at least three different step_ functions.\n\n\n\n\n\n\nTip\n\n\n\nUse the recipes reference page for a list of step_ functions."
  },
  {
    "objectID": "hw/hw-03.html#exercise-4",
    "href": "hw/hw-03.html#exercise-4",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe distribution of Theme is shown below. The bars are ordered by the frequency they occur in the data set.\n\nlegos |&gt;\n  count(Theme) |&gt;\nggplot(aes(x = fct_reorder(Theme, n), y = n)) +\n  geom_col() + \n    labs(title = \"Lego Set Theme\", \n         x = \"Theme\", \n         y = \"Number of LEGO sets\") + \n  coord_flip()\n\n\n\n\n\n\n\n\nWhat is one reason we should avoid putting the variable Theme in a model as is?"
  },
  {
    "objectID": "hw/hw-03.html#exercise-5",
    "href": "hw/hw-03.html#exercise-5",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 5",
    "text": "Exercise 5\nWe will use v-fold cross validation to compare two models. We’ll start by preparing the data, creating the folds, and defining the model specification that will be used for both models.\n\nSplit the data into training (75%) and testing (25%) sets. Use a seed of 6.\nSplit the training data into 12 folds. Set the seed to 6 again.\nSpecify the model."
  },
  {
    "objectID": "hw/hw-03.html#exercise-6",
    "href": "hw/hw-03.html#exercise-6",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse the training data to create a recipe for the first model. The model uses the variables Size, Theme, and Pages to predict Amazon_Price. Conduct the following feature engineering steps:\n\nUse step_other() to collapse Theme into fewer categories. Define the threshold such that any levels of Theme with fewer than 20 observations is defined as “Other”.\nMean-center Pages.\nMake dummy variables for all categorical predictors.\nRemove any predictors with zero variance.\n\nThen create the workflow that brings together this recipe and the model specification from the previous exercise."
  },
  {
    "objectID": "hw/hw-03.html#exercise-7",
    "href": "hw/hw-03.html#exercise-7",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 7",
    "text": "Exercise 7\nConduct 12-fold cross validation using the workflow from the previous exercise. Calculate and display mean RMSE across the 12 folds.\n\n\n\n\n\n\nNote\n\n\n\nWe will just use RMSE to compare models for this assignment; however, in practice, it is best to take into account multiple model fit statistics to get a more holistic evaluation and comparison of the models."
  },
  {
    "objectID": "hw/hw-03.html#exercise-8",
    "href": "hw/hw-03.html#exercise-8",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let’s consider a new model that includes all the variables used in model from Exercise 6 along with Year, Pieces and Minifigures.\n\nUse the training data to create a recipe that uses all the feature engineering steps in Exercise 6 with the addition of the following steps:\n\nCreate a new variable called since2018 that calculates the number of years since 2018.\nRemove Year as a potential predictor.\nMean-center Pieces.\n\nCreate the workflow that brings together this recipe and the model specification from Exercise 5.\nConduct 12-fold cross validation using this model workflow. Calculate and display mean RMSE across the 12 folds."
  },
  {
    "objectID": "hw/hw-03.html#exercise-9",
    "href": "hw/hw-03.html#exercise-9",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 9",
    "text": "Exercise 9\nCompare the cross validation results from Exercises 7 and 8. Which model do you select based on RMSE? Briefly explain your choice."
  },
  {
    "objectID": "hw/hw-03.html#exercise-10",
    "href": "hw/hw-03.html#exercise-10",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 10",
    "text": "Exercise 10\nRefit the selected model on the entire training data. Neatly display the model using 3 digits.\nThen, calculate VIF for the model and use it to comment on whether there are potential issues with multicollinearity.\n\n\n\n\n\n\nTip\n\n\n\nWhen we fit a model using recipe and workflow, we need to extract the model object before using augment or vif functions. Fill in the name of the selected model in both blanks in the code below to extract the model object and calculate VIF\n\nlegos_fit_model &lt;- extract_fit_parsnip(______)\nvif(legos_fit_model$fit)"
  },
  {
    "objectID": "hw/hw-03.html#exercise-11",
    "href": "hw/hw-03.html#exercise-11",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 11",
    "text": "Exercise 11\nCalculate RMSE on the training data and on the testing data. Use it to comment on how well the model performs on new data and whether there are signs of model overfit."
  },
  {
    "objectID": "hw/hw-03.html#exercise-12",
    "href": "hw/hw-03.html#exercise-12",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 12",
    "text": "Exercise 12\nThough we do not check the model conditions in this assignment, complete this exercise assuming the model conditions are met.\nDescribe the effect of Theme on the price of LEGO sets, including an indication of which levels are statistically significant. Use a threshold of 0.05 to determine significance.\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-03.html#exercise-13",
    "href": "hw/hw-03.html#exercise-13",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 13",
    "text": "Exercise 13\nThe World Bank collects “world development indicators” about the past and current development of countries. These data are made available on the World Bank’s website. It can be used to understand the relationships between these various factors and trends over time.\n\nFor this analysis, we focus on indicators from 2011 on 165 countries. The variables of interest are:\n\ngdp.per.capita: gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.\nsanit.access.factor: Population access to sanitation facilities (Low, High)\nedu.expend: Government expenditure on education, total (% of government expenditure)\nlife.expect: Life expectancy at birth (in years)\n\nYou fit a model using sanitation access, education expenditures, and life expectancy to understand variability in GDP. The model takes the form\n\\[\n\\begin{aligned}\\widehat{\\log(GDP)} = \\hat{\\beta}_0 &+ \\hat{\\beta}_1 ~ sanit.access.factor + \\hat{\\beta}_2 ~ edu.expend + \\hat{\\beta}_3 ~life.expect \\\\ &+ \\hat{\\beta}_4 ~ sanit.access.factor \\times life.expect\\end{aligned}\n\\]\nThe output from R is shown below.\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.491\n1.638\n2.742\n0.007\n\n\nsanit.access.factorhigh\n-6.993\n1.971\n-3.548\n0.001\n\n\nedu.expend\n0.097\n0.038\n2.550\n0.012\n\n\nlife.expect\n0.030\n0.029\n1.061\n0.291\n\n\nsanit.access.factorhigh:life.expect\n0.122\n0.032\n3.853\n0.000\n\n\n\n\n\n\nInterpret the coefficient of edu.expend in the context of the data.\nInterpret the coefficient of life.expect for countries with high sanitation access in the context of the data.\n\n\n\n\n\n\nImportant\n\n\n\nWrite all interpretations in terms of the original units, not the log-transformed units."
  },
  {
    "objectID": "hw/hw-03.html#exercise-14",
    "href": "hw/hw-03.html#exercise-14",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 14",
    "text": "Exercise 14\nBelow are plots from the exploratory data analysis of the relationships between the predictor variables. Based on these plots, what appears to be a potential issue with the model from Exercise 13? Briefly explain your response."
  },
  {
    "objectID": "hw/hw-03.html#footnotes",
    "href": "hw/hw-03.html#footnotes",
    "title": "Homework 03: Coffee grades",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "hw/hw-05.html",
    "href": "hw/hw-05.html",
    "title": "HW 05: County Health",
    "section": "",
    "text": "Important\n\n\n\nDue: Friday, October 4th, 11:59pm\nIn this homework, you’ll use simple and multiple linear regression to analyze the relationship between the number of doctors in a county, the number of beds, and the number of hospitals."
  },
  {
    "objectID": "hw/hw-05.html#learning-goals",
    "href": "hw/hw-05.html#learning-goals",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Learning goals",
    "text": "Learning goals\nIn this assignment, you will…\n\nFit and interpret multinomial logistic regression models.\nEvaluate conditions for multinomial logistic regression\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-05.html#getting-started",
    "href": "hw/hw-05.html#getting-started",
    "title": "HW 05: Candy Competition",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-fa23 organization on GitHub. Click on the repo with the prefix lab-05. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo.\nDo not make any changes to the .qmd file until the instructions tell you do to so."
  },
  {
    "objectID": "hw/hw-05.html#packages",
    "href": "hw/hw-05.html#packages",
    "title": "HW 05: Candy Competition",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fivethirtyeight)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-05.html#the-data",
    "href": "hw/hw-05.html#the-data",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "The Data",
    "text": "The Data\nFor this assignment, we will analyze data from the eye witness identification experiment in Carlson and Carlson (2014). In this experiment, participants were asked to watch a video of a mock crime (from the first person perspective), spend a few minutes completing a random task, and then identify the perpetrator of the mock crime from a line up shown on the screen. Every lineup in this analysis included the true perpetrator from the video. After viewing the line-up , each participant could make one of the following decisions (id):\n\ncorrect: correctly identified the true perpetrator\nfoil: incorrectly identified the “foil”, i.e. a person who looks very similar to the perpetrator\nreject: incorrectly concluded the true perpetrator is not in the lineup\n\nThe main objective of the analysis is to understand how different conditions of the mock crime and suspect lineup affect the decision made by the participant. We will consider the following conditions to describe the decisions:\n\nlineup: How potential suspects are shown to the participants\n\nSimultaneous Lineup: Participants were shown photos of all 6 potential suspects at the same time and were required to make a single decision (identify someone from the lineup or reject the lineup).\nSequential 5 Lineup: Photos of the 6 suspects were shown one at a time. The participant was required to make a decision (choose or don’t choose) as each photo was shown. Once a decision was made, participants were not allowed to reexamine a photo. If the participant made an identification, the remaining photos were not shown. In each of these lineups the true perpetrator was always the 5th photo in the lineup.\n\nweapon: Whether or not a weapon was present in the video of the mock crime.\nfeature: Whether or not the perpetrator had a distinctive marking on his face. In this experiment, the distinctive feature was a large “N” sticker on one cheek. (The letter “N” was chosen to represent the first author’s alma mater - University of Nebraska.)\n\nThe data may be found in eyewitness.csv in the data folder.\n\new &lt;- read_csv(\"data/eyewitness.csv\")\new &lt;- ew |&gt;\n  mutate(id = as_factor(id))"
  },
  {
    "objectID": "hw/hw-05.html#exercise-1",
    "href": "hw/hw-05.html#exercise-1",
    "title": "HW 05: County Health",
    "section": "Exercise 1",
    "text": "Exercise 1\nDescribe was an observational unit represents for this data set. How many are there?"
  },
  {
    "objectID": "hw/hw-05.html#exercise-2",
    "href": "hw/hw-05.html#exercise-2",
    "title": "HW 05: County Health",
    "section": "Exercise 2",
    "text": "Exercise 2\nIn Example 1.7 of Stat2, they consider a simple linear model to predict the number of doctors (MDs) from the number of hospitals (Hospitals) in a metropolitan area. In that example, they found that a square root transformation on the response variable produced a more linear relationship. Create a new variable in the CountyHealth data frame called sqrtMDs. Hint: use the sqrt function inside the mutate function."
  },
  {
    "objectID": "hw/hw-05.html#exercise-3",
    "href": "hw/hw-05.html#exercise-3",
    "title": "HW 05: County Health",
    "section": "Exercise 3",
    "text": "Exercise 3\nUse the function ggpairs from the package GGally to generate a grid scatter plots and correlations. Note that you will need to select the variables you want to use. Which explanatory variable (Hospitals or Beds) has the highest correlation with SqrtMDs? Is this consistent with your visual assessment?"
  },
  {
    "objectID": "hw/hw-05.html#exercise-4",
    "href": "hw/hw-05.html#exercise-4",
    "title": "HW 05: County Health",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit a simple linear model using SqrtMDs as the response variable and Hospitals as the predictor. You may use sqrt(MDs) in your lm call instead of SqrtMDs if you like. How much of the variability in the SqrtMDs values is explained by Hospitals? How much of the variability in MDs is explained by the model you just fit. To figure this out:\n\nAugment your model.\nConvert the fitted and observed response variables back to number of MDs rather than square-root of the number of MDs.\nCompute the \\(R^2\\).\n\nWhy are these two numbers different?"
  },
  {
    "objectID": "hw/hw-05.html#exercise-5",
    "href": "hw/hw-05.html#exercise-5",
    "title": "HW 05: County Health",
    "section": "Exercise 5",
    "text": "Exercise 5\nDo you think taking the square root of Beds would improve this model? Support your argument with plots and/or numbers."
  },
  {
    "objectID": "hw/hw-05.html#exercise-6",
    "href": "hw/hw-05.html#exercise-6",
    "title": "HW 05: County Health",
    "section": "Exercise 6",
    "text": "Exercise 6\nRepeat exercise 4 above with Beds as the predictor instead of Hospitals."
  },
  {
    "objectID": "hw/hw-05.html#exercise-7",
    "href": "hw/hw-05.html#exercise-7",
    "title": "HW 05: County Health",
    "section": "Exercise 7",
    "text": "Exercise 7\nFor the model you just fit, are there any high-influence outliers? Justify your answer using something from the lecture on outliers. There appear to be at least two high-leverage points. Which observations are these?"
  },
  {
    "objectID": "hw/hw-05.html#exercise-8",
    "href": "hw/hw-05.html#exercise-8",
    "title": "HW 05: County Health",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit a multiple linear model using SqrtMDs are the response variable and both Hospitals and Beds as the predictors. Interpret both slopes and the intercept in the context of the problem."
  },
  {
    "objectID": "hw/hw-05.html#footnotes",
    "href": "hw/hw-05.html#footnotes",
    "title": "HW 05: County Health",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow, clarity, and professionalism.↩︎"
  },
  {
    "objectID": "labs/ForStudents/lab-02/lab-02.html",
    "href": "labs/ForStudents/lab-02/lab-02.html",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nWednesday, September"
  },
  {
    "objectID": "labs/ForStudents/lab-02/lab-02.html#learning-goals",
    "href": "labs/ForStudents/lab-02/lab-02.html#learning-goals",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will…\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use simulation-based inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "labs/ForStudents/lab-02/lab-02.html#exercise-10",
    "href": "labs/ForStudents/lab-02/lab-02.html#exercise-10",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nThere is a statistically significant linear relationship between average air temperature and average ice duration on Lake Monona \\((\\beta_1 \\neq 0)\\).\n\nDoes the confidence interval you calculated in the previous exercise support or refute this claim? Briefly explain.\n\nRender, commit, and push your final changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/ForStudents/lab-02/lab-02.html#footnotes",
    "href": "labs/ForStudents/lab-02/lab-02.html#footnotes",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://lter.github.io/lterdatasampler/index.html↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "labs/lab-01.html",
    "href": "labs/lab-01.html",
    "title": "Lab 01: Park access",
    "section": "",
    "text": "Important\n\n\n\nDue\n\nFriday, September 8, 11:59pm (Tuesday labs)\nSunday, September 10, 11:59pm (Thursday labs)"
  },
  {
    "objectID": "labs/lab-01.html#learning-goals",
    "href": "labs/lab-01.html#learning-goals",
    "title": "Lab 01: Park access",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab, you will…\n\nBe familiar with the workflow using RStudio and GitHub\nGain practice writing a reproducible report using Quarto\nPractice version control using GitHub\nBe able to create data visualizations using ggplot2 and use those visualizations to describe distributions\nBe gain to fit, interpret, and evaluate simple linear regression models"
  },
  {
    "objectID": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "href": "labs/lab-01.html#clone-the-repo-start-new-rstudio-project",
    "title": "Lab 01: Park access",
    "section": "Clone the repo & start new RStudio project",
    "text": "Clone the repo & start new RStudio project\n\nGo to the course organization at github.com/sta210-fa23 organization on GitHub. Click on the repo with the prefix lab-01-. It contains the starter documents you need to complete the lab.\n\nIf you do not see your lab-01 repo, click here to create your repo. Then, click here to (re)submit your GitHub username.\n\nClick on the green CODE button, select Use SSH (this might already be selected by default, and if it is, you’ll see the text Clone with SSH). Click on the clipboard icon to copy the repo URL.\nIn RStudio, go to File \\(\\rightarrow\\) New Project \\(\\rightarrow\\) Version Control \\(\\rightarrow\\) Git.\nCopy and paste the URL of your assignment repo into the dialog box Repository URL. Again, please make sure to have SSH highlighted under Clone when you copy the address.\nClick Create Project, and the files from your GitHub repo will be displayed in the Files pane in RStudio.\nClick lab-01.qmd to open the template Quarto file. This is where you will write up your code and narrative for the lab."
  },
  {
    "objectID": "labs/lab-01.html#r-and-r-studio",
    "href": "labs/lab-01.html#r-and-r-studio",
    "title": "Lab 01: Park access",
    "section": "R and R Studio",
    "text": "R and R Studio\nBelow are the components of the RStudio IDE.\n\nBelow are the components of an Quarto (.Rmd) file.\n\n\n\n\n\n\nYAML\nThe top portion of your Quarto file (between the three dashed lines) is called YAML. It stands for “YAML Ain’t Markup Language”. It is a human friendly data serialization standard for all programming languages. All you need to know is that this area is called the YAML (we will refer to it as such) and that it contains meta information about your document.\n\n\n\n\n\n\nImportant\n\n\n\nOpen the Quarto (.qmd) file in your project, change the author name to your name, and render the document. Examine the rendered document.\n\n\n\n\nCommitting changes\nNow, go to the Git pane in your RStudio instance. This will be in the top right hand corner in a separate tab.\nIf you have made changes to your Quarto (.qmd) file, you should see it listed here. Click on it to select it in this list and then click on Diff. This shows you the difference between the last committed state of the document and its current state including changes. You should see deletions in red and additions in green.\nIf you’re happy with these changes, we’ll prepare the changes to be pushed to your remote repository. First, stage your changes by checking the appropriate box on the files you want to prepare. Next, write a meaningful commit message (for instance, “updated author name”) in the Commit message box. Finally, click Commit. Note that every commit needs to have a commit message associated with it.\nYou don’t have to commit after every change, as this would get quite tedious. You should commit states that are meaningful to you for inspection, comparison, or restoration.\nIn the first few assignments we will tell you exactly when to commit and in some cases, what commit message to use. As the semester progresses we will let you make these decisions.\nNow let’s make sure all the changes went to GitHub. Go to your GitHub repo and refresh the page. You should see your commit message next to the updated files. If you see this, all your changes are on GitHub and you’re good to go!\n\n\nPush changes\nNow that you have made an update and committed this change, it’s time to push these changes to your repo on GitHub.\nIn order to push your changes to GitHub, you must have staged your commit to be pushed. click on Push."
  },
  {
    "objectID": "labs/lab-01.html#footnotes",
    "href": "labs/lab-01.html#footnotes",
    "title": "Lab 01: Park access",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "labs/lab-03.html",
    "href": "labs/lab-03.html",
    "title": "Lab 03: Coffee grades",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, September 22, 11:59pm (Tuesday labs)\nSunday, September 24, 11:59pm (Thursday labs)"
  },
  {
    "objectID": "labs/lab-03.html#learning-goals",
    "href": "labs/lab-03.html#learning-goals",
    "title": "Lab 03: Coffee grades",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will…\n\nbe able to use mathematical models to conduct inference for the slope\nbe able to assess conditions for simple linear regression"
  },
  {
    "objectID": "labs/lab-03.html#grading-50-pts",
    "href": "labs/lab-03.html#grading-50-pts",
    "title": "Lab 03: Coffee grades",
    "section": "Grading (50 pts)",
    "text": "Grading (50 pts)\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n4\n\n\nEx 2\n3\n\n\nEx 3\n4\n\n\nEx 4\n3\n\n\nEx 5\n8\n\n\nEx 6\n5\n\n\nEx 7\n8\n\n\nEx 8\n6\n\n\nEx 9\n4\n\n\nEx 10\n2\n\n\nWorkflow & formatting\n31"
  },
  {
    "objectID": "labs/lab-03.html#footnotes",
    "href": "labs/lab-03.html#footnotes",
    "title": "Lab 03: Coffee grades",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "labs/lab-05.html",
    "href": "labs/lab-05.html",
    "title": "Lab 05: Candy Competition",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nFriday, October 20, 11:59pm (Tuesday labs)\nFriday, October 20, 11:59pm (Thursday labs)"
  },
  {
    "objectID": "labs/lab-05.html#introduction",
    "href": "labs/lab-05.html#introduction",
    "title": "Lab 05: Candy Competition",
    "section": "Introduction",
    "text": "Introduction\nIn today’s lab you will analyze data about candy that was collected from an online experiment conducted at FiveThirtyEight.\n\nLearning goals\nBy the end of the lab you will be able to\n\ndescribe the components of a recipe\nfit a model using recipes\ncompare models\ncontinue developing a collaborative workflow with your teammates"
  },
  {
    "objectID": "labs/lab-05.html#getting-started",
    "href": "labs/lab-05.html#getting-started",
    "title": "Lab 05: Candy Competition",
    "section": "Getting started",
    "text": "Getting started\n\nA repository has already been created for you and your teammates. Everyone in your team has access to the same repo.\nGo to the sta210-fa23 organization on GitHub. Click on the repo with the prefix lab-05. It contains the starter documents you need to complete the lab.\nEach person on the team should clone the repository and open a new project in RStudio. Throughout the lab, each person should get a chance to make commits and push to the repo.\nDo not make any changes to the .qmd file until the instructions tell you do to so."
  },
  {
    "objectID": "labs/lab-05.html#workflow-using-git-and-github-as-a-team",
    "href": "labs/lab-05.html#workflow-using-git-and-github-as-a-team",
    "title": "Lab 05: Candy Competition",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nThere are no Team Member markers in this lab; however, you should use a similar workflow as in Lab 04. Only one person should type in the group’s .qmd file at a time. Once that person has finished typing the group’s responses, they should render, commit, and push the changes to GitHub. All other teammates can pull to see the updates in RStudio.\nEvery teammate must have at least one commit in the lab. Everyone is expected to contribute to discussion even when they are not typing."
  },
  {
    "objectID": "labs/lab-05.html#packages",
    "href": "labs/lab-05.html#packages",
    "title": "Lab 05: Candy Competition",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(fivethirtyeight)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "labs/lab-05.html#data-candy",
    "href": "labs/lab-05.html#data-candy",
    "title": "Lab 05: Candy Competition",
    "section": "Data: Candy",
    "text": "Data: Candy\nThe data from this lab comes from the the article FiveThirtyEight The Ultimate Halloween Candy Power Ranking by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy matchups (e.g. Reeses vs. Skittles). Click here to check out some of the match ups.\nThe data set contains the characteristics and win percentage from 85 candies in the experiment. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nchocolate\nDoes it contain chocolate?\n\n\nfruity\nIs it fruit flavored?\n\n\ncaramel\nIs there caramel in the candy?\n\n\npeanutalmondy\nDoes it contain peanuts, peanut butter or almonds?\n\n\nnougat\nDoes it contain nougat?\n\n\ncrispedricewafer\nDoes it contain crisped rice, wafers, or a cookie component?\n\n\nhard\nIs it a hard candy?\n\n\nbar\nIs it a candy bar?\n\n\npluribus\nIs it one of many candies in a bag or box?\n\n\nsugarpercent\nThe percentile of sugar it falls under within the data set. Values 0 - 1.\n\n\npricepercent\nThe unit price percentile compared to the rest of the set. Values 0 - 1.\n\n\nwinpercent\nThe overall win percentage according to 269,000 matchups. Values 0 - 100.\n\n\n\nUse the code below to get a glimpse of the candy_rankings data frame in the fivethirtyeight R package.\n\nglimpse(candy_rankings)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter…\n$ chocolate        &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31…\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "labs/lab-05.html#exercises",
    "href": "labs/lab-05.html#exercises",
    "title": "Lab 05: Candy Competition",
    "section": "Exercises",
    "text": "Exercises\nThe goal of this analysis is to use multiple linear regression to understand the factors that make a good candy.\n\nExercise 1\n\nUse ggplot to make a graph of your choice exploring the relationship between at least 3 variables in the data set.\nWrite two observations from your graph.\n\n\n\nExercise 2\nSplit the data into training (80%) and testing sets (20%). Call the training set candy_train and the testing set candy_test. Use a seed of 2.\n\n\nExercise 3\nBelow is a recipe for a model that uses the characteristics of candy to understand variability in the win percentage. The lines of the recipe code are labeled Line 1 - Line 9. Describe what each line of code does. The explanation should be written comprehensively and specifically enough that someone could replicate the data manipulation steps based on your description.\nFor example, if a line of code is step_center(X), a comprehensive and specific explanation something similar to the following: “This line of code mean centers the variable \\(X\\) by subtracting \\(\\bar{X}\\) from each value of \\(X\\) in the training data.”\n\n\n\n\n\n\nTip\n\n\n\nUse the Recipes Function Reference page as a resource to learn more about the step_ functions.\n\n\n\n#Line 1\ncandy_rec &lt;- recipe(winpercent ~ ., data = candy_train) |&gt; \n#Line 2\n  update_role(competitorname, new_role = \"ID\") |&gt; \n# Line 3\n  step_cut(sugarpercent, breaks = c(0, 0.25, 0.5, 0.75, 1)) |&gt; \n#Line 4\n  step_mutate(pricepercent = pricepercent * 100) |&gt; \n#Line 5\n  step_dummy(all_nominal_predictors()) |&gt; \n#Line 6\n  step_interact(terms =~ pricepercent:chocolate) |&gt; \n#Line 7\n  step_interact(terms =~ peanutyalmondy:chocolate) |&gt; \n#Line 8\n  step_rm(fruity, caramel, hard, pluribus, bar, nougat, crispedricewafer) |&gt; \n#Line 9\n  step_zv(all_predictors()) \n\n\n\nExercise 4\nFill in the code to use prep and bake for a preview of what will happen when the recipe in Exercise 3 is applied.\n\ncandy_rec |&gt;\n  prep() |&gt;\n  bake(_____) |&gt;\n  glimpse()\n\nHow many terms (not including the intercept) will be in the model produced by this recipe?\n\n\nExercise 5\nFill in the code to specify model, build the model workflow using the recipe in Exercise 2, and fit the model to the training data. Then, neatly display the model using 3 digits.\n\n#specify the model\ncandy_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#build model workflow\ncandy_workflow &lt;- workflow() |&gt;\n  add_model(_____) |&gt;\n  add_recipe(_____) \n\n# fit the model \ncandy_fit &lt;- candy_workflow |&gt;\n  fit(data = _____)\n\n\n\nExercise 6\nInterpret the following in the context of the data:\n\nIntercept\nCoefficient of sugarpercent_X.0.75.1.\nCoefficient of pricepercent_x_chocolateTRUE\nEffect of pricepercent for chocolate candy\n\n\n\nExercise 7\nLet’s consider another model. Use the recipe workflow to fit a new model that meets the following criteria:\n\nIncludes variables chocolate, pricepercent, crispedricewafer, peanutyalmondy, sugarpercent\nUpdate pricepercent so it ranges from 0 to 100 (instead of 0 to 1)\nMakes sugarpercent a factor where the levels equal the four quartiles: 0 - 0.25, 0.25 - 0.50, 0.50 - 0.75, 0.75 - 1\nIncludes the interaction between pricepercent and peanutyalmondy\n\nNeatly display the model using 3 digits.\n\n\n\n\n\n\nTip\n\n\n\nSee the Function Reference page on recipes.tidymodels.org to find the appropriate recipe functions.\n\n\n\n\nExercise 8\n\nConsider the model from Exercise 5 “Model 1” and the model fit in Exercise 7 “Model 2”. Use the glance() function to calculate \\(R^2\\) for both of these models.\nWhich model would you choose based on \\(R^2\\)? Briefly explain your choice.\n\n\n\nExercise 9\nWe will use RMSE to evalulate the predictive performance of each model on the testing data.\n\nUse the code below to calculate predicted values and RMSE for Model 1 on the testing data. Then get the RMSE for Model 2 on the testing data.\n\n\npredict_test1 &lt;- predict(candy_fit, candy_test) |&gt; \n  bind_cols(candy_test)\n\nrmse1 &lt;- rmse(predict_test1, truth = winpercent, estimate = .pred)\n\n\nWhich model would you choose based on RMSE on the testing data? Briefly explain your choice.\n\n\n\nExercise 10\n\nUse the model you selected in Exercise 9 to describe what generally makes a good candy, i.e., one with a high win percentage."
  },
  {
    "objectID": "labs/lab-05.html#submission",
    "href": "labs/lab-05.html#submission",
    "title": "Lab 05: Candy Competition",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nOne team member submit the assignment:\n\nGo to http://www.gradescope.com and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nSelect all team members’ names, so they receive credit on the assignment. Click here for video on adding team members to assignment on Gradescope.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "labs/lab-05.html#grading",
    "href": "labs/lab-05.html#grading",
    "title": "Lab 05: Candy Competition",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n6\n\n\nEx 2\n3\n\n\nEx 3\n5\n\n\nEx 4\n1\n\n\nEx 5\n2\n\n\nEx 6\n8\n\n\nEx 7\n8\n\n\nEx 8\n4\n\n\nEx 9\n4\n\n\nEx 10\n4\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "labs/lab-05.html#footnotes",
    "href": "labs/lab-05.html#footnotes",
    "title": "Lab 05: Candy Competition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and collaboration. This includes having at least one meaningful commit from each team member and updating the team name and date in the YAML.↩︎"
  },
  {
    "objectID": "links.html",
    "href": "links.html",
    "title": "Useful links",
    "section": "",
    "text": "RStudio\n🔗 on CofI Posit Workbench\n\n\nDiscussion forum\n🔗 on Canvas\n\n\nCanvas Course\n🔗 on Canvas",
    "crumbs": [
      "Useful links"
    ]
  },
  {
    "objectID": "prepare/nov-27.html",
    "href": "prepare/nov-27.html",
    "title": "November 27 lecture",
    "section": "",
    "text": "Examples of full multinomial modeling workflow:\n\njuliasilge.com/blog/multinomial-volcano-eruptions\njuliasilge.com/blog/nber-papers"
  },
  {
    "objectID": "prepare/nov-27.html#additional-resources",
    "href": "prepare/nov-27.html#additional-resources",
    "title": "November 27 lecture",
    "section": "",
    "text": "Examples of full multinomial modeling workflow:\n\njuliasilge.com/blog/multinomial-volcano-eruptions\njuliasilge.com/blog/nber-papers"
  },
  {
    "objectID": "prepare/sep-11.html",
    "href": "prepare/sep-11.html",
    "title": "September 3 lecture",
    "section": "",
    "text": "📖 IMS, Sec 24.2: Randomization test for the slope\n📖 IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "prepare/sep-11.html#assigned-reading",
    "href": "prepare/sep-11.html#assigned-reading",
    "title": "September 3 lecture",
    "section": "",
    "text": "📖 IMS, Sec 24.2: Randomization test for the slope\n📖 IMS, Sec 24.3: Bootstrap confidence interval for the slope"
  },
  {
    "objectID": "prepare/week-01.html",
    "href": "prepare/week-01.html",
    "title": "August 23 Lecture",
    "section": "",
    "text": "Read Chapter 0 of Stat2"
  },
  {
    "objectID": "prepare/week-01.html#assigned-reading",
    "href": "prepare/week-01.html#assigned-reading",
    "title": "August 23 Lecture",
    "section": "",
    "text": "Read Chapter 0 of Stat2"
  },
  {
    "objectID": "project-instructions.html",
    "href": "project-instructions.html",
    "title": "Final project",
    "section": "",
    "text": "TL;DR: Pick a data set and do a regression analysis. That is your final project.\nIn this project, you will select or collect a data set of interest to you, pose a research question(s) that you will attempt to answer using multiple linear and/or multiple logistic regression, and write a paper in a formal scientific style. The data set can come from reputable online sources, research you have conducted, friends or professors who have collected scientific data, etc.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#overview",
    "href": "project-instructions.html#overview",
    "title": "Final project",
    "section": "",
    "text": "TL;DR: Pick a data set and do a regression analysis. That is your final project.\nIn this project, you will select or collect a data set of interest to you, pose a research question(s) that you will attempt to answer using multiple linear and/or multiple logistic regression, and write a paper in a formal scientific style. The data set can come from reputable online sources, research you have conducted, friends or professors who have collected scientific data, etc.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#timeline-grading",
    "href": "project-instructions.html#timeline-grading",
    "title": "Final project",
    "section": "Timeline & Grading",
    "text": "Timeline & Grading\nStage I: Proposal and Data Assembly (5%)\n\ndue Friday, October 11 (will accept through Monday, October 13th without penalty)\n\nStage II: Exploratory Data Analysis (5%)\n\ndue Wednesday, October 30\n\nStage III: Project Paper (25%)\n\ndue Monday, November 18\n\nStage IV: Peer Review (5%)\n\ndue Monday, November 25\n\nStage V: Revised Project Paper (45%)\n\ndue Friday December 6\n\nStage VI: Poster Presentation (15%)\n\ndue to printer Monday, December 9, 9AM\npresentation on December 11, 8:30am-11am",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#data-sources",
    "href": "project-instructions.html#data-sources",
    "title": "Final project",
    "section": "Data sources",
    "text": "Data sources\nBe sure that your data is rich enough so that there are opportunities for model fitting choices, controlling for covariates, discovering interesting interactions, and generally providing interesting answers to real, compelling research questions. Many of you are majoring or minoring in something other than statistics – I encourage you to find a research topic in these or other fields in which you have knowledge and/or interest. I have posted several data repositories on the course website that you may use as a launching pad if you’re searching for data. Some of these data sets have sample analyses and citations for relevant papers; although those can be helpful starting points, I will ask that your project is original in terms of the analyses you perform and references you find. A key aspect of this project is to use statistics to answer a research question in context. The dataset you choose must have enough documentation to give sufficient context for your project. You should be able to answer these questions from the data documentation:\n\nHow were data collected? From whom, by whom, where and when.\nHow is each variable defined and measured?\n\nBecause you’ll want to use several explanatory variables and explore interactions, it is best if your dataset contains at least 100-150 observations. (Rule of thumb says at least 10 observations per variable to avoid over-fitting). There is no maximum size, though R does start to slow down for really big data; if you have a truly humongous data set you may want to take a random sample to work with for your project.\n\nLogistics\nThe four primary deliverables for the final project are\n\nA written, reproducible report detailing your analysis to be submitted in either Microsoft word or a PDF and uploaded to Canvas.\nThe .qmd and PDF from a Quarto appendix you create.\nFormal peer review on another team’s work and presentation feedback.\nA PDF of a poster to be uploaded to Canvas and a physical poster which you will present at during the final exam period.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#project-proposal",
    "href": "project-instructions.html#project-proposal",
    "title": "Final project",
    "section": "Stage I: Project proposal",
    "text": "Stage I: Project proposal\nYour proposal, to be turned in as a Word document or PDF on Canvas, will include the following:\n\nIdentify the original data source. Include a brief summary of how, from whom, and by whom, the data were collected. The data can come from one source or multiple sources. Students can collect their own data through experiment, survey, or study, but this is not required. If you decide to do this, start early and come see me, you may need to get IRB approval. Upload a copy of the data set: not a link to it, but an actual downloaded file in a format that R can open. (This is so I can look at it if I have questions about your proposal). If you plan to collect your own data, you must provide details on the design and implementation of the experiment or survey (in that case, you don’t actually need to have the data set “in hand” yet).\nIdentify the important research question(s) which will guide your project (e.g. “Do youth who participate in physical exercise class have lower BMI?”, “Are males more likely to drink and drive after adjusting for confounding variables?”) and describe why your chosen project is interesting to you.\nProvide a list of variables of interest and their definitions (including units). You should also include rationale for inclusion for each variable and identify the variable type, and whether it may need recoding. A table is a good way to summarize this information; for example:\n\n\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nOriginal Definition\nUnits\nRange or Levels\nPossible recoding\nRationale\n\n\nbmi\nBody mass index\nKg/m^2\n&gt;0\n\nResponse variable\n\n\npe\nHow many days per week attends Physical education class\nDays/week\n0-5, integers\nCurrently categorical var. Recode to same values but numeric\nMain explanatory var of interest\n\n\nage\nAge of student\nyears\n12-19, integers\n\nPossible con founding var\n\n\nlunch\nPercent of students at the school receiving free /reduced lunch\n%\n0-100\n\nPossible con founding var (soc io-econ status)\n\n\n\n\n\nFind references for at least two articles in the refereed literature that are relevant to your question of interest. You should avoid articles that are too technical to be relevant to the project or to be informative for the non-specialist. Be sure you obtain the entire paper and not just an abstract! You will eventually use these references in the introduction of your paper. Your proposal should include:\n\nThe citation for each reference (in a standard format) and a link, if appropriate.\nA few sentences that summarize the primary findings and how they relate to your proposal.\n\nBriefly outline how you plan to address your research question(s) with your data. (e.g, “I plan to run multiple linear regression models with BMI as response and daily attendance and age as explanatory variables, possibly examining interactions between the two. I will also adjust for socioeconomic status at the school level.”)\n\n\nUtilizing the Librarians for Help with Research\nFrom Christine Schutz:\n\nAn essential (and favorite) aspect of a librarian’s work is to provide assistance and coaching to students doing research for their projects and papers. Our two librarians - Christine Schutz and Lance McGrath – can help students to focus their source quests and locate appropriate and useful sources. Since students don’t always realize that not everyone who works in a library is a librarian (or even what that means) and that librarians aren’t in the library at all hours of the night (even though the library is open), please nudge them to reach out to us by email or Teams chat to set up a time to meet (or we can sometimes help over email/chat). From the history of the sufganiyot (this was last Fall’s favorite) to the Lamington crayfish (Euastacus sulcatus) and why they are blue (an all-time favorite of mine), student research projects are the best part of the job (and I know Lance feels the same way), please send those students our way.\n\nYou are more than welcome to contact me with any questions about your project but please do not hesitate reaching out to Christine or Lance with any questions.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#exploratory-analysis",
    "href": "project-instructions.html#exploratory-analysis",
    "title": "Final project",
    "section": "Stage II: Exploratory Data Analysis",
    "text": "Stage II: Exploratory Data Analysis\nThe second stage of your project is to conduct Exploratory Data Analysis.\n\nYou may need to “clean” your dataset first. Make note of any problematic data and observations that need to be removed. Consider implications about any decisions you make about missing data. This website shares some simple approaches to missing data (and the relative advantages/disadvantages of each approach).\nProduce descriptive statistics (5-number summaries and histograms or boxplots for continuous variables; tables of counts/proportions and bar charts for categorical variables) for all relevant variables in your data set.\nExplore the relationships between important pairs of variables both graphically and numerically. Depending on the type of your response and explanatory variables, you may consider graphs such as boxplots, scatterplots/jitterplots, or segmented bar charts. You may consider summary statistics (like mean, median, or standard deviation) by group, correlations, and two-way tables with proportions. At this stage plots can be loose in terms of titles and labels, but for your final paper it is essential that your figures have (meaningful) captions and axis labels!\nYou should not be fitting any models at this point.\n\n\nYour EDA report, to be turned in on Canvas, will meet these guidelines:\nIn no more than 3 pages, summarize the main findings of your exploratory analysis, referring to specific plots and summary statistics where necessary. In addition, describe your plans for building models to address important research questions, including which variables will be important to consider in light of your exploratory analyses. This report should be meaningful and readable to someone familiar with statistics but unfamiliar with your particular research topic and dataset (i.e. your professor). Give concise but precise statements interpreting summary statistics, etc. – in the context of your data set and research questions you pose. Avoid vague terms like “this data”, “these results”, etc. Also avoid cryptic variable names that you may have used in R. A report like this might be something you’d share with collaborators or store as a reference as you proceed with your analysis.\n\nThe Main Body of your EDA report should follow these guidelines:\n\nNo more than 3 pages\nBegin with a short paragraph introducing your project and primary research questions. (This introduction will be expanded into several paragraphs for the final paper.)\nUse your graphical and numerical summaries to tell a story, supporting your conclusions with summary statistics. Weave numerical summaries seamlessly into your text, and refer to graphs where appropriate.\nYou do not need to include EVERY plot or table you made in the report. Include at least 2 interesting plots/tables (if not more!). Name each figure (e.g. Figure 1) so they are easily referred to in your report, and format the figures neatly within your report (without taking up too much space). These exploratory plots/tables don’t have to be perfect in terms of titles and labels, but for your final paper it is essential that your figures have (meaningful) captions and axis labels!\nPreview directions you plan to go with modeling. What models will you begin by fitting, and what variables will be involved. This should be the last paragraph of the report.\nWrite well! Complete sentences, good flow, proper grammar, etc.\n\nYour EDA report should also include an Annotated Appendix and References section (not included in the 3 page limit) which include these elements:\n\nClear definitions of important variables and the (properly cited) source of the data.\n\nTables and figures that are informative but were not referenced specifically in the main report. Include a short annotation – one or two sentences on what they show.\nA citation for each reference article (in a standard format) you included in your proposal. Also include a link, if appropriate.\n\nYou should also upload to Canvas:\n\nA Quarto document with R code and (commented) output so that I can trace how you constructed your final data set, what the results of your exploratory data analyses were, and what plots you generated. Comments should be short, but clarify what you’ve done and why.\nThe rendered pdf document that was created from the Quarto above.\nThe data file (as .txt, .csv, or .xls).",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#draft-report",
    "href": "project-instructions.html#draft-report",
    "title": "Final project",
    "section": "Stage III: Project Report",
    "text": "Stage III: Project Report\nYour report should be a thoughtful, concise, and polished document, no longer than 8 pages. Relevant tables and/or figures should be formatted neatly into your report. Be sure to label and reference your graphs and tables so they are interpretable on their own. An annotated appendix containing less relevant figures and tables along with important R code and output should be attached to the end of your report (see below for more details). The report should include four LABELED sections plus bibliography and annotated appendix. There is no page limit on the bibliography and appendix. The rubric that I use to grade the reports is here.\n\nIntroduction\nA few paragraphs that contain background information, motivation for your research, and a statement of your research goals. Be sure to incorporate your supporting references into the text. The purpose of the background is to place your work in the greater context of the literature in the area you are investigating. Then you should explicitly identify the research question that you will investigate with your analysis.\n\n\nMethods\nA few paragraphs that…\n\nBriefly describe your data, where it came from (source), definitions of important variables, and how it was collected (random/representative sampling? From what population?)\nIndicate any modifications made to the data, recoding, or decisions about missing data. Be sure to report the sample size and the number of observations removed, if any.\nBriefly but thoroughly describe the statistical methods used to investigate the association between your outcome and predictor variables. What summary statistics were calculated? (Just list: e.g. proportions, and conditional proportions for categorical variables; means, medians, and standard deviation for quantitative variables). What statistical tests were performed? What type of modeling was done?\n\nDo not report results in the Methods section!\n\nNote: If you are using a method not covered in this class (e.g., a nonparametric method, time series), you may choose to expand Methods a bit to describe your statistical method.\n\n\nResults\nThe densest (but not necessarily longest) section of your report, which should include…\n\nA general description of your data. (This is where you integrate your exploratory data analysis from Stage II.)\nThe final model, presented in a table with variables, coefficients, standard errors, p-values, and confidence intervals for each coefficient.\nA description of the results from your analyses, including interpretations of parameter estimates and/or confidence intervals in context.\nTables that summarize results and figures that illustrate results. These tables and figures should be well-labeled, numbered (e.g. Figure 1), and have good, descriptive captions. Each report should have a minimum of two tables/figures. Rarely are residual plots part of the main body of the report unless they are an integral part of the story.\nIt may be helpful to refer to the articles you’ve cited as a guide for the writing style of the results section.\nWhile you should interpret confidence intervals and/or coefficients in this section, you should not editorialize here! Save that for the Discussion.\n\\(R^2\\) (for linear regression) or misclassification rate (for logistic regression) should be reported here or in the Discussion (or both).\n\n\n\nDiscussion\nA few paragraphs that:\n\nBegin with an accurate summary statement; describe how the results help answer your research questions and what was most interesting from your analysis. In fact, the first paragraph of the Discussion is very important – in professional journals, it is often the first and sometimes the only paragraph that is read in a paper. After the first sentence highlights primary results, the remainder of the first paragraph might compare your results to others in the literature or include interesting secondary results.\nDiscuss possible implications of the results in the context of the research question.\nMake a statement regarding potential confounding variables in your study.\nMake a statement about the generalizability of your results. Don’t give generic statements of possible causation and generalizability, but thoughtfully discuss relevant issues – confounding variables, representativeness of the sample, etc. You should demonstrate your knowledge of these issues and their importance in context; the research topic is the focus here, not statistics for statistics sake.\n\nIdentify any limitations of your study. Discuss the potential impact of such limitations on the conclusions.\nIdentify strengths and weaknesses of your analysis. This is where you should (briefly) mention whether the conditions of the model are met.\nMake suggestions for future research. Identify important next steps that a researcher could take to build on your work.\nDo not include test statistics or p-values in this section, although you can of course reference Results when discussing your overall conclusions.\nSee the end of this document for some additional advice about writing Discussions.\n\n\n\nBibliography\nInclude full citations for any papers cited in your introduction or other sections of your report. Also cite the dataset you use as appropriate. You may use any bibliographic style you are familiar with (APA, MLA, Chicago, etc), but you MUST be consistent with this style (use the same style for both the bibliography and in-text citations).\n\n\nAnnotated Appendix\n\nTables and figures that are informative but were not referenced specifically in the main report. Include a short annotation – one or two sentences on what they show.\nR code and output (commented and annotated) so that I can trace how you constructed your final data set, what models you ran to produce the results quoted in your report, and what intermediate models you also considered.\nDescription of statistical modeling steps that were not included in the main body of your report. Possible entries here include:\nHow you handled missing data or recoded variables\nEvaluation of assumptions (including residual plots or empirical logit plots)\nHow you went from the model output in R to interpretations in your report (e.g. exponentiate coefficients, then take inverse)\nSteps in your model building process – how you decided on the explanatory variables you ultimately included in your final model.\nTwo tips on constructing this section:\nAnticipate questions someone might have after reading your report, and make sure those questions can be answered with information in the appendix.\nThe easiest way to create the appendix is to use Quarto, then knit it into a PDF document (just to create this section, not the entire report).\n\nThe appendix should be uploaded as a separate document on Moodle and titled “Appendix”.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#peer-review",
    "href": "project-instructions.html#peer-review",
    "title": "Final project",
    "section": "Stage IV: Peer Review",
    "text": "Stage IV: Peer Review\nAn important part of any work is reflection and review. When you submit your paper to Canvas, it will be reviewed by another student, and you will be asked to review as well. More detailed instructions on this exercise will be given at the time of the review. You will be evaluated on your effort in the review, including how constructive and insightful your comments are.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#written-report",
    "href": "project-instructions.html#written-report",
    "title": "Final project",
    "section": "Stage V: Project Revision",
    "text": "Stage V: Project Revision\nAfter submitting your project report, you will receive feedback from your professor and 1-2 peers. You will have the opportunity to revise your project paper and resubmit it. A portion of your revision grade is based on the adequacy of your response to comments. (This means that if you just resubmit the first copy of your paper without changes, it will receive a lower grade.)",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#poster-presentation",
    "href": "project-instructions.html#poster-presentation",
    "title": "Final project",
    "section": "Stage VI: Poster Presentation",
    "text": "Stage VI: Poster Presentation\nThe following is from Passion-Driven Statistics:\nYou have conducted a quantitative research project. Now you will learn how to present your results as a research poster and presentation. Posters offer the opportunity to engage with an audience and to meaningfully disseminate your research findings to others.\nLearn keys to a successful poster presentation. Consider your audience and frame your research question and results in an understandable and interesting way. Understand you should be brief, use large font size, and incorporate graphics instead of text whenever possible. See how being clear and concise with a logical layout will ensure the viewing experience is intellectually and aesthetically satisfying for your audience. Click HERE to a watch the video lesson.\nYou will create a 3 column, 40” X 36” poster including an introduction, research questions, methods, results, and discussion.\nA rubric and poster template will be posted closer to the assignment.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-instructions.html#writing-tips",
    "href": "project-instructions.html#writing-tips",
    "title": "Final project",
    "section": "Writing Tips",
    "text": "Writing Tips\nA major goal of this semester is to be able to write a paper based on a statistical analysis that follows the format typically used for academic journal articles, specifically those in the natural and social sciences.\nThe purpose of an article in this style is NOT to dazzle me with your knowledge of statistical methodology (e.g., the meaning of a p-value, how to write down the null and alternative hypotheses… do this on the exams!). The purpose of this paper is to show me you know how to USE statistics to make a persuasive quantitative argument and to communicate results to other scientists with a similar level of statistics background.\nThe audience for the paper is a scientist interested in your research question. This scientist has knowledge of statistical techniques and interpretation, but is NOT a statistician.\nAn article in this style includes four labelled sections:\nIntroduction: What are we doing? Why are we doing this?\nMethods: Where do the data come from? What did we do to it?\nResults: What did we learn from the data? Tell me a story supported with statistics, not about statistics. The results section will also include labeled Figures and Tables with informative captions. Tell me a story with pictures and tables! (But not so many that it’s overwhelming.)\nDiscussion: Why do we care about what we found? How generalizable are the results? What other questions would we like to answer?\n\nSome tips for writing the results section\n\nThe Results section should include:\n\nA general description of your data (completed via your exploratory data analysis)\nA description of the results from your analyses, including interpretations of parameter estimates and confidence intervals in context.\nTables that summarize results and figures that illustrate results. These tables and figures should be numbered (e.g. Figure 1) and have good, descriptive captions. They should also be referenced and mentioned in the body of the paper – you should not have a table/figure that is floating on the page with no reference in the paper. Rarely are residual plots part of the main body of the report unless they are an integral part of the story.\nWhile you should interpret confidence intervals and coefficients in this section, you should not editorialize here! Save that for the Discussion.\n\n\n\nHere is one potential Results writing strategy:\nFor the first paragraph, start with simple descriptions about relevant variables: 50% of the subjects are male, 20% smoke cigarettes, and 40% are overweight. Often, studies will include demographic or other baseline characteristics about the subjects in the study, even if they are not the main variables of interest. This baseline descriptive information can be helpful for determining generalizability of the study (for instance, if a random sample is not used what population is this study representative of?) Often these baseline characteristics are just included in a table (e.g. mean/SD or proportion for each variable) and only a couple important ones are discussed in text.\nYour final model should be presented in a table with variables, coefficients, standard errors, p-values, and confidence intervals for each coefficient (similar to the output from R, although not cut-and-pasted from R). This table should look professional! Among other things, this means that you should choose a reasonable and consistent number of significant figures (3 or 4) and confidence intervals should be reported using the (#, #) notation. Somewhere (in this table or in prose), you should also report the \\(R^2\\) value.\nIn a second paragraph, discuss the relationships seen in the model – that is, interpretations of all statistically significant coefficients. Try to make it flow together nicely: report things in an order that makes sense and that emphasizes your point. Don’t just create a list of interpretations. Think about tables or figures that support this story. If you’re making the same comparison across many categories (e.g. how Pulse changes for different income groups), consider reporting the general pattern, or pointing out a few important values in the text (with the rest in a table or graph). THEN add p-values and/or confidence intervals in parentheses. Again, insert references to tables and figures into the text (“as seen in Figure 1, ….”). You do not need to interpret all confidence intervals, but you should interpret one or two especially significant or interesting CIs.\nBefore including a figure in the Results section, ask yourself: Does this add to my story? Does this help the reader understand the interesting relationships that are present here? If the answer to both these questions is “No”, then you absolutely should not include it. It may be helpful to look at the Results sections of the articles you cite in your report.\n\n\n\nSome tips for writing the Discussion section.\nAs stated above, in the Discussion you should do the following (among other things):\n\nIdentify any limitations of your study. Discuss the potential impact of such limitations on the conclusions.\nIdentify strengths and weaknesses of your analysis.\n\nIt can often be difficult to differentiate between these two, but they are two separate issues.\nLimitations of the study often have to do with study design and/or data collection… is it generalizable? If not, why not? Are there potential measurement errors or other biases? (e.g. students feel social pressure to lie about drug use, sex, etc when asked in survey). Are you trying to measure something that isn’t easily defined (e.g. “Very happy”, “Somewhat happy”, etc)?\nStrengths and weaknesses of the analysis are statistical decisions that you made which may strengthen or weaken your potential conclusions. This is also where we mention that model conditions are met (hopefully!), or discuss possible problems with those conditions.\n\nExample of a strength\nVery large sample size means very high power (especially interesting if you fail to reject null hypothesis… this means it has very low probability of a type II error… and even though we can’t officially accept \\(H_0\\), this is stronger evidence that \\(H_0\\) may really be true.)\n\n\nExample weaknesses\n\nHad to combine two or more categories of responses into one in order to use your chosen statistical method (e.g. had to make the explanatory variable binary for logistic regression), which means you now can’t make inferences about those groups separately.\nWe treated a variable as continuous even when it was recorded as {1, 2, 3, 4, 5, 6 or more}. The numeric value of the last group is questionable (would analysis change if we replaced it with the numeric value of 7 instead of 6? 6.5? 8? which of these is most representative of the answers “6 or more”?… we don’t and can’t know). These limitations and weaknesses are natural suggestions for further research. When suggesting further research, feel free to make suggestions of things you can’t or don’t know how to do (yet!), e.g. include another variable (not in your data set), or examine a non-linear relationship.\n\n\n\n\nOther hints\n\nRelevant output from a multiple regression model is appropriate, but any tables should be cleaned up, easy to read, and include only relevant information – that is, they should not be copy/pasted from the R output. R code/R output should never be included in the body of a paper. Similarly, variables should be given logical names in your written document; you should not use “codes” from the raw data set.\nThere are three documents on Canvas, all by Jane Miller, that will be extremely useful in writing a good Results section (and writing a good statistical report in general). She includes examples of “poor”, “better” and “best” phrasings – paying attention to these and following her advice will save you from many common pitfalls! I strongly encourage you to read all three of these (short) pieces before you sit down to write your Results.\nWhile Quarto is perfectly suited to homework and informal reports, it is not the best choice for producing a formal paper, because you don’t want the R code cluttering things up and you’ll want more control over figures and tables than R markdown can provide.\nNever include R code or output in the body of your paper\n\nCI’s should be reported as (#, #), including in Tables.\nEverything should look professional and organized, including page numbers, numbered and captions tables/figures, etc.\nRemember your audience: a scientist interested in your research question. This scientist has knowledge of statistical techniques and interpretation, but is NOT a statistician.\nGo read the documents by Jane Miller on Canvas, especially her advice about the “Goldilocks Principle” when interpreting coefficients.\nIn the caption of the Table with your model results, state the response variable, including units if applicable.\nKeep the number of decimal places less than 4. If you have a very small value use scientific notation; e.g., \\(3.45\\times 10^{-7}\\).\nDon’t use codes in your prose unless absolutely necessary. For example, if you’re using “Return on Investment” = ROI, this is fine because it’s a well-known and easy-to-remember acronym. . But “sem_trim_15” is meaningless, so don’t use it! Even for acronyms it is good practice to write it out with the acronym the first time (e.g. “blah blah blah Return on Investment (ROI) blah blah”) If you must use codes, italicize them for easy reading.",
    "crumbs": [
      "Project",
      "Instructions"
    ]
  },
  {
    "objectID": "project-tips.html",
    "href": "project-tips.html",
    "title": "Final project tips + resources",
    "section": "",
    "text": "Data sources\n\nSome resources that may be helpful as you find data:\n\nR Data Sources for Regression Analysis\nFiveThirtyEight data\nTidyTuesday\n\n\n\nOther data repositories\n\nWorld Health Organization\nThe National Bureau of Economic Research\nInternational Monetary Fund\nGeneral Social Survey\nUnited Nations Data\nUnited Nations Statistics Division\nU.K. Data\nU.S. Data\nU.S. Census Data\nEuropean Statistics\nStatistics Canada\nPew Research\nUNICEF\nCDC\nWorld Bank\nElection Studies\nKaggle\nUC Irvine ML Repository\nGapminder\nNHANES\n\n\n\n\nTips\n\nAsk questions if any of the expectations are unclear.\nMake sure each team member is contributing, both in terms of quality and quantity of contribution (we will be reviewing commits from different team members).\nAll team members are expected to contribute equally to the completion of this assignment and group assessments will be given at its completion - anyone judged to not have sufficient contributed to the final product will have their grade penalized. While different teams members may have different backgrounds and abilities, it is the responsibility of every team member to understand how and why all code and approaches in the assignment works.\n\n\nDo a little more to make the plot look professional!\n\nInformative title and axis labels\nFlipped coordinates to make names readable\nArranged bars based on count\nCapitalized manufacturer names\nOptional: Added color - Use a coordinated color scheme throughout paper / presentation\nOptional: Applied a theme - Use same theme throughout paper / presentation\n\n\nmpg |&gt;\n  count(manufacturer) |&gt;\n  mutate(manufacturer = str_to_title(manufacturer)) |&gt;\n  gf_col(n ~ fct_reorder(manufacturer, n) , stat = \"identity\", fill = \"steelblue\") |&gt; \n  gf_refine(coord_flip()) |&gt; \n  gf_labs(x = \"Manufacturer\", \n       y = \"Count\", \n       title = \"The most common manufacturer is Dodge\") |&gt; \n  gf_refine(theme_bw() )\n\n\n\n\n\n\n\n\n\n\nTables and model output\n\nUse the kable function from the knitr package to neatly output all tables and model output. This will also ensure all model coefficients are displayed.\n\nUse the digits argument to display only 3 or 4 significant digits.\nUse the caption argument to add captions to your table.\n\n\n\nmodel &lt;- lm(mpg ~ hp, data = mtcars)\ntidy(model) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n30.099\n1.634\n18.421\n0\n\n\nhp\n-0.068\n0.010\n-6.742\n0\n\n\n\n\n\n\n\nGuidelines for communicating results\n\nDon’t use variable names in your narrative! Use descriptive terms, so the reader understands your narrative without relying on the data dictionary.\n\n❌ There is a negative linear relationship between mpg and hp.\n✅ There is a negative linear relationship between a car’s fuel economy (in miles per gallon) and its horsepower.\n\nKnow your audience: Your report should be written for a general audience who has an understanding of statistics at the level of MAT 212.\nAvoid subject matter jargon: Don’t assume the audience knows all of the specific terminology related to your subject area. If you must use jargon, include a brief definition the first time you introduce a term.\nTell the “so what”: Your report and presentation should be more than a list of interpretations and technical definitions. Focus on what the results mean, i.e. what you want the audience to know about your topic after reading your report or viewing your presentation.\n\n❌ For every one unit increase in horsepower, we expect the miles per gallon to decrease by 0.068 units, on average.\n✅ If the priority is to have good fuel economy, then one should choose a car with lower horsepower. Based on our model, the fuel economy is expected to decrease, on average, by 0.68 miles per gallon for every 10 additional horsepower.\n\nTell a story: All visualizations, tables, model output, and narrative should tell a cohesive story!\nUse one voice: Though multiple people are writing the report, it should read as if it’s from a single author. At least one team member should read through the report before submission to ensure it reads like a cohesive document.\n\n\n\n\nAdditional resources\n\nExploring RStudio’s Visual Markdown Editor\nR for Data Science\nData Visualization: A Practical Introduction",
    "crumbs": [
      "Project",
      "Tips + resources"
    ]
  },
  {
    "objectID": "slides/01-welcome-old.html#meet-prof.-tackett",
    "href": "slides/01-welcome-old.html#meet-prof.-tackett",
    "title": "Welcome to STA 210!",
    "section": "Meet Prof. Tackett!",
    "text": "Meet Prof. Tackett!\n\n\nEducation and career journey\n\nBS in Math and MS in Statistics from University of Tennessee\nStatistician at Capital One\nPhD in Statistics from University of Virginia\nAssistant Professor of the Practice, Department of Statistical Science at Duke\n\nWork focuses on statistics education and sense of belonging in introductory math and statistics classes\nCo-leader of the Bass Connections team Mental Health and the Justice System in Durham County\nMom of (almost) 8-month-old twins 🙂"
  },
  {
    "objectID": "slides/01-welcome-old.html#meet-the-teaching-assistants-tas",
    "href": "slides/01-welcome-old.html#meet-the-teaching-assistants-tas",
    "title": "Welcome to STA 210!",
    "section": "Meet the Teaching Assistants (TAs)",
    "text": "Meet the Teaching Assistants (TAs)\n\nSam Rosen (PhD): Head TA + Lab 01\nBethany Astor (MS): Lab 02\nJon Campbell (MS)\nDonald Cayton (MS): Lab 02\nAllison Li (UG)\nMitchelle Mojekwu (UG): Lab 04\nBen Thorpe (UG)\nLinxuan Wang (MS): Lab 03\nXiaojun Zheng (PhD): Lab 04"
  },
  {
    "objectID": "slides/01-welcome-old.html#what-is-regression-analysis",
    "href": "slides/01-welcome-old.html#what-is-regression-analysis",
    "title": "Welcome to STA 210!",
    "section": "What is regression analysis?",
    "text": "What is regression analysis?\n\n\n“In statistical modeling, regression analysis is a set of statistical processes for estimating the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a dependent variable and one or more independent variables (or ‘predictors’). More specifically, regression analysis helps one understand how the typical value of the dependent variable (or ‘criterion variable’) changes when any one of the independent variables is varied, while the other independent variables are held fixed.”\n\nSource: Wikipedia (previous definition)"
  },
  {
    "objectID": "slides/01-welcome-old.html#examples-of-regression-in-practice",
    "href": "slides/01-welcome-old.html#examples-of-regression-in-practice",
    "title": "Welcome to STA 210!",
    "section": "Examples of regression in practice",
    "text": "Examples of regression in practice\n\nNew Yorkers Will Pay $56 A Month To Trim A Minute Off Their Commute\nHow FiveThirtyEight’s 2020 Presidential Forecast Works — And What’s Different Because Of COVID-19\nEffect of Forensic Evidence on Criminal Justice Case Processing\nWhy it’s so freaking hard to make a good COVID-19 model (from March 2020)"
  },
  {
    "objectID": "slides/01-welcome-old.html#course-faq",
    "href": "slides/01-welcome-old.html#course-faq",
    "title": "Welcome to STA 210!",
    "section": "Course FAQ",
    "text": "Course FAQ\nQ - What background is assumed for the course?\nA - Introductory statistics or probability course at Duke\n\nQ - Will we be doing computing?\nA - Yes. We will use the computing language R for analysis, Quarto for writing up results, and GitHub for version control and collaboration\n\n\nQ - Will we learn the mathematical theory of regression?\nA - Yes and No. The course is primarily focused on application; however, we will discuss some of the mathematics of simple linear regression. There a 0.5-credit course STA 211: Mathematics of Regression to take simultaneously or after this course to dive into more of the mathematics."
  },
  {
    "objectID": "slides/01-welcome-old.html#course-learning-objectives",
    "href": "slides/01-welcome-old.html#course-learning-objectives",
    "title": "Welcome to STA 210!",
    "section": "Course learning objectives",
    "text": "Course learning objectives\nBy the end of the semester, you will be able to…\n\nanalyze real-world data to answer questions about multivariable relationships.\nuse R to fit and evaluate linear and logistic regression models.\nassess whether a proposed model is appropriate and describe its limitations.\nuse Quarto to write reproducible reports and GitHub for version control and collaboration.\neffectively communicate statistical results through writing and oral presentations."
  },
  {
    "objectID": "slides/01-welcome-old.html#course-topics",
    "href": "slides/01-welcome-old.html#course-topics",
    "title": "Welcome to STA 210!",
    "section": "Course topics",
    "text": "Course topics\n\n\nUnit 1: Quantitative Response Variable\n\nSimple Linear Regression\nMultiple Linear Regression\n\n\nUnit 2: Categorical Response Variable\n\nLogistic Regression\nMultinomial logistic regression\n\n\nUnit 3: Looking Ahead\n\nSpecial topics\nPresenting statistical results"
  },
  {
    "objectID": "slides/01-welcome-old.html#course-toolkit",
    "href": "slides/01-welcome-old.html#course-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Course toolkit",
    "text": "Course toolkit\n\nCourse website: sta210-fa23.netlify.app\n\nCentral hub for the course!\nTour of the website\n\nSakai: sakai.duke.edu\n\nGradebook\nAnnouncements\nGradescope\nEd Discussion\n\nGitHub: github.com/sta210-fa23\n\nDistribute assignments\nPlatform for version control and collaboration"
  },
  {
    "objectID": "slides/01-welcome-old.html#computing-toolkit",
    "href": "slides/01-welcome-old.html#computing-toolkit",
    "title": "Welcome to STA 210!",
    "section": "Computing toolkit",
    "text": "Computing toolkit\n\n\n\n\n\n\n\n\n\n\n\nAll analyses using R, a statistical programming language\nWrite reproducible reports in Quarto\nAccess RStudio through STA 210 Docker Containers\n\n\n\n\n\n\n\n\n\n\n\n\nAccess assignments\nFacilitates version control and collaboration\nAll work in STA 210 course organization"
  },
  {
    "objectID": "slides/01-welcome-old.html#prepare-participate-practice-perform",
    "href": "slides/01-welcome-old.html#prepare-participate-practice-perform",
    "title": "Welcome to STA 210!",
    "section": "Prepare, Participate, Practice, Perform",
    "text": "Prepare, Participate, Practice, Perform\n\n\n\nPrepare: Introduce new content and prepare for lectures by completing the readings (and sometimes watching the videos)\nParticipate: Attend and actively participate in lectures and labs, office hours, team meetings\nPractice: Practice applying statistical concepts and computing with application exercises during lecture, graded for completion\nPerform: Put together what you’ve learned to analyze real-world data\n\nLab assignments (first individual, later team-based)\nHomework assignments (individual)\nTwo exams\nFinal group project"
  },
  {
    "objectID": "slides/01-welcome-old.html#grading",
    "href": "slides/01-welcome-old.html#grading",
    "title": "Welcome to STA 210!",
    "section": "Grading",
    "text": "Grading\n\n\n\nCategory\nPercentage\n\n\n\n\nHomework\n35%\n\n\nFinal project\n15%\n\n\nLab\n15%\n\n\nExam 01\n15%\n\n\nExam 02\n15%\n\n\nApplication Exercises\n2.5%\n\n\nTeamwork\n2.5%\n\n\n\nSee the syllabus for details on how the final letter grade will be calculated."
  },
  {
    "objectID": "slides/01-welcome-old.html#support",
    "href": "slides/01-welcome-old.html#support",
    "title": "Welcome to STA 210!",
    "section": "Support",
    "text": "Support\n\nAttend office hours to meet with a member of the teaching team\n\nProf. Tackett’s office hours start Fri, Sep 1, 1 - 3pm\nFull office hours schedule starts Tue, Sep 5\n\nAsk and answer questions on course discussion forum\nUse email for questions regarding personal matters and/or grades\nSee the Course Support page for more details"
  },
  {
    "objectID": "slides/01-welcome-old.html#diversity-inclusion",
    "href": "slides/01-welcome-old.html#diversity-inclusion",
    "title": "Welcome to STA 210!",
    "section": "Diversity & inclusion",
    "text": "Diversity & inclusion\n\nIt is my intent that students from all diverse backgrounds and perspectives be well-served by this course, that students’ learning needs be addressed both in and out of class, and that the diversity that the students bring to this class be viewed as a resource, strength and benefit.\n\nIf you have a name that differs from those that appear in your official Duke records, please let me know.\nPlease let me know your preferred pronouns, if you are comfortable sharing.\nIf you feel like your performance in the class is being impacted by your experiences outside of class, please don’t hesitate to come and talk with me. If you prefer to speak with someone outside of the course, your advisers and deans are excellent resources.\nI (like many people) am still in the process of learning about diverse perspectives and identities. If something was said or done in class (by anyone) that made you feel uncomfortable, please talk to me about it."
  },
  {
    "objectID": "slides/01-welcome-old.html#accessibility",
    "href": "slides/01-welcome-old.html#accessibility",
    "title": "Welcome to STA 210!",
    "section": "Accessibility",
    "text": "Accessibility\n\nThe Student Disability Access Office (SDAO) is available to ensure that students are able to engage with their courses and related assignments.\nIf you have documented accommodations from SDAO, please send the documentation as soon as possible.\nI am committed to making all course activities and materials accessible. If any course component is not accessible to you in any way, please don’t hesitate to let me know."
  },
  {
    "objectID": "slides/01-welcome-old.html#covid-19-and-other-illness",
    "href": "slides/01-welcome-old.html#covid-19-and-other-illness",
    "title": "Welcome to STA 210!",
    "section": "COVID-19 and other illness",
    "text": "COVID-19 and other illness\n\nPlease do not come to class if you have tested positive for COVID-19, have possible symptoms and have not yet been tested, or have other illness.\nLecture recordings are available for excused absences. See Lecture recording request in the syllabus for more information and a link to the request form.\nRead and follow the university guidelines regarding COVID-19 at coronavirus.duke.edu."
  },
  {
    "objectID": "slides/01-welcome-old.html#late-work-waivers-and-regrade-requests",
    "href": "slides/01-welcome-old.html#late-work-waivers-and-regrade-requests",
    "title": "Welcome to STA 210!",
    "section": "Late work, waivers, and regrade requests",
    "text": "Late work, waivers, and regrade requests\n\nWe have policies! We will discuss them in detail when the first assignment is released.\nRead more about them in the Course policies section of the syllabus and refer back to them as needed\nIf you have questions, email sta210@duke.edu"
  },
  {
    "objectID": "slides/01-welcome-old.html#academic-integrity",
    "href": "slides/01-welcome-old.html#academic-integrity",
    "title": "Welcome to STA 210!",
    "section": "Academic integrity",
    "text": "Academic integrity\n\nTo uphold the Duke Community Standard:\n\nI will not lie, cheat, or steal in my academic endeavors;\nI will conduct myself honorably in all my endeavors; and\nI will act if the Standard is compromised.\n\n\n\nBy participating in this course, you are agreeing that all your work and conduct will be in accordance with the Duke Community Standard."
  },
  {
    "objectID": "slides/01-welcome-old.html#collaboration-sharing-code",
    "href": "slides/01-welcome-old.html#collaboration-sharing-code",
    "title": "Welcome to STA 210!",
    "section": "Collaboration & sharing code",
    "text": "Collaboration & sharing code\n\nWe have policies! We will discuss them in detail when the first assignment is released.\nRead about them in the Academic honesty section of the syllabus and refer to them as needed"
  },
  {
    "objectID": "slides/01-welcome-old.html#use-of-artificial-intelligence-ai",
    "href": "slides/01-welcome-old.html#use-of-artificial-intelligence-ai",
    "title": "Welcome to STA 210!",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n\nYou should treat AI tools, such as ChatGPT, the same as other online resources.\nThere are two guiding principles that govern how you can use AI in this course:1\n\n(1) Cognitive dimension: Working with AI should not reduce your ability to think clearly. We will practice using AI to facilitate—rather than hinder—learning.\n(2) Ethical dimension: Students using AI should be transparent about their use and make sure it aligns with academic integrity.\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎"
  },
  {
    "objectID": "slides/01-welcome-old.html#use-of-artificial-intelligence-ai-1",
    "href": "slides/01-welcome-old.html#use-of-artificial-intelligence-ai-1",
    "title": "Welcome to STA 210!",
    "section": "Use of artificial intelligence (AI)",
    "text": "Use of artificial intelligence (AI)\n✅ AI tools for code: You may make use of the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code.\n❌ No AI tools for narrative: Unless instructed otherwise, AI is not permitted for writing narrative on assignments.\n\n\n\n\n\n\nImportant\n\n\nIn general, you may use AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content."
  },
  {
    "objectID": "slides/01-welcome-old.html#five-tips-for-success",
    "href": "slides/01-welcome-old.html#five-tips-for-success",
    "title": "Welcome to STA 210!",
    "section": "Five tips for success",
    "text": "Five tips for success\n\nComplete all the preparation work (readings and videos) before class.\nAsk questions.\nDo the homework and labs; get started on homework early when possible.\nDon’t procrastinate and don’t let a week pass by with lingering questions.\nStay up-to-date on announcements on Ed Discussion and sent via email."
  },
  {
    "objectID": "slides/01-welcome-old.html#application-exercise",
    "href": "slides/01-welcome-old.html#application-exercise",
    "title": "Welcome to STA 210!",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 AE 01 - Movie Budgets and Revenues"
  },
  {
    "objectID": "slides/01-welcome-old.html#for-this-week",
    "href": "slides/01-welcome-old.html#for-this-week",
    "title": "Welcome to STA 210!",
    "section": "For this week…",
    "text": "For this week…\n\nRead the syllabus\nSee the course schedule for an overview of the semester\nLabs start this week!\n\nSection 001: Labs on Tuesday\nSection 002: Labs on Thursday\n\nThis week’s lab is focused on introductions and computing\nWednesday’s lecture: The Big Picture\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/02-big-picture-old.html#announcements",
    "href": "slides/02-big-picture-old.html#announcements",
    "title": "The big picture",
    "section": "Announcements",
    "text": "Announcements\n\nResources for extra R review\n\nLearn R: An interactive introduction to data analysis R (focus on Chapters 4 - 6)\nDuke Library Center for Data and Visualization Sciences workshops\n\nR for Lunch: data wrangling with dplyr (Fri, Sep 1, 12:30 - 1:30)\nR for Lunch: visualization with ggplot2 (Fri, Sep 8, 12:30 - 1:30)\nSee the CDVS website for more information and to register.\n\n\nLast day of in-person work for this class is Dec 7\nLecture recordings request policy\nReadings for next week will be posted later this week"
  },
  {
    "objectID": "slides/02-big-picture-old.html#topics",
    "href": "slides/02-big-picture-old.html#topics",
    "title": "The big picture",
    "section": "Topics",
    "text": "Topics\n\nData analysis life cycle\nReproducible data analysis\nAnalyzing multivariable relationships"
  },
  {
    "objectID": "slides/02-big-picture-old.html#reproducibility-checklist",
    "href": "slides/02-big-picture-old.html#reproducibility-checklist",
    "title": "The big picture",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n✔️ Can the tables and figures be exactly reproduced from the code and data?\n✔️ Does the code actually do what you think it does?\n✔️ In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n✔️ Can the code be used for other data?\n✔️ Can you extend the code to do other things?"
  },
  {
    "objectID": "slides/02-big-picture-old.html#why-is-reproducibility-important",
    "href": "slides/02-big-picture-old.html#why-is-reproducibility-important",
    "title": "The big picture",
    "section": "Why is reproducibility important?",
    "text": "Why is reproducibility important?\n\nResults produced are more reliable and trustworthy (Ostblom and Timbers 2022)\nFacilitates more effective collaboration (Ostblom and Timbers 2022)\nContributing to science, which builds and organizes knowledge in terms of testable hypotheses (Alexander 2023)\nPossible to identify and correct errors or biases in the analysis process (Alexander 2023)"
  },
  {
    "objectID": "slides/02-big-picture-old.html#when-things-go-wrong",
    "href": "slides/02-big-picture-old.html#when-things-go-wrong",
    "title": "The big picture",
    "section": "When things go wrong",
    "text": "When things go wrong\n\n\n\nReproducibility error\nConsequence\nSource(s)\n\n\n\n\nLimitations in Excel data formats\nLoss of 16,000 COVID case records in the UK\n(Kelion 2020)\n\n\nAutomatic formatting in Excel\nImportant genes disregarded in scientific studies\n(Ziemann, Eren, and El-Osta 2016)\n\n\nDeletion of a cell caused rows to shift\nMix-up of which patient group received the treatment\n(Wallensteen et al. 2018)\n\n\nUsing binary instead of explanatory labels\nMix-up of the intervention with the control group\n(Aboumatar and Wise 2019)\n\n\nUsing the same notation for missing data and zero values\nPaper retraction\n(Whitehouse et al. 2021)\n\n\nIncorrectly copying data in a spreadsheet\nDelay in the opening of a hospital\n(Picken 2020)\n\n\n\nSource: Ostblom and Timbers (2022)"
  },
  {
    "objectID": "slides/02-big-picture-old.html#toolkit",
    "href": "slides/02-big-picture-old.html#toolkit",
    "title": "The big picture",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub\n\n\n\n\n\n\n\nNote\n\n\nYou will start using these computing tools in Lab 01."
  },
  {
    "objectID": "slides/02-big-picture-old.html#r-and-rstudio",
    "href": "slides/02-big-picture-old.html#r-and-rstudio",
    "title": "The big picture",
    "section": "R and RStudio",
    "text": "R and RStudio\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\nSource: Statistical Inference via Data Science"
  },
  {
    "objectID": "slides/02-big-picture-old.html#rstudio-ide",
    "href": "slides/02-big-picture-old.html#rstudio-ide",
    "title": "The big picture",
    "section": "RStudio IDE",
    "text": "RStudio IDE"
  },
  {
    "objectID": "slides/02-big-picture-old.html#quarto",
    "href": "slides/02-big-picture-old.html#quarto",
    "title": "The big picture",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports – the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)"
  },
  {
    "objectID": "slides/02-big-picture-old.html#quarto-1",
    "href": "slides/02-big-picture-old.html#quarto-1",
    "title": "The big picture",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "slides/02-big-picture-old.html#how-will-we-use-quarto",
    "href": "slides/02-big-picture-old.html#how-will-we-use-quarto",
    "title": "The big picture",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou’ll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester"
  },
  {
    "objectID": "slides/02-big-picture-old.html#what-is-versioning",
    "href": "slides/02-big-picture-old.html#what-is-versioning",
    "title": "The big picture",
    "section": "What is versioning?",
    "text": "What is versioning?"
  },
  {
    "objectID": "slides/02-big-picture-old.html#what-is-versioning-1",
    "href": "slides/02-big-picture-old.html#what-is-versioning-1",
    "title": "The big picture",
    "section": "What is versioning?",
    "text": "What is versioning?\nwith human readable messages"
  },
  {
    "objectID": "slides/02-big-picture-old.html#why-do-we-need-version-control",
    "href": "slides/02-big-picture-old.html#why-do-we-need-version-control",
    "title": "The big picture",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\n\n\n\n\n\n\n\n\nProvides a clear record of how the analysis methods evolved. This makes analysis auditable and thus more trustworthy and reliable. (Ostblom and Timbers 2022)"
  },
  {
    "objectID": "slides/02-big-picture-old.html#git-and-github",
    "href": "slides/02-big-picture-old.html#git-and-github",
    "title": "The big picture",
    "section": "git and GitHub",
    "text": "git and GitHub\n\n\ngit is a version control system – like “Track Changes” features from Microsoft Word.\nGitHub is the home for your git-based projects on the internet (like DropBox but much better).\nThere are a lot of git commands and very few people know them all. 99% of the time you will use git to add, commit, push, and pull."
  },
  {
    "objectID": "slides/02-big-picture-old.html#carbohydrates-in-starbucks-food",
    "href": "slides/02-big-picture-old.html#carbohydrates-in-starbucks-food",
    "title": "The big picture",
    "section": "Carbohydrates in Starbucks food",
    "text": "Carbohydrates in Starbucks food\n\nStarbucks often displays the total calories in their food items but not the other nutritional information.\nCarbohydrates are a body’s main fuel source. The Dietary Guidelines for America recommend that carbohydrates make up 45% to 65% of total daily calories.1\nOur goal is to understand the relationship between the amount of carbohydrates and calories in Starbucks food items. We’d also like to assess if the relationship differs based on the type of food item (bakery, salad, sandwich, etc.)\n\nSource: Mayo Clinic"
  },
  {
    "objectID": "slides/02-big-picture-old.html#starbucks-data",
    "href": "slides/02-big-picture-old.html#starbucks-data",
    "title": "The big picture",
    "section": "Starbucks data",
    "text": "Starbucks data\n\n\nObservations: 77 Starbucks food items\nVariables:\n\ncarb: Total carbohydrates (in grams)\ncalories: Total calories\nbakery: 1: bakery food item, 0: other food type"
  },
  {
    "objectID": "slides/02-big-picture-old.html#terminology",
    "href": "slides/02-big-picture-old.html#terminology",
    "title": "The big picture",
    "section": "Terminology",
    "text": "Terminology\n\ncarb is the response variable\n\nvariable whose variation we want to understand / variable we wish to predict\nalso known as outcome or dependent variable\n\n\n\n\ncalories, bakery are the predictor variables\n\nvariables used to account for variation in the response\nalso known as explanatory, independent, or input variables"
  },
  {
    "objectID": "slides/02-big-picture-old.html#univariate-exploratory-data-analysis",
    "href": "slides/02-big-picture-old.html#univariate-exploratory-data-analysis",
    "title": "The big picture",
    "section": "Univariate exploratory data analysis",
    "text": "Univariate exploratory data analysis\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`."
  },
  {
    "objectID": "slides/02-big-picture-old.html#bivariate-exploratory-data-analysis",
    "href": "slides/02-big-picture-old.html#bivariate-exploratory-data-analysis",
    "title": "The big picture",
    "section": "Bivariate exploratory data analysis",
    "text": "Bivariate exploratory data analysis\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/02-big-picture-old.html#function-between-response-and-predictors",
    "href": "slides/02-big-picture-old.html#function-between-response-and-predictors",
    "title": "The big picture",
    "section": "Function between response and predictors",
    "text": "Function between response and predictors\n\n\\[\\text{carb} = f(\\text{calories}, \\text{bakery}) + \\epsilon\\]\n\n\nGoal: Determine \\(f\\)\nHow do we determine \\(f\\)?\n\nMake an assumption about the functional form \\(f\\) (parametric model)\nUse the data to fit a model based on that form"
  },
  {
    "objectID": "slides/02-big-picture-old.html#determine-f",
    "href": "slides/02-big-picture-old.html#determine-f",
    "title": "The big picture",
    "section": "Determine \\(f\\)",
    "text": "Determine \\(f\\)\n\nChoose the functional form of \\(f\\), i.e., choose the appropriate model given the response variable\n\n\nSuppose \\(f\\) takes the form of a linear model\n\\[y = f(\\mathbf{X}) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p + \\epsilon\\]\n\n\n\nUse the data to fit (or train) the model, i.e, estimate the model parameters, \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)"
  },
  {
    "objectID": "slides/02-big-picture-old.html#carb-vs.-calories",
    "href": "slides/02-big-picture-old.html#carb-vs.-calories",
    "title": "The big picture",
    "section": "Carb vs. Calories",
    "text": "Carb vs. Calories\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\\[\\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\epsilon\\]"
  },
  {
    "objectID": "slides/02-big-picture-old.html#carb-vs.-calories-bakery",
    "href": "slides/02-big-picture-old.html#carb-vs.-calories-bakery",
    "title": "The big picture",
    "section": "Carb vs. Calories + Bakery",
    "text": "Carb vs. Calories + Bakery\n\n\\[\\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\epsilon\\]"
  },
  {
    "objectID": "slides/02-big-picture-old.html#carb-vs.-calories-bakery-with-interaction",
    "href": "slides/02-big-picture-old.html#carb-vs.-calories-bakery-with-interaction",
    "title": "The big picture",
    "section": "Carb vs. Calories + Bakery (with interaction)",
    "text": "Carb vs. Calories + Bakery (with interaction)\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\\[{\\small \\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\beta_3 ~ \\text{calories} \\times \\text{bakery}  + \\epsilon}\\]"
  },
  {
    "objectID": "slides/02-big-picture-old.html#statistical-model-vs.-regression-equation",
    "href": "slides/02-big-picture-old.html#statistical-model-vs.-regression-equation",
    "title": "The big picture",
    "section": "Statistical model vs. regression equation",
    "text": "Statistical model vs. regression equation\nStatistical model (also known as data-generating model)\n\\[{\\small \\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\beta_3 ~ \\text{calories} \\times \\text{bakery}  + \\epsilon}\\]\nModels the process for generating values of the response in the population (function + error)\n\n\nRegression equation\nEstimate of the function using the sample data\n\\[{\\small \\hat{\\text{carb}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 ~\\text{calories} + \\hat{\\beta}_2 ~\\text{bakery} + \\hat{\\beta}_3 ~ \\text{calories} \\times \\text{bakery}}\\]"
  },
  {
    "objectID": "slides/02-big-picture-old.html#why-fit-a-model",
    "href": "slides/02-big-picture-old.html#why-fit-a-model",
    "title": "The big picture",
    "section": "Why fit a model?",
    "text": "Why fit a model?\n\nPrediction: Expected value of the response variable for given values of the predictor variables\nInference: Conclusion about the relationship between the response and predictor variables\n\n\n\nWhat is an example of a prediction question that can be answered using the model of carb vs. calories and bakery?\nWhat is an example of an inference question that can be answered using the model of carb vs. calories and bakery?"
  },
  {
    "objectID": "slides/02-big-picture-old.html#recap",
    "href": "slides/02-big-picture-old.html#recap",
    "title": "The big picture",
    "section": "Recap",
    "text": "Recap\n\nReproducibility\n\nIt is best practice conduct all data analysis in a reproducible way\nWe will implement a reproducible workflow using R, Quarto, and git/GitHub\n\n\n\n\nMultivariable relationships\n\nWe can use exploratory data analysis to describe the relationship between two variables\nWe make an assumption about the relationship between variables when doing linear regression\nThe two main objectives for fitting a linear regression model are (1) prediction and (2) inference"
  },
  {
    "objectID": "slides/02-big-picture-old.html#references",
    "href": "slides/02-big-picture-old.html#references",
    "title": "The big picture",
    "section": "References",
    "text": "References\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule\n\n\n\n\nAlexander, Rohan. 2023. “Telling Stories with Data,” June. https://doi.org/10.1201/9781003229407.\n\n\nOstblom, Joel, and Tiffany Timbers. 2022. “Opinionated Practices for Teaching Reproducibility: Motivation, Guided Instruction and Practice.” Journal of Statistics and Data Science Education 30 (3): 241–50. https://doi.org/10.1080/26939169.2022.2074922."
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#announcements",
    "href": "slides/03-multivariate-relationships.html#announcements",
    "title": "Analyzing Multivariable Relationships",
    "section": "Announcements",
    "text": "Announcements\n\nDue Wednesday:\n\nHW 0 (3 .qmd, 3 .html, 6 files total uploaded to Canvas)\nRead Chapter 1.1 of text (see webstie for optional additional reading)\n\nOther Stuff:\n\nTuesday (tomorrow) office hours rescheduled to 12:30-1:30pm"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#carbohydrates-in-starbucks-food",
    "href": "slides/03-multivariate-relationships.html#carbohydrates-in-starbucks-food",
    "title": "Analyzing Multivariable Relationships",
    "section": "Carbohydrates in Starbucks food",
    "text": "Carbohydrates in Starbucks food\n\nStarbucks often displays the total calories in their food items but not the other nutritional information.\nCarbohydrates are a body’s main fuel source. The Dietary Guidelines for America recommend that carbohydrates make up 45% to 65% of total daily calories.1\nOur goal is to understand the relationship between the amount of carbohydrates and calories in Starbucks food items. We’d also like to assess if the relationship differs based on the type of food item (bakery, salad, sandwich, etc.)\n\nSource: Mayo Clinic"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#starbucks-data",
    "href": "slides/03-multivariate-relationships.html#starbucks-data",
    "title": "Analyzing Multivariable Relationships",
    "section": "Starbucks data",
    "text": "Starbucks data\n\n\nObservations: 77 Starbucks food items\nVariables:\n\ncarb: Total carbohydrates (in grams)\ncalories: Total calories\nbakery: 1: bakery food item, 0: other food type"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#terminology",
    "href": "slides/03-multivariate-relationships.html#terminology",
    "title": "Analyzing Multivariable Relationships",
    "section": "Terminology",
    "text": "Terminology\n\ncarb is the response variable\n\nvariable whose variation we want to understand / variable we wish to predict\nalso known as outcome or target variable\n\n\n\n\ncalories, bakery are the explanatory variables\n\nvariables used to account for variation in the response\nalso known as predictor, or input variables"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#univariate-exploratory-data-analysis",
    "href": "slides/03-multivariate-relationships.html#univariate-exploratory-data-analysis",
    "title": "Analyzing Multivariable Relationships",
    "section": "Univariate exploratory data analysis",
    "text": "Univariate exploratory data analysis"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#bivariate-exploratory-data-analysis",
    "href": "slides/03-multivariate-relationships.html#bivariate-exploratory-data-analysis",
    "title": "Analyzing Multivariable Relationships",
    "section": "Bivariate exploratory data analysis",
    "text": "Bivariate exploratory data analysis"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#function-between-response-and-predictors",
    "href": "slides/03-multivariate-relationships.html#function-between-response-and-predictors",
    "title": "Analyzing Multivariable Relationships",
    "section": "Function between response and predictors",
    "text": "Function between response and predictors\n\n\\[\\text{carb} = f(\\text{calories}, \\text{bakery}) + \\epsilon\\]\n\n\nGoal: Determine \\(f\\)\nHow do we determine \\(f\\)?\n\nMake an assumption about the functional form \\(f\\) (parametric model)\nUse the data to fit a model based on that form"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#determine-f",
    "href": "slides/03-multivariate-relationships.html#determine-f",
    "title": "Analyzing Multivariable Relationships",
    "section": "Determine \\(f\\)",
    "text": "Determine \\(f\\)\n\nChoose the functional form of \\(f\\), i.e., choose the appropriate model given the response variable\n\n\nSuppose \\(f\\) takes the form of a linear model\n\\[y = f(\\mathbf{X}) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_p x_p + \\epsilon\\]\n\n\n\nUse the data to fit (or train) the model, i.e, estimate the model parameters, \\(\\beta_0, \\beta_1, \\ldots, \\beta_p\\)"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#carb-vs.-calories",
    "href": "slides/03-multivariate-relationships.html#carb-vs.-calories",
    "title": "Analyzing Multivariable Relationships",
    "section": "Carb vs. Calories",
    "text": "Carb vs. Calories\n\n\\[\\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\epsilon\\]"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#carb-vs.-calories-bakery",
    "href": "slides/03-multivariate-relationships.html#carb-vs.-calories-bakery",
    "title": "Analyzing Multivariable Relationships",
    "section": "Carb vs. Calories + Bakery",
    "text": "Carb vs. Calories + Bakery\n\n\\[\\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\epsilon\\]"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#carb-vs.-calories-bakery-with-interaction",
    "href": "slides/03-multivariate-relationships.html#carb-vs.-calories-bakery-with-interaction",
    "title": "Analyzing Multivariable Relationships",
    "section": "Carb vs. Calories + Bakery (with interaction)",
    "text": "Carb vs. Calories + Bakery (with interaction)\n\n\\[{\\small \\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\beta_3 ~ \\text{calories} \\times \\text{bakery}  + \\epsilon}\\]"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#example-palmers-penguins",
    "href": "slides/03-multivariate-relationships.html#example-palmers-penguins",
    "title": "Analyzing Multivariable Relationships",
    "section": "Example: Palmer’s Penguins",
    "text": "Example: Palmer’s Penguins\nSize measurements, clutch observations, and blood isotope ratios for adult foraging Adélie, Chinstrap, and Gentoo penguins observed on islands in the Palmer Archipelago near Palmer Station, Antarctica."
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#example-palmers-penguins-1",
    "href": "slides/03-multivariate-relationships.html#example-palmers-penguins-1",
    "title": "Analyzing Multivariable Relationships",
    "section": "Example: Palmer’s Penguins",
    "text": "Example: Palmer’s Penguins\n\n\nWhat does the relationship between Length and Depth look like?"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#example-palmers-penguins-2",
    "href": "slides/03-multivariate-relationships.html#example-palmers-penguins-2",
    "title": "Analyzing Multivariable Relationships",
    "section": "Example: Palmer’s Penguins",
    "text": "Example: Palmer’s Penguins\n\n\nWhat does the relationship between Length and Depth look like now?"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#simpsons-paradox-1",
    "href": "slides/03-multivariate-relationships.html#simpsons-paradox-1",
    "title": "Analyzing Multivariable Relationships",
    "section": "Simpson’s Paradox",
    "text": "Simpson’s Paradox\nSimpson’s Paradox is when there is a clear relationship between two varibles but when you introduce a third variable that relationship disappears of reverses."
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#statistical-model-vs.-regression-equation",
    "href": "slides/03-multivariate-relationships.html#statistical-model-vs.-regression-equation",
    "title": "Analyzing Multivariable Relationships",
    "section": "Statistical model vs. regression equation",
    "text": "Statistical model vs. regression equation\nStatistical model (also known as data-generating model)\n\\[{\\small \\text{carb} = \\beta_0 + \\beta_1 ~\\text{calories} + \\beta_2 ~\\text{bakery} + \\beta_3 ~ \\text{calories} \\times \\text{bakery}  + \\epsilon}\\]\nModels the process for generating values of the response in the population (function + error)\n\n\nRegression equation\nEstimate of the function using the sample data\n\\[{\\small \\hat{\\text{carb}} = \\hat{\\beta}_0 + \\hat{\\beta}_1 ~\\text{calories} + \\hat{\\beta}_2 ~\\text{bakery} + \\hat{\\beta}_3 ~ \\text{calories} \\times \\text{bakery}}\\]"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#why-fit-a-model",
    "href": "slides/03-multivariate-relationships.html#why-fit-a-model",
    "title": "Analyzing Multivariable Relationships",
    "section": "Why fit a model?",
    "text": "Why fit a model?\n\nPrediction: Expected value of the response variable for given values of the predictor variables\nInference: Conclusion about the relationship between the response and predictor variables\n\n\n\nWhat is an example of a prediction question that can be answered using the model of carb vs. calories and bakery?\nWhat is an example of an inference question that can be answered using the model of carb vs. calories and bakery?"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#recap",
    "href": "slides/03-multivariate-relationships.html#recap",
    "title": "Analyzing Multivariable Relationships",
    "section": "Recap",
    "text": "Recap\n\nWe can use exploratory data analysis to describe the relationship between two variables\nWe make an assumption about the relationship between variables when doing linear regression\nThe two main objectives for fitting a linear regression model are (1) prediction and (2) inference"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#rest-of-class",
    "href": "slides/03-multivariate-relationships.html#rest-of-class",
    "title": "Analyzing Multivariable Relationships",
    "section": "Rest of class",
    "text": "Rest of class\n\nWork on HW 0!"
  },
  {
    "objectID": "slides/03-multivariate-relationships.html#references",
    "href": "slides/03-multivariate-relationships.html#references",
    "title": "Analyzing Multivariable Relationships",
    "section": "References",
    "text": "References\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#announcements",
    "href": "slides/03-slr-tidymodels.html#announcements",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Announcements",
    "text": "Announcements\n\nNo office hours today. Office hours start Tuesday, September 6. Click here for full schedule\nCheck your email for an email to join the course GitHub organization. You will receive one by Tuesday, September 6.\nSee Week 02 for this week’s activities."
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#topics",
    "href": "slides/03-slr-tidymodels.html#topics",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Topics",
    "text": "Topics\n\nIntroduce the computing toolkit - RStudio and GitHub\nUse tidymodels to fit and summarize regression models in R\nComplete an application exercise on exploratory data analysis and modeling"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#computational-setup",
    "href": "slides/03-slr-tidymodels.html#computational-setup",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(tidymodels)      # for modeling\nlibrary(fivethirtyeight) # for the fandango dataset\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#reproducibility-checklist",
    "href": "slides/03-slr-tidymodels.html#reproducibility-checklist",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Reproducibility checklist",
    "text": "Reproducibility checklist\n\nWhat does it mean for an analysis to be reproducible?\n\n\nNear term goals:\n✔️ Are the tables and figures reproducible from the code and data?\n✔️ Does the code actually do what you think it does?\n✔️ In addition to what was done, is it clear why it was done?\n\n\nLong term goals:\n✔️ Can the code be used for other data?\n✔️ Can you extend the code to do other things?"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#toolkit",
    "href": "slides/03-slr-tidymodels.html#toolkit",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Toolkit",
    "text": "Toolkit\n\nScriptability \\(\\rightarrow\\) R\nLiterate programming (code, narrative, output in one place) \\(\\rightarrow\\) Quarto\nVersion control \\(\\rightarrow\\) Git / GitHub\n\nMore on this in this week’s lab"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#what-are-r-and-rstudio",
    "href": "slides/03-slr-tidymodels.html#what-are-r-and-rstudio",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "What are R and RStudio?",
    "text": "What are R and RStudio?\n\nR is a statistical programming language\nRStudio is a convenient interface for R (an integrated development environment, IDE)\n\n\n\nSource: Modern Dive"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#rstudio-ide",
    "href": "slides/03-slr-tidymodels.html#rstudio-ide",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "RStudio IDE",
    "text": "RStudio IDE"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#quarto",
    "href": "slides/03-slr-tidymodels.html#quarto",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Quarto",
    "text": "Quarto\n\nFully reproducible reports – the analysis is run from the beginning each time you render\nCode goes in chunks and narrative goes outside of chunks\nVisual editor to make document editing experience similar to a word processor (Google docs, Word, Pages, etc.)"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#quarto-1",
    "href": "slides/03-slr-tidymodels.html#quarto-1",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Quarto",
    "text": "Quarto"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#how-will-we-use-quarto",
    "href": "slides/03-slr-tidymodels.html#how-will-we-use-quarto",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "How will we use Quarto?",
    "text": "How will we use Quarto?\n\nEvery application exercise and assignment is written in a Quarto document\nYou’ll have a template Quarto document to start with\nThe amount of scaffolding in the template will decrease over the semester"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#application-exercise",
    "href": "slides/03-slr-tidymodels.html#application-exercise",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-fa22/ae-02-bikeshare"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#recap-of-last-lecture",
    "href": "slides/03-slr-tidymodels.html#recap-of-last-lecture",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Recap of last lecture",
    "text": "Recap of last lecture\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative outcome variable.\nUsed the least squares method to estimate the slope and intercept.\nWe interpreted the slope and intercept.\n\n\nSlope: For every one unit increase in \\(x\\), we expect y to be higher/lower by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units.\n\n\nPredicted the response given a value of the predictor variable.\nDefined extrapolation and why we should avoid it."
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#movie-ratings",
    "href": "slides/03-slr-tidymodels.html#movie-ratings",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Movie ratings",
    "text": "Movie ratings\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#data-prep",
    "href": "slides/03-slr-tidymodels.html#data-prep",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(\n    critics = rottentomatoes, \n    audience = rottentomatoes_user\n  )"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#tidymodels",
    "href": "slides/03-slr-tidymodels.html#tidymodels",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "tidymodels",
    "text": "tidymodels\n\nThe tidymodels framework is a collection of packages for modeling and machine learning using tidyverse principles.\n\n\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.1.0 ──\n\n\n✔ broom        1.0.5     ✔ rsample      1.1.1\n✔ dials        1.2.0     ✔ tune         1.1.1\n✔ infer        1.0.4     ✔ workflows    1.1.3\n✔ modeldata    1.1.0     ✔ workflowsets 1.0.1\n✔ parsnip      1.1.0     ✔ yardstick    1.2.0\n✔ recipes      1.0.6     \n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use suppressPackageStartupMessages() to eliminate package startup messages"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#why-tidymodels",
    "href": "slides/03-slr-tidymodels.html#why-tidymodels",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Why tidymodels?",
    "text": "Why tidymodels?\n\nConsistent syntax for different model types (linear, logistic, random forest, Bayesian, etc.)\nStreamline modeling workflow\n\nSplit data into train and test sets\nTransform and create new variables\nAssess model performance\nUse model for prediction and inference"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#step-1-specify-model",
    "href": "slides/03-slr-tidymodels.html#step-1-specify-model",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Step 1: Specify model",
    "text": "Step 1: Specify model\n\nlinear_reg()\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#step-2-set-model-fitting-engine",
    "href": "slides/03-slr-tidymodels.html#step-2-set-model-fitting-engine",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Step 2: Set model fitting engine",
    "text": "Step 2: Set model fitting engine\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") # lm: linear model\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#a-closer-look-at-model-output",
    "href": "slides/03-slr-tidymodels.html#a-closer-look-at-model-output",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "A closer look at model output",
    "text": "A closer look at model output\n\nmovie_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores)\n\nmovie_fit\n\nparsnip model object\n\n\nCall:\nstats::lm(formula = audience ~ critics, data = data)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187  \n\n\n\\[\\widehat{\\text{audience}} = 32.3155 + 0.5187 \\times \\text{critics}\\]\n\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is likely just due to rounding in the hand calculation."
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#the-regression-output",
    "href": "slides/03-slr-tidymodels.html#the-regression-output",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "The regression output",
    "text": "The regression output\nWe’ll focus on the first column for now…\n\nlinear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(audience ~ critics, data = movie_scores) |&gt;\n  tidy() \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#prediction",
    "href": "slides/03-slr-tidymodels.html#prediction",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Prediction",
    "text": "Prediction\n\n# create a data frame for a new movie\nnew_movie &lt;- tibble(critics = 70)\n\n# predict the outcome for a new movie\npredict(movie_fit, new_movie)\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1  68.6"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#application-exercise-1",
    "href": "slides/03-slr-tidymodels.html#application-exercise-1",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-fa22/ae-02-bikeshare\n\n\n\nClose document tab"
  },
  {
    "objectID": "slides/03-slr-tidymodels.html#recap",
    "href": "slides/03-slr-tidymodels.html#recap",
    "title": "SLR: Fitting models in R with tidymodels",
    "section": "Recap",
    "text": "Recap\n\nIntroduced the computing toolkit - RStudio and GitHub\nUsed tidymodels to fit and summarize regression models in R\nCompleted an application exercise on exploratory data analysis and modeling\n\n\n\n\n\n🔗 Week 02"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#announcements",
    "href": "slides/04-slr-evaluation.html#announcements",
    "title": "SLR: Prediction + model evaluation",
    "section": "Announcements",
    "text": "Announcements\n\nOffice hours have started. Click here for full schedule.\nAccept the email invitation to join the sta210-fa22 GitHub organization by today at 11:59pm.\n\nYou may also go to the course organization and click to accept on the banner at the top of the page.\nIf you don’t see the email or banner invitation, please email Prof. Tackett (maria.tackett@duke.edu).\n\nLab 01 this week - will need access to RStudio and to be a member of the course GitHub organization.\nSee Week 02 for this week’s activities."
  },
  {
    "objectID": "slides/04-slr-evaluation.html#topics",
    "href": "slides/04-slr-evaluation.html#topics",
    "title": "SLR: Prediction + model evaluation",
    "section": "Topics",
    "text": "Topics\n\nMotivate the importance of model evaluation\nDescribe how \\(R^2\\) and RMSE are used to evaluate models\nAssess model’s predictive importance using data splitting and bootstrapping"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#computational-setup",
    "href": "slides/04-slr-evaluation.html#computational-setup",
    "title": "SLR: Prediction + model evaluation",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(usdata)      # for the county_2019 dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(glue)        # for constructing character strings\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#application-exercise",
    "href": "slides/04-slr-evaluation.html#application-exercise",
    "title": "SLR: Prediction + model evaluation",
    "section": "Application exercise",
    "text": "Application exercise\n\n📋 github.com/sta210-fa22/ae-02-bikeshare"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#data-source",
    "href": "slides/04-slr-evaluation.html#data-source",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data source",
    "text": "Data source\n\nThe data come from usdata::county_2019\nThese data have been compiled from the 2019 American Community Survey\n\n\n\nJoining with `by = join_by(region, subregion)`"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#uninsurance-rate",
    "href": "slides/04-slr-evaluation.html#uninsurance-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance rate",
    "text": "Uninsurance rate"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#high-school-graduation-rate",
    "href": "slides/04-slr-evaluation.html#high-school-graduation-rate",
    "title": "SLR: Prediction + model evaluation",
    "section": "High school graduation rate",
    "text": "High school graduation rate"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#examining-the-relationship",
    "href": "slides/04-slr-evaluation.html#examining-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Examining the relationship",
    "text": "Examining the relationship\n\n\nThe NC Labor and Economic Analysis Division (LEAD) “collects data, conducts research and analysis and publishes reports about the state’s economy and labor market. Information and data produced by LEAD help stakeholders make more informed decisions on business recruitment, education and workforce policies and career development, as well as gain a more extensive view of North Carolina’s economy.”\nSuppose that an analyst working for LEAD is interested in the relationship between uninsurance and high school graduation rates in NC counties.\n\n\n\n\nWhat type of visualization should the analyst make to examine the relationship between these two variables?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#data-prep",
    "href": "slides/04-slr-evaluation.html#data-prep",
    "title": "SLR: Prediction + model evaluation",
    "section": "Data prep",
    "text": "Data prep\n\ncounty_2019_nc &lt;- county_2019 |&gt;\n  as_tibble() |&gt;\n  filter(state == \"North Carolina\") |&gt;\n  select(name, hs_grad, uninsured)\n\ncounty_2019_nc\n\n# A tibble: 100 × 3\n   name             hs_grad uninsured\n   &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;\n 1 Alamance County     86.3      11.2\n 2 Alexander County    82.4       8.9\n 3 Alleghany County    77.5      11.3\n 4 Anson County        80.7      11.1\n 5 Ashe County         85.1      12.6\n 6 Avery County        83.6      15.9\n 7 Beaufort County     87.7      12  \n 8 Bertie County       78.4      11.9\n 9 Bladen County       81.3      12.9\n10 Brunswick County    91.3       9.8\n# ℹ 90 more rows"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#uninsurance-vs.-hs-graduation-rates",
    "href": "slides/04-slr-evaluation.html#uninsurance-vs.-hs-graduation-rates",
    "title": "SLR: Prediction + model evaluation",
    "section": "Uninsurance vs. HS graduation rates",
    "text": "Uninsurance vs. HS graduation rates\n\n\nCode\nggplot(county_2019_nc,\n       aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  ) +\n  geom_point(data = county_2019_nc |&gt; filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured), shape = \"circle open\", color = \"#8F2D56\", size = 4, stroke = 2) +\n  geom_text(data = county_2019_nc |&gt; filter(name == \"Durham County\"), aes(x = hs_grad, y = uninsured, label = name), color = \"#8F2D56\", fontface = \"bold\", nudge_y = 3, nudge_x = 2)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#modeling-the-relationship",
    "href": "slides/04-slr-evaluation.html#modeling-the-relationship",
    "title": "SLR: Prediction + model evaluation",
    "section": "Modeling the relationship",
    "text": "Modeling the relationship\n\n\nCode\nggplot(county_2019_nc, aes(x = hs_grad, y = uninsured)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#8F2D56\") +\n  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +\n  labs(\n    x = \"High school graduate\", y = \"Uninsured\",\n    title = \"Uninsurance vs. HS graduation rates\",\n    subtitle = \"North Carolina counties, 2015 - 2019\"\n  )\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#fitting-the-model",
    "href": "slides/04-slr-evaluation.html#fitting-the-model",
    "title": "SLR: Prediction + model evaluation",
    "section": "Fitting the model",
    "text": "Fitting the model\nWith fit():\n\nnc_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(uninsured ~ hs_grad, data = county_2019_nc)\n\ntidy(nc_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   33.9      3.99        8.50 2.12e-13\n2 hs_grad       -0.262    0.0468     -5.61 1.88e- 7"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#augmenting-the-data",
    "href": "slides/04-slr-evaluation.html#augmenting-the-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Augmenting the data",
    "text": "Augmenting the data\nWith augment() to add columns for predicted values (.fitted), residuals (.resid), etc.:\n\nnc_aug &lt;- augment(nc_fit$fit)\nnc_aug\n\n# A tibble: 100 × 8\n   uninsured hs_grad .fitted  .resid   .hat .sigma    .cooksd .std.resid\n       &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1      11.2    86.3   11.3  -0.0633 0.0107   2.10 0.00000501    -0.0305\n 2       8.9    82.4   12.3  -3.39   0.0138   2.07 0.0186        -1.63  \n 3      11.3    77.5   13.6  -2.27   0.0393   2.09 0.0252        -1.11  \n 4      11.1    80.7   12.7  -1.63   0.0199   2.09 0.00633       -0.790 \n 5      12.6    85.1   11.6   1.02   0.0100   2.10 0.00122        0.492 \n 6      15.9    83.6   12.0   3.93   0.0112   2.06 0.0203         1.89  \n 7      12      87.7   10.9   1.10   0.0133   2.10 0.00191        0.532 \n 8      11.9    78.4   13.3  -1.44   0.0328   2.09 0.00830       -0.700 \n 9      12.9    81.3   12.6   0.324  0.0174   2.10 0.000218       0.157 \n10       9.8    91.3    9.95 -0.151  0.0291   2.10 0.0000806     -0.0734\n# ℹ 90 more rows"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-i",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-i",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model I",
    "text": "Visualizing the model I\n\n\n\n\nBlack circles: Observed values (y = uninsured)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-ii",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-ii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model II",
    "text": "Visualizing the model II\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-iii",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-iii",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model III",
    "text": "Visualizing the model III\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#visualizing-the-model-iv",
    "href": "slides/04-slr-evaluation.html#visualizing-the-model-iv",
    "title": "SLR: Prediction + model evaluation",
    "section": "Visualizing the model IV",
    "text": "Visualizing the model IV\n\n\n\n\nBlack circles: Observed values (y = uninsured)\nPink solid line: Least squares regression line\nMaroon triangles: Predicted values (y = .fitted)\nGray dashed lines: Residuals\n\n\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#evaluating-the-model-fit",
    "href": "slides/04-slr-evaluation.html#evaluating-the-model-fit",
    "title": "SLR: Prediction + model evaluation",
    "section": "Evaluating the model fit",
    "text": "Evaluating the model fit\n\nHow can we evaluate whether the model for predicting uninsurance rate from high school graduation rate for NC counties is a good fit?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#two-statistics",
    "href": "slides/04-slr-evaluation.html#two-statistics",
    "title": "SLR: Prediction + model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#r2",
    "href": "slides/04-slr-evaluation.html#r2",
    "title": "SLR: Prediction + model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() using the augmented data:\n\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.243"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#interpreting-r2",
    "href": "slides/04-slr-evaluation.html#interpreting-r2",
    "title": "SLR: Prediction + model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\n🗳️ Vote on Ed Discussion\n\n\nThe \\(R^2\\) of the model for predicting uninsurance rate from high school graduation rate for NC counties is 24.3%. Which of the following is the correct interpretation of this value?\n\n\nHigh school graduation rates correctly predict 24.3% of uninsurance rates in NC counties.\n24.3% of the variability in uninsurance rates in NC counties can be explained by high school graduation rates.\n24.3% of the variability in high school graduation rates in NC counties can be explained by uninsurance rates.\n24.3% of the time uninsurance rates in NC counties can be predicted by high school graduation rates.\n\n\n\nVote - Section 001\nVote - Section 002"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#alternative-approach-for-r2",
    "href": "slides/04-slr-evaluation.html#alternative-approach-for-r2",
    "title": "SLR: Prediction + model evaluation",
    "section": "Alternative approach for \\(R^2\\)",
    "text": "Alternative approach for \\(R^2\\)\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(nc_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.243         0.235  2.09      31.5 0.000000188     1  -214.  435.  443.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\nglance(nc_fit)$r.squared\n\n[1] 0.2430694"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#rmse",
    "href": "slides/04-slr-evaluation.html#rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rmse() using the augmented data:\n\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        2.07\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#obtaining-r2-and-rmse",
    "href": "slides/04-slr-evaluation.html#obtaining-r2-and-rmse",
    "title": "SLR: Prediction + model evaluation",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\n\nUse rsq() and rmse(), respectively\n\nrsq(nc_aug, truth = uninsured, estimate = .fitted)\nrmse(nc_aug, truth = uninsured, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#purpose-of-model-evaluation",
    "href": "slides/04-slr-evaluation.html#purpose-of-model-evaluation",
    "title": "SLR: Prediction + model evaluation",
    "section": "Purpose of model evaluation",
    "text": "Purpose of model evaluation\n\n\\(R^2\\) tells us how our model is doing to predict the data we already have\nBut generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e. out-of-sample prediction\nWe have a couple ways of simulating out-of-sample prediction before actually getting new data to evaluate the performance of our models"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#spending-our-data",
    "href": "slides/04-slr-evaluation.html#spending-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Spending our data",
    "text": "Spending our data\n\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we’ve done so far)"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#simulation-data-splitting",
    "href": "slides/04-slr-evaluation.html#simulation-data-splitting",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: data splitting",
    "text": "Simulation: data splitting\n\n\n\n\nTake a random sample of 10% of the data and set aside (testing data)\nFit a model on the remaining 90% of the data (training data)\nUse the coefficients from this model to make predictions for the testing data\nRepeat 10 times\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#predictive-performance",
    "href": "slides/04-slr-evaluation.html#predictive-performance",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different testing datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#bootstrapping-our-data",
    "href": "slides/04-slr-evaluation.html#bootstrapping-our-data",
    "title": "SLR: Prediction + model evaluation",
    "section": "Bootstrapping our data",
    "text": "Bootstrapping our data\n\n\nThe idea behind bootstrapping is that if a given observation exists in a sample, there may be more like it in the population\nWith bootstrapping, we simulate resampling from the population by resampling from the sample we observed\nBootstrap samples are the sampled with replacement from the original sample and same size as the original sample\n\nFor example, if our sample consists of the observations {A, B, C}, bootstrap samples could be {A, A, B}, {A, C, A}, {B, C, C}, {A, B, C}, etc."
  },
  {
    "objectID": "slides/04-slr-evaluation.html#simulation-bootstrapping",
    "href": "slides/04-slr-evaluation.html#simulation-bootstrapping",
    "title": "SLR: Prediction + model evaluation",
    "section": "Simulation: bootstrapping",
    "text": "Simulation: bootstrapping\n\n\n\n\nTake a bootstrap sample – sample with replacement from the original data, same size as the original data\nFit model to the sample and make predictions for that sample\nRepeat many times\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#predictive-performance-1",
    "href": "slides/04-slr-evaluation.html#predictive-performance-1",
    "title": "SLR: Prediction + model evaluation",
    "section": "Predictive performance",
    "text": "Predictive performance\n\n\n\n\n\nHow consistent are the predictions for different bootstrap datasets?\nHow consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?\n\n\n\n\n\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#recap",
    "href": "slides/04-slr-evaluation.html#recap",
    "title": "SLR: Prediction + model evaluation",
    "section": "Recap",
    "text": "Recap\n\nMotivated the importance of model evaluation\nDescribed how \\(R^2\\) and RMSE are used to evaluate models\nAssessed model’s predictive importance using data splitting and bootstrapping"
  },
  {
    "objectID": "slides/04-slr-evaluation.html#next-week",
    "href": "slides/04-slr-evaluation.html#next-week",
    "title": "SLR: Prediction + model evaluation",
    "section": "Next week",
    "text": "Next week\nInference on the slope using\n\nSimulation-based methods\nMathematical models\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/05-slr-prediction.html#finish-wednedays-ae",
    "href": "slides/05-slr-prediction.html#finish-wednedays-ae",
    "title": "Simple Linear Regression",
    "section": "Finish Wedneday’s AE",
    "text": "Finish Wedneday’s AE\n\n📋 AE 01 - Movie Budgets and Revenues"
  },
  {
    "objectID": "slides/05-slr-prediction.html#last-time",
    "href": "slides/05-slr-prediction.html#last-time",
    "title": "Simple Linear Regression",
    "section": "Last Time",
    "text": "Last Time\n\n\nUsed simple linear regression to describe the relationship between a quantitative predictor and quantitative response variable.\nUsed the least squares method to estimate the slope and intercept.\nInterpreted the slope and intercept.\n\nSlope: For every one unit increase in \\(x\\), we expect y to change by \\(\\hat{\\beta}_1\\) units, on average.\nIntercept: If \\(x\\) is 0, then we expect \\(y\\) to be \\(\\hat{\\beta}_0\\) units"
  },
  {
    "objectID": "slides/05-slr-prediction.html#topics",
    "href": "slides/05-slr-prediction.html#topics",
    "title": "Simple Linear Regression",
    "section": "Topics",
    "text": "Topics\n\nPredict the response given a value of the predictor variable.\nUse R to fit and summarize regression models."
  },
  {
    "objectID": "slides/05-slr-prediction.html#computation-set-up",
    "href": "slides/05-slr-prediction.html#computation-set-up",
    "title": "Simple Linear Regression",
    "section": "Computation set up",
    "text": "Computation set up\n\n# load packages\nlibrary(tidyverse)       # for data wrangling\nlibrary(ggformula)       # for plotting\nlibrary(fivethirtyeight) # for the fandango dataset\nlibrary(broom)           # for formatting model output\nlibrary(knitr)           # for formatting tables\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))\n\n# set default figure parameters for knitr\nknitr::opts_chunk$set(\n  fig.width = 8,\n  fig.asp = 0.618,\n  fig.retina = 3,\n  dpi = 300,\n  out.width = \"80%\"\n)"
  },
  {
    "objectID": "slides/05-slr-prediction.html#movie-scores",
    "href": "slides/05-slr-prediction.html#movie-scores",
    "title": "Simple Linear Regression",
    "section": "Movie scores",
    "text": "Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/05-slr-prediction.html#data-prep",
    "href": "slides/05-slr-prediction.html#data-prep",
    "title": "Simple Linear Regression",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/05-slr-prediction.html#movie-scores-data",
    "href": "slides/05-slr-prediction.html#movie-scores-data",
    "title": "Simple Linear Regression",
    "section": "Movie scores data",
    "text": "Movie scores data\nThe data set contains the “Tomatometer” score (critics) and audience score (audience) for 146 movies rated on rottentomatoes.com.\n\n\nCode\nmovie_scores |&gt; \ngf_point(audience ~ critics, alpha = 0.5) + \n  labs(x = \"Critics Score\" , \n       y = \"Audience Score\")"
  },
  {
    "objectID": "slides/05-slr-prediction.html#movie-ratings-data",
    "href": "slides/05-slr-prediction.html#movie-ratings-data",
    "title": "Simple Linear Regression",
    "section": "Movie ratings data",
    "text": "Movie ratings data\nGoal: Fit a line to describe the relationship between the critics score and audience score."
  },
  {
    "objectID": "slides/05-slr-prediction.html#recall-our-model",
    "href": "slides/05-slr-prediction.html#recall-our-model",
    "title": "Simple Linear Regression",
    "section": "Recall: Our Model",
    "text": "Recall: Our Model\n\\[\\begin{aligned}\n\\widehat{Y} &= 32.3142 + 0.5187 \\times X\\\\\n\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics}\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slr-prediction.html#making-a-prediction",
    "href": "slides/05-slr-prediction.html#making-a-prediction",
    "title": "Simple Linear Regression",
    "section": "Making a prediction",
    "text": "Making a prediction\nSuppose that a movie has a critics score of 70. According to this model, what is the movie’s predicted audience score?\n\\[\\begin{aligned}\n\\widehat{\\text{audience}} &= 32.3142 + 0.5187 \\times \\text{critics} \\\\\n&= 32.3142 + 0.5187 \\times 70 \\\\\n&= 68.6232\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/05-slr-prediction.html#fit-model-estimate-parameters",
    "href": "slides/05-slr-prediction.html#fit-model-estimate-parameters",
    "title": "Simple Linear Regression",
    "section": "Fit model & estimate parameters",
    "text": "Fit model & estimate parameters\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\nmovie_fit\n\n\nCall:\nlm(formula = audience ~ critics, data = movie_scores)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187"
  },
  {
    "objectID": "slides/05-slr-prediction.html#look-at-the-regression-output",
    "href": "slides/05-slr-prediction.html#look-at-the-regression-output",
    "title": "Simple Linear Regression",
    "section": "Look at the regression output",
    "text": "Look at the regression output\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\nmovie_fit\n\n\nCall:\nlm(formula = audience ~ critics, data = movie_scores)\n\nCoefficients:\n(Intercept)      critics  \n    32.3155       0.5187  \n\n\n\\[\\widehat{\\text{audience}} = 32.3155 + 0.5187 \\times \\text{critics}\\]\n\n\nNote: The intercept is off by a tiny bit from the hand-calculated intercept, this is just due to rounding in the hand calculation."
  },
  {
    "objectID": "slides/05-slr-prediction.html#the-regression-output",
    "href": "slides/05-slr-prediction.html#the-regression-output",
    "title": "Simple Linear Regression",
    "section": "The regression output",
    "text": "The regression output\nWe’ll focus on the first column for now…\n\nmovie_fit |&gt; \n  tidy() \n\n# A tibble: 2 × 5\n  term        estimate std.error statistic  p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)   32.3      2.34        13.8 4.03e-28\n2 critics        0.519    0.0345      15.0 2.70e-31"
  },
  {
    "objectID": "slides/05-slr-prediction.html#format-output-with-kable",
    "href": "slides/05-slr-prediction.html#format-output-with-kable",
    "title": "Simple Linear Regression",
    "section": "Format output with kable",
    "text": "Format output with kable\nUse the kable function from the knitr package to produce a table and specify number of significant digits\n\nmovie_fit |&gt; \n  tidy() |&gt;\n  kable(digits = 4)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n32.3155\n2.3425\n13.7953\n0\n\n\ncritics\n0.5187\n0.0345\n15.0281\n0"
  },
  {
    "objectID": "slides/05-slr-prediction.html#visualize-model",
    "href": "slides/05-slr-prediction.html#visualize-model",
    "title": "Simple Linear Regression",
    "section": "Visualize Model",
    "text": "Visualize Model\n\nmovie_scores |&gt; \n  gf_point(audience ~ critics) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/05-slr-prediction.html#prediction-1",
    "href": "slides/05-slr-prediction.html#prediction-1",
    "title": "Simple Linear Regression",
    "section": "Prediction",
    "text": "Prediction\n\n# create a data frame for a new movie\nnew_movie &lt;- tibble(critics = 70)\n\n# predict the outcome for a new movie\npredict(movie_fit, new_movie)\n\n       1 \n68.62297"
  },
  {
    "objectID": "slides/05-slr-prediction.html#application-exercise",
    "href": "slides/05-slr-prediction.html#application-exercise",
    "title": "Simple Linear Regression",
    "section": "Application Exercise",
    "text": "Application Exercise\n\n📋 AE-02 Bike rentals in DC"
  },
  {
    "objectID": "slides/05-slr-prediction.html#recap",
    "href": "slides/05-slr-prediction.html#recap",
    "title": "Simple Linear Regression",
    "section": "Recap",
    "text": "Recap\n\n\nPredicted the response given a value of the predictor variable.\nUsed lm and the broom package to fit and summarize regression models in R.\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#ae-02-follow-up-1",
    "href": "slides/06-slr-bootstrap.html#ae-02-follow-up-1",
    "title": "SLR: Simulation-based inference",
    "section": "",
    "text": "Goal: Use simple linear regression to model the relationship between temperature and daily bike rentals in the winter season"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#ae-02-follow-up-2",
    "href": "slides/06-slr-bootstrap.html#ae-02-follow-up-2",
    "title": "SLR: Simulation-based inference",
    "section": "",
    "text": "Statistical Model:\n\\[count = \\beta_0 +\\beta_1 ~ temp\\_orig + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)\\]\n. . .\n\nwinter_fit &lt;- lm(count ~ temp_orig, data = winter)\n\ntidy(winter_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-111.038\n238.312\n-0.466\n0.642\n\n\ntemp_orig\n222.416\n18.459\n12.049\n0.000"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#ae-02-follow-up-3",
    "href": "slides/06-slr-bootstrap.html#ae-02-follow-up-3",
    "title": "SLR: Simulation-based inference",
    "section": "",
    "text": "Use the output to write out the estimated regression equation.\n\\[\n\\hat{count} =  -111.038 + 222.416 ~temp\\_orig\n\\]\n\n\nLaTex:\n\\$\\$\\hat{count} = -111.038 + 222.416 ~ temp\\_orig\\$\\$\n\n\n\n\n\n\nYour turn!\n\n\n\n\nInterpret the slope in the context of the data.\nWhy is there no error term in the regression equation?"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#topics",
    "href": "slides/06-slr-bootstrap.html#topics",
    "title": "SLR: Simulation-based inference",
    "section": "Topics",
    "text": "Topics\n\nFind range of plausible values for the slope using bootstrap confidence intervals"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#computational-setup",
    "href": "slides/06-slr-bootstrap.html#computational-setup",
    "title": "SLR: Simulation-based inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)  # for modeling\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for neatly formatted tables\nlibrary(kableExtra)  # also for neatly formatted tablesf\n\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#data-san-antonio-income-organic-food-access",
    "href": "slides/06-slr-bootstrap.html#data-san-antonio-income-organic-food-access",
    "title": "SLR: Simulation-based inference",
    "section": "Data: San Antonio Income & Organic Food Access",
    "text": "Data: San Antonio Income & Organic Food Access\n\n\n\nAverage household income (per zip code) and number of organic vegetable offerings in San Antonio, TX\nData from HEB website, compiles by high school student Linda Saucedo, Fall 2019\nSource: Skew The Script\n\n\n\n\n\nGoal: Use the average household income to understand variation in access to organic foods."
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#exploratory-data-analysis",
    "href": "slides/06-slr-bootstrap.html#exploratory-data-analysis",
    "title": "SLR: Simulation-based inference",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\n\nCode\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\ngf_point(Number_Organic ~ Avg_Income_K, data = heb, alpha = 0.7) |&gt; \n  gf_labs(\n    x = \"Average Household Income (in thousands)\",\n    y = \"Number of Organic Vegetables\",\n  ) |&gt; \n  gf_refine(scale_x_continuous(labels = label_dollar()))"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#modeling",
    "href": "slides/06-slr-bootstrap.html#modeling",
    "title": "SLR: Simulation-based inference",
    "section": "Modeling",
    "text": "Modeling\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits=2) #neatly format table to 2 digits\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n. . .\n\nIntercept: HEBs in Zip Codes with an average household income of $0 are expected to have -14.72 organic vegetable options, on average.\n\nIs this interpretation useful?\n\nSlope: For each additional $1,000 in average household income, we expect the number of organic options available at nearby HEBs to increase by 0.96, on average."
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#from-sample-to-population",
    "href": "slides/06-slr-bootstrap.html#from-sample-to-population",
    "title": "SLR: Simulation-based inference",
    "section": "From sample to population",
    "text": "From sample to population\n\nFor each additional $1,000 in average household income, we expect the number of organic options available at nearby HEBs to increase by 0.96, on average.\n\n\n\nEstimate is valid for the single sample of 37 HEBs\nWhat if we’re not interested quantifying the relationship between the size and price of a house in this single sample?\nWhat if we want to say something about the relationship between these variables for all supermarkets in America?"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#statistical-inference",
    "href": "slides/06-slr-bootstrap.html#statistical-inference",
    "title": "SLR: Simulation-based inference",
    "section": "Statistical inference",
    "text": "Statistical inference\n\nStatistical inference refers to ideas, methods, and tools for to generalizing the single observed sample to make statements (inferences) about the population it comes from\nFor our inferences to be valid, the sample should be random and representative of the population we’re interested in"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#inference-for-simple-linear-regression",
    "href": "slides/06-slr-bootstrap.html#inference-for-simple-linear-regression",
    "title": "SLR: Simulation-based inference",
    "section": "Inference for simple linear regression",
    "text": "Inference for simple linear regression\n\n\nCalculate a confidence interval for the slope, \\(\\beta_1\\)\nConduct a hypothesis test for the slope, \\(\\beta_1\\)\nWhy not \\(\\beta_0\\)?\nWe can but it isn’t super interesting typically\n\n\n. . .\n\n\nWhat is a confidence interval?\nWhat is a hypothesis test?"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#confidence-interval",
    "href": "slides/06-slr-bootstrap.html#confidence-interval",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval",
    "text": "Confidence interval\n\n\nConfidence interval: plausible range of values for a population parameter\nsingle point estimate \\(\\implies\\) fishing in a murky lake with a spear\nconfidence interval \\(\\implies\\) fishing with a net\n\nWe can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish\nIf we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter\nHigh confidence \\(\\implies\\) wider interval (larger net)\n\nRemember: single CI \\(\\implies\\) either you hit parameter or you don’t\n\nFavorite Visualization"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#confidence-interval-for-the-slope-1",
    "href": "slides/06-slr-bootstrap.html#confidence-interval-for-the-slope-1",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\nA confidence interval will allow us to make a statement like “For each $1K in average income, the model predicts the number of organic vegetables available at local supermarkets to be higher, on average, by 0.96, plus or minus X options.”\n. . .\n\nShould X be 1? 2? 3?\nIf we were to take another sample of 37 would we expect the slope calculated based on that sample to be exactly 0.96? Off by 1? 2? 3?\nThe answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is\nWe need a way to quantify the variability of the sample statistic"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#quantify-the-variability-of-the-slope",
    "href": "slides/06-slr-bootstrap.html#quantify-the-variability-of-the-slope",
    "title": "SLR: Simulation-based inference",
    "section": "Quantify the variability of the slope",
    "text": "Quantify the variability of the slope\nfor estimation\n\n\nTwo approaches:\n\nVia simulation (what we’ll do today)\nVia mathematical models (what we’ll do in the soon)\n\nBootstrapping to quantify the variability of the slope for the purpose of estimation:\n\nGenerate new samples by sampling with replacement from the original sample\nFit models to each of the new samples and estimate the slope\nUse features of the distribution of the bootstrapped slopes to construct a confidence interval"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#original-sample",
    "href": "slides/06-slr-bootstrap.html#original-sample",
    "title": "SLR: Simulation-based inference",
    "section": "Original Sample",
    "text": "Original Sample"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#bootstrap-sample-1",
    "href": "slides/06-slr-bootstrap.html#bootstrap-sample-1",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 1",
    "text": "Bootstrap sample 1"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#bootstrap-sample-2",
    "href": "slides/06-slr-bootstrap.html#bootstrap-sample-2",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 2",
    "text": "Bootstrap sample 2"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#bootstrap-sample-3",
    "href": "slides/06-slr-bootstrap.html#bootstrap-sample-3",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 3",
    "text": "Bootstrap sample 3"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#bootstrap-sample-4",
    "href": "slides/06-slr-bootstrap.html#bootstrap-sample-4",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 4",
    "text": "Bootstrap sample 4"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#bootstrap-sample-5",
    "href": "slides/06-slr-bootstrap.html#bootstrap-sample-5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap sample 5",
    "text": "Bootstrap sample 5"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#bootstrap-samples-1---5",
    "href": "slides/06-slr-bootstrap.html#bootstrap-samples-1---5",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 5",
    "text": "Bootstrap samples 1 - 5"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#bootstrap-samples-1---100",
    "href": "slides/06-slr-bootstrap.html#bootstrap-samples-1---100",
    "title": "SLR: Simulation-based inference",
    "section": "Bootstrap samples 1 - 100",
    "text": "Bootstrap samples 1 - 100"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#slopes-of-bootstrap-samples",
    "href": "slides/06-slr-bootstrap.html#slopes-of-bootstrap-samples",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional $1k in average household income, the model predicts the number of organic vegetables available to be higher, on average, by 0.96, plus or minus ___."
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#slopes-of-bootstrap-samples-1",
    "href": "slides/06-slr-bootstrap.html#slopes-of-bootstrap-samples-1",
    "title": "SLR: Simulation-based inference",
    "section": "Slopes of bootstrap samples",
    "text": "Slopes of bootstrap samples\n\nFill in the blank: For each additional $1k in average household income, the model predicts the number of organic vegetables available to be higher, on average, by 0.96, plus or minus ___."
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#confidence-level",
    "href": "slides/06-slr-bootstrap.html#confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Confidence level",
    "text": "Confidence level\n\nHow confident are you that the true slope is between 0.8 and 1.2? How about 0.9 and 1.0? How about 1.0 and 1.4?"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#confidence-interval-1",
    "href": "slides/06-slr-bootstrap.html#confidence-interval-1",
    "title": "SLR: Simulation-based inference",
    "section": "95% confidence interval",
    "text": "95% confidence interval\n\n\n\n\n\n\n\n\n\n\n\n95% bootstrapped confidence interval: bounded by the middle 95% of the bootstrap distribution\nWe are 95% confident that for each additional $1K in average household income, the model predicts the number of organic vegetables options at local supermarkets to be higher, on average, by 0.81 to 1.31."
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#computing-the-ci-for-the-slope-i",
    "href": "slides/06-slr-bootstrap.html#computing-the-ci-for-the-slope-i",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope I",
    "text": "Computing the CI for the slope I\nCalculate the observed slope:\n\nlibrary(infer) # package that does Simulation-Based Inference\n\nobserved_fit &lt;- heb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  fit()\n\nobserved_fit\n\n# A tibble: 2 × 2\n  term         estimate\n  &lt;chr&gt;           &lt;dbl&gt;\n1 intercept     -14.7  \n2 Avg_Income_K    0.959"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#computing-the-ci-for-the-slope-ii",
    "href": "slides/06-slr-bootstrap.html#computing-the-ci-for-the-slope-ii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope II",
    "text": "Computing the CI for the slope II\nTake 100 bootstrap samples and fit models to each one:\n\nset.seed(1120)\n\nboot_fits &lt;- heb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  generate(reps = 100, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_fits\n\n# A tibble: 200 × 3\n# Groups:   replicate [100]\n   replicate term         estimate\n       &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1         1 intercept     -40.9  \n 2         1 Avg_Income_K    1.25 \n 3         2 intercept     -23.9  \n 4         2 Avg_Income_K    1.09 \n 5         3 intercept     -18.6  \n 6         3 Avg_Income_K    1.02 \n 7         4 intercept      -1.96 \n 8         4 Avg_Income_K    0.828\n 9         5 intercept     -15.1  \n10         5 Avg_Income_K    0.951\n# ℹ 190 more rows"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#computing-the-ci-for-the-slope-iii",
    "href": "slides/06-slr-bootstrap.html#computing-the-ci-for-the-slope-iii",
    "title": "SLR: Simulation-based inference",
    "section": "Computing the CI for the slope III",
    "text": "Computing the CI for the slope III\nPercentile method: Compute the 95% CI as the middle 95% of the bootstrap distribution:\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\" #default method\n)\n\n# A tibble: 2 × 3\n  term         lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K    0.822     1.27\n2 intercept     -34.5      -2.56"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#precision-vs.-accuracy",
    "href": "slides/06-slr-bootstrap.html#precision-vs.-accuracy",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nIf we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?\n\n. . ."
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#precision-vs.-accuracy-1",
    "href": "slides/06-slr-bootstrap.html#precision-vs.-accuracy-1",
    "title": "SLR: Simulation-based inference",
    "section": "Precision vs. accuracy",
    "text": "Precision vs. accuracy\n\nHow can we get best of both worlds – high precision and high accuracy?"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#changing-confidence-level",
    "href": "slides/06-slr-bootstrap.html#changing-confidence-level",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\nHow would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?\n\n\nget_confidence_interval(\n  boot_fits, \n  point_estimate = observed_fit, \n  level = 0.95,\n  type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term         lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K    0.822     1.27\n2 intercept     -34.5      -2.56"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#changing-confidence-level-1",
    "href": "slides/06-slr-bootstrap.html#changing-confidence-level-1",
    "title": "SLR: Simulation-based inference",
    "section": "Changing confidence level",
    "text": "Changing confidence level\n\n## confidence level: 90%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.90, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term         lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K    0.829     1.23\n2 intercept     -31.7      -3.76\n\n## confidence level: 99%\nget_confidence_interval(\n  boot_fits, point_estimate = observed_fit, \n  level = 0.99, type = \"percentile\"\n)\n\n# A tibble: 2 × 3\n  term         lower_ci upper_ci\n  &lt;chr&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K    0.795    1.36 \n2 intercept     -43.3     -0.535"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html#recap",
    "href": "slides/06-slr-bootstrap.html#recap",
    "title": "SLR: Simulation-based inference",
    "section": "Recap",
    "text": "Recap\n\nPopulation: Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = \\(N\\))\nSample: Subset of the population, ideally random and representative (sample size = \\(n\\))\nSample statistic \\(\\ne\\) population parameter, but if the sample is good, it can be a good estimate\nStatistical inference: Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\nWe report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\nSince we can’t continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability"
  },
  {
    "objectID": "slides/06-slr-math-models.html#topics",
    "href": "slides/06-slr-math-models.html#topics",
    "title": "SLR: Mathematical models for inference",
    "section": "Topics",
    "text": "Topics\n\nDefine mathematical models to conduct inference for the slope\nUse mathematical models to\n\ncalculate confidence interval for the slope\nconduct a hypothesis test for the slope"
  },
  {
    "objectID": "slides/06-slr-math-models.html#computational-setup",
    "href": "slides/06-slr-math-models.html#computational-setup",
    "title": "SLR: Mathematical models for inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# HEB Dataset\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/06-slr-math-models.html#the-regression-model-revisited",
    "href": "slides/06-slr-math-models.html#the-regression-model-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00"
  },
  {
    "objectID": "slides/06-slr-math-models.html#inference-revisited",
    "href": "slides/06-slr-math-models.html#inference-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "Inference, revisited",
    "text": "Inference, revisited\n\n\nEarlier we computed a confidence interval and conducted a hypothesis test via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we’ll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the confidence interval and the p-value for the hypothesis test"
  },
  {
    "objectID": "slides/06-slr-math-models.html#mathematical-representation-of-the-model",
    "href": "slides/06-slr-math-models.html#mathematical-representation-of-the-model",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\n\nindependent: Knowing the error term for one observation doesn’t tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/06-slr-math-models.html#mathematical-representation-visualized",
    "href": "slides/06-slr-math-models.html#mathematical-representation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/06-slr-math-models.html#regression-standard-error",
    "href": "slides/06-slr-math-models.html#regression-standard-error",
    "title": "SLR: Mathematical models for inference",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error, the average distance between the observed values and the regression line\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\nWhy divide by \\(n - 2\\)?\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/06-slr-math-models.html#standard-error-of-hatbeta_1",
    "href": "slides/06-slr-math-models.html#standard-error-of-hatbeta_1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\nThe standard error of \\(\\hat{\\beta}_1\\) quantifies the sampling variability in the estimated slopes\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00"
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-for-the-slope",
    "href": "slides/06-slr-math-models.html#hypothesis-test-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test for the slope",
    "text": "Hypothesis test for the slope\nHypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\nT = \\frac{\\text{Estimate - Null Value}}{\\text{Standard error}} \\\\\n\\]\n\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\n\\text{p-value} = P(|t| &gt; |\\text{test statistic}|),\n\\]\ncalculated from a \\(t\\) distribution with \\(n - 2\\) degrees of freedom"
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-test-statistic",
    "href": "slides/06-slr-math-models.html#hypothesis-test-test-statistic",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Test statistic",
    "text": "Hypothesis test: Test statistic\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\n\\[\nT = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}} = \\frac{0.96 - 0}{0.13} = 7.38\n\\]\n\nHow should we interpret this test statistic?"
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-p-value",
    "href": "slides/06-slr-math-models.html#hypothesis-test-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00"
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-p-value-1",
    "href": "slides/06-slr-math-models.html#hypothesis-test-p-value-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\nA more exact p-value\n\n2 * pt(q = 7.38, df = 35, lower.tail = FALSE)\n\n[1] 1.243448e-08\n\n\n\nInterpret this p-value."
  },
  {
    "objectID": "slides/06-slr-math-models.html#understanding-the-p-value",
    "href": "slides/06-slr-math-models.html#understanding-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/06-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "href": "slides/06-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Conclusion, in context",
    "text": "Hypothesis test: Conclusion, in context\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\n\nThe data provide convincing evidence that the population slope \\(\\beta_1\\) is different from 0.\nThe data provide convincing evidence of a linear relationship between average household income and the number of organic vegetable options available."
  },
  {
    "objectID": "slides/06-slr-math-models.html#confidence-interval-for-the-slope",
    "href": "slides/06-slr-math-models.html#confidence-interval-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/06-slr-math-models.html#confidence-interval-critical-value",
    "href": "slides/06-slr-math-models.html#confidence-interval-critical-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(heb) - 2)\n\n[1] 2.030108\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(heb) - 2)\n\n[1] 1.689572\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(heb) - 2)\n\n[1] 2.723806"
  },
  {
    "objectID": "slides/06-slr-math-models.html#ci-for-the-slope-calculation",
    "href": "slides/06-slr-math-models.html#ci-for-the-slope-calculation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\n\\[\\hat{\\beta}_1 = 0.96 \\hspace{15mm} t^* = 2.03 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 0.13\\]\n\n\\[\n0.96 \\pm 2.03 \\times 0.13 = (0.70, 1.22)\n\\]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#ci-for-the-slope-computation",
    "href": "slides/06-slr-math-models.html#ci-for-the-slope-computation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(heb_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n-33.59\n4.16\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n0.70\n1.22"
  },
  {
    "objectID": "slides/06-slr-math-models.html#intervals-for-predictions-1",
    "href": "slides/06-slr-math-models.html#intervals-for-predictions-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Intervals for predictions",
    "text": "Intervals for predictions\n\nQuestion: “What is the predicted number of organic vegetable options in a neighborhood with an average income of $70k?”\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead"
  },
  {
    "objectID": "slides/06-slr-math-models.html#two-types-of-predictions",
    "href": "slides/06-slr-math-models.html#two-types-of-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: “What is the average predicted number of organic vegetable options in a neighborhood with an average income of $70k?”\nPrediction for an individual observation: “What is the predicted number of organic vegetable options at a single HEB in a neighborhood with an average income of $70k?”\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/06-slr-math-models.html#uncertainty-in-predictions",
    "href": "slides/06-slr-math-models.html#uncertainty-in-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}}\\]\n\nPrediction interval for an individual observation: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}}\\]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#standard-errors",
    "href": "slides/06-slr-math-models.html#standard-errors",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#standard-errors-1",
    "href": "slides/06-slr-math-models.html#standard-errors-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/06-slr-math-models.html#confidence-interval",
    "href": "slides/06-slr-math-models.html#confidence-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcome:\n\nnew_neighborhood &lt;- tibble(Avg_Income_K = 70)\n\npredict(heb_fit, newdata = new_neighborhood, interval = \"confidence\", level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n52.4175\n46.58558\n58.24942\n\n\n\n\n\n\n\nWe are 95% confident that the mean number of organic vegetable options offered by HEB in a neighborhood with an average income of $70k is between 46.59 and 58.25."
  },
  {
    "objectID": "slides/06-slr-math-models.html#prediction-interval",
    "href": "slides/06-slr-math-models.html#prediction-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction interval for an individual outcome:\n\npredict(heb_fit, newdata = new_neighborhood, interval = \"prediction\", level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n52.4175\n16.48941\n88.34559\n\n\n\n\n\n\nWe are 95% confident that the number of organic vegetable options offered by HEB in a neighborhood with an average income of $70k is between 16.49 and 88.35."
  },
  {
    "objectID": "slides/06-slr-math-models.html#comparing-intervals",
    "href": "slides/06-slr-math-models.html#comparing-intervals",
    "title": "SLR: Mathematical models for inference",
    "section": "Comparing intervals",
    "text": "Comparing intervals"
  },
  {
    "objectID": "slides/06-slr-math-models.html#extrapolation",
    "href": "slides/06-slr-math-models.html#extrapolation",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\nUsing the model to predict for values outside the range of the original data is extrapolation.\n\n\n\n\nCalculate the prediction interval for the number of organic options in an extremely wealthy neighborhood with an average household income of $500k.\n\nNo, thanks!"
  },
  {
    "objectID": "slides/06-slr-math-models.html#extrapolation-1",
    "href": "slides/06-slr-math-models.html#extrapolation-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nWhy do we want to avoid extrapolation?\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#computational-set-up",
    "href": "slides/07-slr-conditions-eval.html#computational-set-up",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)   # for plotting using formulas\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# HEB Dataset\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#regression-model-revisited",
    "href": "slides/07-slr-conditions-eval.html#regression-model-revisited",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.719\n9.298\n-1.583\n0.122\n\n\nAvg_Income_K\n0.959\n0.128\n7.505\n0.000"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#mathematical-representation-visualized",
    "href": "slides/07-slr-conditions-eval.html#mathematical-representation-visualized",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#model-conditions-1",
    "href": "slides/07-slr-conditions-eval.html#model-conditions-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#linearity",
    "href": "slides/07-slr-conditions-eval.html#linearity",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Linearity",
    "text": "Linearity\n\nIf the linear model, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\), adequately describes the relationship between \\(X\\) and \\(Y\\), then the residuals should reflect random (chance) error\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nLinearity satisfied if there is no distinguishable pattern in the residuals plot, i.e. the residuals should be randomly scattered\nA non-random pattern (e.g. a parabola) suggests a linear model does not adequately describe the relationship between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#linearity-1",
    "href": "slides/07-slr-conditions-eval.html#linearity-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Linearity",
    "text": "Linearity\n✅ The residuals vs. fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#residuals-vs.-fitted-values-code",
    "href": "slides/07-slr-conditions-eval.html#residuals-vs.-fitted-values-code",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Residuals vs. fitted values (code)",
    "text": "Residuals vs. fitted values (code)\n\nheb_aug &lt;- augment(heb_fit)\n\ngf_point(.resid ~ .fitted, data = heb_aug) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#non-linear-relationships",
    "href": "slides/07-slr-conditions-eval.html#non-linear-relationships",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#constant-variance",
    "href": "slides/07-slr-conditions-eval.html#constant-variance",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n\nIf the spread of the distribution of \\(Y\\) is equal for all values of \\(X\\) then the spread of the residuals should be approximately equal for each value of \\(X\\)\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nConstant variance satisfied if the vertical spread of the residuals is approximately equal as you move from left to right (i.e. there is no “fan” pattern)\nA fan pattern suggests the constant variance assumption is not satisfied and transformation or some other remedy is required (more on this later in the semester)\nCAREFUL: Inconsistent distribution of \\(X\\)s can make it seem as if there is non-constant variance"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#constant-variance-1",
    "href": "slides/07-slr-conditions-eval.html#constant-variance-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n✅ The vertical spread of the residuals is relatively constant across the plot"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#non-constant-variance",
    "href": "slides/07-slr-conditions-eval.html#non-constant-variance",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Non-constant variance",
    "text": "Non-constant variance"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#normality",
    "href": "slides/07-slr-conditions-eval.html#normality",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality\n\nThe linear model assumes that the distribution of \\(Y\\) is Normal for every value of \\(X\\)\nThis is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied\nNormality satisfied if a histogram of the residuals is approximately normal\n\nCan also check that the points on a normal QQ-plot falls along a diagonal line\n\nMost inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#normality-1",
    "href": "slides/07-slr-conditions-eval.html#normality-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#check-normality-using-a-qq-plot",
    "href": "slides/07-slr-conditions-eval.html#check-normality-using-a-qq-plot",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Check normality using a QQ-plot",
    "text": "Check normality using a QQ-plot\n\n\n\n\nCode\ngf_histogram(~.resid, data = heb_aug,\n             bins=7, color = \"white\") |&gt; \n  gf_labs(\n    x = \"Residual\",\n    y = \"Count\",\n    title = \"Histogram of residuals\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngf_qq(~.resid, data = heb_aug) |&gt; \n  gf_qqline() |&gt;  \n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")\n\n\n\n\n\n\n\n\n\n\n\n\nAssess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot).\nIf so, the residuals are normally distributed."
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#normality-2",
    "href": "slides/07-slr-conditions-eval.html#normality-2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality\n\n❌ The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied.\n✅ The sample size \\(n =  37 &gt; 30\\), so the sample size is large enough to relax this condition and proceed with inference."
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#independence",
    "href": "slides/07-slr-conditions-eval.html#independence",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nTwo common violations of the independence assumption:\n\nSerial Effect: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation)\nCluster Effect: If there are subgroups represented in the data that are not accounted for in the model (e.g., type of supermarket), you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#independence-1",
    "href": "slides/07-slr-conditions-eval.html#independence-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Independence",
    "text": "Independence\nRecall the description of the data:\n\nAverage household income (per zip code) and number of organic vegetable offerings in San Antonio, TX\nData from HEB website, compiled by high school student Linda Saucedo, Fall 2019\n\n\n❌ Based on the information we have, it’s unclear if the data are independent. In fact, I’d guess that they are likely geographically correlated."
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#recap",
    "href": "slides/07-slr-conditions-eval.html#recap",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\n\nLinearity\nConstant variance\n\n\n\n\n\nNormality\nIndependence\n\n\n\n\n\n\nWhich of these conditions are required for fitting a SLR? Which for simulation-based inference for the slope for an SLR? Which for inference with mathematical models?\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#comparing-inferential-methods",
    "href": "slides/07-slr-conditions-eval.html#comparing-inferential-methods",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Comparing inferential methods",
    "text": "Comparing inferential methods\n\n\nWhat are the advantages of using simulation-based inference methods? What are the advantages of using inference methods based on mathematical models?\nUnder what scenario(s) would you prefer to use simulation-based methods? Under what scenario(s) would you prefer to use methods based on mathematical models?\n\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#two-statistics",
    "href": "slides/07-slr-conditions-eval.html#two-statistics",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\), Coefficient of Determination : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#r2",
    "href": "slides/07-slr-conditions-eval.html#r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with glance() or summary() using the fitted model:\n\n\nglance(heb_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic       p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.617         0.606  17.5      56.3 0.00000000863     1  -157.  321.  325.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(heb_fit)$r.squared\n\n[1] 0.6167334"
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#interpreting-r2",
    "href": "slides/07-slr-conditions-eval.html#interpreting-r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\n🗳️ Discussion\n\n\nThe \\(R^2\\) of the model for Number_Organic from Average_Income_K is 61.7%. Which of the following is the correct interpretation of this value?\n\n\nAvg_Income_K correctly predicts 61.7% of Number_Organic in San Anontio HEBs.\n61.7% of the variability in Number_Organic can be explained by Avg_Income_K.\n61.7% of the variability in Avg_Income_K can be explained by Number_Organic.\n61.7% of the time Number_Organic can be predicted by Avg_Income_K."
  },
  {
    "objectID": "slides/07-slr-conditions-eval.html#rmse",
    "href": "slides/07-slr-conditions-eval.html#rmse",
    "title": "SLR: Conditions + Model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with glance() using the augmented data:\n\nglance(heb_fit)$sigma\n\n[1] 17.46292\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this when we get to regression with multiple predictors)\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#announcements",
    "href": "slides/08-slr-model-eval.html#announcements",
    "title": "SLR: Model evaluation",
    "section": "Announcements",
    "text": "Announcements\n\nHW 02 due Mon, Oct 2 at 11:59pm. (released after section 002)\nAll lecture recordings available until Wed, Oct 4 at 9am.\n\nClick here for link to videos. You can also find the link in the navigation bar of the course website.\n\nLab groups start this week. You will get your assigned group when you go to lab.\nLooking ahead: Exam 01:\n\nClosed note in-class: Wed, Oct 4\nOpen note take-home: Wed, Oct 4 - Fri, Oct 6\n\nReleased after Section 002\n\nExam review: Mon, Oct 2"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#statistician-of-the-day-robert-santos",
    "href": "slides/08-slr-model-eval.html#statistician-of-the-day-robert-santos",
    "title": "SLR: Model evaluation",
    "section": "Statistician of the day: Robert Santos",
    "text": "Statistician of the day: Robert Santos\n\n\n\n\nRobert Santos received an MA in Statistics from the University of Michigan, Ann Arbor. He served as president of the American Statistical Association in 2021. As a survey researcher, he worked at National Opinion Research Center (NORC, University of Chicago) and the Urban Institute in Washington, DC.\nAs a Mexican-American, he is the first non-white person to serve as the Director of the US Census Bureau (appointed by Joe Biden and approved by the US Senate in 2022).\nSource: hardin47.github.io/CURV/scholars/santos.html"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#robert-santos",
    "href": "slides/08-slr-model-eval.html#robert-santos",
    "title": "SLR: Model evaluation",
    "section": "Robert Santos",
    "text": "Robert Santos\nSantos is a survey researcher with much of his recent focus on the US Census. In particular, he has written extensively about miscounting particular groups of people in the Census and the relationship between race and ethnicity in surveys.\nFrom his article “Is It Time to Postpone the 2020 Census?“ (written during his time at the Urban Institute)\n\n“This would create a worst-case scenario when it comes to political representation and allocation of federal resources…And the 2020 counts would then be baked in to population projections used to calibrate federal statistics and surveys, thus informing federal funds allocations and eligibility thresholds for the next 10 years.\n\nRelated work: Interactive feature Who’s at Risk of Being Miscounted?"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#computational-set-up",
    "href": "slides/08-slr-model-eval.html#computational-set-up",
    "title": "SLR: Model evaluation",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#model-conditions-1",
    "href": "slides/08-slr-model-eval.html#model-conditions-1",
    "title": "SLR: Model evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#augmented-data-frame",
    "href": "slides/08-slr-model-eval.html#augmented-data-frame",
    "title": "SLR: Model evaluation",
    "section": "Augmented data frame",
    "text": "Augmented data frame\n\ndf_fit &lt;- linear_reg() |&gt;\n  fit(price ~ area, data = duke_forest)\n\ndf_aug &lt;- augment(df_fit$fit)\n\nhead(df_aug)\n\n# A tibble: 6 × 8\n    price  area  .fitted  .resid   .hat  .sigma  .cooksd .std.resid\n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 1520000  6040 1079931. 440069. 0.133  162605. 0.604         2.80 \n2 1030000  4475  830340. 199660. 0.0435 168386. 0.0333        1.21 \n3  420000  1745  394951.  25049. 0.0226 169664. 0.000260      0.150\n4  680000  2091  450132. 229868. 0.0157 168011. 0.0150        1.37 \n5  428500  1772  399257.  29243. 0.0220 169657. 0.000345      0.175\n6  456000  1950  427645.  28355. 0.0182 169659. 0.000266      0.170"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#two-statistics",
    "href": "slides/08-slr-model-eval.html#two-statistics",
    "title": "SLR: Model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#r2",
    "href": "slides/08-slr-model-eval.html#r2",
    "title": "SLR: Model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() using the augmented data:\n\n\nrsq(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.445"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#interpreting-r2",
    "href": "slides/08-slr-model-eval.html#interpreting-r2",
    "title": "SLR: Model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\n🗳️ Discussion\n\n\nThe \\(R^2\\) of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n\n\nArea correctly predicts 44.5% of price for houses in Duke Forest.\n44.5% of the variability in price for houses in Duke Forest can be explained by area.\n44.5% of the variability in area for houses in Duke Forest can be explained by price\n44.5% of the time price for houses in Duke Forest can be predicted by area."
  },
  {
    "objectID": "slides/08-slr-model-eval.html#alternative-approach-for-r2",
    "href": "slides/08-slr-model-eval.html#alternative-approach-for-r2",
    "title": "SLR: Model evaluation",
    "section": "Alternative approach for \\(R^2\\)",
    "text": "Alternative approach for \\(R^2\\)\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(df_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared   sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.445         0.439 168798.      77.0 6.29e-14     1 -1318. 2641. 2649.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n\n\nglance(df_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#rmse",
    "href": "slides/08-slr-model-eval.html#rmse",
    "title": "SLR: Model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rmse() using the augmented data:\n\nrmse(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     167067.\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/08-slr-model-eval.html#obtaining-r2-and-rmse",
    "href": "slides/08-slr-model-eval.html#obtaining-r2-and-rmse",
    "title": "SLR: Model evaluation",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\n\nUse rsq() and rmse(), respectively\n\nrsq(df_aug, truth = price, estimate = .fitted)\nrmse(df_aug, truth = price, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/09-mlr.html#computational-setup",
    "href": "slides/09-mlr.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/09-mlr.html#data-peer-to-peer-lender",
    "href": "slides/09-mlr.html#data-peer-to-peer-lender",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n\n\n\nannual_income\ndebt_to_income\nverified_income\ninterest_rate\n\n\n\n\n59000\n0.5575254\nNot Verified\n10.90\n\n\n60000\n1.3056833\nNot Verified\n9.92\n\n\n75000\n1.0562800\nVerified\n26.30\n\n\n75000\n0.5743467\nNot Verified\n9.92\n\n\n254000\n0.2381496\nNot Verified\n9.43\n\n\n67000\n1.0770448\nSource Verified\n9.92\n\n\n28800\n0.0997222\nSource Verified\n17.09\n\n\n80000\n0.3509125\nNot Verified\n6.08\n\n\n34000\n0.6975000\nNot Verified\n7.97\n\n\n80000\n0.1668542\nSource Verified\n12.62"
  },
  {
    "objectID": "slides/09-mlr.html#variables",
    "href": "slides/09-mlr.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/09-mlr.html#outcome-interest_rate",
    "href": "slides/09-mlr.html#outcome-interest_rate",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n5.31\n7.96\n9.93\n13.715\n26.3\n11.5672\n5.052115\n50\n0"
  },
  {
    "objectID": "slides/09-mlr.html#predictors",
    "href": "slides/09-mlr.html#predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/09-mlr.html#data-manipulation-1-rescale-income",
    "href": "slides/09-mlr.html#data-manipulation-1-rescale-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_k = annual_income / 1000)\n\ngf_histogram(~annual_income_k, data = loan50, binwidth = 20) |&gt; \n  gf_labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/09-mlr.html#outcome-vs.-predictors",
    "href": "slides/09-mlr.html#outcome-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/09-mlr.html#single-vs.-multiple-predictors",
    "href": "slides/09-mlr.html#single-vs.-multiple-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Single vs. multiple predictors",
    "text": "Single vs. multiple predictors\nSo far we’ve used a single predictor variable to understand variation in a quantitative response variable\n\nNow we want to use multiple predictor variables to understand variation in a quantitative response variable"
  },
  {
    "objectID": "slides/09-mlr.html#multiple-linear-regression-mlr",
    "href": "slides/09-mlr.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{interest_rate}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{debt_to_income} \\\\ & + \\hat{\\beta}_2 \\text{verified_income} \\\\ &+ \\hat{\\beta}_3 \\text{annual_income_k}\n\\end{aligned}\n\\]\nSimilar to simple linear regression, this model assumes that at each combination of the predictor variables, the values interest_rate follow a Normal distribution."
  },
  {
    "objectID": "slides/09-mlr.html#multiple-linear-regression-1",
    "href": "slides/09-mlr.html#multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nRecall: The simple linear regression model assumes\n\\[\nY|X\\sim N(\\beta_0 + \\beta_1 X, \\sigma_{\\epsilon}^2)\n\\]\n\nSimilarly: The multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/09-mlr.html#multiple-linear-regression-2",
    "href": "slides/09-mlr.html#multiple-linear-regression-2",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/09-mlr.html#model-equation",
    "href": "slides/09-mlr.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{interest_rate}} =  10.726 &+0.671 \\times \\text{debt_to_income}\\\\\n&+ 2.211 \\times \\text{source_verified}\\\\  \n&+ 6.880 \\times \\text{verified}\\\\\n& -0.021 \\times \\text{annual_income_k}\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\nWe will talk about why there are two terms in the model for verified_income shortly!"
  },
  {
    "objectID": "slides/09-mlr.html#interpreting-hatbeta_j",
    "href": "slides/09-mlr.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(y\\) when \\(x_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nExample: The estimated coefficient for debt_to_income is 0.671. This means for each point in an borrower’s debt to income ratio, the interest rate on the loan is expected to be greater by 0.671%, holding annual income and income verification constant."
  },
  {
    "objectID": "slides/09-mlr.html#prediction",
    "href": "slides/09-mlr.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted interest rate for an borrower with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000?\n\n\n\n10.726 + 0.671 * 0.558 + 2.211 * 0 + 6.880 * 0 - 0.021 * 59\n\n[1] 9.861418\n\n\n\nThe predicted interest rate for an borrower with with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000 is 9.86%."
  },
  {
    "objectID": "slides/09-mlr.html#prediction-revisited",
    "href": "slides/09-mlr.html#prediction-revisited",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction, revisited",
    "text": "Prediction, revisited\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_borrower &lt;- tibble(\n  debt_to_income  = 0.558, \n  verified_income = \"Not Verified\", \n  annual_income_k = 59\n)\n\npredict(int_fit, new_borrower)\n\n       1 \n9.890888 \n\n\n\n\n\n\n\n\nNote\n\n\nDifference in predicted value due to rounding the coefficients on the previous slide."
  },
  {
    "objectID": "slides/09-mlr.html#confidence-interval-for-hatmu_y",
    "href": "slides/09-mlr.html#confidence-interval-for-hatmu_y",
    "title": "Multiple linear regression (MLR)",
    "section": "Confidence interval for \\(\\hat{\\mu}_y\\)",
    "text": "Confidence interval for \\(\\hat{\\mu}_y\\)\n\nCalculate a 90% confidence interval for the estimated mean interest rate for borrowers with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000.\n\n\n\npredict(int_fit, new_borrower, interval = \"confidence\", level = 0.90)\n\n       fit      lwr      upr\n1 9.890888 8.159246 11.62253"
  },
  {
    "objectID": "slides/09-mlr.html#prediction-interval-for-haty",
    "href": "slides/09-mlr.html#prediction-interval-for-haty",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 90% confidence interval for the predicted interest rate for an individual applicant with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000.\n\n\n\npredict(int_fit, new_borrower, interval = \"prediction\", level = 0.90)\n\n       fit      lwr      upr\n1 9.890888 2.180521 17.60126"
  },
  {
    "objectID": "slides/09-mlr.html#cautions",
    "href": "slides/09-mlr.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/09-mlr.html#interpreting-results",
    "href": "slides/09-mlr.html#interpreting-results",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_k\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/09-mlr.html#indicator-variables-1",
    "href": "slides/09-mlr.html#indicator-variables-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category"
  },
  {
    "objectID": "slides/09-mlr.html#data-manipulation-2-create-indicator-variables-for-verified_income",
    "href": "slides/09-mlr.html#data-manipulation-2-create-indicator-variables-for-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 2: Create indicator variables for verified_income",
    "text": "Data manipulation 2: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n\n\n\n\n\n\nverified_income\nnot_verified\nsource_verified\nverified\n\n\n\n\nNot Verified\n1\n0\n0\n\n\nVerified\n0\n0\n1\n\n\nSource Verified\n0\n1\n0"
  },
  {
    "objectID": "slides/09-mlr.html#indicators-in-the-model",
    "href": "slides/09-mlr.html#indicators-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe reference level is the category that doesn’t have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding.\n\n\n\nloan50 |&gt;\n  select(verified_income, source_verified, verified) |&gt;\n  slice(1, 3, 6) |&gt; \n  kable()\n\n\n\n\nverified_income\nsource_verified\nverified\n\n\n\n\nNot Verified\n0\n0\n\n\nVerified\n0\n1\n\n\nSource Verified\n1\n0"
  },
  {
    "objectID": "slides/09-mlr.html#interpreting-verified_income",
    "href": "slides/09-mlr.html#interpreting-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_k\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n\n\n\nThe reference level is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\n\n\n\n\n\nInterpret the coefficient of Verified in the context of the data."
  },
  {
    "objectID": "slides/09-mlr.html#interaction-terms-1",
    "href": "slides/09-mlr.html#interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/09-mlr.html#interest-rate-vs.-annual-income",
    "href": "slides/09-mlr.html#interest-rate-vs.-annual-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/09-mlr.html#interaction-terms-3",
    "href": "slides/09-mlr.html#interaction-terms-3",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/09-mlr.html#interest-rate-vs.-annual-income-1",
    "href": "slides/09-mlr.html#interest-rate-vs.-annual-income-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/09-mlr.html#interaction-term-in-model",
    "href": "slides/09-mlr.html#interaction-term-in-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_int_fit &lt;- lm(interest_rate ~ debt_to_income + verified_income + annual_income_k + verified_income * annual_income_k,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.560\n2.034\n4.700\n0.000\n\n\ndebt_to_income\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n3.577\n2.539\n1.409\n0.166\n\n\nverified_incomeVerified\n9.923\n3.654\n2.716\n0.009\n\n\nannual_income_k\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_k\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_k\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/09-mlr.html#interpreting-interaction-terms",
    "href": "slides/09-mlr.html#interpreting-interaction-terms",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/09-mlr.html#data-manipulation-3-create-interaction-variables",
    "href": "slides/09-mlr.html#data-manipulation-3-create-interaction-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 3: Create interaction variables",
    "text": "Data manipulation 3: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_k_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, …\n$ debt_to_income                                   &lt;dbl&gt; 0.55752542, 1.3056833…\n$ annual_income_k                                  &lt;dbl&gt; 59.0, 60.0, 75.0, 75.…\n$ `verified_incomeNot Verified`                    &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 0, …\n$ `verified_incomeSource Verified`                 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, …\n$ verified_incomeVerified                          &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, …\n$ `annual_income_k:verified_incomeNot Verified`    &lt;dbl&gt; 59, 60, 0, 75, 254, 0…\n$ `annual_income_k:verified_incomeSource Verified` &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0…\n$ `annual_income_k:verified_incomeVerified`        &lt;dbl&gt; 0, 0, 75, 0, 0, 0, 0,…\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/10-mlr-inference.html#topics",
    "href": "slides/10-mlr-inference.html#topics",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Topics",
    "text": "Topics\n\n\nInference for multiple linear regression\nChecking model conditions\nVariable transformations"
  },
  {
    "objectID": "slides/10-mlr-inference.html#computational-setup",
    "href": "slides/10-mlr-inference.html#computational-setup",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/10-mlr-inference.html#data-rail_trail",
    "href": "slides/10-mlr-inference.html#data-rail_trail",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n\n\n\n\nvolume\nhightemp\navgtemp\nseason\ncloudcover\nprecip\nday_type\n\n\n\n\n501\n83\n66.5\nSummer\n7.6\n0.00\nWeekday\n\n\n419\n73\n61.0\nSummer\n6.3\n0.29\nWeekday\n\n\n397\n74\n63.0\nSpring\n7.5\n0.32\nWeekday\n\n\n385\n95\n78.0\nSummer\n2.6\n0.00\nWeekend\n\n\n200\n44\n48.0\nSpring\n10.0\n0.14\nWeekday\n\n\n375\n69\n61.5\nSpring\n6.6\n0.02\nWeekday\n\n\n417\n66\n52.5\nSpring\n2.4\n0.00\nWeekday\n\n\n629\n66\n52.0\nSpring\n0.0\n0.00\nWeekend\n\n\n533\n80\n67.5\nSummer\n3.8\n0.00\nWeekend\n\n\n547\n79\n62.0\nSummer\n4.1\n0.00\nWeekday\n\n\n\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/10-mlr-inference.html#variables",
    "href": "slides/10-mlr-inference.html#variables",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\n\nPredictors\n\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of “Fall”, “Spring”, or “Summer”\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of “weekday” or “weekend”"
  },
  {
    "objectID": "slides/10-mlr-inference.html#review-simple-linear-regression-slr",
    "href": "slides/10-mlr-inference.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\ngf_point(volume ~ hightemp, data = rail_trail, alpha = 0.5) |&gt; \n  gf_lm()  |&gt; \n  gf_labs(x = \"High temp (F)\", y = \"Number of riders\")"
  },
  {
    "objectID": "slides/10-mlr-inference.html#slr-model-summary",
    "href": "slides/10-mlr-inference.html#slr-model-summary",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nrt_slr_fit &lt;- lm(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.08\n59.40\n-0.29\n0.77\n\n\nhightemp\n5.70\n0.85\n6.72\n0.00"
  },
  {
    "objectID": "slides/10-mlr-inference.html#slr-hypothesis-test",
    "href": "slides/10-mlr-inference.html#slr-hypothesis-test",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.08\n59.40\n-0.29\n0.77\n\n\nhightemp\n5.70\n0.85\n6.72\n0.00\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t= 6.72\\) . The p-value is calculated using a \\(t\\) distribution with 88 degrees of freedom. The p-value is \\(\\approx 0\\) .\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders, i.e. there is a linear relationship between high temperature and number of daily riders."
  },
  {
    "objectID": "slides/10-mlr-inference.html#multiple-linear-regression",
    "href": "slides/10-mlr-inference.html#multiple-linear-regression",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nrt_mlr_main_fit &lt;- lm(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11"
  },
  {
    "objectID": "slides/10-mlr-inference.html#multiple-linear-regression-1",
    "href": "slides/10-mlr-inference.html#multiple-linear-regression-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes \\[Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)\\]\n\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)\\), we can rewrite the previous statement as\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)\\]"
  },
  {
    "objectID": "slides/10-mlr-inference.html#estimating-sigma_epsilon",
    "href": "slides/10-mlr-inference.html#estimating-sigma_epsilon",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Estimating \\(\\sigma_\\epsilon\\)",
    "text": "Estimating \\(\\sigma_\\epsilon\\)\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)\\) the residual is \\[\n\\begin{aligned}\ne_i &= y_{i} - \\hat{y_i}\\\\\n&= y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\n\\end{aligned}\n\\]\n\nThe estimated value of the regression standard error , \\(\\sigma_{\\epsilon}\\), is\n\\[\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}\\]\n\n\nAs with SLR, we use \\(\\hat{\\sigma}_{\\epsilon}\\) to calculate \\(SE_{\\hat{\\beta}_j}\\), the standard error of each coefficient. See Matrix Form of Linear Regression for more detail."
  },
  {
    "objectID": "slides/10-mlr-inference.html#mlr-hypothesis-test-hightemp",
    "href": "slides/10-mlr-inference.html#mlr-hypothesis-test-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "MLR hypothesis test: hightemp",
    "text": "MLR hypothesis test: hightemp\n\nSet hypotheses: \\(H_0: \\beta_{hightemp} = 0\\) vs. \\(H_A: \\beta_{hightemp} \\ne 0\\), given season is in the model\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.43\\). The p-value is calculated using a \\(t\\) distribution with 86 \\((n - p - 1)\\) degrees of freedom. The p-value is \\(\\approx 0\\).\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature for the day is a useful predictor in a model that already contains the season as a predictor for number of daily riders."
  },
  {
    "objectID": "slides/10-mlr-inference.html#the-model-for-season-spring",
    "href": "slides/10-mlr-inference.html#the-model-for-season-spring",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Spring",
    "text": "The model for season = Spring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 1 - 76.84 \\times 0 \\\\\n&= -120.10 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference.html#the-model-for-season-summer",
    "href": "slides/10-mlr-inference.html#the-model-for-season-summer",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Summer",
    "text": "The model for season = Summer\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 1 \\\\\n&= -202.07 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference.html#the-model-for-season-fall",
    "href": "slides/10-mlr-inference.html#the-model-for-season-fall",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Fall",
    "text": "The model for season = Fall\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 0 \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference.html#the-models",
    "href": "slides/10-mlr-inference.html#the-models",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The models",
    "text": "The models\nSame slope, different intercepts\n\nseason = Spring: \\(-120.10 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Summer: \\(-202.07 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Fall: \\(-125.23 + 7.54 \\times \\texttt{hightemp}\\)"
  },
  {
    "objectID": "slides/10-mlr-inference.html#interaction-terms",
    "href": "slides/10-mlr-inference.html#interaction-terms",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Interaction terms",
    "text": "Interaction terms\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.53\n166.80\n-0.06\n0.95\n\n\nhightemp\n5.48\n2.95\n1.86\n0.07\n\n\nseasonSpring\n-293.95\n190.33\n-1.54\n0.13\n\n\nseasonSummer\n354.18\n255.08\n1.39\n0.17\n\n\nhightemp:seasonSpring\n4.88\n3.26\n1.50\n0.14\n\n\nhightemp:seasonSummer\n-4.54\n3.75\n-1.21\n0.23\n\n\n\n\n\n\n\n\nDo the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms."
  },
  {
    "objectID": "slides/10-mlr-inference.html#confidence-interval-for-beta_j-1",
    "href": "slides/10-mlr-inference.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C\\%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically: We are \\(C\\%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant."
  },
  {
    "objectID": "slides/10-mlr-inference.html#confidence-interval-for-beta_j-2",
    "href": "slides/10-mlr-inference.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\ntidy(rt_mlr_main_fit, conf.int = TRUE) |&gt;\n  kable(\"html\", digits = 2) |&gt; kable_styling(font_size = 24)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00"
  },
  {
    "objectID": "slides/10-mlr-inference.html#ci-for-hightemp",
    "href": "slides/10-mlr-inference.html#ci-for-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "CI for hightemp",
    "text": "CI for hightemp\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\n\n\nWe are 95% confident that for every degree Fahrenheit the day is warmer, the number of riders increases by 5.21 to 9.87, on average, holding season constant."
  },
  {
    "objectID": "slides/10-mlr-inference.html#ci-for-seasonspring",
    "href": "slides/10-mlr-inference.html#ci-for-seasonspring",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "CI for seasonSpring",
    "text": "CI for seasonSpring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\n\n\nWe are 95% confident that the number of riders on a Spring day is lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant.\n\n\nIs season a significant predictor of the number of riders, after accounting for high temperature?"
  },
  {
    "objectID": "slides/10-mlr-inference.html#large-sample-sizes",
    "href": "slides/10-mlr-inference.html#large-sample-sizes",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/10-mlr-inference.html#small-sample-sizes",
    "href": "slides/10-mlr-inference.html#small-sample-sizes",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON’T immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association."
  },
  {
    "objectID": "slides/10-mlr-inference.html#full-model",
    "href": "slides/10-mlr-inference.html#full-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Full model",
    "text": "Full model\nIncluding all available predictors\nFit:\n\nrt_full_fit &lt;- lm(volume ~ ., data = rail_trail)\n\n\nSummarize:\n\ntidy(rt_full_fit)\n\n# A tibble: 8 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\n\n\nAugment:\n\nrt_full_aug &lt;- augment(rt_full_fit)"
  },
  {
    "objectID": "slides/10-mlr-inference.html#model-conditions",
    "href": "slides/10-mlr-inference.html#model-conditions",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/10-mlr-inference.html#checking-linearity",
    "href": "slides/10-mlr-inference.html#checking-linearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking Linearity",
    "text": "Checking Linearity\n\nLook at a plot of the residuals vs. predicted values\nLook at a plot of the residuals vs. each predictor\nLinearity is met if there is no discernible pattern in each of these plots"
  },
  {
    "objectID": "slides/10-mlr-inference.html#residuals-vs.-predicted-values",
    "href": "slides/10-mlr-inference.html#residuals-vs.-predicted-values",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. predicted values",
    "text": "Residuals vs. predicted values\n\ngf_point(.resid ~ .fitted, data = rt_full_aug, alpha = 0.7) |&gt; \n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") |&gt; \n  gf_labs(x = \"Predicted values\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/10-mlr-inference.html#residuals-vs.-each-predictor",
    "href": "slides/10-mlr-inference.html#residuals-vs.-each-predictor",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. each predictor",
    "text": "Residuals vs. each predictor"
  },
  {
    "objectID": "slides/10-mlr-inference.html#checking-linearity-1",
    "href": "slides/10-mlr-inference.html#checking-linearity-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plot of the residuals vs. predicted values looked OK\nThe plots of residuals vs. hightemp and avgtemp appear to have a parabolic pattern.\nThe linearity condition does not appear to be satisfied given these plots.\n\n\n\nGiven this conclusion, what might be a next step in the analysis?"
  },
  {
    "objectID": "slides/10-mlr-inference.html#checking-constant-variance",
    "href": "slides/10-mlr-inference.html#checking-constant-variance",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/10-mlr-inference.html#checking-constant-variance-1",
    "href": "slides/10-mlr-inference.html#checking-constant-variance-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied.\n\n\n\nWe will talk about to address this later in the notes."
  },
  {
    "objectID": "slides/10-mlr-inference.html#checking-normality",
    "href": "slides/10-mlr-inference.html#checking-normality",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking normality",
    "text": "Checking normality\n\nThe distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size 90 is sufficiently large to relax this condition if it was not satisfied."
  },
  {
    "objectID": "slides/10-mlr-inference.html#checking-independence",
    "href": "slides/10-mlr-inference.html#checking-independence",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable."
  },
  {
    "objectID": "slides/10-mlr-inference.html#checking-independence-1",
    "href": "slides/10-mlr-inference.html#checking-independence-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs. order of data collection:\n\ngf_line(.resid ~ 1:nrow(rt_full_aug), data = rt_full_aug) |&gt; \n  gf_point()  |&gt; \n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\")  |&gt; \n  gf_labs(x = \"Order of data collection\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/10-mlr-inference.html#checking-independence-2",
    "href": "slides/10-mlr-inference.html#checking-independence-2",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\n\nNo clear pattern in the residuals vs. order of data collection plot.\nIndependence condition appears to be satisfied, as far as we can evaluate it."
  },
  {
    "objectID": "slides/10-mlr-inference.html#what-is-multicollinearity",
    "href": "slides/10-mlr-inference.html#what-is-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "What is multicollinearity",
    "text": "What is multicollinearity\nMulticollinearity is the case when two or more predictor variables are strongly correlated with one another"
  },
  {
    "objectID": "slides/10-mlr-inference.html#example",
    "href": "slides/10-mlr-inference.html#example",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\nLet’s assume the true population regression equation is \\(y = 3 + 4x\\)\n\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference.html#example-1",
    "href": "slides/10-mlr-inference.html#example-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the “best” combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)"
  },
  {
    "objectID": "slides/10-mlr-inference.html#why-multicollinearity-is-a-problem",
    "href": "slides/10-mlr-inference.html#why-multicollinearity-is-a-problem",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have perfect collinearities, we are unable to get estimates for the coefficients\n\nWhen we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference\nIt is also difficult to interpret the model coefficients"
  },
  {
    "objectID": "slides/10-mlr-inference.html#detecting-multicollinearity",
    "href": "slides/10-mlr-inference.html#detecting-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when…\n\nThere are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\n\n\n\nOne (or more) predictor variables is an almost perfect linear combination of the others\nThere is a quadratic term in the model without mean-centering the variable first\nThere are interactions between two or more continuous variables\n\nCan reduce this by mean-centering the variables first\n\nThere is a categorical predictor with very few observations in the baseline level"
  },
  {
    "objectID": "slides/10-mlr-inference.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/10-mlr-inference.html#detecting-multicollinearity-in-the-eda",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship\n\nLook at the distribution of categorical predictors and avoid setting the baseline to a category with very few observations"
  },
  {
    "objectID": "slides/10-mlr-inference.html#detecting-multicollinearity-vif",
    "href": "slides/10-mlr-inference.html#detecting-multicollinearity-vif",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/10-mlr-inference.html#detecting-multicollinearity-vif-1",
    "href": "slides/10-mlr-inference.html#detecting-multicollinearity-vif-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\n\nTypically \\(VIF &gt; 5\\) indicates concerning multicollinearity\nVariables with similar values of VIF are typically the ones correlated with each other\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/10-mlr-inference.html#vif-for-rail-trail-model",
    "href": "slides/10-mlr-inference.html#vif-for-rail-trail-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "VIF for rail trail model",
    "text": "VIF for rail trail model\n\nvif(rt_full_fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\n\nhightemp, avgtemp, and seasonSummer are correlated."
  },
  {
    "objectID": "slides/10-mlr-inference.html#solutions-for-multicollinearity",
    "href": "slides/10-mlr-inference.html#solutions-for-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Solutions for Multicollinearity",
    "text": "Solutions for Multicollinearity\n\nDrop some predictors\nCombine some predictors\nDiscount the individual coefficients and t-tests (i.e. predictions are meaningful but coefficients, tests, and confidence intervals are not)\n\n\nLet’s try removing hightemp and avgtemp (separately)"
  },
  {
    "objectID": "slides/10-mlr-inference.html#model-without-hightemp",
    "href": "slides/10-mlr-inference.html#model-without-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 &lt;- lm(volume ~ . - hightemp, data = rail_trail)\n  \nm1 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038"
  },
  {
    "objectID": "slides/10-mlr-inference.html#model-without-avgtemp",
    "href": "slides/10-mlr-inference.html#model-without-avgtemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 &lt;- lm(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/10-mlr-inference.html#choosing-a-model",
    "href": "slides/10-mlr-inference.html#choosing-a-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel without hightemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel without avgtemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\n\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model without avgtemp is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/10-mlr-inference.html#selected-model-for-now",
    "href": "slides/10-mlr-inference.html#selected-model-for-now",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Selected model (for now)",
    "text": "Selected model (for now)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/10-mlr-inference.html#topics-1",
    "href": "slides/10-mlr-inference.html#topics-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Topics",
    "text": "Topics\n\nLog transformation on the response variable\nLog transformation on the predictor variable"
  },
  {
    "objectID": "slides/10-mlr-inference.html#residuals-vs.-fitted-for-the-selected-model",
    "href": "slides/10-mlr-inference.html#residuals-vs.-fitted-for-the-selected-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. fitted for the selected model",
    "text": "Residuals vs. fitted for the selected model\n\nThe constant variance condition is not satisfied. We can transform the response variable to address the violation in this condition."
  },
  {
    "objectID": "slides/10-mlr-inference.html#identifying-a-need-to-transform-y",
    "href": "slides/10-mlr-inference.html#identifying-a-need-to-transform-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Identifying a need to transform \\(Y\\)",
    "text": "Identifying a need to transform \\(Y\\)\n\nTypically, a “fan-shaped” residual plot indicates the need for a transformation of the response variable \\(Y\\)\n\nThere are multiple ways to transform a variable, e.g., \\(\\sqrt{Y}\\), \\(1/Y\\), \\(\\log(Y)\\)\n\\(\\log(Y)\\) the most straightforward to interpret, so we use that transformation when possible\n\n\n\n\nWhen building a model:\n\nChoose a transformation and build the model on the transformed data\nReassess the residual plots\nIf the residuals plots did not sufficiently improve, try a new transformation!"
  },
  {
    "objectID": "slides/10-mlr-inference.html#log-transformation-on-y",
    "href": "slides/10-mlr-inference.html#log-transformation-on-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\n\nIf we apply a log transformation to the response variable, we want to estimate the parameters for the statistical model\n\n\\[\n\\log(Y) = \\beta_0+ \\beta_1 X_1 + \\dots +\\beta_pX_p + \\epsilon, \\hspace{10mm} \\epsilon \\sim N(0,\\sigma^2_\\epsilon)\n\\]\n\nThe regression equation is\n\n\\[\\widehat{\\log(Y)} = \\hat{\\beta}_0+ \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_pX_p\\]"
  },
  {
    "objectID": "slides/10-mlr-inference.html#log-transformation-on-y-1",
    "href": "slides/10-mlr-inference.html#log-transformation-on-y-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\nWe want to interpret the model in terms of the original variable \\(Y\\), not \\(\\log(Y)\\), so we need to write the regression equation in terms of \\(Y\\)\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\nThe predicted value \\(\\hat{Y}\\) is the predicted median of \\(Y\\). Note, when the distribution of \\(Y|X_1, \\ldots, X_p\\) is symmetric, then the median equals the mean. See the slides in the appendix for more detail."
  },
  {
    "objectID": "slides/10-mlr-inference.html#model-interpretation",
    "href": "slides/10-mlr-inference.html#model-interpretation",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n\n\nIntercept: When \\(X_1 = \\dots = X_p =0\\), \\(Y\\) is expected to be \\(\\exp\\{\\hat{\\beta}_0\\}\\)\nSlope: For every one unit increase in \\(X_j\\), the \\(Y\\) is expected to multiply by a factor of \\(\\exp\\{\\hat{\\beta}_j\\}\\), holding all else constant\n\n\nWhy is the interpretation in terms of a multiplicative change?"
  },
  {
    "objectID": "slides/10-mlr-inference.html#model-for-logvolume",
    "href": "slides/10-mlr-inference.html#model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model for \\(log(volume)\\)",
    "text": "Model for \\(log(volume)\\)\n\n#fit model\nlog_rt_fit &lt;- lm(log(volume) ~ hightemp + season + cloudcover + precip + day_type, data = rail_trail)\n\ntidy(log_rt_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327"
  },
  {
    "objectID": "slides/10-mlr-inference.html#interpretation-of-model-for-logvolume",
    "href": "slides/10-mlr-inference.html#interpretation-of-model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Interpretation of model for \\(\\log(volume)\\)",
    "text": "Interpretation of model for \\(\\log(volume)\\)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327\n\n\n\n\n\n\n\nInterpret the intercept in terms of (1) log(volume) and (2) volume.\nInterpret the coefficient of hightemp in terms of (1) log(volume) and (2) volume."
  },
  {
    "objectID": "slides/10-mlr-inference.html#residuals-for-model-with-logvolume",
    "href": "slides/10-mlr-inference.html#residuals-for-model-with-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals for model with \\(\\log(volume)\\)",
    "text": "Residuals for model with \\(\\log(volume)\\)"
  },
  {
    "objectID": "slides/10-mlr-inference.html#compare-residual-plots",
    "href": "slides/10-mlr-inference.html#compare-residual-plots",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Compare residual plots",
    "text": "Compare residual plots"
  },
  {
    "objectID": "slides/10-mlr-inference.html#log-transformation-on-x",
    "href": "slides/10-mlr-inference.html#log-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log Transformation on \\(X\\)",
    "text": "Log Transformation on \\(X\\)\n\nTry a transformation on \\(X\\) if the scatterplot shows some curvature but the variance is constant for all values of \\(X\\)"
  },
  {
    "objectID": "slides/10-mlr-inference.html#respiratory-rate-vs.-age",
    "href": "slides/10-mlr-inference.html#respiratory-rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Respiratory Rate vs. Age",
    "text": "Respiratory Rate vs. Age\n\nA high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a “high” rate, we first want to understand the relationship between a child’s age and their respiratory rate.\nThe data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the Sleuth3 R package and is originally form a 1994 publication “Reference Values for Respiratory Rate in the First 3 Years of Life”.\nVariables:\n\nAge: age in months\nRate: respiratory rate (breaths per minute)"
  },
  {
    "objectID": "slides/10-mlr-inference.html#rate-vs.-age",
    "href": "slides/10-mlr-inference.html#rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Rate vs. Age",
    "text": "Rate vs. Age"
  },
  {
    "objectID": "slides/10-mlr-inference.html#model-with-transformation-on-x",
    "href": "slides/10-mlr-inference.html#model-with-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model with Transformation on \\(X\\)",
    "text": "Model with Transformation on \\(X\\)\nSuppose we have the following regression equation:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\log(X)\\]\n\n\nIntercept: When \\(X = 1\\) \\((\\log(X) = 0)\\), \\(Y\\) is expected to be \\(\\hat{\\beta}_0\\) (i.e. the mean of \\(Y\\) is \\(\\hat{\\beta}_0\\))\nSlope: When \\(X\\) is multiplied by a factor of \\(\\mathbf{C}\\), the mean of \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(C)}\\) units\n\nExample: when \\(X\\) is multiplied by a factor of 2, \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(2)}\\) units"
  },
  {
    "objectID": "slides/10-mlr-inference.html#model-interpretation-1",
    "href": "slides/10-mlr-inference.html#model-interpretation-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n50.135\n0.632\n79.330\n0\n\n\nlog(Age)\n-5.982\n0.263\n-22.781\n0\n\n\n\n\n\n\\[\\hat{\\text{Rate}} = 50.135 - 5.982 \\times \\log\\text{(Age)}\\]\n\n\nInterpret the intercept in the context of the data.\nInterpret the slope in terms of age multiplying by 2 in the context of the data."
  },
  {
    "objectID": "slides/10-mlr-inference.html#learn-more",
    "href": "slides/10-mlr-inference.html#learn-more",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Learn more",
    "text": "Learn more\nSee Log Transformations in Linear Regression for more details about interpreting regression models with log-transformed variables."
  },
  {
    "objectID": "slides/10-mlr-inference.html#why-medianyx-instead-of-mu_yx",
    "href": "slides/10-mlr-inference.html#why-medianyx-instead-of-mu_yx",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\nSuppose we have a set of values\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet’s calculate \\(\\overline{\\log(x)}\\)\n\nlog_x &lt;- log(x)\nmean(log_x)\n\n[1] 2.066476\n\n\n\nLet’s calculate \\(\\log(\\bar{x})\\)\n\nxbar &lt;- mean(x)\nlog(xbar)\n\n[1] 2.228477\n\n\n\n\n\n\n\nNote: \\(\\overline{\\log(x)} \\neq \\log(\\bar{x})\\)"
  },
  {
    "objectID": "slides/10-mlr-inference.html#why-medianyx-instead-of-mu_yx-1",
    "href": "slides/10-mlr-inference.html#why-medianyx-instead-of-mu_yx-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet’s calculate \\(\\text{Median}(\\log(x))\\)\n\nlog_x &lt;- log(x)\nmedian(log_x)\n\n[1] 2.079442\n\n\n\nLet’s calculate \\(\\log(\\text{Median}(x))\\)\n\nmedian_x &lt;- median(x)\nlog(median_x)\n\n[1] 2.079442\n\n\n\n\n\n\n\nNote: \\(\\text{Median} (\\log(x)) = \\log(\\text{Median}(x))\\)"
  },
  {
    "objectID": "slides/10-mlr-inference.html#mean-median-and-log",
    "href": "slides/10-mlr-inference.html#mean-median-and-log",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean, Median, and Log",
    "text": "Mean, Median, and Log\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\\[\\overline{\\log(x)} \\neq \\log(\\bar{x})\\]\n\nmean(log_x) == log(xbar)\n\n[1] FALSE\n\n\n\n\\[\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))\\]\n\nmedian(log_x) == log(median_x)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/10-mlr-inference.html#mean-and-median-of-logy",
    "href": "slides/10-mlr-inference.html#mean-and-median-of-logy",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(Y)\\)",
    "text": "Mean and median of \\(\\log(Y)\\)\n\nRecall that \\(Y = \\beta_0 + \\beta_1 X\\) is the mean value of the response at the given value of the predictor \\(X\\). This doesn’t hold when we log-transform the response variable.\nMathematically, the mean of the logged values is not necessarily equal to the log of the mean value. Therefore at a given value of \\(X\\)\n\n\n\\[\n\\begin{aligned}\\exp\\{\\text{Mean}(\\log(Y|X))\\} \\neq \\text{Mean}(Y|X) \\\\[5pt]\n\\Rightarrow \\exp\\{\\beta_0 + \\beta_1 X\\} \\neq \\text{Mean}(Y|X) \\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference.html#mean-and-median-of-logy-1",
    "href": "slides/10-mlr-inference.html#mean-and-median-of-logy-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(y)\\)",
    "text": "Mean and median of \\(\\log(y)\\)\n\nHowever, the median of the logged values is equal to the log of the median value. Therefore,\n\n\\[\\exp\\{\\text{Median}(\\log(Y|X))\\} = \\text{Median}(Y|X)\\]\n\n\nIf the distribution of \\(\\log(Y)\\) is symmetric about the regression line, for a given value \\(X\\), we can expect \\(Mean(Y)\\) and \\(Median(Y)\\) to be approximately equal.\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/12-model-workflow.html#announcements",
    "href": "slides/12-model-workflow.html#announcements",
    "title": "Feature engineering",
    "section": "Announcements",
    "text": "Announcements\n\nLab 05 due October 20\nProf. Tackett will not have office hours on Friday\n\nEmail to schedule an appointment if you need to meet\nAll other office hours on regular schedule\n\n\n\n🍁 Have a good Fall Break! 🍁"
  },
  {
    "objectID": "slides/12-model-workflow.html#topics",
    "href": "slides/12-model-workflow.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\n\nFeature engineering with recipes\nWorkflows to bring together models and recipes\nRMSE and \\(R^2\\) for model evaluation on training and tests sets"
  },
  {
    "objectID": "slides/12-model-workflow.html#computational-setup",
    "href": "slides/12-model-workflow.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(gghighlight)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/12-model-workflow.html#the-office",
    "href": "slides/12-model-workflow.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/12-model-workflow.html#data-goal",
    "href": "slides/12-model-workflow.html#data-goal",
    "title": "Feature engineering",
    "section": "Data & goal",
    "text": "Data & goal\n\nData: The data come from data.world, by way of TidyTuesday\nGoal: Predict imdb_rating from other variables in the dataset\n\n\n\n# A tibble: 188 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ℹ 178 more rows"
  },
  {
    "objectID": "slides/12-model-workflow.html#train-test",
    "href": "slides/12-model-workflow.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings) # prop = 3/4 by default\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/12-model-workflow.html#training-data",
    "href": "slides/12-model-workflow.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 × 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ℹ 131 more rows"
  },
  {
    "objectID": "slides/12-model-workflow.html#recap-feature-engineering",
    "href": "slides/12-model-workflow.html#recap-feature-engineering",
    "title": "Feature engineering",
    "section": "Recap: Feature engineering",
    "text": "Recap: Feature engineering\n\nWe prefer simple (parsimonious) models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance)"
  },
  {
    "objectID": "slides/12-model-workflow.html#recap-modeling-workflow-revisited",
    "href": "slides/12-model-workflow.html#recap-modeling-workflow-revisited",
    "title": "Feature engineering",
    "section": "Recap: Modeling workflow, revisited",
    "text": "Recap: Modeling workflow, revisited\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/12-model-workflow.html#initiate-a-recipe",
    "href": "slides/12-model-workflow.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloguing names and types of variables\n  )\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-1-alter-roles",
    "href": "slides/12-model-workflow.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn’t a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec |&gt;\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-2-add-features",
    "href": "slides/12-model-workflow.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec |&gt;\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date"
  },
  {
    "objectID": "slides/12-model-workflow.html#working-with-recipes",
    "href": "slides/12-model-workflow.html#working-with-recipes",
    "title": "Feature engineering",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don’t get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n\n\n\n\n\n\n\nNote\n\n\nThis is optional, we’ll show the results for demonstrative purposes. It doesn’t need to be part of your modeling pipeline, but it can be assuring to see the effects of the recipe steps as you build the recipe."
  },
  {
    "objectID": "slides/12-model-workflow.html#step-2-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-2-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 2: Prep and bake",
    "text": "Step 2: Prep and bake\n\n# determine required parameters to be estimated\noffice_rec |&gt;\n  prep() |&gt;\n# apply recipe computations to data\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 8\n$ season         &lt;dbl&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7, 6, 5, 2, 2…\n$ episode        &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1, 20,…\n$ title          &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Performance Review…\n$ total_votes    &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 1445, 1…\n$ air_date       &lt;date&gt; 2012-03-08, 2013-01-31, 2005-11-15, 2012-10-25, 2007-0…\n$ imdb_rating    &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8.7, …\n$ air_date_dow   &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu, Thu, Thu, …\n$ air_date_month &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr, Jan, May, …"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-3-add-more-features",
    "href": "slides/12-model-workflow.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec |&gt;\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-3-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-3-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 3: Prep and bake",
    "text": "Step 3: Prep and bake\n\noffice_rec |&gt;\n  prep() |&gt;\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 11\n$ season                     &lt;dbl&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7…\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n$ air_date_dow               &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu…\n$ air_date_month             &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr…\n$ air_date_USThanksgivingDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USChristmasDay    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USNewYearsDay     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USIndependenceDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-4-convert-numbers-to-factors",
    "href": "slides/12-model-workflow.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec |&gt;\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-4-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-4-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 4: Prep and bake",
    "text": "Step 4: Prep and bake\n\noffice_rec |&gt;\n  prep() |&gt;\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 11\n$ season                     &lt;fct&gt; 8, 9, 2, 9, 3, 7, 3, 9, 9, 8, 5, 5, 9, 6, 7…\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n$ air_date_dow               &lt;fct&gt; Thu, Thu, Tue, Thu, Thu, Thu, Thu, Thu, Thu…\n$ air_date_month             &lt;fct&gt; Mar, Jan, Nov, Oct, May, Sep, Feb, May, Apr…\n$ air_date_USThanksgivingDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USChristmasDay    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USNewYearsDay     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USIndependenceDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-5-make-dummy-variables",
    "href": "slides/12-model-workflow.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec |&gt;\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-5-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-5-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 5: Prep and bake",
    "text": "Step 5: Prep and bake\n\n# determine required parameters to be estimated\noffice_rec |&gt;\n  prep() |&gt;\n# apply recipe computations to data\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 33\n$ episode                    &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26…\n$ title                      &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Perfor…\n$ total_votes                &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2…\n$ imdb_rating                &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0…\n$ air_date_USThanksgivingDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USChristmasDay    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USNewYearsDay     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_USIndependenceDay &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ season_X2                  &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ season_X3                  &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ season_X4                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ season_X5                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0…\n$ season_X6                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ season_X7                  &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1…\n$ season_X8                  &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…\n$ season_X9                  &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0…\n$ air_date_dow_Mon           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_dow_Tue           &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_dow_Wed           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_dow_Thu           &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…\n$ air_date_dow_Fri           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_dow_Sat           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Feb         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Mar         &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Apr         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1…\n$ air_date_month_May         &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0…\n$ air_date_month_Jun         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Jul         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Aug         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Sep         &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0…\n$ air_date_month_Oct         &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Nov         &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…\n$ air_date_month_Dec         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-6-remove-zero-variance-predictors",
    "href": "slides/12-model-workflow.html#step-6-remove-zero-variance-predictors",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance predictors",
    "text": "Step 6: Remove zero variance predictors\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec |&gt;\n  step_zv(all_predictors())\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/12-model-workflow.html#step-6-prep-and-bake",
    "href": "slides/12-model-workflow.html#step-6-prep-and-bake",
    "title": "Feature engineering",
    "section": "Step 6: Prep and bake",
    "text": "Step 6: Prep and bake\n\noffice_rec |&gt;\n  prep() |&gt;\n  bake(office_train) |&gt;\n  glimpse()\n\nRows: 141\nColumns: 22\n$ episode            &lt;dbl&gt; 18, 14, 8, 5, 22, 1, 15, 21, 18, 12, 25, 26, 12, 1,…\n$ title              &lt;fct&gt; \"Last Day in Florida\", \"Vandalism\", \"Performance Re…\n$ total_votes        &lt;dbl&gt; 1429, 1402, 2416, 1515, 2783, 1897, 2283, 2041, 144…\n$ imdb_rating        &lt;dbl&gt; 7.8, 7.6, 8.2, 7.1, 9.1, 8.4, 8.3, 8.9, 8.0, 8.0, 8…\n$ season_X2          &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ season_X3          &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ season_X4          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ season_X5          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, …\n$ season_X6          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, …\n$ season_X7          &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ season_X8          &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, …\n$ season_X9          &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, …\n$ air_date_dow_Tue   &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ air_date_dow_Thu   &lt;dbl&gt; 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ air_date_month_Feb &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ air_date_month_Mar &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ air_date_month_Apr &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ air_date_month_May &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, …\n$ air_date_month_Sep &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, …\n$ air_date_month_Oct &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, …\n$ air_date_month_Nov &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, …\n$ air_date_month_Dec &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …"
  },
  {
    "objectID": "slides/12-model-workflow.html#putting-it-all-together",
    "href": "slides/12-model-workflow.html#putting-it-all-together",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  # make title's role ID\n  update_role(title, new_role = \"ID\") |&gt;\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) |&gt;\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) |&gt;\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) |&gt;\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/12-model-workflow.html#putting-it-all-together-1",
    "href": "slides/12-model-workflow.html#putting-it-all-together-1",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/12-model-workflow.html#recipe-workflow",
    "href": "slides/12-model-workflow.html#recipe-workflow",
    "title": "Feature engineering",
    "section": "Recipe workflow",
    "text": "Recipe workflow\n\nArtwork by @allison_horst"
  },
  {
    "objectID": "slides/12-model-workflow.html#specify-model",
    "href": "slides/12-model-workflow.html#specify-model",
    "title": "Feature engineering",
    "section": "Specify model",
    "text": "Specify model\n\noffice_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\noffice_spec\n\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/12-model-workflow.html#build-workflow",
    "href": "slides/12-model-workflow.html#build-workflow",
    "title": "Feature engineering",
    "section": "Build workflow",
    "text": "Build workflow\nWorkflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\noffice_wflow &lt;- workflow() |&gt;\n  add_model(office_spec) |&gt;\n  add_recipe(office_rec)\n\n\nSee next slide for workflow…"
  },
  {
    "objectID": "slides/12-model-workflow.html#view-workflow",
    "href": "slides/12-model-workflow.html#view-workflow",
    "title": "Feature engineering",
    "section": "View workflow",
    "text": "View workflow\n\noffice_wflow\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n5 Recipe Steps\n\n• step_date()\n• step_holiday()\n• step_num2factor()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/12-model-workflow.html#fit-model-to-training-data",
    "href": "slides/12-model-workflow.html#fit-model-to-training-data",
    "title": "Feature engineering",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\noffice_fit &lt;- office_wflow |&gt;\n  fit(data = office_train)\n\ntidy(office_fit)\n\n# A tibble: 21 × 5\n   term         estimate std.error statistic  p.value\n   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)  6.40     0.510        12.5   1.51e-23\n 2 episode     -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes  0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2    0.811    0.327         2.48  1.44e- 2\n 5 season_X3    1.04     0.343         3.04  2.91e- 3\n 6 season_X4    1.09     0.295         3.70  3.32e- 4\n 7 season_X5    1.08     0.348         3.11  2.34e- 3\n 8 season_X6    1.00     0.367         2.74  7.18e- 3\n 9 season_X7    1.02     0.352         2.89  4.52e- 3\n10 season_X8    0.497    0.348         1.43  1.55e- 1\n# ℹ 11 more rows\n\n\n\n\nSo many predictors!"
  },
  {
    "objectID": "slides/12-model-workflow.html#model-fit-summary",
    "href": "slides/12-model-workflow.html#model-fit-summary",
    "title": "Feature engineering",
    "section": "Model fit summary",
    "text": "Model fit summary\n\ntidy(office_fit) |&gt; print(n = 21)\n\n# A tibble: 21 × 5\n   term                estimate std.error statistic  p.value\n   &lt;chr&gt;                  &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)         6.40     0.510        12.5   1.51e-23\n 2 episode            -0.00393  0.0171       -0.230 8.18e- 1\n 3 total_votes         0.000375 0.0000414     9.07  2.75e-15\n 4 season_X2           0.811    0.327         2.48  1.44e- 2\n 5 season_X3           1.04     0.343         3.04  2.91e- 3\n 6 season_X4           1.09     0.295         3.70  3.32e- 4\n 7 season_X5           1.08     0.348         3.11  2.34e- 3\n 8 season_X6           1.00     0.367         2.74  7.18e- 3\n 9 season_X7           1.02     0.352         2.89  4.52e- 3\n10 season_X8           0.497    0.348         1.43  1.55e- 1\n11 season_X9           0.621    0.345         1.80  7.41e- 2\n12 air_date_dow_Tue    0.382    0.422         0.904 3.68e- 1\n13 air_date_dow_Thu    0.284    0.389         0.731 4.66e- 1\n14 air_date_month_Feb -0.0597   0.132        -0.452 6.52e- 1\n15 air_date_month_Mar -0.0752   0.156        -0.481 6.31e- 1\n16 air_date_month_Apr  0.0954   0.177         0.539 5.91e- 1\n17 air_date_month_May  0.156    0.213         0.734 4.64e- 1\n18 air_date_month_Sep -0.0776   0.223        -0.348 7.28e- 1\n19 air_date_month_Oct -0.176    0.174        -1.01  3.13e- 1\n20 air_date_month_Nov -0.156    0.149        -1.05  2.98e- 1\n21 air_date_month_Dec  0.170    0.149         1.14  2.55e- 1"
  },
  {
    "objectID": "slides/12-model-workflow.html#make-predictions-for-training-data",
    "href": "slides/12-model-workflow.html#make-predictions-for-training-data",
    "title": "Feature engineering",
    "section": "Make predictions for training data",
    "text": "Make predictions for training data\n\noffice_train_pred &lt;- predict(office_fit, office_train) |&gt;\n  bind_cols(office_train)\n\noffice_train_pred\n\n# A tibble: 141 × 7\n   .pred season episode title               imdb_rating total_votes air_date  \n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1  7.57      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2  7.77      9      14 Vandalism                   7.6        1402 2013-01-31\n 3  8.31      2       8 Performance Review          8.2        2416 2005-11-15\n 4  7.67      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5  8.84      3      22 Beach Games                 9.1        2783 2007-05-10\n 6  8.33      7       1 Nepotism                    8.4        1897 2010-09-23\n 7  8.46      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8  8.14      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9  7.87      9      18 Promos                      8          1445 2013-04-04\n10  7.74      8      12 Pool Party                  8          1612 2012-01-19\n# ℹ 131 more rows"
  },
  {
    "objectID": "slides/12-model-workflow.html#r-squared",
    "href": "slides/12-model-workflow.html#r-squared",
    "title": "Feature engineering",
    "section": "R-squared",
    "text": "R-squared\nPercentage of variability in the IMDB ratings explained by the model.\n\n\nrsq(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.670\n\n\n\n\n\nAre models with high or low \\(R^2\\) more preferable?"
  },
  {
    "objectID": "slides/12-model-workflow.html#rmse",
    "href": "slides/12-model-workflow.html#rmse",
    "title": "Feature engineering",
    "section": "RMSE",
    "text": "RMSE\nAn alternative model performance statistic: root mean square error.\n\\[ RMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}} \\]\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.302\n\n\n\n\n\nAre models with high or low RMSE are more preferable?"
  },
  {
    "objectID": "slides/12-model-workflow.html#interpreting-rmse",
    "href": "slides/12-model-workflow.html#interpreting-rmse",
    "title": "Feature engineering",
    "section": "Interpreting RMSE",
    "text": "Interpreting RMSE\n\nIs this RMSE considered low or high?\n\n\nrmse(office_train_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.302\n\n\n\n\nDepends…\n\noffice_train |&gt;\n  summarise(min = min(imdb_rating), max = max(imdb_rating))\n\n# A tibble: 1 × 2\n    min   max\n  &lt;dbl&gt; &lt;dbl&gt;\n1   6.7   9.7"
  },
  {
    "objectID": "slides/12-model-workflow.html#but-really",
    "href": "slides/12-model-workflow.html#but-really",
    "title": "Feature engineering",
    "section": "But, really…",
    "text": "But, really…\nwho cares about predictions on training data?"
  },
  {
    "objectID": "slides/12-model-workflow.html#make-predictions-for-testing-data",
    "href": "slides/12-model-workflow.html#make-predictions-for-testing-data",
    "title": "Feature engineering",
    "section": "Make predictions for testing data",
    "text": "Make predictions for testing data\n\noffice_test_pred &lt;- predict(office_fit, office_test) |&gt;\n  bind_cols(office_test)\n\noffice_test_pred\n\n# A tibble: 47 × 7\n   .pred season episode title               imdb_rating total_votes air_date  \n   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1  8.03      1       2 Diversity Day               8.3        3566 2005-03-29\n 2  7.98      1       3 Health Care                 7.9        2983 2005-04-05\n 3  8.41      2       4 The Fire                    8.4        2713 2005-10-11\n 4  8.35      2       5 Halloween                   8.2        2561 2005-10-18\n 5  8.35      2       9 E-Mail Surveillance         8.4        2527 2005-11-22\n 6  8.68      2      12 The Injury                  9          3282 2006-01-12\n 7  8.32      2      14 The Carpet                  7.9        2342 2006-01-26\n 8  8.93      2      22 Casino Night                9.3        3644 2006-05-11\n 9  8.80      3       1 Gay Witch Hunt              8.9        3087 2006-09-21\n10  8.37      3       5 Initiation                  8.2        2254 2006-10-19\n# ℹ 37 more rows"
  },
  {
    "objectID": "slides/12-model-workflow.html#evaluate-performance-for-testing-data",
    "href": "slides/12-model-workflow.html#evaluate-performance-for-testing-data",
    "title": "Feature engineering",
    "section": "Evaluate performance for testing data",
    "text": "Evaluate performance for testing data\nRMSE of model fit to testing data\n\nrmse(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard       0.411\n\n\nR-sq of model fit to testing data\n\nrsq(office_test_pred, truth = imdb_rating, estimate = .pred)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.468"
  },
  {
    "objectID": "slides/12-model-workflow.html#training-vs.-testing",
    "href": "slides/12-model-workflow.html#training-vs.-testing",
    "title": "Feature engineering",
    "section": "Training vs. testing",
    "text": "Training vs. testing\n\n\n\n\n\n\n\n\n\nmetric\ntrain\ntest\ncomparison\n\n\n\n\nRMSE\n0.302\n0.411\nRMSE lower for training\n\n\nR-squared\n0.67\n0.468\nR-squared higher for training"
  },
  {
    "objectID": "slides/12-model-workflow.html#evaluating-performance-on-training-data",
    "href": "slides/12-model-workflow.html#evaluating-performance-on-training-data",
    "title": "Feature engineering",
    "section": "Evaluating performance on training data",
    "text": "Evaluating performance on training data\n\nThe training set does not have the capacity to be a good arbiter of performance.\nIt is not an independent piece of information; predicting the training set can only reflect what the model already knows.\nSuppose you give a class a test, then give them the answers, then provide the same test. The student scores on the second test do not accurately reflect what they know about the subject; these scores would probably be higher than their results on the first test."
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#announcements",
    "href": "slides/14-cross-validation-pt2.html#announcements",
    "title": "Cross validation application",
    "section": "Announcements",
    "text": "Announcements\n\nSee Ed Discussion for upcoming events and internship opportunities\nStatistics Experience due Mon, Nov 20 at 11:59pm\nProf. Tackett office hours Fridays 1:30 - 3:30pm for the rest of the semester\nStart the final project in lab this week - start thinking about the data your team wants to use"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#mid-semester-survey",
    "href": "slides/14-cross-validation-pt2.html#mid-semester-survey",
    "title": "Cross validation application",
    "section": "Mid-semester survey",
    "text": "Mid-semester survey\nThank you to everyone who filled out the mid-semester survey!\nAspect of class most helpful with learning\n\nApplication exercises\nLectures\nDiscussing content with others\n\nSomething to do in class to better help with learning\n\nZooming out more / reminder of the big picture\nTaking time to finish AEs (perhaps do some of this in lab)\nMore conceptual questions on assignments, specifically HW\n\nThings you do that are helpful with learning\n\nAttend office hours!\nReview course materials\nLots practice - review AEs, HW, labs"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#mid-semester-survey-1",
    "href": "slides/14-cross-validation-pt2.html#mid-semester-survey-1",
    "title": "Cross validation application",
    "section": "Mid-semester survey",
    "text": "Mid-semester survey\nWhy we do in-class exams\n\nOpportunity to demonstrate understanding of concepts and how they apply to application\n\nThis is what will make you stand out as a statistician/ data scientist!\n\nIn-class provides the most “level” playing field to demonstrate conceptual understanding, given all the online resources available now\nLots of other opportunities to demonstrate application skills through labs, HW, final project, and take-home portion of exam"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#statistician-of-the-day-felicity-enders",
    "href": "slides/14-cross-validation-pt2.html#statistician-of-the-day-felicity-enders",
    "title": "Cross validation application",
    "section": "Statistician of the day: Felicity Enders",
    "text": "Statistician of the day: Felicity Enders\n\n\n\n\n\n\n\n\nDr. Felicity Enders received her PhD from Johns Hopkins Bloomberg School of Public Health. She is a Professor of Biostatistics at the Mayo Clinic. With close to 200 publications, she has worked closely with clinicians, with particular focus on women’s health and psychology. Across the medical spectrum, Dr. Enders has provided advanced statistical modeling collaboration in clinical trials.\nShe is also passionate about biostatistics education and works to dissolve the hidden curriculum for research, particularly statistical knowledge needed for non-statisticians.\n\n\nSource: hardin47.github.io/CURV/scholars/enders"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#felicity-enders",
    "href": "slides/14-cross-validation-pt2.html#felicity-enders",
    "title": "Cross validation application",
    "section": "Felicity Enders",
    "text": "Felicity Enders\nDr. Enders was a statistician on an interdisciplinary research team that used logistic regression to identify demographic, clinical, and laboratory variables associated with the presence (or absence) of advanced fibrosis with the aim to create a scoring system that could be used by clinicians.\n“Data from each of the 4 countries were randomly separated into 2/3 and 1/3 of patients for model building and model validation, respectively. Hence, data on 480 patients were used to build a model, whereas data on 253 patients were used to validate the model.”\n“…cross-validation was used with 20 subgroups, so that at most 5% of the data under consideration was excluded at any one time. By employing cross-validation, the possibility of an unusually positive or negative validation subset could be assessed.”\nAngulo, Paul, et al. “The NAFLD fibrosis score: a noninvasive system that identifies liver fibrosis in patients with NAFLD.” Hepatology 45.4 (2007): 846-854."
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#topics",
    "href": "slides/14-cross-validation-pt2.html#topics",
    "title": "Cross validation application",
    "section": "Topics",
    "text": "Topics\n\n\nCross validation application exercise"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#computational-setup",
    "href": "slides/14-cross-validation-pt2.html#computational-setup",
    "title": "Cross validation application",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#data-restaurant-tips",
    "href": "slides/14-cross-validation-pt2.html#data-restaurant-tips",
    "title": "Cross validation application",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 × 5\n     Tip Party Meal   Age    Alcohol\n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;chr&gt;  \n 1  2.99     1 Dinner Yadult No     \n 2  2        1 Dinner Yadult No     \n 3  5        1 Dinner SenCit No     \n 4  4        3 Dinner Middle No     \n 5 10.3      2 Dinner SenCit No     \n 6  4.85     2 Dinner Middle No     \n 7  5        4 Dinner Yadult No     \n 8  4        3 Dinner Middle No     \n 9  5        2 Dinner Middle No     \n10  1.58     1 Dinner SenCit No     \n# ℹ 159 more rows"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#variables",
    "href": "slides/14-cross-validation-pt2.html#variables",
    "title": "Cross validation application",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\nAlcohol: Whether the party ordered alcohol with the meal (Yes, No)\n\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#outcome-tip",
    "href": "slides/14-cross-validation-pt2.html#outcome-tip",
    "title": "Cross validation application",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#predictors",
    "href": "slides/14-cross-validation-pt2.html#predictors",
    "title": "Cross validation application",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#outcome-vs.-predictors",
    "href": "slides/14-cross-validation-pt2.html#outcome-vs.-predictors",
    "title": "Cross validation application",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#analysis-goal",
    "href": "slides/14-cross-validation-pt2.html#analysis-goal",
    "title": "Cross validation application",
    "section": "Analysis goal",
    "text": "Analysis goal\n\nUse cross validation to evaluate and select a model to predict the tip amount\n\n\nv-fold cross validation – commonly used resampling technique:\n\nRandomly split your training data into v partitions\nUse v-1 partitions for analysis, and the remaining 1 partition for assessment\nRepeat v times, updating which partition is used for assessment each time"
  },
  {
    "objectID": "slides/14-cross-validation-pt2.html#modeling-workflow",
    "href": "slides/14-cross-validation-pt2.html#modeling-workflow",
    "title": "Cross validation application",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\nSplit data into training and test sets.\nUse cross validation on the training set to fit, evaluate, and compare candidate models. Choose a final model based on summary of cross validation results.\nRefit the model using the entire training set and do “final” evaluation on the test set (make sure you have not overfit the model).\n\nAdjust as needed if there is evidence of overfit.\n\nUse model fit on training set for inference and prediction.\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/16-logistic-intro.html#announcements",
    "href": "slides/16-logistic-intro.html#announcements",
    "title": "Logistic regression",
    "section": "Announcements",
    "text": "Announcements\n\nHW 03 due Wed, Nov 1 at 11:59pm\nSee Ed Discussion for\n\nCode to reformat Theme variable in HW 03 data set\nExplanation on interpreting models with quadratic terms"
  },
  {
    "objectID": "slides/16-logistic-intro.html#spring-2024-statistics-courses",
    "href": "slides/16-logistic-intro.html#spring-2024-statistics-courses",
    "title": "Logistic regression",
    "section": "Spring 2024 statistics courses",
    "text": "Spring 2024 statistics courses\n\nSTA 211: Mathematics of Regression\n\nPrereqs: MATH 216/218/221, STA 210\n\nSTA 230 or STA 240: Probability\n\nPrereqs: MATH 22/112/122/202/212/219/222\n\nSTA 310: Generalized Linear Models\n\nPrereqs: STA 210 and STA 230/240\n\nSTA 313: Advanced Data Visualization\n\nPrereqs: STA 198 or STA 199 or STA 210\n\nSTA 323: Statistical Computing\n\nPrereqs: STA 210 and STA 230/240\n\nSTA 360: Bayesian Inference and Modern Statistical Methods\n\nPrereqs: STA 210 and STA 230/240 and MATH 202/212/219/222 and CS 101/102/201 and MATH 216/218/221, 211 (co-req)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#interpreting-models-with-log-transformed-variables",
    "href": "slides/16-logistic-intro.html#interpreting-models-with-log-transformed-variables",
    "title": "Logistic regression",
    "section": "Interpreting models with log-transformed variables",
    "text": "Interpreting models with log-transformed variables\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327\n\n\n\n\n\n\n\nInterpret the intercept in terms of (1) log(volume) and (2) volume.\nInterpret the coefficient of hightemp in terms of (1) log(volume) and (2) volume."
  },
  {
    "objectID": "slides/16-logistic-intro.html#topics",
    "href": "slides/16-logistic-intro.html#topics",
    "title": "Logistic regression",
    "section": "Topics",
    "text": "Topics\n\nLogistic regression for binary response variable\nRelationship between odds and probabilities\nUse logistic regression model to calculate predicted odds and probabilities"
  },
  {
    "objectID": "slides/16-logistic-intro.html#computational-setup",
    "href": "slides/16-logistic-intro.html#computational-setup",
    "title": "Logistic regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(Stat2Data) #contains data set\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/16-logistic-intro.html#types-of-outcome-variables",
    "href": "slides/16-logistic-intro.html#types-of-outcome-variables",
    "title": "Logistic regression",
    "section": "Types of outcome variables",
    "text": "Types of outcome variables\nQuantitative outcome variable:\n\nSales price of a house in Duke Forest\nModel: Expected sales price given the number of bedrooms, lot size, etc.\n\n\nCategorical outcome variable:\n\nIndicator of being high risk of getting coronary heart disease in the next 10 years\nModel: Probability an adult is high risk of heart disease in the next 10 years given their age, total cholesterol, etc."
  },
  {
    "objectID": "slides/16-logistic-intro.html#models-for-categorical-outcomes",
    "href": "slides/16-logistic-intro.html#models-for-categorical-outcomes",
    "title": "Logistic regression",
    "section": "Models for categorical outcomes",
    "text": "Models for categorical outcomes\n\n\nLogistic regression\n2 Outcomes\n1: Yes, 0: No\n\nMultinomial logistic regression\n3+ Outcomes\n1: Democrat, 2: Republican, 3: Independent"
  },
  {
    "objectID": "slides/16-logistic-intro.html#election-forecasts",
    "href": "slides/16-logistic-intro.html#election-forecasts",
    "title": "Logistic regression",
    "section": "2022 election forecasts",
    "text": "2022 election forecasts\n\nSource: FiveThirtyEight 2022 Election Forecasts"
  },
  {
    "objectID": "slides/16-logistic-intro.html#nba-finals-predictions",
    "href": "slides/16-logistic-intro.html#nba-finals-predictions",
    "title": "Logistic regression",
    "section": "2020 NBA finals predictions",
    "text": "2020 NBA finals predictions\n\nSource: FiveThirtyEight 2019-20 NBA Predictions"
  },
  {
    "objectID": "slides/16-logistic-intro.html#do-teenagers-get-7-hours-of-sleep",
    "href": "slides/16-logistic-intro.html#do-teenagers-get-7-hours-of-sleep",
    "title": "Logistic regression",
    "section": "Do teenagers get 7+ hours of sleep?",
    "text": "Do teenagers get 7+ hours of sleep?\n\n\nStudents in grades 9 - 12 surveyed about health risk behaviors including whether they usually get 7 or more hours of sleep.\nSleep7\n1: yes\n0: no\n\n\ndata(YouthRisk2009) #from Stat2Data package\nsleep &lt;- YouthRisk2009 |&gt;\n  as_tibble() |&gt;\n  filter(!is.na(Age), !is.na(Sleep7))\nsleep |&gt;\n  relocate(Age, Sleep7)\n\n# A tibble: 446 × 6\n     Age Sleep7 Sleep           SmokeLife SmokeDaily MarijuaEver\n   &lt;int&gt;  &lt;int&gt; &lt;fct&gt;           &lt;fct&gt;     &lt;fct&gt;            &lt;int&gt;\n 1    16      1 8 hours         Yes       Yes                  1\n 2    17      0 5 hours         Yes       Yes                  1\n 3    18      0 5 hours         Yes       Yes                  1\n 4    17      1 7 hours         Yes       No                   1\n 5    15      0 4 or less hours No        No                   0\n 6    17      0 6 hours         No        No                   0\n 7    17      1 7 hours         No        No                   0\n 8    16      1 8 hours         Yes       No                   0\n 9    16      1 8 hours         No        No                   0\n10    18      0 4 or less hours Yes       Yes                  1\n# ℹ 436 more rows"
  },
  {
    "objectID": "slides/16-logistic-intro.html#plot-the-data",
    "href": "slides/16-logistic-intro.html#plot-the-data",
    "title": "Logistic regression",
    "section": "Plot the data",
    "text": "Plot the data\n\nggplot(sleep, aes(x = Age, y = Sleep7)) +\n  geom_point() + \n  labs(y = \"Getting 7+ hours of sleep\")"
  },
  {
    "objectID": "slides/16-logistic-intro.html#lets-fit-a-linear-regression-model",
    "href": "slides/16-logistic-intro.html#lets-fit-a-linear-regression-model",
    "title": "Logistic regression",
    "section": "Let’s fit a linear regression model",
    "text": "Let’s fit a linear regression model\nOutcome: \\(Y\\) = 1: yes, 0: no"
  },
  {
    "objectID": "slides/16-logistic-intro.html#lets-use-proportions",
    "href": "slides/16-logistic-intro.html#lets-use-proportions",
    "title": "Logistic regression",
    "section": "Let’s use proportions",
    "text": "Let’s use proportions\nOutcome: Probability of getting 7+ hours of sleep"
  },
  {
    "objectID": "slides/16-logistic-intro.html#what-happens-if-we-zoom-out",
    "href": "slides/16-logistic-intro.html#what-happens-if-we-zoom-out",
    "title": "Logistic regression",
    "section": "What happens if we zoom out?",
    "text": "What happens if we zoom out?\nOutcome: Probability of getting 7+ hours of sleep\n\n🛑 This model produces predictions outside of 0 and 1."
  },
  {
    "objectID": "slides/16-logistic-intro.html#lets-try-another-model",
    "href": "slides/16-logistic-intro.html#lets-try-another-model",
    "title": "Logistic regression",
    "section": "Let’s try another model",
    "text": "Let’s try another model\n\n✅ This model (called a logistic regression model) only produces predictions between 0 and 1."
  },
  {
    "objectID": "slides/16-logistic-intro.html#the-code",
    "href": "slides/16-logistic-intro.html#the-code",
    "title": "Logistic regression",
    "section": "The code",
    "text": "The code\n\nggplot(sleep_age, aes(x = Age, y = prop)) +\n  geom_point() + \n  geom_hline(yintercept = c(0,1), lty = 2) + \n  stat_smooth(method =\"glm\", method.args = list(family = \"binomial\"), \n              fullrange = TRUE, se = FALSE) +\n  labs(y = \"P(7+ hours of sleep)\") +\n  xlim(1, 40) +\n  ylim(-0.5, 1.5)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#different-types-of-models",
    "href": "slides/16-logistic-intro.html#different-types-of-models",
    "title": "Logistic regression",
    "section": "Different types of models",
    "text": "Different types of models\n\n\n\n\n\n\n\n\nMethod\nOutcome\nModel\n\n\n\n\nLinear regression\nQuantitative\n\\(Y = \\beta_0 + \\beta_1~ X\\)\n\n\nLinear regression (transform Y)\nQuantitative\n\\(\\log(Y) = \\beta_0 + \\beta_1~ X\\)\n\n\nLogistic regression\nBinary\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1 ~ X\\)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#linear-vs.-logistic-regression",
    "href": "slides/16-logistic-intro.html#linear-vs.-logistic-regression",
    "title": "Logistic regression",
    "section": "Linear vs. logistic regression",
    "text": "Linear vs. logistic regression\n\nState whether a linear regression model or logistic regression model is more appropriate for each scenario.\n\nUse age and education to predict if a randomly selected person will vote in the next election.\nUse budget and run time (in minutes) to predict a movie’s total revenue.\nUse age and sex to calculate the probability a randomly selected adult will visit Duke Health in the next year.\n\nSubmit your responses on Ed Discussion [10:05am][1:25pm]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#binary-response-variable",
    "href": "slides/16-logistic-intro.html#binary-response-variable",
    "title": "Logistic regression",
    "section": "Binary response variable",
    "text": "Binary response variable\n\n\n\\(Y = 1: \\text{ yes}, 0: \\text{ no}\\)\n\\(\\pi\\): probability that \\(Y=1\\), i.e., \\(P(Y = 1)\\)\n\\(\\frac{\\pi}{1-\\pi}\\): odds that \\(Y = 1\\)\n\\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\): log odds\nGo from \\(\\pi\\) to \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\) using the logit transformation"
  },
  {
    "objectID": "slides/16-logistic-intro.html#odds",
    "href": "slides/16-logistic-intro.html#odds",
    "title": "Logistic regression",
    "section": "Odds",
    "text": "Odds\n\nSuppose there is a 70% chance it will rain tomorrow\n\nProbability it will rain is \\(\\mathbf{p = 0.7}\\)\nProbability it won’t rain is \\(\\mathbf{1 - p = 0.3}\\)\nOdds it will rain are 7 to 3, 7:3, \\(\\mathbf{\\frac{0.7}{0.3} \\approx 2.33}\\)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#are-teenagers-getting-enough-sleep",
    "href": "slides/16-logistic-intro.html#are-teenagers-getting-enough-sleep",
    "title": "Logistic regression",
    "section": "Are teenagers getting enough sleep?",
    "text": "Are teenagers getting enough sleep?\n\nsleep |&gt;\n  count(Sleep7) |&gt;\n  mutate(p = round(n / sum(n), 3))\n\n# A tibble: 2 × 3\n  Sleep7     n     p\n   &lt;int&gt; &lt;int&gt; &lt;dbl&gt;\n1      0   150 0.336\n2      1   296 0.664\n\n\n\n\\(P(\\text{7+ hours of sleep}) = P(Y = 1) = p = 0.664\\)\n\n\n\\(P(\\text{&lt; 7 hours of sleep}) = P(Y = 0) = 1 - p = 0.336\\)\n\n\n\\(P(\\text{odds of 7+ hours of sleep}) = \\frac{0.664}{0.336} = 1.976\\)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#from-odds-to-probabilities",
    "href": "slides/16-logistic-intro.html#from-odds-to-probabilities",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\n\nodds\n\\[\\omega = \\frac{\\pi}{1-\\pi}\\]\n\nprobability\n\\[\\pi = \\frac{\\omega}{1 + \\omega}\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#from-odds-to-probabilities-1",
    "href": "slides/16-logistic-intro.html#from-odds-to-probabilities-1",
    "title": "Logistic regression",
    "section": "From odds to probabilities",
    "text": "From odds to probabilities\n\nLogistic model: log odds = \\(\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\)\nOdds = \\(\\exp\\big\\{\\log\\big(\\frac{\\pi}{1-\\pi}\\big)\\big\\} = \\frac{\\pi}{1-\\pi}\\)\nCombining (1) and (2) with what we saw earlier\n\n\n\\[\\text{probability} = \\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#logistic-regression-model",
    "href": "slides/16-logistic-intro.html#logistic-regression-model",
    "title": "Logistic regression",
    "section": "Logistic regression model",
    "text": "Logistic regression model\nLogit form: \\[\\log\\big(\\frac{\\pi}{1-\\pi}\\big) = \\beta_0 + \\beta_1~X\\]\n\nProbability form:\n\\[\n\\pi = \\frac{\\exp\\{\\beta_0 + \\beta_1~X\\}}{1 + \\exp\\{\\beta_0 + \\beta_1~X\\}}\n\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#risk-of-coronary-heart-disease",
    "href": "slides/16-logistic-intro.html#risk-of-coronary-heart-disease",
    "title": "Logistic regression",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use age to predict if a randomly selected adult is high risk of having coronary heart disease in the next 10 years.\n\nhigh_risk:\n\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\n\nage: Age at exam time (in years)"
  },
  {
    "objectID": "slides/16-logistic-intro.html#data-heart_disease",
    "href": "slides/16-logistic-intro.html#data-heart_disease",
    "title": "Logistic regression",
    "section": "Data: heart_disease",
    "text": "Data: heart_disease\n\n\n# A tibble: 4,240 × 2\n     age high_risk\n   &lt;dbl&gt; &lt;fct&gt;    \n 1    39 0        \n 2    46 0        \n 3    48 0        \n 4    61 1        \n 5    46 0        \n 6    43 0        \n 7    63 1        \n 8    45 0        \n 9    52 0        \n10    43 0        \n# ℹ 4,230 more rows"
  },
  {
    "objectID": "slides/16-logistic-intro.html#high-risk-vs.-age",
    "href": "slides/16-logistic-intro.html#high-risk-vs.-age",
    "title": "Logistic regression",
    "section": "High risk vs. age",
    "text": "High risk vs. age\n\nggplot(heart_disease, aes(x = high_risk, y = age)) +\n  geom_boxplot(fill = \"steelblue\") +\n  labs(x = \"High risk - 1: yes, 0: no\",\n       y = \"Age\", \n       title = \"Age vs. High risk of heart disease\")"
  },
  {
    "objectID": "slides/16-logistic-intro.html#lets-fit-the-model",
    "href": "slides/16-logistic-intro.html#lets-fit-the-model",
    "title": "Logistic regression",
    "section": "Let’s fit the model",
    "text": "Let’s fit the model\n\nheart_disease_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age, data = heart_disease, family = \"binomial\")\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0"
  },
  {
    "objectID": "slides/16-logistic-intro.html#the-model",
    "href": "slides/16-logistic-intro.html#the-model",
    "title": "Logistic regression",
    "section": "The model",
    "text": "The model\n\ntidy(heart_disease_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-5.561\n0.284\n-19.599\n0\n\n\nage\n0.075\n0.005\n14.178\n0\n\n\n\n\n\n\n\\[\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.561 + 0.075 \\times \\text{age}\\] where \\(\\hat{\\pi}\\) is the predicted probability of being high risk of having heart disease in the next 10 years"
  },
  {
    "objectID": "slides/16-logistic-intro.html#predicted-log-odds",
    "href": "slides/16-logistic-intro.html#predicted-log-odds",
    "title": "Logistic regression",
    "section": "Predicted log odds",
    "text": "Predicted log odds\n\naugment(heart_disease_fit$fit) |&gt; select(.fitted, .resid)\n\n# A tibble: 4,240 × 2\n   .fitted .resid\n     &lt;dbl&gt;  &lt;dbl&gt;\n 1  -2.65  -0.370\n 2  -2.13  -0.475\n 3  -1.98  -0.509\n 4  -1.01   1.62 \n 5  -2.13  -0.475\n 6  -2.35  -0.427\n 7  -0.858  1.56 \n 8  -2.20  -0.458\n 9  -1.68  -0.585\n10  -2.35  -0.427\n# ℹ 4,230 more rows\n\n\n\nFor observation 1\n\\[\\text{predicted odds} = \\hat{\\omega} = \\frac{\\hat{\\pi}}{1-\\hat{\\pi}} = \\exp\\{-2.650\\} = 0.071\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#predicted-probabilities",
    "href": "slides/16-logistic-intro.html#predicted-probabilities",
    "title": "Logistic regression",
    "section": "Predicted probabilities",
    "text": "Predicted probabilities\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"prob\")\n\n# A tibble: 4,240 × 2\n   .pred_0 .pred_1\n     &lt;dbl&gt;   &lt;dbl&gt;\n 1   0.934  0.0660\n 2   0.894  0.106 \n 3   0.878  0.122 \n 4   0.733  0.267 \n 5   0.894  0.106 \n 6   0.913  0.0870\n 7   0.702  0.298 \n 8   0.900  0.0996\n 9   0.843  0.157 \n10   0.913  0.0870\n# ℹ 4,230 more rows\n\n\n\nFor observation 1\n\\[\\text{predicted probability} = \\hat{\\pi} = \\frac{\\exp\\{-2.650\\}}{1 + \\exp\\{-2.650\\}} = 0.066\\]"
  },
  {
    "objectID": "slides/16-logistic-intro.html#predicted-classes",
    "href": "slides/16-logistic-intro.html#predicted-classes",
    "title": "Logistic regression",
    "section": "Predicted classes",
    "text": "Predicted classes\n\npredict(heart_disease_fit, new_data = heart_disease, type = \"class\")\n\n# A tibble: 4,240 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ℹ 4,230 more rows"
  },
  {
    "objectID": "slides/16-logistic-intro.html#default-prediction",
    "href": "slides/16-logistic-intro.html#default-prediction",
    "title": "Logistic regression",
    "section": "Default prediction",
    "text": "Default prediction\nFor a logistic regression, the default prediction is the class.\n\npredict(heart_disease_fit, new_data = heart_disease)\n\n# A tibble: 4,240 × 1\n   .pred_class\n   &lt;fct&gt;      \n 1 0          \n 2 0          \n 3 0          \n 4 0          \n 5 0          \n 6 0          \n 7 0          \n 8 0          \n 9 0          \n10 0          \n# ℹ 4,230 more rows"
  },
  {
    "objectID": "slides/16-logistic-intro.html#observed-vs.-predicted",
    "href": "slides/16-logistic-intro.html#observed-vs.-predicted",
    "title": "Logistic regression",
    "section": "Observed vs. predicted",
    "text": "Observed vs. predicted\n\nWhat does the following table show?\n\n\npredict(heart_disease_fit, new_data = heart_disease) |&gt;\n  bind_cols(heart_disease) |&gt;\n  count(high_risk, .pred_class)\n\n# A tibble: 2 × 3\n  high_risk .pred_class     n\n  &lt;fct&gt;     &lt;fct&gt;       &lt;int&gt;\n1 0         0            3596\n2 1         0             644\n\n\n\n\nThe .pred_class is the class with the highest predicted probability. What is a limitation to using this method to determine the predicted class?"
  },
  {
    "objectID": "slides/16-logistic-intro.html#recap",
    "href": "slides/16-logistic-intro.html#recap",
    "title": "Logistic regression",
    "section": "Recap",
    "text": "Recap\n\nIntroduced logistic regression for binary response variable\nDescribed relationship between odds and probabilities\nUsed logistic regression model to calculate predicted odds and probabilities\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#announcements",
    "href": "slides/18-logistic-prediction.html#announcements",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Announcements",
    "text": "Announcements\n\nLab this week: Work on project\nProject draft due in your GitHub repo at 9am on\n\nNovember 14 (Tuesday labs)\nNovember 16 (Thursday labs)\nWill do peer review in lab those days\n\nTeam Feedback #1 due Friday, November 10 at 11:5pm\n\nWill receive email from Teammates"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#odds-ratios-practice",
    "href": "slides/18-logistic-prediction.html#odds-ratios-practice",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Odds ratios practice",
    "text": "Odds ratios practice\nLet’s take a look at one of the models from Lab 06 using flipper length and species to predict the odds a penguin is large (has a body mass above average).\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-39.151\n7.149\n-5.477\n0.000\n\n\nspeciesChinstrap\n-1.870\n0.580\n-3.221\n0.001\n\n\nspeciesGentoo\n0.512\n0.843\n0.607\n0.544\n\n\nflipper_length_mm\n0.195\n0.037\n5.295\n0.000\n\n\n\n\n\n\n\nInterpret the coefficient of flipper_length_mm in terms of the odds a penguin is large.\nInterpret the coefficient of speciesChinstrap in terms of the odds a penguin is large.\n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#topics",
    "href": "slides/18-logistic-prediction.html#topics",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Topics",
    "text": "Topics\n\n\nBuilding predictive logistic regression models\nSensitivity and specificity\nMaking classification decisions"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#computational-setup",
    "href": "slides/18-logistic-prediction.html#computational-setup",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#openintroemail",
    "href": "slides/18-logistic-prediction.html#openintroemail",
    "title": "Logistic Regression: Prediction + classification",
    "section": "openintro::email",
    "text": "openintro::email\nThese data represent incoming emails for the first three months of 2012 for an email account.\n\n\nOutcome: spam - Indicator for whether the email was spam.\nPredictors: spam, to_multiple, from, cc, sent_email, time, image, attach, dollar, winner, inherit, viagra, password, num_char, line_breaks, format, re_subj, exclaim_subj, urgent_subj, exclaim_mess, number.\n\n\nClick here for more detailed information on the variables."
  },
  {
    "objectID": "slides/18-logistic-prediction.html#training-and-testing-split",
    "href": "slides/18-logistic-prediction.html#training-and-testing-split",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Training and testing split",
    "text": "Training and testing split\n\n# Fix random numbers by setting the seed \n# Enables analysis to be reproducible when random numbers are used \nset.seed(1109)\n\n# Put 75% of the data into the training set \nemail_split &lt;- initial_split(email)\n\n# Create data frames for the two sets\nemail_train &lt;- training(email_split)\nemail_test  &lt;- testing(email_split)"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#exploratory-data-analysis",
    "href": "slides/18-logistic-prediction.html#exploratory-data-analysis",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\nThe sample is unbalanced with respect to spam."
  },
  {
    "objectID": "slides/18-logistic-prediction.html#reminder-modeling-workflow",
    "href": "slides/18-logistic-prediction.html#reminder-modeling-workflow",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Reminder: Modeling workflow",
    "text": "Reminder: Modeling workflow\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\n\nUse cross-validation if deciding between multiple models\n\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#initiate-a-recipe",
    "href": "slides/18-logistic-prediction.html#initiate-a-recipe",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\nemail_rec &lt;- recipe(\n  spam ~ .,          # formula\n  data = email_train  # data to use for cataloging names and types of variables\n  )\nsummary(email_rec)\n\n\n\n# A tibble: 21 × 4\n   variable     type      role      source  \n   &lt;chr&gt;        &lt;list&gt;    &lt;chr&gt;     &lt;chr&gt;   \n 1 to_multiple  &lt;chr [3]&gt; predictor original\n 2 from         &lt;chr [3]&gt; predictor original\n 3 cc           &lt;chr [2]&gt; predictor original\n 4 sent_email   &lt;chr [3]&gt; predictor original\n 5 time         &lt;chr [1]&gt; predictor original\n 6 image        &lt;chr [2]&gt; predictor original\n 7 attach       &lt;chr [2]&gt; predictor original\n 8 dollar       &lt;chr [2]&gt; predictor original\n 9 winner       &lt;chr [3]&gt; predictor original\n10 inherit      &lt;chr [2]&gt; predictor original\n11 viagra       &lt;chr [2]&gt; predictor original\n12 password     &lt;chr [2]&gt; predictor original\n13 num_char     &lt;chr [2]&gt; predictor original\n14 line_breaks  &lt;chr [2]&gt; predictor original\n15 format       &lt;chr [3]&gt; predictor original\n16 re_subj      &lt;chr [3]&gt; predictor original\n17 exclaim_subj &lt;chr [2]&gt; predictor original\n18 urgent_subj  &lt;chr [3]&gt; predictor original\n19 exclaim_mess &lt;chr [2]&gt; predictor original\n20 number       &lt;chr [3]&gt; predictor original\n21 spam         &lt;chr [3]&gt; outcome   original"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#remove-certain-variables",
    "href": "slides/18-logistic-prediction.html#remove-certain-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Remove certain variables",
    "text": "Remove certain variables\n\nemail_rec &lt;- email_rec |&gt;\n  step_rm(from, sent_email)"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#feature-engineer-date",
    "href": "slides/18-logistic-prediction.html#feature-engineer-date",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Feature engineer date",
    "text": "Feature engineer date\n\nemail_rec &lt;- email_rec |&gt;\n  step_date(time, features = c(\"dow\", \"month\")) |&gt;\n  step_rm(time)"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#discretize-numeric-variables",
    "href": "slides/18-logistic-prediction.html#discretize-numeric-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Discretize numeric variables",
    "text": "Discretize numeric variables\n\nemail_rec &lt;- email_rec |&gt;\n  step_cut(cc, attach, dollar, breaks = c(0, 1))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#create-dummy-variables",
    "href": "slides/18-logistic-prediction.html#create-dummy-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Create dummy variables",
    "text": "Create dummy variables\n\nemail_rec &lt;- email_rec |&gt;\n  step_dummy(all_nominal(), -all_outcomes())"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#remove-zero-variance-variables",
    "href": "slides/18-logistic-prediction.html#remove-zero-variance-variables",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Remove zero variance variables",
    "text": "Remove zero variance variables\nVariables that contain only a single value\n\nemail_rec &lt;- email_rec |&gt;\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#recipe-all-in-one-place",
    "href": "slides/18-logistic-prediction.html#recipe-all-in-one-place",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Recipe: All in one place",
    "text": "Recipe: All in one place\n\nemail_rec &lt;- recipe(spam ~ ., data = email_train) |&gt;\n  step_rm(from, sent_email) |&gt;\n  step_date(time, features = c(\"dow\", \"month\")) |&gt;               \n  step_rm(time) |&gt;\n  step_cut(cc, attach, dollar, breaks = c(0, 1)) |&gt;\n  step_dummy(all_nominal_predictors()) |&gt;\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#define-model",
    "href": "slides/18-logistic-prediction.html#define-model",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Define model",
    "text": "Define model\n\nemail_spec &lt;- logistic_reg() |&gt; \n  set_engine(\"glm\")\nemail_spec\n\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#define-workflow",
    "href": "slides/18-logistic-prediction.html#define-workflow",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Define workflow",
    "text": "Define workflow\nRemember: Workflows bring together models and recipes so that they can be easily applied to both the training and test data.\n\nemail_wflow &lt;- workflow() |&gt; \n  add_model(email_spec) |&gt; \n  add_recipe(email_rec)\n\n\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: logistic_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n6 Recipe Steps\n\n• step_rm()\n• step_date()\n• step_rm()\n• step_cut()\n• step_dummy()\n• step_zv()\n\n── Model ───────────────────────────────────────────────────────────────────────\nLogistic Regression Model Specification (classification)\n\nComputational engine: glm"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#fit-model-to-training-data",
    "href": "slides/18-logistic-prediction.html#fit-model-to-training-data",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Fit model to training data",
    "text": "Fit model to training data\n\nemail_fit &lt;- email_wflow |&gt; \n  fit(data = email_train)\n\ntidy(email_fit) |&gt; print(n = 31)\n\n# A tibble: 27 × 5\n   term           estimate std.error statistic  p.value\n   &lt;chr&gt;             &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n 1 (Intercept)    -1.24      0.274     -4.51   6.43e- 6\n 2 image          -1.36      0.679     -2.00   4.59e- 2\n 3 inherit         0.352     0.185      1.90   5.69e- 2\n 4 viagra          1.96     40.6        0.0482 9.62e- 1\n 5 password       -0.941     0.387     -2.43   1.51e- 2\n 6 num_char        0.0572    0.0257     2.23   2.58e- 2\n 7 line_breaks    -0.00554   0.00147   -3.77   1.66e- 4\n 8 exclaim_subj   -0.245     0.303     -0.807  4.19e- 1\n 9 exclaim_mess    0.00916   0.00195    4.69   2.67e- 6\n10 to_multiple_X1 -2.91      0.388     -7.50   6.37e-14\n11 cc_X.1.68.     -0.105     0.446     -0.236  8.14e- 1\n12 attach_X.1.21.  2.33      0.385      6.06   1.37e- 9\n13 dollar_X.1.64.  0.0136    0.241      0.0565 9.55e- 1\n14 winner_yes      2.46      0.480      5.12   3.02e- 7\n15 format_X1      -1.02      0.173     -5.88   4.07e- 9\n16 re_subj_X1     -2.93      0.436     -6.72   1.81e-11\n17 urgent_subj_X1  4.37      1.25       3.51   4.54e- 4\n18 number_small   -0.728     0.178     -4.08   4.45e- 5\n19 number_big      0.261     0.255      1.03   3.05e- 1\n20 time_dow_Mon    0.123     0.320      0.386  7.00e- 1\n21 time_dow_Tue    0.309     0.294      1.05   2.94e- 1\n22 time_dow_Wed   -0.133     0.297     -0.447  6.55e- 1\n23 time_dow_Thu    0.104     0.303      0.343  7.32e- 1\n24 time_dow_Fri    0.280     0.292      0.960  3.37e- 1\n25 time_dow_Sat    0.439     0.323      1.36   1.74e- 1\n26 time_month_Feb  1.06      0.192      5.54   3.06e- 8\n27 time_month_Mar  0.575     0.198      2.91   3.60e- 3"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#make-predictions-for-test-data",
    "href": "slides/18-logistic-prediction.html#make-predictions-for-test-data",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Make predictions for test data",
    "text": "Make predictions for test data\n\nemail_pred &lt;- predict(email_fit, email_test, type = \"prob\") |&gt; \n  bind_cols(email_test) \nemail_pred\n\n# A tibble: 981 × 23\n   .pred_0  .pred_1 spam  to_multiple from     cc sent_email time               \n     &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt;       &lt;fct&gt; &lt;int&gt; &lt;fct&gt;      &lt;dttm&gt;             \n 1   0.921 0.0786   0     0           1         0 0          2012-01-01 01:16:41\n 2   0.961 0.0391   0     0           1         0 0          2012-01-01 05:00:01\n 3   0.999 0.000988 0     0           1         1 1          2012-01-01 14:38:32\n 4   0.999 0.000591 0     0           1         1 1          2012-01-01 18:40:14\n 5   0.991 0.00878  0     0           1         0 0          2012-01-02 00:42:16\n 6   0.910 0.0902   0     0           1         0 0          2012-01-01 21:05:45\n 7   1.00  0.000108 0     1           1         3 0          2012-01-02 08:41:11\n 8   0.975 0.0248   0     0           1         0 0          2012-01-02 20:07:17\n 9   0.952 0.0477   0     0           1         0 0          2012-01-02 23:31:03\n10   0.992 0.00819  0     1           1         0 0          2012-01-03 08:36:16\n# ℹ 971 more rows\n# ℹ 15 more variables: image &lt;dbl&gt;, attach &lt;dbl&gt;, dollar &lt;dbl&gt;, winner &lt;fct&gt;,\n#   inherit &lt;dbl&gt;, viagra &lt;dbl&gt;, password &lt;dbl&gt;, num_char &lt;dbl&gt;,\n#   line_breaks &lt;int&gt;, format &lt;fct&gt;, re_subj &lt;fct&gt;, exclaim_subj &lt;dbl&gt;,\n#   urgent_subj &lt;fct&gt;, exclaim_mess &lt;dbl&gt;, number &lt;fct&gt;"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#a-closer-look-at-predictions",
    "href": "slides/18-logistic-prediction.html#a-closer-look-at-predictions",
    "title": "Logistic Regression: Prediction + classification",
    "section": "A closer look at predictions",
    "text": "A closer look at predictions\n\nWhich of the following 10 emails will be misclassified?\n\n\nemail_pred |&gt;\n  arrange(desc(.pred_1)) |&gt;\n  select(contains(\"pred\"), spam) |&gt; slice(1:10)\n\n# A tibble: 10 × 3\n   .pred_0 .pred_1 spam \n     &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;\n 1  0.0750   0.925 0    \n 2  0.110    0.890 0    \n 3  0.116    0.884 1    \n 4  0.127    0.873 1    \n 5  0.170    0.830 1    \n 6  0.189    0.811 1    \n 7  0.204    0.796 1    \n 8  0.208    0.792 1    \n 9  0.224    0.776 1    \n10  0.295    0.705 1"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#false-positive-and-negative",
    "href": "slides/18-logistic-prediction.html#false-positive-and-negative",
    "title": "Logistic Regression: Prediction + classification",
    "section": "False positive and negative",
    "text": "False positive and negative\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail classified as spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail classified as not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\n\nFalse negative rate = P(classified as not spam | Email spam)\n\n= FN / (TP + FN)\n\nFalse positive rate = P(classified as spam | Email not spam)\n\n= FP / (FP + TN)"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#sensitivity-and-specificity-1",
    "href": "slides/18-logistic-prediction.html#sensitivity-and-specificity-1",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Sensitivity and specificity",
    "text": "Sensitivity and specificity\n\n\n\n\n\n\n\n\n\nEmail is spam\nEmail is not spam\n\n\n\n\nEmail classified as spam\nTrue positive\nFalse positive (Type 1 error)\n\n\nEmail classified as not spam\nFalse negative (Type 2 error)\nTrue negative\n\n\n\n\n\nSensitivity = P(classified as spam | Email spam) = TP / (TP + FN)\n\nSensitivity = 1 − False negative rate\n\nSpecificity = P(classified as not spam | Email not spam) = TN / (FP + TN)\n\nSpecificity = 1 − False positive rate\n\n\n\n\n\nIf you were designing a spam filter, would you want sensitivity and specificity to be high or low? What are the trade-offs associated with each decision?"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#evaluate-the-performance",
    "href": "slides/18-logistic-prediction.html#evaluate-the-performance",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Evaluate the performance",
    "text": "Evaluate the performance\nReceiver operating characteristic (ROC) curve+ plots the true positive rate (sensitivity) vs. false positive rate (1 - specificity).\n\n\n\n\nemail_pred |&gt;\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  ) |&gt;\n  autoplot()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n+ Originally developed for operators of military radar receivers, hence the name."
  },
  {
    "objectID": "slides/18-logistic-prediction.html#roc-curve-under-the-hood",
    "href": "slides/18-logistic-prediction.html#roc-curve-under-the-hood",
    "title": "Logistic Regression: Prediction + classification",
    "section": "ROC curve, under the hood",
    "text": "ROC curve, under the hood\n\nemail_pred |&gt;\n  roc_curve(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 978 × 3\n    .threshold specificity sensitivity\n         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 -Inf            0                 1\n 2    3.36e-10     0                 1\n 3    2.27e- 9     0.00226           1\n 4    8.69e- 7     0.00339           1\n 5    9.89e- 7     0.00452           1\n 6    1.43e- 6     0.00565           1\n 7    9.16e- 6     0.00678           1\n 8    1.03e- 5     0.00791           1\n 9    2.58e- 5     0.00904           1\n10    3.35e- 5     0.0102            1\n# ℹ 968 more rows"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#roc-curve",
    "href": "slides/18-logistic-prediction.html#roc-curve",
    "title": "Logistic Regression: Prediction + classification",
    "section": "ROC curve",
    "text": "ROC curve"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#evaluate-the-performance-auc",
    "href": "slides/18-logistic-prediction.html#evaluate-the-performance-auc",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Evaluate the performance: AUC",
    "text": "Evaluate the performance: AUC\n\nemail_pred |&gt;\n  roc_auc(\n    truth = spam,\n    .pred_1,\n    event_level = \"second\"\n  )\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 roc_auc binary         0.836\n\n\n\nThe area under the curve (AUC) can be used to assess how well the logistic model fits the data\n\nAUC=0.5: model is a very bad fit (no better than a coin flip)\nAUC close to 1: model is a good fit"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#cutoff-probability-0.5",
    "href": "slides/18-logistic-prediction.html#cutoff-probability-0.5",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Cutoff probability: 0.5",
    "text": "Cutoff probability: 0.5\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.5.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n877\n82\n\n\nEmail classified as spam\n8\n14\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.5\nemail_pred |&gt;\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) |&gt;\n  count(spam_pred, spam) |&gt;\n  pivot_wider(names_from = spam, values_from = n) |&gt;\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#confusion-matrix",
    "href": "slides/18-logistic-prediction.html#confusion-matrix",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Confusion matrix",
    "text": "Confusion matrix\nCross-tabulation of observed and predicted classes:\n\ncutoff_prob &lt;- 0.5\nemail_pred |&gt;\n  mutate(spam_predicted = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0))) |&gt;\n  conf_mat(truth = spam, estimate = spam_predicted)\n\n          Truth\nPrediction   0   1\n         0 877  82\n         1   8  14"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#classification",
    "href": "slides/18-logistic-prediction.html#classification",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#cutoff-probability-0.25",
    "href": "slides/18-logistic-prediction.html#cutoff-probability-0.25",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Cutoff probability: 0.25",
    "text": "Cutoff probability: 0.25\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.25.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n830\n52\n\n\nEmail classified as spam\n55\n44\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.25\nemail_pred |&gt;\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) |&gt;\n  count(spam_pred, spam) |&gt;\n  pivot_wider(names_from = spam, values_from = n) |&gt;\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#classification-1",
    "href": "slides/18-logistic-prediction.html#classification-1",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#cutoff-probability-0.75",
    "href": "slides/18-logistic-prediction.html#cutoff-probability-0.75",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Cutoff probability: 0.75",
    "text": "Cutoff probability: 0.75\n\nOutputCode\n\n\nSuppose we decide to label an email as spam if the model predicts the probability of spam to be more than 0.75.\n\n\n\n\n\n\nEmail is not spam\nEmail is spam\n\n\n\n\nEmail classified as not spam\n883\n89\n\n\nEmail classified as spam\n2\n7\n\n\n\n\n\n\n\n\ncutoff_prob &lt;- 0.75\nemail_pred |&gt;\n  mutate(\n    spam_pred = as_factor(if_else(.pred_1 &gt;= cutoff_prob, 1, 0)),\n    spam      = if_else(spam == 1, \"Email is spam\", \"Email is not spam\"),\n    spam_pred = if_else(spam_pred == 1, \"Email classified as spam\", \"Email classified as not spam\")\n    ) |&gt;\n  count(spam_pred, spam) |&gt;\n  pivot_wider(names_from = spam, values_from = n) |&gt;\n  kable(col.names = c(\"\", \"Email is not spam\", \"Email is spam\"))"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#classification-2",
    "href": "slides/18-logistic-prediction.html#classification-2",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Classification",
    "text": "Classification"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#use-roc-curve",
    "href": "slides/18-logistic-prediction.html#use-roc-curve",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Use ROC curve",
    "text": "Use ROC curve\nUse the ROC curve to determine the best cutoff probability\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# A tibble: 10 × 3\n   .threshold specificity sensitivity\n        &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1     0.0769       0.736       0.792\n 2     0.0770       0.736       0.781\n 3     0.0780       0.737       0.781\n 4     0.0785       0.737       0.771\n 5     0.0786       0.738       0.771\n 6     0.0787       0.739       0.771\n 7     0.0789       0.739       0.760\n 8     0.0802       0.740       0.760\n 9     0.0802       0.741       0.760\n10     0.0805       0.742       0.760"
  },
  {
    "objectID": "slides/18-logistic-prediction.html#recap",
    "href": "slides/18-logistic-prediction.html#recap",
    "title": "Logistic Regression: Prediction + classification",
    "section": "Recap",
    "text": "Recap\n\nBuilt predictive logistic regression models\nDefined and calculated sensitivity and specificity\nMade classification decisions based on sensitivity and specificity\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/20-logistic-inf.html#announcements",
    "href": "slides/20-logistic-inf.html#announcements",
    "title": "LR: Inference + conditions",
    "section": "Announcements",
    "text": "Announcements\n\nDue dates\n\nDraft report due in GitHub repo on 9am on the day of your lab\nHW 04 due Wed, Nov 15 at 11:59pm\nStatistics experience due Mon, Nov 20 at 11:59pm\n\nNext week:\n\n(Optional) Project meetings Nov 20 & 21. Click here to sign up. Must sign up by Fri, Nov 17\nNo lecture Mon, Nov 20\nNo lab Tue, Nov 21\nThanksgiving Break: Wed, Nov 22 - Fri, Nov 24"
  },
  {
    "objectID": "slides/20-logistic-inf.html#statistician-of-the-day---alejandra-castillo",
    "href": "slides/20-logistic-inf.html#statistician-of-the-day---alejandra-castillo",
    "title": "LR: Inference + conditions",
    "section": "Statistician of the day - Alejandra Castillo",
    "text": "Statistician of the day - Alejandra Castillo\n\n\nAlejandra Castillo did her undergraduate work at Pomona College in Mathematics and her MS (2019) and PhD (2023) at Oregon State University in Statistics.\nDr. Castillo’s research lies at the intersection of unsupervised learning, dimension reduction, and inference, with applications in clinical trial design. Some of her work explores how to use baseline demographic information collected before randomization to a clinical trial, particularly as the baseline information changes during the course of the trial.\n\n\n\n\n\n\n\n\nMS Thesis: Castillo A. On the Use of Baseline Values in Randomized Clinical Trials, MS Thesis, Oregon State, 2019.\nSource: hardin47.github.io/CURV/scholars/castillo"
  },
  {
    "objectID": "slides/20-logistic-inf.html#topics",
    "href": "slides/20-logistic-inf.html#topics",
    "title": "LR: Inference + conditions",
    "section": "Topics",
    "text": "Topics\n\n\nEstimating coefficients in logistic regression\nInference for coefficients in logistic regression\nChecking model conditions for logistic regression"
  },
  {
    "objectID": "slides/20-logistic-inf.html#computational-setup",
    "href": "slides/20-logistic-inf.html#computational-setup",
    "title": "LR: Inference + conditions",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(knitr)\nlibrary(kableExtra)  # for table embellishments\nlibrary(Stat2Data)   # for empirical logit\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/20-logistic-inf.html#risk-of-coronary-heart-disease",
    "href": "slides/20-logistic-inf.html#risk-of-coronary-heart-disease",
    "title": "LR: Inference + conditions",
    "section": "Risk of coronary heart disease",
    "text": "Risk of coronary heart disease\nThis data set is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to examine the relationship between various health characteristics and the risk of having heart disease.\n\nhigh_risk:\n\n1: High risk of having heart disease in next 10 years\n0: Not high risk of having heart disease in next 10 years\n\nage: Age at exam time (in years)\neducation: 1 = Some High School, 2 = High School or GED, 3 = Some College or Vocational School, 4 = College\ncurrentSmoker: 0 = nonsmoker, 1 = smoker"
  },
  {
    "objectID": "slides/20-logistic-inf.html#data-prep",
    "href": "slides/20-logistic-inf.html#data-prep",
    "title": "LR: Inference + conditions",
    "section": "Data prep",
    "text": "Data prep\n\nheart_disease &lt;- read_csv(here::here(\"slides\", \"data/framingham.csv\")) |&gt;\n  select(age, education, TenYearCHD, totChol, currentSmoker) |&gt;\n  drop_na() |&gt;\n  mutate(\n    high_risk = as.factor(TenYearCHD),\n    education = as.factor(education),\n    currentSmoker = as.factor(currentSmoker)\n  )\n\nheart_disease\n\n# A tibble: 4,086 × 6\n     age education TenYearCHD totChol currentSmoker high_risk\n   &lt;dbl&gt; &lt;fct&gt;          &lt;dbl&gt;   &lt;dbl&gt; &lt;fct&gt;         &lt;fct&gt;    \n 1    39 4                  0     195 0             0        \n 2    46 2                  0     250 0             0        \n 3    48 1                  0     245 1             0        \n 4    61 3                  1     225 1             1        \n 5    46 3                  0     285 1             0        \n 6    43 2                  0     228 0             0        \n 7    63 1                  1     205 0             1        \n 8    45 2                  0     313 1             0        \n 9    52 1                  0     260 0             0        \n10    43 1                  0     225 1             0        \n# ℹ 4,076 more rows"
  },
  {
    "objectID": "slides/20-logistic-inf.html#statistical-model",
    "href": "slides/20-logistic-inf.html#statistical-model",
    "title": "LR: Inference + conditions",
    "section": "Statistical model",
    "text": "Statistical model\nThe form of the statistical model for logistic regression is\n\\[\n\\log\\Big(\\frac{\\pi}{1-\\pi}\\Big) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\dots + \\beta_pX_p\n\\]\nwhere \\(\\pi\\) is the probability \\(Y = 1\\).\n\nNotice there is no error term when writing the statistical model for logistic regression. Why?\n\nRecall that the statistical model is the “data-generating” model\nEach individual observed \\(Y\\) is generated from a Bernoulli distribution, \\(Bernoulli(\\pi)\\) (similarly we can think of \\(n\\) observed \\(Y\\)’s as generated from a Binomial distribution, \\(Binomial(n,p)\\))\nTherefore, the randomness is not produced by an error term but rather in the distribution used to generate \\(Y\\)"
  },
  {
    "objectID": "slides/20-logistic-inf.html#estimating-coefficients-1",
    "href": "slides/20-logistic-inf.html#estimating-coefficients-1",
    "title": "LR: Inference + conditions",
    "section": "Estimating coefficients",
    "text": "Estimating coefficients\nRecall the log likelihood function\n\\[\n\\log L = \\sum\\limits_{i=1}^n[y_i \\log(\\hat{\\pi}_i) + (1 - y_i)\\log(1 - \\hat{\\pi}_i)]\n\\]\nwhere\n\\(\\hat{\\pi} = \\frac{exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\dots + \\hat{\\beta}_pX_p\\}}{1 + exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1X_1 + \\dots + \\hat{\\beta}_pX_p\\}}\\)\n\n\nThe coefficients \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p\\) are estimated using maximum likelihood estimation\nBasic idea: Find the values of \\(\\hat{\\beta}_0, \\ldots, \\hat{\\beta}_p\\) that give the observed data the maximum probability of occurring"
  },
  {
    "objectID": "slides/20-logistic-inf.html#modeling-risk-of-coronary-heart-disease",
    "href": "slides/20-logistic-inf.html#modeling-risk-of-coronary-heart-disease",
    "title": "LR: Inference + conditions",
    "section": "Modeling risk of coronary heart disease",
    "text": "Modeling risk of coronary heart disease\nUsing age and education:\n\nrisk_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + education, \n      data = heart_disease, family = \"binomial\")"
  },
  {
    "objectID": "slides/20-logistic-inf.html#model-output",
    "href": "slides/20-logistic-inf.html#model-output",
    "title": "LR: Inference + conditions",
    "section": "Model output",
    "text": "Model output\n\ntidy(risk_fit, conf.int = TRUE) |&gt; \n  kable(format = \"markdown\", digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\\[\n\\small{\\log\\Big(\\frac{\\hat{\\pi}}{1-\\hat{\\pi}}\\Big) = -5.508 + 0.076 ~ \\text{age} - 0.245 ~ \\text{ed2} - 0.236 ~ \\text{ed3} - 0.024 ~ \\text{ed4}}\n\\]"
  },
  {
    "objectID": "slides/20-logistic-inf.html#inference-for-coefficients-1",
    "href": "slides/20-logistic-inf.html#inference-for-coefficients-1",
    "title": "LR: Inference + conditions",
    "section": "Inference for coefficients",
    "text": "Inference for coefficients\nThere are two approaches for testing coefficients in logistic regression\n\nDrop-in-deviance test. Use to test…\n\na single coefficient\na categorical predictor with 3+ levels\na group of predictor variables\n\n(Wald) hypothesis test. Use to test\n\na single coefficient"
  },
  {
    "objectID": "slides/20-logistic-inf.html#hypothesis-test-for-beta_j",
    "href": "slides/20-logistic-inf.html#hypothesis-test-for-beta_j",
    "title": "LR: Inference + conditions",
    "section": "Hypothesis test for \\(\\beta_j\\)",
    "text": "Hypothesis test for \\(\\beta_j\\)\nHypotheses: \\(H_0: \\beta_j = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_j \\neq 0\\), given the other variables in the model\n\nTest Statistic: \\[z = \\frac{\\hat{\\beta}_j - 0}{SE_{\\hat{\\beta}_j}}\\]\n\n\nP-value: \\(P(|Z| &gt; |z|)\\), where \\(Z \\sim N(0, 1)\\), the Standard Normal distribution"
  },
  {
    "objectID": "slides/20-logistic-inf.html#confidence-interval-for-beta_j",
    "href": "slides/20-logistic-inf.html#confidence-interval-for-beta_j",
    "title": "LR: Inference + conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\nWe can calculate the C% confidence interval for \\(\\beta_j\\) as the following:\n\\[\n\\Large{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}}\n\\]\nwhere \\(z^*\\) is calculated from the \\(N(0,1)\\) distribution\n\n\n\n\n\n\n\nNote\n\n\nThis is an interval for the change in the log-odds for every one unit increase in \\(x_j\\)"
  },
  {
    "objectID": "slides/20-logistic-inf.html#interpretation-in-terms-of-the-odds",
    "href": "slides/20-logistic-inf.html#interpretation-in-terms-of-the-odds",
    "title": "LR: Inference + conditions",
    "section": "Interpretation in terms of the odds",
    "text": "Interpretation in terms of the odds\nThe change in odds for every one unit increase in \\(x_j\\).\n\\[\n\\Large{exp\\{\\hat{\\beta}_j \\pm z^* SE_{\\hat{\\beta}_j}\\}}\n\\]\n\nInterpretation: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), the odds multiply by a factor of \\(exp\\{\\hat{\\beta}_j - z^* SE_{\\hat{\\beta}_j}\\}\\) to \\(exp\\{\\hat{\\beta}_j + z^* SE_{\\hat{\\beta}_j}\\}\\), holding all else constant."
  },
  {
    "objectID": "slides/20-logistic-inf.html#coefficient-for-age",
    "href": "slides/20-logistic-inf.html#coefficient-for-age",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\n\nHypotheses:\n\\[\nH_0: \\beta_{age} = 0 \\hspace{2mm} \\text{ vs } \\hspace{2mm} H_a: \\beta_{age} \\neq 0\n\\], given education is in the model"
  },
  {
    "objectID": "slides/20-logistic-inf.html#coefficient-for-age-1",
    "href": "slides/20-logistic-inf.html#coefficient-for-age-1",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\nTest statistic:\n\\[z = \\frac{0.07559 - 0}{0.00554} = 13.64\n\\]"
  },
  {
    "objectID": "slides/20-logistic-inf.html#coefficient-for-age-2",
    "href": "slides/20-logistic-inf.html#coefficient-for-age-2",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\nP-value:\n\\[\nP(|Z| &gt; |13.64|) \\approx 0\n\\]\n\n\n2 * pnorm(13.64,lower.tail = FALSE)\n\n[1] 2.315606e-42"
  },
  {
    "objectID": "slides/20-logistic-inf.html#coefficient-for-age-3",
    "href": "slides/20-logistic-inf.html#coefficient-for-age-3",
    "title": "LR: Inference + conditions",
    "section": "Coefficient for age",
    "text": "Coefficient for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\nConclusion:\nThe p-value is very small, so we reject \\(H_0\\). The data provide sufficient evidence that age is a statistically significant predictor of whether someone is high risk of having heart disease, after accounting for education."
  },
  {
    "objectID": "slides/20-logistic-inf.html#ci-for-age",
    "href": "slides/20-logistic-inf.html#ci-for-age",
    "title": "LR: Inference + conditions",
    "section": "CI for age",
    "text": "CI for age\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-5.508\n0.311\n-17.692\n0.000\n-6.125\n-4.904\n\n\nage\n0.076\n0.006\n13.648\n0.000\n0.065\n0.087\n\n\neducation2\n-0.245\n0.113\n-2.172\n0.030\n-0.469\n-0.026\n\n\neducation3\n-0.236\n0.135\n-1.753\n0.080\n-0.504\n0.024\n\n\neducation4\n-0.024\n0.150\n-0.161\n0.872\n-0.323\n0.264\n\n\n\n\n\n\n\n\nInterpret the 95% confidence interval for age in terms of the odds of being high risk for heart disease."
  },
  {
    "objectID": "slides/20-logistic-inf.html#the-model",
    "href": "slides/20-logistic-inf.html#the-model",
    "title": "LR: Inference + conditions",
    "section": "The model",
    "text": "The model\nLet’s predict high_risk from age, total cholesterol, and whether the patient is a current smoker:\n\nrisk_fit &lt;- logistic_reg() |&gt;\n  set_engine(\"glm\") |&gt;\n  fit(high_risk ~ age + totChol + currentSmoker, \n      data = heart_disease, family = \"binomial\")\n\ntidy(risk_fit, conf.int = TRUE) |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-6.673\n0.378\n-17.647\n0.000\n-7.423\n-5.940\n\n\nage\n0.082\n0.006\n14.344\n0.000\n0.071\n0.094\n\n\ntotChol\n0.002\n0.001\n1.940\n0.052\n0.000\n0.004\n\n\ncurrentSmoker1\n0.443\n0.094\n4.733\n0.000\n0.260\n0.627"
  },
  {
    "objectID": "slides/20-logistic-inf.html#conditions-for-logistic-regression",
    "href": "slides/20-logistic-inf.html#conditions-for-logistic-regression",
    "title": "LR: Inference + conditions",
    "section": "Conditions for logistic regression",
    "text": "Conditions for logistic regression\n\nLinearity: The log-odds have a linear relationship with the predictors.\nRandomness: The data were obtained from a random process\nIndependence: The observations are independent from one another."
  },
  {
    "objectID": "slides/20-logistic-inf.html#empirical-logit",
    "href": "slides/20-logistic-inf.html#empirical-logit",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit",
    "text": "Empirical logit\nThe empirical logit is the log of the observed odds:\n\\[\n\\text{logit}(\\hat{p}) = \\log\\Big(\\frac{\\hat{p}}{1 - \\hat{p}}\\Big) = \\log\\Big(\\frac{\\# \\text{Yes}}{\\# \\text{No}}\\Big)\n\\]"
  },
  {
    "objectID": "slides/20-logistic-inf.html#calculating-empirical-logit-categorical-predictor",
    "href": "slides/20-logistic-inf.html#calculating-empirical-logit-categorical-predictor",
    "title": "LR: Inference + conditions",
    "section": "Calculating empirical logit (categorical predictor)",
    "text": "Calculating empirical logit (categorical predictor)\nIf the predictor is categorical, we can calculate the empirical logit for each level of the predictor.\n\nheart_disease |&gt;\n  count(currentSmoker, high_risk) |&gt;\n  group_by(currentSmoker) |&gt;\n  mutate(prop = n/sum(n)) |&gt;\n  filter(high_risk == \"1\") |&gt;\n  mutate(emp_logit = log(prop/(1-prop)))\n\n# A tibble: 2 × 5\n# Groups:   currentSmoker [2]\n  currentSmoker high_risk     n  prop emp_logit\n  &lt;fct&gt;         &lt;fct&gt;     &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 0             1           301 0.145     -1.77\n2 1             1           318 0.158     -1.67"
  },
  {
    "objectID": "slides/20-logistic-inf.html#calculating-empirical-logit-quantitative-predictor",
    "href": "slides/20-logistic-inf.html#calculating-empirical-logit-quantitative-predictor",
    "title": "LR: Inference + conditions",
    "section": "Calculating empirical logit (quantitative predictor)",
    "text": "Calculating empirical logit (quantitative predictor)\n\nDivide the range of the predictor into intervals with approximately equal number of cases. (If you have enough observations, use 5 - 10 intervals.)\nCompute the empirical logit for each interval\n\n\nYou can then calculate the mean value of the predictor in each interval and create a plot of the empirical logit versus the mean value of the predictor in each interval."
  },
  {
    "objectID": "slides/20-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor",
    "href": "slides/20-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\nCreated using dplyr and ggplot functions."
  },
  {
    "objectID": "slides/20-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor-1",
    "href": "slides/20-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor-1",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\nCreated using dplyr and ggplot functions.\n\nheart_disease |&gt; \n  mutate(age_bin = cut_interval(age, n = 10)) |&gt;\n  group_by(age_bin) |&gt;\n  mutate(mean_age = mean(age)) |&gt;\n  count(mean_age, high_risk) |&gt;\n  mutate(prop = n/sum(n)) |&gt;\n  filter(high_risk == \"1\") |&gt;\n  mutate(emp_logit = log(prop/(1-prop))) |&gt;\n  ggplot(aes(x = mean_age, y = emp_logit)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(x = \"Mean Age\", \n       y = \"Empirical logit\")"
  },
  {
    "objectID": "slides/20-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor-2",
    "href": "slides/20-logistic-inf.html#empirical-logit-plot-in-r-quantitative-predictor-2",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (quantitative predictor)",
    "text": "Empirical logit plot in R (quantitative predictor)\nUsing the emplogitplot1 function from the Stat2Data R package\n\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)"
  },
  {
    "objectID": "slides/20-logistic-inf.html#empirical-logit-plot-in-r-interactions",
    "href": "slides/20-logistic-inf.html#empirical-logit-plot-in-r-interactions",
    "title": "LR: Inference + conditions",
    "section": "Empirical logit plot in R (interactions)",
    "text": "Empirical logit plot in R (interactions)\nUsing the emplogitplot2 function from the Stat2Data R package\n\nemplogitplot2(high_risk ~ age + currentSmoker, data = heart_disease, \n              ngroups = 10, \n              putlegend = \"bottomright\")"
  },
  {
    "objectID": "slides/20-logistic-inf.html#checking-linearity",
    "href": "slides/20-logistic-inf.html#checking-linearity",
    "title": "LR: Inference + conditions",
    "section": "Checking linearity",
    "text": "Checking linearity\n\n\n\nemplogitplot1(high_risk ~ age, \n              data = heart_disease, \n              ngroups = 10)\n\n\n\n\n\n\n\n\n\n\nemplogitplot1(high_risk ~ totChol, \n              data = heart_disease, \n              ngroups = 10)\n\n\n\n\n\n\n\n\n\n\n\n✅ The linearity condition is satisfied. There is a linear relationship between the empirical logit and the predictor variables."
  },
  {
    "objectID": "slides/20-logistic-inf.html#checking-randomness",
    "href": "slides/20-logistic-inf.html#checking-randomness",
    "title": "LR: Inference + conditions",
    "section": "Checking randomness",
    "text": "Checking randomness\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\nWas the sample randomly selected?\nIf the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n\n✅ The randomness condition is satisfied. We do not have reason to believe that the participants in this study differ systematically from adults in the U.S. in regards to health characteristics and risk of heart disease."
  },
  {
    "objectID": "slides/20-logistic-inf.html#checking-independence",
    "href": "slides/20-logistic-inf.html#checking-independence",
    "title": "LR: Inference + conditions",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can check the independence condition based on the context of the data and how the observations were collected.\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n\n✅ The independence condition is satisfied. It is reasonable to conclude that the participants’ health characteristics are independent of one another.\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#announcements",
    "href": "slides/22-multinomial-logistic-pt2.html#announcements",
    "title": "Multinomial logistic regression",
    "section": "Announcements",
    "text": "Announcements\n\nDue dates\n\nHW 05 due Wed, Nov 29 at 11:59pm\n\nProject\n\n(Optional) Round 1 submission due Fri, Dec 01 at 11:59pm\nPresentations in lab Dec 5 & 7\n\nExam 02 dates\n\nIn-class: Mon, Dec 4\nTake-home: Mon, Dec 4 - Wed, Dec 6\nReview: Wed, Nov 29\n\nClick here to access lecture recordings. Available until Mon, Dec 04 at 9am"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#statistician-of-the-day-mike-dairyko",
    "href": "slides/22-multinomial-logistic-pt2.html#statistician-of-the-day-mike-dairyko",
    "title": "Multinomial logistic regression",
    "section": "Statistician of the day: Mike Dairyko",
    "text": "Statistician of the day: Mike Dairyko\n\n\nMike Dairyko was a student at Pomona College where he studied Mathematics. Through a linear algebra class, he found his way to two different summer REU programs and eventually to a PhD in Applied Mathematics from Iowa State University (2018). Being introduced to machine learning methods caused him to pursue data science jobs after graduation.\nDr. Dairyko served as a Senior Manager of Data Science at the Milwaukee Brewers and is now the Director of Ticketing Analytics at the Milwaukee Bucks. Helping the organization get the most out of budgeting, revenue, and ticket sales allows him to fully use his training in mathematics and data science.\n\n\n\n\nSource: hardin47.github.io/CURV/scholars/dairyko"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#statistician-of-the-day",
    "href": "slides/22-multinomial-logistic-pt2.html#statistician-of-the-day",
    "title": "Multinomial logistic regression",
    "section": "Statistician of the day",
    "text": "Statistician of the day\nAbout his role as a Senior Manager of Data Science for the MilwaukeeBrewers:\n\n“During the season, one of my main priorities is to produce game-by-game ticket and revenue projections. To do so, my group incorporates historical data—such as team performance, weather, and schedules—into multiple regression-based models and then consolidates the outputs in an easily-digestible format. A large codebase both automates and maintains this process; the codebase is regularly tweaked to ensure that it is agile enough to handle the constant usage and flow of new information.”\n\n“Our projections are most accurate when we utilize both qualitative and quantitative forecasts.”\n\n\n\nSource: From Academia to Major League Baseball: The Journey of a Data Scientist"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#statistician-of-the-day-1",
    "href": "slides/22-multinomial-logistic-pt2.html#statistician-of-the-day-1",
    "title": "Multinomial logistic regression",
    "section": "Statistician of the day",
    "text": "Statistician of the day\nAbout his role as a Senior Manager of Data Science for the Milwaukee Brewers:\n“Mathematicians are ultimately trained to develop problem-solving skills and apply them with persistence and creativity…Carefully reviewing the work—and perhaps redoing it a different way or approaching the issue from another angle—eventually leads to success…I do use the problem-solving strategies, persistence, and creativity that I have honed throughout my mathematical journey every single day.”\n“I would encourage students to become comfortable with navigating a programming language such as R or Python…Briefly stepping outside of mathematics and establishing computer science and statistics expertise is also useful…Finally, participating in conferences with data science content is an excellent way to gain exposure to more advanced topics in the field and build a network within the community.”\n\n\nSource: From Academia to Major League Baseball: The Journey of a Data Scientist"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#topics",
    "href": "slides/22-multinomial-logistic-pt2.html#topics",
    "title": "Multinomial logistic regression",
    "section": "Topics",
    "text": "Topics\n\n\nPredictions\nModel selection\nChecking conditions"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#computational-setup",
    "href": "slides/22-multinomial-logistic-pt2.html#computational-setup",
    "title": "Multinomial logistic regression",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(NHANES)\nlibrary(knitr)\nlibrary(patchwork)\nlibrary(colorblindr)\nlibrary(pROC)\nlibrary(Stat2Data)\nlibrary(nnet)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 20))"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#nhanes-data",
    "href": "slides/22-multinomial-logistic-pt2.html#nhanes-data",
    "title": "Multinomial logistic regression",
    "section": "NHANES Data",
    "text": "NHANES Data\n\n\nNational Health and Nutrition Examination Survey is conducted by the National Center for Health Statistics (NCHS).\nThe goal is to “assess the health and nutritional status of adults and children in the United States”.\nThis survey includes an interview and a physical examination."
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#variables",
    "href": "slides/22-multinomial-logistic-pt2.html#variables",
    "title": "Multinomial logistic regression",
    "section": "Variables",
    "text": "Variables\nGoal: Use a person’s age and whether they do regular physical activity to predict their self-reported health rating.\n\nOutcome: HealthGen: Self-reported rating of participant’s health in general. Excellent, Vgood, Good, Fair, or Poor.\nPredictors:\n\nAge: Age at time of screening (in years). Participants 80 or older were recorded as 80.\nPhysActive: Participant does moderate to vigorous-intensity sports, fitness or recreational activities."
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#the-data",
    "href": "slides/22-multinomial-logistic-pt2.html#the-data",
    "title": "Multinomial logistic regression",
    "section": "The data",
    "text": "The data\n\nnhanes_adult &lt;- NHANES |&gt;\n  filter(Age &gt;= 18) |&gt;\n  select(HealthGen, Age, PhysActive, Education) |&gt;\n  drop_na() |&gt;\n  mutate(obs_num = 1:n())\n\n\nglimpse(nhanes_adult)\n\nRows: 6,465\nColumns: 5\n$ HealthGen  &lt;fct&gt; Good, Good, Good, Good, Vgood, Vgood, Vgood, Vgood, Vgood, …\n$ Age        &lt;int&gt; 34, 34, 34, 49, 45, 45, 45, 66, 58, 54, 50, 33, 60, 56, 56,…\n$ PhysActive &lt;fct&gt; No, No, No, No, Yes, Yes, Yes, Yes, Yes, Yes, Yes, No, No, …\n$ Education  &lt;fct&gt; High School, High School, High School, Some College, Colleg…\n$ obs_num    &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#model-in-r",
    "href": "slides/22-multinomial-logistic-pt2.html#model-in-r",
    "title": "Multinomial logistic regression",
    "section": "Model in R",
    "text": "Model in R\n\nhealth_fit &lt;- multinom_reg() |&gt;\n  set_engine(\"nnet\") |&gt;\n  fit(HealthGen ~ Age + PhysActive, data = nhanes_adult)"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#model-summary",
    "href": "slides/22-multinomial-logistic-pt2.html#model-summary",
    "title": "Multinomial logistic regression",
    "section": "Model summary",
    "text": "Model summary\n\ntidy(health_fit) |&gt; kable(digits = 3)\n\n\n\n\ny.level\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nVgood\n(Intercept)\n1.265\n0.154\n8.235\n0.000\n\n\nVgood\nAge\n0.000\n0.003\n-0.014\n0.989\n\n\nVgood\nPhysActiveYes\n-0.332\n0.095\n-3.496\n0.000\n\n\nGood\n(Intercept)\n1.989\n0.150\n13.285\n0.000\n\n\nGood\nAge\n-0.003\n0.003\n-1.187\n0.235\n\n\nGood\nPhysActiveYes\n-1.011\n0.092\n-10.979\n0.000\n\n\nFair\n(Intercept)\n1.033\n0.174\n5.938\n0.000\n\n\nFair\nAge\n0.001\n0.003\n0.373\n0.709\n\n\nFair\nPhysActiveYes\n-1.662\n0.109\n-15.190\n0.000\n\n\nPoor\n(Intercept)\n-1.338\n0.299\n-4.475\n0.000\n\n\nPoor\nAge\n0.019\n0.005\n3.827\n0.000\n\n\nPoor\nPhysActiveYes\n-2.670\n0.236\n-11.308\n0.000"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#calculating-probabilities",
    "href": "slides/22-multinomial-logistic-pt2.html#calculating-probabilities",
    "title": "Multinomial logistic regression",
    "section": "Calculating probabilities",
    "text": "Calculating probabilities\n\nSuppose the repsonse variable has \\(K\\) categories and \\(k = 1\\) is the baseline category. For categories \\(2,\\ldots,K\\), the probability that the \\(i^{th}\\) observation is in the \\(j^{th}\\) category is\n\\[\n\\hat{\\pi}_{ij} = \\frac{\\exp\\{\\hat{\\beta}_{0j} + \\hat{\\beta}_{1j}x_{i1} + \\dots + \\hat{\\beta}_{pj}x_{ip}\\}}{1 + \\sum\\limits_{k=2}^K \\exp\\{\\hat{\\beta}_{0k} + \\hat{\\beta}_{1k}x_{i1} + \\dots \\hat{\\beta}_{pk}x_{ip}\\}}\n\\]\nFor the baseline category, \\(k=1\\), we calculate the probability \\(\\hat{\\pi}_{i1}\\) as\n\\[\n\\hat{\\pi}_{i1} = 1- \\sum\\limits_{k=2}^K \\hat{\\pi}_{ik}\n\\]"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#predicted-health-rating",
    "href": "slides/22-multinomial-logistic-pt2.html#predicted-health-rating",
    "title": "Multinomial logistic regression",
    "section": "Predicted health rating",
    "text": "Predicted health rating\nWe can use our model to predict a person’s perceived health rating given their age and whether they exercise.\n\nhealth_aug &lt;- augment(health_fit, new_data = nhanes_adult)\nhealth_aug\n\n# A tibble: 6,465 × 11\n   .pred_class .pred_Excellent .pred_Vgood .pred_Good .pred_Fair .pred_Poor\n   &lt;fct&gt;                 &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 2 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 3 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 4 Good                 0.0691       0.244      0.435     0.205     0.0467 \n 5 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 6 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 7 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 8 Vgood                0.157        0.400      0.342     0.0904    0.0102 \n 9 Vgood                0.156        0.397      0.349     0.0890    0.00872\n10 Vgood                0.156        0.396      0.352     0.0883    0.00804\n# ℹ 6,455 more rows\n# ℹ 5 more variables: HealthGen &lt;fct&gt;, Age &lt;int&gt;, PhysActive &lt;fct&gt;,\n#   Education &lt;fct&gt;, obs_num &lt;int&gt;"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#actual-vs.-predicted-health-rating",
    "href": "slides/22-multinomial-logistic-pt2.html#actual-vs.-predicted-health-rating",
    "title": "Multinomial logistic regression",
    "section": "Actual vs. predicted health rating",
    "text": "Actual vs. predicted health rating\nFor each observation, the predicted perceived health rating is the category with the highest predicted probability.\n\nhealth_aug |&gt; select(contains(\"pred\"))\n\n# A tibble: 6,465 × 6\n   .pred_class .pred_Excellent .pred_Vgood .pred_Good .pred_Fair .pred_Poor\n   &lt;fct&gt;                 &lt;dbl&gt;       &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n 1 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 2 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 3 Good                 0.0687       0.243      0.453     0.201     0.0348 \n 4 Good                 0.0691       0.244      0.435     0.205     0.0467 \n 5 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 6 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 7 Vgood                0.155        0.393      0.359     0.0868    0.00671\n 8 Vgood                0.157        0.400      0.342     0.0904    0.0102 \n 9 Vgood                0.156        0.397      0.349     0.0890    0.00872\n10 Vgood                0.156        0.396      0.352     0.0883    0.00804\n# ℹ 6,455 more rows"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#confusion-matrix",
    "href": "slides/22-multinomial-logistic-pt2.html#confusion-matrix",
    "title": "Multinomial logistic regression",
    "section": "Confusion matrix",
    "text": "Confusion matrix\n\nhealth_conf &lt;- health_aug |&gt; \n  count(HealthGen, .pred_class, .drop = FALSE) |&gt;\n  pivot_wider(names_from = .pred_class, values_from = n)\n\nhealth_conf\n\n# A tibble: 5 × 6\n  HealthGen Excellent Vgood  Good  Fair  Poor\n  &lt;fct&gt;         &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n1 Excellent         0   528   210     0     0\n2 Vgood             0  1341   743     0     0\n3 Good              0  1226  1316     0     0\n4 Fair              0   296   625     0     0\n5 Poor              0    24   156     0     0"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#actual-vs.-predicted-health-rating-1",
    "href": "slides/22-multinomial-logistic-pt2.html#actual-vs.-predicted-health-rating-1",
    "title": "Multinomial logistic regression",
    "section": "Actual vs. predicted health rating",
    "text": "Actual vs. predicted health rating\n\nWhy do you think no observations were predicted to have a rating of “Excellent”, “Fair”, or “Poor”?"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#roc-curves",
    "href": "slides/22-multinomial-logistic-pt2.html#roc-curves",
    "title": "Multinomial logistic regression",
    "section": "ROC curves",
    "text": "ROC curves\nROC curves for multiclass outcomes use a one-vs-all approach: calculate multiple curves, one per level vs. all other levels.\n\nhealth_aug |&gt; \n  roc_curve(\n    truth = HealthGen, \n    .pred_Excellent:.pred_Poor\n  ) |&gt; \n  autoplot()"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#roc-curves-1",
    "href": "slides/22-multinomial-logistic-pt2.html#roc-curves-1",
    "title": "Multinomial logistic regression",
    "section": "ROC curves",
    "text": "ROC curves"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#roc-curve-under-the-hood",
    "href": "slides/22-multinomial-logistic-pt2.html#roc-curve-under-the-hood",
    "title": "Multinomial logistic regression",
    "section": "ROC curve: under the hood",
    "text": "ROC curve: under the hood\nAn additional column, .level, identifies the “one” column in the one-vs-all calculation:\n\nhealth_aug |&gt; \n  roc_curve(\n    truth = HealthGen, \n    .pred_Excellent:.pred_Poor\n  )\n\n# A tibble: 620 × 4\n   .level    .threshold specificity sensitivity\n   &lt;chr&gt;          &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1 Excellent  -Inf          0             1    \n 2 Excellent     0.0681     0             1    \n 3 Excellent     0.0682     0.00664       0.997\n 4 Excellent     0.0682     0.0138        0.992\n 5 Excellent     0.0683     0.0192        0.989\n 6 Excellent     0.0683     0.0243        0.981\n 7 Excellent     0.0684     0.0297        0.970\n 8 Excellent     0.0684     0.0353        0.969\n 9 Excellent     0.0684     0.0426        0.951\n10 Excellent     0.0685     0.0492        0.947\n# ℹ 610 more rows"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#comparing-nested-models",
    "href": "slides/22-multinomial-logistic-pt2.html#comparing-nested-models",
    "title": "Multinomial logistic regression",
    "section": "Comparing nested models",
    "text": "Comparing nested models\n\nSuppose there are two models:\n\nReduced model includes predictors \\(x_1, \\ldots, x_q\\)\nFull model includes predictors \\(x_1, \\ldots, x_q, x_{q+1}, \\ldots, x_p\\)\n\nWe want to test the following hypotheses:\n\n\\(H_0: \\beta_{q+1} = \\dots = \\beta_p = 0\\)\n\\(H_A: \\text{ at least 1 }\\beta_j \\text{ is not } 0\\)\n\nTo do so, we will use the drop-in-deviance test (very similar to logistic regression)"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#add-education-to-the-model",
    "href": "slides/22-multinomial-logistic-pt2.html#add-education-to-the-model",
    "title": "Multinomial logistic regression",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nWe consider adding the participants’ Education level to the model.\n\nEducation takes values 8thGrade, 9-11thGrade, HighSchool, SomeCollege, and CollegeGrad\n\nModels we’re testing:\n\nReduced model: Age, PhysActive\nFull model: Age, PhysActive, Education\n\n\n\n\\[\n\\begin{align}\n&H_0: \\beta_{9-11thGrade} = \\beta_{HighSchool} = \\beta_{SomeCollege} = \\beta_{CollegeGrad} = 0\\\\\n&H_a: \\text{ at least one }\\beta_j \\text{ is not equal to }0\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#add-education-to-the-model-1",
    "href": "slides/22-multinomial-logistic-pt2.html#add-education-to-the-model-1",
    "title": "Multinomial logistic regression",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nreduced_fit &lt;- multinom_reg() |&gt;\n  set_engine(\"nnet\") |&gt;\n  fit(HealthGen ~ Age + PhysActive,\n  data = nhanes_adult)\n\nfull_fit &lt;- multinom_reg() |&gt;\n  set_engine(\"nnet\") |&gt;\n  fit(HealthGen ~ Age + PhysActive + Education,\n  data = nhanes_adult)"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#add-education-to-the-model-2",
    "href": "slides/22-multinomial-logistic-pt2.html#add-education-to-the-model-2",
    "title": "Multinomial logistic regression",
    "section": "Add Education to the model?",
    "text": "Add Education to the model?\n\nanova(reduced_fit$fit, full_fit$fit, test = \"Chisq\") |&gt;\n  kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nResid. df\nResid. Dev\nTest\nDf\nLR stat.\nPr(Chi)\n\n\n\n\nAge + PhysActive\n25848\n16994.23\n\nNA\nNA\nNA\n\n\nAge + PhysActive + Education\n25832\n16505.10\n1 vs 2\n16\n489.132\n0\n\n\n\n\n\n\nAt least one coefficient associated with Education is non-zero. Therefore, we will include Education in the model."
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#model-with-education",
    "href": "slides/22-multinomial-logistic-pt2.html#model-with-education",
    "title": "Multinomial logistic regression",
    "section": "Model with Education",
    "text": "Model with Education\n\ntidy(full_fit, conf.int = T) |&gt; print(n = 28) \n\n# A tibble: 28 × 8\n   y.level term         estimate std.error statistic  p.value conf.low conf.high\n   &lt;chr&gt;   &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n 1 Vgood   (Intercept)   5.82e-1   0.301      1.93   5.36e- 2 -0.00914   1.17   \n 2 Vgood   Age           1.12e-3   0.00266    0.419  6.75e- 1 -0.00411   0.00634\n 3 Vgood   PhysActiveY… -2.64e-1   0.0985    -2.68   7.33e- 3 -0.457    -0.0711 \n 4 Vgood   Education9 …  7.68e-1   0.308      2.49   1.27e- 2  0.164     1.37   \n 5 Vgood   EducationHi…  7.01e-1   0.280      2.51   1.21e- 2  0.153     1.25   \n 6 Vgood   EducationSo…  7.88e-1   0.271      2.90   3.71e- 3  0.256     1.32   \n 7 Vgood   EducationCo…  4.08e-1   0.268      1.52   1.28e- 1 -0.117     0.933  \n 8 Good    (Intercept)   2.04e+0   0.272      7.51   5.77e-14  1.51      2.57   \n 9 Good    Age          -1.72e-3   0.00263   -0.651  5.15e- 1 -0.00688   0.00345\n10 Good    PhysActiveY… -7.58e-1   0.0961    -7.88   3.16e-15 -0.946    -0.569  \n11 Good    Education9 …  3.60e-1   0.275      1.31   1.90e- 1 -0.179     0.899  \n12 Good    EducationHi…  8.52e-2   0.247      0.345  7.30e- 1 -0.399     0.569  \n13 Good    EducationSo… -1.13e-2   0.239     -0.0472 9.62e- 1 -0.480     0.457  \n14 Good    EducationCo… -8.91e-1   0.236     -3.77   1.65e- 4 -1.35     -0.427  \n15 Fair    (Intercept)   2.12e+0   0.288      7.35   1.91e-13  1.55      2.68   \n16 Fair    Age           3.35e-4   0.00312    0.107  9.14e- 1 -0.00578   0.00645\n17 Fair    PhysActiveY… -1.19e+0   0.115    -10.4    3.50e-25 -1.42     -0.966  \n18 Fair    Education9 … -2.24e-1   0.279     -0.802  4.22e- 1 -0.771     0.323  \n19 Fair    EducationHi… -8.32e-1   0.252     -3.31   9.44e- 4 -1.33     -0.339  \n20 Fair    EducationSo… -1.34e+0   0.246     -5.46   4.71e- 8 -1.82     -0.861  \n21 Fair    EducationCo… -2.51e+0   0.253     -9.91   3.67e-23 -3.00     -2.01   \n22 Poor    (Intercept)  -2.00e-1   0.411     -0.488  6.26e- 1 -1.01      0.605  \n23 Poor    Age           1.79e-2   0.00509    3.53   4.21e- 4  0.00797   0.0279 \n24 Poor    PhysActiveY… -2.27e+0   0.242     -9.38   6.81e-21 -2.74     -1.79   \n25 Poor    Education9 … -3.60e-1   0.353     -1.02   3.08e- 1 -1.05      0.332  \n26 Poor    EducationHi… -1.15e+0   0.334     -3.44   5.86e- 4 -1.81     -0.494  \n27 Poor    EducationSo… -1.07e+0   0.316     -3.40   6.77e- 4 -1.69     -0.454  \n28 Poor    EducationCo… -2.32e+0   0.366     -6.34   2.27e-10 -3.04     -1.60"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#compare-models-using-aic-bic",
    "href": "slides/22-multinomial-logistic-pt2.html#compare-models-using-aic-bic",
    "title": "Multinomial logistic regression",
    "section": "Compare models using AIC & BIC",
    "text": "Compare models using AIC & BIC\n\n\nAIC\nReduced model:\n\nglance(reduced_fit)$AIC\n\n[1] 17018.23\n\n\nFull model:\n\nglance(full_fit)$AIC\n\n[1] 16561.1\n\n\n\nBIC\nReduced model:\n\nglance(reduced_fit)$deviance + log(nrow(nhanes_adult)) * glance(reduced_fit)$edf\n\n[1] 17099.52\n\n\nFull model:\n\nglance(full_fit)$deviance + log(nrow(nhanes_adult)) * glance(full_fit)$edf\n\n[1] 16750.77"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#conditions-for-inference",
    "href": "slides/22-multinomial-logistic-pt2.html#conditions-for-inference",
    "title": "Multinomial logistic regression",
    "section": "Conditions for inference",
    "text": "Conditions for inference\nWe want to check the following conditions for inference for the multinomial logistic regression model:\n\nLinearity: Is there a linear relationship between the log-odds and the predictor variables?\nRandomness: Was the sample randomly selected? Or can we reasonably treat it as random?\nIndependence: Are the observations independent?"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#checking-linearity",
    "href": "slides/22-multinomial-logistic-pt2.html#checking-linearity",
    "title": "Multinomial logistic regression",
    "section": "Checking linearity",
    "text": "Checking linearity\nSimilar to logistic regression, we will check linearity by examining empirical logit plots between each level of the response and the quantitative predictor variables.\n\nnhanes_adult &lt;- nhanes_adult |&gt;\n  mutate(\n    Excellent = factor(if_else(HealthGen == \"Excellent\", \"1\", \"0\")),\n    Vgood = factor(if_else(HealthGen == \"Vgood\", \"1\", \"0\")),\n    Good = factor(if_else(HealthGen == \"Good\", \"1\", \"0\")),\n    Fair = factor(if_else(HealthGen == \"Fair\", \"1\", \"0\")),\n    Poor = factor(if_else(HealthGen == \"Poor\", \"1\", \"0\"))\n  )"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#checking-linearity-1",
    "href": "slides/22-multinomial-logistic-pt2.html#checking-linearity-1",
    "title": "Multinomial logistic regression",
    "section": "Checking linearity",
    "text": "Checking linearity\nemplogitplot1(Excellent ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Excellent vs. Age\")\nemplogitplot1(Vgood ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Vgood vs. Age\")"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#checking-linearity-2",
    "href": "slides/22-multinomial-logistic-pt2.html#checking-linearity-2",
    "title": "Multinomial logistic regression",
    "section": "Checking linearity",
    "text": "Checking linearity\nemplogitplot1(Good ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Good vs. Age\")\nemplogitplot1(Fair ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Fair vs. Age\")"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#checking-linearity-3",
    "href": "slides/22-multinomial-logistic-pt2.html#checking-linearity-3",
    "title": "Multinomial logistic regression",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nemplogitplot1(Poor ~ Age, data = nhanes_adult, \n              ngroups = 10, main = \"Poor vs. Age\")\n\n\n\n✅ The linearity condition is satisfied. There is generally a linear relationship between the empirical logit and the quantitative predictor variable, Age, for each level of the response."
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#checking-randomness",
    "href": "slides/22-multinomial-logistic-pt2.html#checking-randomness",
    "title": "Multinomial logistic regression",
    "section": "Checking randomness",
    "text": "Checking randomness\nWe can check the randomness condition based on the context of the data and how the observations were collected.\n\nWas the sample randomly selected?\nIf the sample was not randomly selected, ask whether there is reason to believe the observations in the sample differ systematically from the population of interest.\n\n\n✅ The randomness condition is satisfied. The participants were randomly selected, and thus we do not have reason to believe that the participants in this study differ systematically from adults in the U.S."
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#checking-independence",
    "href": "slides/22-multinomial-logistic-pt2.html#checking-independence",
    "title": "Multinomial logistic regression",
    "section": "Checking independence",
    "text": "Checking independence\nWe can check the independence condition based on the context of the data and how the observations were collected.\nIndependence is most often violated if the data were collected over time or there is a strong spatial relationship between the observations.\n\n✅ The independence condition is satisfied. The participants were randomly selected, so it is reasonable to conclude that the participants’ health and behavior characteristics are independent of one another."
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#recap",
    "href": "slides/22-multinomial-logistic-pt2.html#recap",
    "title": "Multinomial logistic regression",
    "section": "Recap",
    "text": "Recap\n\nPredictions\nModel selection for inference\nChecking conditions for inference"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#full-multinomial-modeling-workflow",
    "href": "slides/22-multinomial-logistic-pt2.html#full-multinomial-modeling-workflow",
    "title": "Multinomial logistic regression",
    "section": "Full multinomial modeling workflow",
    "text": "Full multinomial modeling workflow\n\njuliasilge.com/blog/multinomial-volcano-eruptions\njuliasilge.com/blog/nber-papers"
  },
  {
    "objectID": "slides/22-multinomial-logistic-pt2.html#questions-for-exam-02-review",
    "href": "slides/22-multinomial-logistic-pt2.html#questions-for-exam-02-review",
    "title": "Multinomial logistic regression",
    "section": "Questions for Exam 02 review",
    "text": "Questions for Exam 02 review\n\nSubmit your questions for the Exam 02 review: https://forms.office.com/r/cTyeRjVmfb\n\nExam 02 will cover multiple linear regression, logistic regression, and multinomial logistic regression.\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/anova-table.html",
    "href": "slides/anova-table.html",
    "title": "ANOVA Output in R",
    "section": "",
    "text": "We will use the Tips data set for this example.\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(knitr)\nThe variables of interest in this analysis are"
  },
  {
    "objectID": "slides/anova-table.html#model-fit",
    "href": "slides/anova-table.html#model-fit",
    "title": "ANOVA Output in R",
    "section": "Model fit",
    "text": "Model fit\n\ntip_fit &lt;- lm(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.17\n0.37\n-0.46\n0.64\n\n\nParty\n1.84\n0.12\n14.76\n0.00\n\n\nAgeMiddle\n1.01\n0.41\n2.47\n0.01\n\n\nAgeSenCit\n1.39\n0.48\n2.86\n0.00"
  },
  {
    "objectID": "slides/anova-table.html#anova",
    "href": "slides/anova-table.html#anova",
    "title": "ANOVA Output in R",
    "section": "ANOVA",
    "text": "ANOVA\nBelow is the ANOVA output for the model fit above.\n\nanova(tip_fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA\n\n\n\n\n\nWe will focus on the sum of squares in this document. The sum of squares are as follows:\n\\[\\begin{aligned}\n&SS_{Party} = 1188.64 \\\\\n&SS_{Age|Party} = 38.03 \\\\\n&SS_{Error} = SS_{Residuals} = 686.44 \\\\\n&SS_{Total} = 1913.11\n\\end{aligned}\\]"
  },
  {
    "objectID": "slides/anova-table.html#sum-of-squares-in-anova-table",
    "href": "slides/anova-table.html#sum-of-squares-in-anova-table",
    "title": "ANOVA Output in R",
    "section": "Sum of squares in ANOVA table",
    "text": "Sum of squares in ANOVA table\nR uses a sequential method to calculate sum of squares for the variables in the model. This means that the sum of squares attributed to each variable is the variation in the response explained by that variable after accounting for the total variation explained by the other variables already in the model.\nThe order of the sequence is determined by the order of the variables in the model fit code. This order is reflected in the order the variables appear in the ANOVA output. The sequential sum of squares attributed to each variable will change if the order of the variables in the model changes; however, the sum of squares attributed to the model overall will not change, regardless of the order of the variables.\nLet’s take a look at the sum of squares for Party, the first variable in the model. This value is calculated as the total variation in Tips explained by Party only. We can calculate this value by looking at the ANOVA table for simple linear regression model where Party is the only predictor.\n\nparty_fit &lt;- lm(Tip ~ Party, data = tips)\n\nanova(party_fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n274\n0\n\n\nResiduals\n167\n724.47\n4.34\nNA\nNA\n\n\n\n\n\nNotice that the sum of squares in this table is the value of \\(SS_{Party}\\) above.\nNext, let’s add Age to the model. The sum of squares associated with Age is the additional variation in Tips explained by Age after accounting for variation explained by Party. This can be understood as the additional model variation in the model with Party and Age compared to a model that only includes Party. We can calculate this additional variation as follows:\n\nanova(party_fit, tip_fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf.residual\nrss\ndf\nsumsq\nstatistic\np.value\n\n\n\n\nTip ~ Party\n167\n724.47\nNA\nNA\nNA\nNA\n\n\nTip ~ Party + Age\n165\n686.44\n2\n38.03\n4.57\n0.01\n\n\n\n\n\nNotice that the sum of squares in this table is the value of \\(SS_{Age|Party}\\) above.\n\n\n\n\n\n\nNote\n\n\n\nWhen we input two model is the anova() function, e.g., anova(Model 1, Model 2), the output produced is the additional sum of squares accounted for by the new variable(s) in Model 2 after accounting for the variables in Model 1. In this case, it is the additional sum of squares accounted for by Age after accounting for Party.\n\n\nWhen we use the ANOVA table, we are most interested in the variation in the response explained by the entire model, not the contribution from each variable. Therefore, we will primarily consider the \\(SS_{Model}\\), Sum of Squares Model. Because sum of squares are additive, it can be calculated as\n\\[\n\\begin{aligned}\nSS_{Model} &= SS_{Total} - SS_{Error} \\\\\n&= 1913.11 - 686.44 \\\\\n& = 1226.67\n\\end{aligned}\n\\]It can also be calculated as\n\\[\n\\begin{aligned}\nSS_{Model} &= SS_{Party} + SS_{Age | Party} \\\\\n& = 1188.64 + 38.03 \\\\\n& = 1226.67\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/lab-01.html#do-you-have-the-lab-01-repo",
    "href": "slides/lab-01.html#do-you-have-the-lab-01-repo",
    "title": "Lab 01",
    "section": "Do you have the lab-01 repo?",
    "text": "Do you have the lab-01 repo?\n\nGo to the GitHub course organization: https://github.com/sta210-fa23\nYou should see a repo with the prefix lab-01- followed by your GitHub username\nIf you do not have this repo\n\nClickhere to create your lab-01 repo, AND\nGo to the course organization and accept the invite in the banner at the top of the page. If you do not see the invite, click here to (re)submit your GitHub username."
  },
  {
    "objectID": "slides/lab-01.html#demo",
    "href": "slides/lab-01.html#demo",
    "title": "Lab 01",
    "section": "Demo",
    "text": "Demo\nFollow along as your TA demonstrates the following:\n\nConfigure Git using SSH\nClone repo (using SSH) and start new project in RStudio\nRender document and produce PDF\nUpdate name in YAML in the Quarto doc\n\nRender, commit, push changes to GitHub\nSee updates in your GitHub repo\n\nAdd an R code chunk to the Quarto doc"
  },
  {
    "objectID": "slides/lab-01.html#tips-for-working-on-lab",
    "href": "slides/lab-01.html#tips-for-working-on-lab",
    "title": "Lab 01",
    "section": "Tips for working on lab",
    "text": "Tips for working on lab\n\nYou do not have to finish the lab in class, they will always be due Friday (Tuesday labs) or Sunday (Thursday labs). One work strategy is to get through portions that you think will be most challenging (which initially might be the coding component) during lab when a TA can help you on the spot and leave the narrative writing until later.\nDo not pressure each other to finish early (particularly once you start working on teams); use the time wisely to really learn the material and produce a quality report."
  },
  {
    "objectID": "slides/lab-01.html#when-youre-done-with-lab",
    "href": "slides/lab-01.html#when-youre-done-with-lab",
    "title": "Lab 01",
    "section": "When you’re done with lab",
    "text": "When you’re done with lab\n\nMake sure all your final changes have been pushed to your GitHub repo\n\nSubmit the PDF of your responses to Gradescope\n\nYou can access Gradescope through Sakai or the course website\nLogin using your Duke NetID credentials\nSee Lab 01 instructions for details on submitting an assignment on Gradescope"
  },
  {
    "objectID": "slides/lab-01.html#lab-01-park-access",
    "href": "slides/lab-01.html#lab-01-park-access",
    "title": "Lab 01",
    "section": "Lab 01: Park access",
    "text": "Lab 01: Park access\n\nToday’s lab focuses on exploratory data analysis and simple linear regression. Read the lab instructions carefully.\nThere are markers throughout suggesting when to render, commit, and push changes to GitHub. These are to help you start using version control in your workflow.\nThere are points for having a neatly formatted document and implementing a reproducible workflow. See the Workflow & Formatting section in the Lab 01 instructions.\n\n🔗 sta210-fa23.netlify.app/labs/lab-01.html\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/lab-03.html#reminders",
    "href": "slides/lab-03.html#reminders",
    "title": "Lab 03",
    "section": "Reminders",
    "text": "Reminders\nMake sure to do the following as you complete the assignment:\n\nWrite all narrative in complete sentences.\nUse informative axis titles and labels on all graphs.\nImplement version control in your reproducible workflow.\n\nThroughout the assignment periodically render your Quarto document to produce the updated PDF, commit the changes in the Git pane, and push the updated files to GitHub.\nBenchmark: Push changes to GitHub at least three times as you work on the assignment."
  },
  {
    "objectID": "slides/lab-03.html#todays-lab",
    "href": "slides/lab-03.html#todays-lab",
    "title": "Lab 03",
    "section": "Today’s lab",
    "text": "Today’s lab\n\nFocused on mathematical inference for simple linear regression and checking model conditions.\n\nWe will talk about model conditions in the September 20 lecture.\n\nRemember to mark all pages in your Gradescope submission. The first page should be marked for the “Workflow & formatting” section.\nUse lectures and application exercises as reference as you complete the lab.\n\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/lab-05.html#team-workflow",
    "href": "slides/lab-05.html#team-workflow",
    "title": "Lab 05",
    "section": "Team workflow",
    "text": "Team workflow\n\nOnly one team member should type at a time. There are no markers in today’s lab, but you should use a similar workflow as Lab 04.\n\nEvery team member should still be engaged in discussion for all questions, even if it’s not your turn type.\n\nDon’t forget to pull to get your teammates’ updates before making changes to the .qmd file.\n\n\n\n\n\n\nImportant\n\n\nOnly one submission per team on Gradescope. Carefully read the submission instructions at the end of the lab instructions."
  },
  {
    "objectID": "slides/lab-05.html#tips-for-working-on-a-team",
    "href": "slides/lab-05.html#tips-for-working-on-a-team",
    "title": "Lab 05",
    "section": "Tips for working on a team",
    "text": "Tips for working on a team\n\nDo not pressure each other to finish early; use the time wisely to really learn the material and produce a quality report.\nThe labs are structured to help you learn the steps of a data analysis. Do not split up the lab among the team members; work on it together in its entirety.\nEveryone has something to contribute! Use the lab groups as an opportunity to share ideas and learn from each other."
  },
  {
    "objectID": "slides/lab-05.html#sta-210-mid-semester-survey",
    "href": "slides/lab-05.html#sta-210-mid-semester-survey",
    "title": "Lab 05",
    "section": "STA 210 Mid Semester Survey",
    "text": "STA 210 Mid Semester Survey\nPlease take a few minutes to complete the STA 210 Mid Semester Survey.\n\nClick here to access the survey."
  },
  {
    "objectID": "slides/lab-05.html#resources-for-lab-04",
    "href": "slides/lab-05.html#resources-for-lab-04",
    "title": "Lab 05",
    "section": "Resources for Lab 04",
    "text": "Resources for Lab 04\n\nLecture notes:\n\nFeature engineering\nFeature engineering: Model workflow\n\nRecipes package function reference\nTidy Modeling in R - Chapter 8: Feature engineering with recipes\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#computational-set-up",
    "href": "slides/xx-slr-transformations-outliers.html#computational-set-up",
    "title": "SLR: Transformations + Outliers",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(ggformula)   # for creating plots using formulas\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(moderndive)  # for house_price dataset\nlibrary(fivethirtyeight) # for fandango dataset\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#data-house_prices",
    "href": "slides/xx-slr-transformations-outliers.html#data-house_prices",
    "title": "SLR: Transformations + Outliers",
    "section": "Data: house_prices",
    "text": "Data: house_prices\n\n\nContains house sale prices for King County, which includes Seattle, from homes sold between May 2014 and May 2015\nObtained from Kaggle.com\n\n\n\nglimpse(house_prices)\n\nRows: 21,613\nColumns: 21\n$ id            &lt;chr&gt; \"7129300520\", \"6414100192\", \"5631500400\", \"2487200875\", …\n$ date          &lt;date&gt; 2014-10-13, 2014-12-09, 2015-02-25, 2014-12-09, 2015-02…\n$ price         &lt;dbl&gt; 221900, 538000, 180000, 604000, 510000, 1225000, 257500,…\n$ bedrooms      &lt;int&gt; 3, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 5, 4, 3, 4, 2,…\n$ bathrooms     &lt;dbl&gt; 1.00, 2.25, 1.00, 3.00, 2.00, 4.50, 2.25, 1.50, 1.00, 2.…\n$ sqft_living   &lt;int&gt; 1180, 2570, 770, 1960, 1680, 5420, 1715, 1060, 1780, 189…\n$ sqft_lot      &lt;int&gt; 5650, 7242, 10000, 5000, 8080, 101930, 6819, 9711, 7470,…\n$ floors        &lt;dbl&gt; 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1…\n$ waterfront    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ view          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,…\n$ condition     &lt;fct&gt; 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4,…\n$ grade         &lt;fct&gt; 7, 7, 6, 7, 8, 11, 7, 7, 7, 7, 8, 7, 7, 7, 7, 9, 7, 7, 7…\n$ sqft_above    &lt;int&gt; 1180, 2170, 770, 1050, 1680, 3890, 1715, 1060, 1050, 189…\n$ sqft_basement &lt;int&gt; 0, 400, 0, 910, 0, 1530, 0, 0, 730, 0, 1700, 300, 0, 0, …\n$ yr_built      &lt;int&gt; 1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 20…\n$ yr_renovated  &lt;int&gt; 0, 1991, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ zipcode       &lt;fct&gt; 98178, 98125, 98028, 98136, 98074, 98053, 98003, 98198, …\n$ lat           &lt;dbl&gt; 47.5112, 47.7210, 47.7379, 47.5208, 47.6168, 47.6561, 47…\n$ long          &lt;dbl&gt; -122.257, -122.319, -122.233, -122.393, -122.045, -122.0…\n$ sqft_living15 &lt;int&gt; 1340, 1690, 2720, 1360, 1800, 4760, 2238, 1650, 1780, 23…\n$ sqft_lot15    &lt;int&gt; 5650, 7639, 8062, 5000, 7503, 101930, 6819, 9711, 8113, …"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#variables",
    "href": "slides/xx-slr-transformations-outliers.html#variables",
    "title": "SLR: Transformations + Outliers",
    "section": "Variables",
    "text": "Variables\n\nOutcome\n\nprice: the sale price\n\nPredictor\n\nsqft_living: the square footage of the home"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#recap-fit-the-model",
    "href": "slides/xx-slr-transformations-outliers.html#recap-fit-the-model",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\nhp_fit &lt;- lm(price ~ sqft_living, data = house_prices)\ntidy(hp_fit) |&gt;  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-43580.74\n4402.69\n-9.90\n0\n\n\nsqft_living\n280.62\n1.94\n144.92\n0\n\n\n\n\n\n\nModel: \\(\\text{price} = -43580.74 + 280.62\\times\\text{sqft_living}\\)\nInterpretation: If the square footage of the house increases by 1, the price increases by and average of $280.62 and a (theoretical) house with 0 square feet with cost $-43,580.74."
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#recap-fit-the-model-1",
    "href": "slides/xx-slr-transformations-outliers.html#recap-fit-the-model-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\n\nCode\ngf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#recap-model-conditions",
    "href": "slides/xx-slr-transformations-outliers.html#recap-model-conditions",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Model conditions",
    "text": "Recap: Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other\n\n\n\nHow should we check these assumptions?"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#recap-residual-histogram",
    "href": "slides/xx-slr-transformations-outliers.html#recap-residual-histogram",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Residual Histogram",
    "text": "Recap: Residual Histogram\n\n\nCode\nhp_aug &lt;- augment(hp_fit)\n\ngf_histogram(~.resid, data = hp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Residual Histogram\")"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#recap-qq-plot-of-residuals",
    "href": "slides/xx-slr-transformations-outliers.html#recap-qq-plot-of-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: QQ-Plot of Residuals",
    "text": "Recap: QQ-Plot of Residuals\n\n\nCode\ngf_qq(~.resid, data = hp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#recap-residuals-vs.-fitted-values",
    "href": "slides/xx-slr-transformations-outliers.html#recap-residuals-vs.-fitted-values",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Residuals vs. Fitted Values",
    "text": "Recap: Residuals vs. Fitted Values\n\n\nCode\ngf_point(.resid ~ .fitted, data = hp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#are-model-conditions-satisfied",
    "href": "slides/xx-slr-transformations-outliers.html#are-model-conditions-satisfied",
    "title": "SLR: Transformations + Outliers",
    "section": "Are model conditions satisfied?",
    "text": "Are model conditions satisfied?\n\nLinearity: ❓\nConstant variance: ❌\nNormality: ❌\nIndependence: ✅"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#what-to-do-when-regression-conditions-are-violated",
    "href": "slides/xx-slr-transformations-outliers.html#what-to-do-when-regression-conditions-are-violated",
    "title": "SLR: Transformations + Outliers",
    "section": "What to do when regression conditions are violated",
    "text": "What to do when regression conditions are violated\nExamples:\n\nLack of normality in residuals\nPatterns in residuals\nHeteroscedasticity (nonconstant variance)\nOutliers: influential points, large residuals"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#data-transformations",
    "href": "slides/xx-slr-transformations-outliers.html#data-transformations",
    "title": "SLR: Transformations + Outliers",
    "section": "Data Transformations",
    "text": "Data Transformations\nCan be used to:\n\nAddress nonlinear patterns\nStabilize variance\nRemove skewness from resiudals\nMinimize effects of outliers"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#common-transformations",
    "href": "slides/xx-slr-transformations-outliers.html#common-transformations",
    "title": "SLR: Transformations + Outliers",
    "section": "Common Transformations",
    "text": "Common Transformations\nFor either the response \\(Y\\) or predictor \\(X\\):\n\nLogarithm \\(Z \\to \\log(Z)\\)\n\nNote: “log” means “log base \\(e\\)”\n\nSquare Root \\(Z \\to \\sqrt{Z}\\)\nExponential \\(Z \\to e^Z\\)\nPower functions \\(Z \\to Z^2, Z^3, Z^4, \\ldots\\)\nReciprocal \\(Z \\to 1/Z\\)"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-planets",
    "href": "slides/xx-slr-transformations-outliers.html#example-planets",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nplanets &lt;- read_csv(\"data/planets.csv\")\n\n\n\\(Y =\\) length of the “year” for planets\n\\(X =\\) distance from the Sun"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-planets-1",
    "href": "slides/xx-slr-transformations-outliers.html#example-planets-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Distance ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-planets-2",
    "href": "slides/xx-slr-transformations-outliers.html#example-planets-2",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(log(Distance) ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-planets-3",
    "href": "slides/xx-slr-transformations-outliers.html#example-planets-3",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(Distance ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-planets-4",
    "href": "slides/xx-slr-transformations-outliers.html#example-planets-4",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Distance) ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-mammal-species",
    "href": "slides/xx-slr-transformations-outliers.html#example-mammal-species",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nSpeciesArea &lt;- read_csv(\"data/SpeciesArea.csv\")\n\n\n\\(Y =\\) number of mammal species on an island\n\\(X =\\) area of the island"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-mammal-species-1",
    "href": "slides/xx-slr-transformations-outliers.html#example-mammal-species-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Species ~ Area, data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-mammal-species-2",
    "href": "slides/xx-slr-transformations-outliers.html#example-mammal-species-2",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Species) ~ log(Area), data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#why-a-log-transformation",
    "href": "slides/xx-slr-transformations-outliers.html#why-a-log-transformation",
    "title": "SLR: Transformations + Outliers",
    "section": "Why a Log Transformation?",
    "text": "Why a Log Transformation?\n\nSome relationship are multiplicative (not linear)\n\nExample: Area of a circle\n\\[\n\\begin{aligned}\nA &= \\pi r^2 \\text{ (not linear)}\\\\\n\\log(A) &= \\log(\\pi r^2)\n= \\log(\\pi) + 2\\log(r)\\\\\n\\log(A) &= \\beta_0 + \\beta_1\\times \\log(r)\\\\\n\\implies & \\log(A) \\text{ is a linear function of } \\log(r)\n\\end{aligned}\n\\]\nLook for:\n\nStrongly right-skewed distributions\nCurvative in scatterplot\nIncreasing variability in scatterplot"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#back-to-house_sales",
    "href": "slides/xx-slr-transformations-outliers.html#back-to-house_sales",
    "title": "SLR: Transformations + Outliers",
    "section": "Back to house_sales",
    "text": "Back to house_sales\n\n\nCode\np1 &lt;- gf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np2 &lt;- gf_point(log(price) ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np3 &lt;- gf_point(price ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np4 &lt;- gf_point(log(price) ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\n(p1 + p2)/ (p3 + p4)"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#fitting-transformed-models",
    "href": "slides/xx-slr-transformations-outliers.html#fitting-transformed-models",
    "title": "SLR: Transformations + Outliers",
    "section": "Fitting Transformed Models",
    "text": "Fitting Transformed Models\n\n\n\nlogprice_model &lt;- lm(log(price) ~ sqft_living, data = house_prices)\ntidy(logprice_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.2184641\n0.0063741\n1916.8830\n0\n\n\nsqft_living\n0.0003987\n0.0000028\n142.2326\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &= 12.22  + 3.99\\times 10^{-4}\\times X\\\\\nY &= e^{12.22 + 3.99\\times 10^{-4}\\times X}\\\\\n&= 202805\\times e^{3.99\\times 10^{-4}\\times X}\n\\end{aligned}\n\\]\n\n\nloglog_model &lt;- lm(log(price) ~ log(sqft_living), data = house_prices)\ntidy(loglog_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.729916\n0.0470620\n143.0011\n0\n\n\nlog(sqft_living)\n0.836771\n0.0062233\n134.4587\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &=6.73 + 0.837\\times \\log(X)\\\\\n\\log(Y) &= \\log(e^{6.73})  + \\log(X^{0.837})\\\\\nY &= 873.15\\times X^{0.837}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#residuals-histograms",
    "href": "slides/xx-slr-transformations-outliers.html#residuals-histograms",
    "title": "SLR: Transformations + Outliers",
    "section": "Residuals Histograms",
    "text": "Residuals Histograms\n\n\nCode\nlp_aug &lt;- augment(logprice_model)\nll_aug &lt;- augment(loglog_model)\n\np1 &lt;- gf_histogram(~.resid, data = lp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log Price Residuals\")\n\np2 &lt;- gf_histogram(~.resid, data = ll_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log-Log Residuals\")\n\n(p1 + p2)"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#qq-plots-of-residuals",
    "href": "slides/xx-slr-transformations-outliers.html#qq-plots-of-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "QQ-Plots of Residuals",
    "text": "QQ-Plots of Residuals\n\n\nCode\np1 &lt;- gf_qq(~.resid, data = lp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log Price QQ\")\n\np2 &lt;- gf_qq(~.resid, data = ll_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log-Log QQ\")\n\np1 + p2"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#residuals-vs.-fitted-values",
    "href": "slides/xx-slr-transformations-outliers.html#residuals-vs.-fitted-values",
    "title": "SLR: Transformations + Outliers",
    "section": "Residuals vs. Fitted Values",
    "text": "Residuals vs. Fitted Values\n\n\nCode\np1 &lt;- gf_point(.resid ~ .fitted, data = lp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log Price Model\"\n  )\n\np2 &lt;- gf_point(.resid ~ .fitted, data = ll_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log-Log Model\"\n  )\n\np1 + p2"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#types-of-unusual-points-in-slr",
    "href": "slides/xx-slr-transformations-outliers.html#types-of-unusual-points-in-slr",
    "title": "SLR: Transformations + Outliers",
    "section": "Types of “Unusual” Points in SLR",
    "text": "Types of “Unusual” Points in SLR\n\nOutlier: a data point that is far from the regression line\nInfluential point: a data point that has a large effect on the regression fit\n\n\n\n\nHow do we measure “far”?\nHow do we measure “effect on the fit”?"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#detecting-unusual-cases-overview",
    "href": "slides/xx-slr-transformations-outliers.html#detecting-unusual-cases-overview",
    "title": "SLR: Transformations + Outliers",
    "section": "Detecting Unusual Cases: Overview",
    "text": "Detecting Unusual Cases: Overview\n\nCompute residuals\n\n“raw”, standardized, studentized\n\nPlots of residuals\n\nboxplot, scatterplot, normal plot\n\nLeverage\n\nunusual values for the predictors"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-movie-scores",
    "href": "slides/xx-slr-transformations-outliers.html#example-movie-scores",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie scores",
    "text": "Example: Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#data-prep",
    "href": "slides/xx-slr-transformations-outliers.html#data-prep",
    "title": "SLR: Transformations + Outliers",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-movie-scores-1",
    "href": "slides/xx-slr-transformations-outliers.html#example-movie-scores-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\n\nCode\nmovie_scores |&gt; \n  gf_point(audience ~ critics) |&gt; \n  gf_lm() |&gt; \n  gf_labs(x = \"Critics Score\", \n       y = \"Audience Score\")"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#boxplot-of-residuals",
    "href": "slides/xx-slr-transformations-outliers.html#boxplot-of-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "Boxplot of Residuals",
    "text": "Boxplot of Residuals\n\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\nmovie_fit_aug &lt;- augment(movie_fit)\n\ngf_boxplot(.resid~\"\", data = movie_fit_aug, \n           fill = \"salmon\", ylab = \"Residuals\", xlab = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nDots (outliers) indicate data points more than 1.5 IQRs above (or below) quartiles"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#standardized-residuals",
    "href": "slides/xx-slr-transformations-outliers.html#standardized-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\n\nRecall: Z-scores\nFact: If \\(X\\) has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then \\((X-\\mu)/\\sigma\\) has mean 0 and stanard deviation 1\nFor residuals: mean 0 and standard deviation \\(\\hat{\\sigma}_\\epsilon\\)\nStandardized residuals: \\(\\frac{y_i-\\hat{y}_i}{\\hat{\\sigma}_\\epsilon}\\)\n\nLook for values beyond \\(\\pm 2\\) or \\(\\pm 3\\)"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-movie-scores-2",
    "href": "slides/xx-slr-transformations-outliers.html#example-movie-scores-2",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\nmovie_fit_aug |&gt;  # Augmented data\n  gf_point(.std.resid ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Standardized Residual\")"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#externally-studentized-residuals",
    "href": "slides/xx-slr-transformations-outliers.html#externally-studentized-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "(Externally) Studentized Residuals",
    "text": "(Externally) Studentized Residuals\n\nConcern: An unusual value may exert great influence on the fit\n\nIts residual might be underestimated because the model “moves” a lot to fit it\nThe standard error may also be inflated due to the outlier error\n\nStudentize: Fit the model without that case, then find new \\(\\hat{\\sigma}_\\epsilon\\)"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#example-movie-scores-3",
    "href": "slides/xx-slr-transformations-outliers.html#example-movie-scores-3",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\nmovie_fit_aug |&gt;  # Augmented data\n  mutate(studentized_residual = rstudent(movie_fit)) |&gt; \n  gf_point(.std.resid ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Studentized Residual\")"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#what-to-do-with-an-outlier",
    "href": "slides/xx-slr-transformations-outliers.html#what-to-do-with-an-outlier",
    "title": "SLR: Transformations + Outliers",
    "section": "What to do with an outlier?",
    "text": "What to do with an outlier?\n\nLook into it\nIf something is unusual about it and you can make a case that it is not a good representation of the population you can throw it out\nIf not and the value is just unusual, keep it"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#influence-vs.-leverage",
    "href": "slides/xx-slr-transformations-outliers.html#influence-vs.-leverage",
    "title": "SLR: Transformations + Outliers",
    "section": "Influence vs. Leverage",
    "text": "Influence vs. Leverage\n\nHigh Influence Point: point that DOES impact the regression line\nHigh Leverage Point: point with “potential” to impact regression line because \\(X\\)-value is unusual"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#high-leverage-low-influence",
    "href": "slides/xx-slr-transformations-outliers.html#high-leverage-low-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "High Leverage, Low Influence",
    "text": "High Leverage, Low Influence"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#high-leverage-high-influence",
    "href": "slides/xx-slr-transformations-outliers.html#high-leverage-high-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "High Leverage, High Influence",
    "text": "High Leverage, High Influence"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#low-leverage-low-influence",
    "href": "slides/xx-slr-transformations-outliers.html#low-leverage-low-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "Low Leverage, Low Influence",
    "text": "Low Leverage, Low Influence"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#low-leverage-high-influence",
    "href": "slides/xx-slr-transformations-outliers.html#low-leverage-high-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence"
  },
  {
    "objectID": "slides/xx-slr-transformations-outliers.html#low-leverage-high-influence-1",
    "href": "slides/xx-slr-transformations-outliers.html#low-leverage-high-influence-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "support.html",
    "href": "support.html",
    "title": "Course support",
    "section": "",
    "text": "We expect everyone will have questions at some point in the semester, so we want to make sure you can identify when that is and feel comfortable seeking help.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#lectures",
    "href": "support.html#lectures",
    "title": "Course support",
    "section": "Lectures",
    "text": "Lectures\nIf you have a question during lecture, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#office-hours",
    "href": "support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nI am here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage you to take advantage of them!\nMake a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#canvas-discussion-forum",
    "href": "support.html#canvas-discussion-forum",
    "title": "Course support",
    "section": "Canvas Discussion Forum",
    "text": "Canvas Discussion Forum\nOutside of class and office hours, any general questions about course content or assignments should be posted on the Canvas Discussion Forum. There is a chance another student has already asked a similar question, so please check the other posts on Ed Discussion before adding a new question. If you know the answer to a question that is posted, I encourage you to respond!",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#tutoring",
    "href": "support.html#tutoring",
    "title": "Course support",
    "section": "Tutoring",
    "text": "Tutoring\nAll tutors are in Jewett 100 during their specified hours. Please see each of the course’s tutors with their names and hours below:\n\nBrayden Banks\n\nMonday and Wednesday 7:00 PM - 8:00 PM\n\nChristian Benischek\n\nWednesday 2:00 PM - 4:00 PM\n\nLogan Bright\n\nTuesday and Thursday 6:30 PM - 7:30 PM\n\nSuthi de Silva\n\nMonday 12:00 PM - 2:00 PM",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "support.html#email",
    "href": "support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nIf you have questions about personal matters that are not appropriate for the class discussion forum (e.g. illness, accommodations, etc.), you may email Professor Fridlander at efriedlander@collegeofidaho.edu. If you email me, please include “MAT 212” in the subject line. Barring extenuating circumstances, I will respond to MAT 212 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday.",
    "crumbs": [
      "Support"
    ]
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#permutation-pipeline-i",
    "href": "slides/07-slr-sim-testing.html#permutation-pipeline-i",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline I",
    "text": "Permutation pipeline I\n\nset.seed(1218)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K)\n\nResponse: Number_Organic (numeric)\nExplanatory: Avg_Income_K (numeric)\n# A tibble: 37 × 2\n   Number_Organic Avg_Income_K\n            &lt;dbl&gt;        &lt;dbl&gt;\n 1             36         71.2\n 2              4         34.2\n 3             28         71.2\n 4             31         48.8\n 5             78         78.1\n 6             14         40.5\n 7             12         38.2\n 8             18         50.4\n 9             38         49.4\n10             84         66.1\n# ℹ 27 more rows"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#permutation-pipeline-ii",
    "href": "slides/07-slr-sim-testing.html#permutation-pipeline-ii",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline II",
    "text": "Permutation pipeline II\n\nset.seed(1218)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  hypothesize(null = \"independence\")\n\nResponse: Number_Organic (numeric)\nExplanatory: Avg_Income_K (numeric)\nNull Hypothesis: independence\n# A tibble: 37 × 2\n   Number_Organic Avg_Income_K\n            &lt;dbl&gt;        &lt;dbl&gt;\n 1             36         71.2\n 2              4         34.2\n 3             28         71.2\n 4             31         48.8\n 5             78         78.1\n 6             14         40.5\n 7             12         38.2\n 8             18         50.4\n 9             38         49.4\n10             84         66.1\n# ℹ 27 more rows"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#permutation-pipeline-iii",
    "href": "slides/07-slr-sim-testing.html#permutation-pipeline-iii",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline III",
    "text": "Permutation pipeline III\n\nset.seed(1218)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\")\n\nResponse: Number_Organic (numeric)\nExplanatory: Avg_Income_K (numeric)\nNull Hypothesis: independence\n# A tibble: 37,000 × 3\n# Groups:   replicate [1,000]\n   Number_Organic Avg_Income_K replicate\n            &lt;dbl&gt;        &lt;dbl&gt;     &lt;int&gt;\n 1             38         71.2         1\n 2             56         34.2         1\n 3             28         71.2         1\n 4             14         48.8         1\n 5             29         78.1         1\n 6             36         40.5         1\n 7             84         38.2         1\n 8             18         50.4         1\n 9             96         49.4         1\n10             26         66.1         1\n# ℹ 36,990 more rows"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#permutation-pipeline-iv",
    "href": "slides/07-slr-sim-testing.html#permutation-pipeline-iv",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline IV",
    "text": "Permutation pipeline IV\n\nset.seed(1218)\n\nheb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()\n\n# A tibble: 2,000 × 3\n# Groups:   replicate [1,000]\n   replicate term         estimate\n       &lt;int&gt; &lt;chr&gt;           &lt;dbl&gt;\n 1         1 intercept     47.8   \n 2         1 Avg_Income_K   0.0555\n 3         2 intercept     58.0   \n 4         2 Avg_Income_K  -0.0914\n 5         3 intercept     57.3   \n 6         3 Avg_Income_K  -0.0817\n 7         4 intercept     78.9   \n 8         4 Avg_Income_K  -0.394 \n 9         5 intercept     34.8   \n10         5 Avg_Income_K   0.244 \n# ℹ 1,990 more rows"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#permutation-pipeline-v",
    "href": "slides/07-slr-sim-testing.html#permutation-pipeline-v",
    "title": "SLR: Randomization test for the slope",
    "section": "Permutation pipeline V",
    "text": "Permutation pipeline V\n\nset.seed(1218)\n\nnull_dist &lt;- heb |&gt;\n  specify(Number_Organic ~ Avg_Income_K) |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  fit()"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#visualize-the-null-distribution",
    "href": "slides/07-slr-sim-testing.html#visualize-the-null-distribution",
    "title": "SLR: Randomization test for the slope",
    "section": "Visualize the null distribution",
    "text": "Visualize the null distribution\n\nnull_dist |&gt;\n  filter(term == \"Avg_Income_K\") |&gt;\n  gf_histogram(~estimate, color = \"white\")"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#reason-around-the-p-value",
    "href": "slides/07-slr-sim-testing.html#reason-around-the-p-value",
    "title": "SLR: Randomization test for the slope",
    "section": "Reason around the p-value",
    "text": "Reason around the p-value\n\nIn a world where the there is no relationship between the the number of organic food options and the nearby average household income (\\(\\beta_1 = 0\\)), what is the probability that we observe a sample of 37 stores where the slope fo the model predicting the number of organic options from average household income is 0.96 or even more extreme?"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#compute-the-p-value",
    "href": "slides/07-slr-sim-testing.html#compute-the-p-value",
    "title": "SLR: Randomization test for the slope",
    "section": "Compute the p-value",
    "text": "Compute the p-value\n\nWhat does this warning mean?\n\n\nget_p_value(\n  null_dist,\n  obs_stat = obs_fit,\n  direction = \"two-sided\"\n)\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\nPlease be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n# A tibble: 2 × 2\n  term         p_value\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Avg_Income_K       0\n2 intercept          0"
  },
  {
    "objectID": "slides/08-slr-math-models.html#topics",
    "href": "slides/08-slr-math-models.html#topics",
    "title": "SLR: Mathematical models for inference",
    "section": "Topics",
    "text": "Topics\n\nDefine mathematical models to conduct inference for the slope\nUse mathematical models to\n\ncalculate confidence interval for the slope\nconduct a hypothesis test for the slope"
  },
  {
    "objectID": "slides/08-slr-math-models.html#computational-setup",
    "href": "slides/08-slr-math-models.html#computational-setup",
    "title": "SLR: Mathematical models for inference",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# HEB Dataset\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/08-slr-math-models.html#the-regression-model-revisited",
    "href": "slides/08-slr-math-models.html#the-regression-model-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "The regression model, revisited",
    "text": "The regression model, revisited\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00"
  },
  {
    "objectID": "slides/08-slr-math-models.html#inference-revisited",
    "href": "slides/08-slr-math-models.html#inference-revisited",
    "title": "SLR: Mathematical models for inference",
    "section": "Inference, revisited",
    "text": "Inference, revisited\n\n\nEarlier we computed a confidence interval and conducted a hypothesis test via simulation:\n\nCI: Bootstrap the observed sample to simulate the distribution of the slope\nHT: Permute the observed sample to simulate the distribution of the slope under the assumption that the null hypothesis is true\n\nNow we’ll do these based on theoretical results, i.e., by using the Central Limit Theorem to define the distribution of the slope and use features (shape, center, spread) of this distribution to compute bounds of the confidence interval and the p-value for the hypothesis test"
  },
  {
    "objectID": "slides/08-slr-math-models.html#mathematical-representation-of-the-model",
    "href": "slides/08-slr-math-models.html#mathematical-representation-of-the-model",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation of the model",
    "text": "Mathematical representation of the model\n\\[\n\\begin{aligned}\nY &= Model + Error \\\\\n&= f(X) + \\epsilon \\\\\n&= \\mu_{Y|X} + \\epsilon \\\\\n&= \\beta_0 + \\beta_1 X + \\epsilon\n\\end{aligned}\n\\]\nwhere the errors are independent and normally distributed:\n\n\nindependent: Knowing the error term for one observation doesn’t tell you anything about the error term for another observation\nnormally distributed: \\(\\epsilon \\sim N(0, \\sigma_\\epsilon^2)\\)"
  },
  {
    "objectID": "slides/08-slr-math-models.html#mathematical-representation-visualized",
    "href": "slides/08-slr-math-models.html#mathematical-representation-visualized",
    "title": "SLR: Mathematical models for inference",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\n\n\n\n\nImage source: Introduction to the Practice of Statistics (5th ed)\n\n\n\n\nMean: \\(\\beta_0 + \\beta_1 X\\), the predicted value based on the regression model\nVariance: \\(\\sigma_\\epsilon^2\\), constant across the range of \\(X\\)\n\nHow do we estimate \\(\\sigma_\\epsilon^2\\)?"
  },
  {
    "objectID": "slides/08-slr-math-models.html#regression-standard-error",
    "href": "slides/08-slr-math-models.html#regression-standard-error",
    "title": "SLR: Mathematical models for inference",
    "section": "Regression standard error",
    "text": "Regression standard error\nOnce we fit the model, we can use the residuals to estimate the regression standard error, the average distance between the observed values and the regression line\n\\[\n\\hat{\\sigma}_\\epsilon = \\sqrt{\\frac{\\sum_\\limits{i=1}^n(y_i - \\hat{y}_i)^2}{n-2}} = \\sqrt{\\frac{\\sum_\\limits{i=1}^ne_i^2}{n-2}}\n\\]\n\n\n\nWhy divide by \\(n - 2\\)?\nWhy do we care about the value of the regression standard error?"
  },
  {
    "objectID": "slides/08-slr-math-models.html#standard-error-of-hatbeta_1",
    "href": "slides/08-slr-math-models.html#standard-error-of-hatbeta_1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard error of \\(\\hat{\\beta}_1\\)",
    "text": "Standard error of \\(\\hat{\\beta}_1\\)\nThe standard error of \\(\\hat{\\beta}_1\\) quantifies the sampling variability in the estimated slopes\n\\[\nSE_{\\hat{\\beta}_1} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{(n-1)s_X^2}}\n\\]\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00"
  },
  {
    "objectID": "slides/08-slr-math-models.html#hypothesis-test-for-the-slope",
    "href": "slides/08-slr-math-models.html#hypothesis-test-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test for the slope",
    "text": "Hypothesis test for the slope\nHypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\nTest statistic: Number of standard errors the estimate is away from the null\n\\[\nT = \\frac{\\text{Estimate - Null Value}}{\\text{Standard error}} \\\\\n\\]\n\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\n\\text{p-value} = P(|T| &gt; |\\text{test statistic}|),\n\\]\ncalculated from a \\(t\\) distribution with \\(n - 2\\) degrees of freedom"
  },
  {
    "objectID": "slides/08-slr-math-models.html#hypothesis-test-test-statistic",
    "href": "slides/08-slr-math-models.html#hypothesis-test-test-statistic",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Test statistic",
    "text": "Hypothesis test: Test statistic\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\n\\[\nT = \\frac{\\hat{\\beta}_1 - 0}{SE_{\\hat{\\beta}_1}} = \\frac{0.96 - 0}{0.13} = 7.38\n\\]\n\nHow should we interpret this test statistic?"
  },
  {
    "objectID": "slides/08-slr-math-models.html#hypothesis-test-p-value",
    "href": "slides/08-slr-math-models.html#hypothesis-test-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00"
  },
  {
    "objectID": "slides/08-slr-math-models.html#hypothesis-test-p-value-1",
    "href": "slides/08-slr-math-models.html#hypothesis-test-p-value-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: p-value",
    "text": "Hypothesis test: p-value\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\nA more exact p-value\n\n2 * pt(q = 7.38, df = 35, lower.tail = FALSE)\n\n[1] 1.243448e-08\n\n\n\nInterpret this p-value."
  },
  {
    "objectID": "slides/08-slr-math-models.html#understanding-the-p-value",
    "href": "slides/08-slr-math-models.html#understanding-the-p-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Understanding the p-value",
    "text": "Understanding the p-value\n\n\n\nMagnitude of p-value\nInterpretation\n\n\n\n\np-value &lt; 0.01\nstrong evidence against \\(H_0\\)\n\n\n0.01 &lt; p-value &lt; 0.05\nmoderate evidence against \\(H_0\\)\n\n\n0.05 &lt; p-value &lt; 0.1\nweak evidence against \\(H_0\\)\n\n\np-value &gt; 0.1\neffectively no evidence against \\(H_0\\)\n\n\n\n\n\n\n\n\n\nImportant\n\n\nThese are general guidelines. The strength of evidence depends on the context of the problem."
  },
  {
    "objectID": "slides/08-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "href": "slides/08-slr-math-models.html#hypothesis-test-conclusion-in-context",
    "title": "SLR: Mathematical models for inference",
    "section": "Hypothesis test: Conclusion, in context",
    "text": "Hypothesis test: Conclusion, in context\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\n\nThe data provide convincing evidence that the population slope \\(\\beta_1\\) is different from 0.\nThe data provide convincing evidence of a linear relationship between average household income and the number of organic vegetable options available."
  },
  {
    "objectID": "slides/08-slr-math-models.html#confidence-interval-for-the-slope",
    "href": "slides/08-slr-math-models.html#confidence-interval-for-the-slope",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/08-slr-math-models.html#confidence-interval-critical-value",
    "href": "slides/08-slr-math-models.html#confidence-interval-critical-value",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(heb) - 2)\n\n[1] 2.030108\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(heb) - 2)\n\n[1] 1.689572\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(heb) - 2)\n\n[1] 2.723806"
  },
  {
    "objectID": "slides/08-slr-math-models.html#ci-for-the-slope-calculation",
    "href": "slides/08-slr-math-models.html#ci-for-the-slope-calculation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\n\\[\\hat{\\beta}_1 = 0.96 \\hspace{15mm} t^* = 2.03 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 0.13\\]\n\n\\[\n0.96 \\pm 2.03 \\times 0.13 = (0.70, 1.22)\n\\]"
  },
  {
    "objectID": "slides/08-slr-math-models.html#ci-for-the-slope-computation",
    "href": "slides/08-slr-math-models.html#ci-for-the-slope-computation",
    "title": "SLR: Mathematical models for inference",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(heb_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n-33.59\n4.16\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n0.70\n1.22"
  },
  {
    "objectID": "slides/08-slr-math-models.html#intervals-for-predictions-1",
    "href": "slides/08-slr-math-models.html#intervals-for-predictions-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Intervals for predictions",
    "text": "Intervals for predictions\n\nQuestion: “What is the predicted number of organic vegetable options in a neighborhood with an average income of $70k?”\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead"
  },
  {
    "objectID": "slides/08-slr-math-models.html#two-types-of-predictions",
    "href": "slides/08-slr-math-models.html#two-types-of-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: “What is the average predicted number of organic vegetable options in a neighborhood with an average income of $70k?”\nPrediction for an individual observation: “What is the predicted number of organic vegetable options at a single HEB in a neighborhood with an average income of $70k?”\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/08-slr-math-models.html#uncertainty-in-predictions",
    "href": "slides/08-slr-math-models.html#uncertainty-in-predictions",
    "title": "SLR: Mathematical models for inference",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}}\\]\n\nPrediction interval for an individual observation: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}}\\]"
  },
  {
    "objectID": "slides/08-slr-math-models.html#standard-errors",
    "href": "slides/08-slr-math-models.html#standard-errors",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/08-slr-math-models.html#standard-errors-1",
    "href": "slides/08-slr-math-models.html#standard-errors-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/08-slr-math-models.html#confidence-interval",
    "href": "slides/08-slr-math-models.html#confidence-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcome:\n\nnew_neighborhood &lt;- tibble(Avg_Income_K = 70)\n\npredict(heb_fit, newdata = new_neighborhood, interval = \"confidence\", level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n52.4175\n46.58558\n58.24942\n\n\n\n\n\n\n\nWe are 95% confident that the mean number of organic vegetable options offered by HEB in a neighborhood with an average income of $70k is between 46.59 and 58.25."
  },
  {
    "objectID": "slides/08-slr-math-models.html#prediction-interval",
    "href": "slides/08-slr-math-models.html#prediction-interval",
    "title": "SLR: Mathematical models for inference",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction interval for an individual outcome:\n\npredict(heb_fit, newdata = new_neighborhood, interval = \"prediction\", level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n52.4175\n16.48941\n88.34559\n\n\n\n\n\n\nWe are 95% confident that the number of organic vegetable options offered by HEB in a neighborhood with an average income of $70k is between 16.49 and 88.35."
  },
  {
    "objectID": "slides/08-slr-math-models.html#comparing-intervals",
    "href": "slides/08-slr-math-models.html#comparing-intervals",
    "title": "SLR: Mathematical models for inference",
    "section": "Comparing intervals",
    "text": "Comparing intervals"
  },
  {
    "objectID": "slides/08-slr-math-models.html#extrapolation",
    "href": "slides/08-slr-math-models.html#extrapolation",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\nUsing the model to predict for values outside the range of the original data is extrapolation.\n\n\n\n\nCalculate the prediction interval for the number of organic options in an extremely wealthy neighborhood with an average household income of $500k.\n\nNo, thanks!"
  },
  {
    "objectID": "slides/08-slr-math-models.html#extrapolation-1",
    "href": "slides/08-slr-math-models.html#extrapolation-1",
    "title": "SLR: Mathematical models for inference",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nWhy do we want to avoid extrapolation?\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "prepare/math-models-prep.html",
    "href": "prepare/math-models-prep.html",
    "title": "September 9th lecture",
    "section": "",
    "text": "Stat2 Textbook: Chapter 2.1, 2.3, 2.3"
  },
  {
    "objectID": "prepare/math-models-prep.html#assigned-reading",
    "href": "prepare/math-models-prep.html#assigned-reading",
    "title": "September 9th lecture",
    "section": "",
    "text": "Stat2 Textbook: Chapter 2.1, 2.3, 2.3"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#from-last-time",
    "href": "slides/07-slr-sim-testing.html#from-last-time",
    "title": "SLR: Randomization test for the slope",
    "section": "From last time",
    "text": "From last time\n\nPopulation: Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = \\(N\\))\nSample: Subset of the population, ideally random and representative (sample size = \\(n\\))\nSample statistic \\(\\ne\\) population parameter, but if the sample is good, it can be a good estimate\nStatistical inference: Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process\nWe report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population\nSince we can’t continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#computational-set-up",
    "href": "slides/09-slr-conditions-eval.html#computational-set-up",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)   # for plotting using formulas\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# HEB Dataset\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#regression-model-revisited",
    "href": "slides/09-slr-conditions-eval.html#regression-model-revisited",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.719\n9.298\n-1.583\n0.122\n\n\nAvg_Income_K\n0.959\n0.128\n7.505\n0.000"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#mathematical-representation-visualized",
    "href": "slides/09-slr-conditions-eval.html#mathematical-representation-visualized",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#model-conditions-1",
    "href": "slides/09-slr-conditions-eval.html#model-conditions-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#linearity",
    "href": "slides/09-slr-conditions-eval.html#linearity",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Linearity",
    "text": "Linearity\n\nIf the linear model, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\), adequately describes the relationship between \\(X\\) and \\(Y\\), then the residuals should reflect random (chance) error\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nLinearity satisfied if there is no distinguishable pattern in the residuals plot, i.e. the residuals should be randomly scattered\nA non-random pattern (e.g. a parabola) suggests a linear model does not adequately describe the relationship between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#linearity-1",
    "href": "slides/09-slr-conditions-eval.html#linearity-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Linearity",
    "text": "Linearity\n✅ The residuals vs. fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#residuals-vs.-fitted-values-code",
    "href": "slides/09-slr-conditions-eval.html#residuals-vs.-fitted-values-code",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Residuals vs. fitted values (code)",
    "text": "Residuals vs. fitted values (code)\n\nheb_aug &lt;- augment(heb_fit)\n\ngf_point(.resid ~ .fitted, data = heb_aug) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#non-linear-relationships",
    "href": "slides/09-slr-conditions-eval.html#non-linear-relationships",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#constant-variance",
    "href": "slides/09-slr-conditions-eval.html#constant-variance",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n\nIf the spread of the distribution of \\(Y\\) is equal for all values of \\(X\\) then the spread of the residuals should be approximately equal for each value of \\(X\\)\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nConstant variance satisfied if the vertical spread of the residuals is approximately equal as you move from left to right (i.e. there is no “fan” pattern)\nA fan pattern suggests the constant variance assumption is not satisfied and transformation or some other remedy is required (more on this later in the semester)\nCAREFUL: Inconsistent distribution of \\(X\\)s can make it seem as if there is non-constant variance"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#constant-variance-1",
    "href": "slides/09-slr-conditions-eval.html#constant-variance-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Constant variance",
    "text": "Constant variance\n✅ The vertical spread of the residuals is relatively constant across the plot"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#non-constant-variance",
    "href": "slides/09-slr-conditions-eval.html#non-constant-variance",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Non-constant variance",
    "text": "Non-constant variance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThink: Is my error/variance proportional to the thing I’m predicting?"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#normality",
    "href": "slides/09-slr-conditions-eval.html#normality",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality\n\nThe linear model assumes that the distribution of \\(Y\\) is Normal for every value of \\(X\\)\nThis is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied\nNormality satisfied if a histogram of the residuals is approximately normal\n\nCan also check that the points on a normal QQ-plot falls along a diagonal line\n\nMost inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#normality-1",
    "href": "slides/09-slr-conditions-eval.html#normality-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#check-normality-using-a-qq-plot",
    "href": "slides/09-slr-conditions-eval.html#check-normality-using-a-qq-plot",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Check normality using a QQ-plot",
    "text": "Check normality using a QQ-plot\n\n\n\n\nCode\ngf_histogram(~.resid, data = heb_aug,\n             bins=7, color = \"white\") |&gt; \n  gf_labs(\n    x = \"Residual\",\n    y = \"Count\",\n    title = \"Histogram of residuals\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngf_qq(~.resid, data = heb_aug) |&gt; \n  gf_qqline() |&gt;  \n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")\n\n\n\n\n\n\n\n\n\n\n\n\nAssess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot).\nIf so, the residuals are normally distributed."
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#normality-2",
    "href": "slides/09-slr-conditions-eval.html#normality-2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Normality",
    "text": "Normality\n\n❌ The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied.\n✅ The sample size \\(n =  37 &gt; 30\\), so the sample size is large enough to relax this condition and proceed with inference."
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#independence",
    "href": "slides/09-slr-conditions-eval.html#independence",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nTwo common violations of the independence assumption:\n\nSerial Effect: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation)\nCluster Effect: If there are subgroups represented in the data that are not accounted for in the model (e.g., type of supermarket), you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#independence-1",
    "href": "slides/09-slr-conditions-eval.html#independence-1",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Independence",
    "text": "Independence\nRecall the description of the data:\n\nAverage household income (per zip code) and number of organic vegetable offerings in San Antonio, TX\nData from HEB website, compiled by high school student Linda Saucedo, Fall 2019\n\n\n❌ Based on the information we have, it’s unclear if the data are independent. In fact, I’d guess that they are likely geographically correlated."
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#recap",
    "href": "slides/09-slr-conditions-eval.html#recap",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\n\nLinearity\nConstant variance\n\n\n\n\n\nNormality\nIndependence\n\n\n\n\n\n\n\nWhich of these conditions are required for fitting a SLR (and not doing any inference)?\nWhich for simulation-based inference for the slope for an SLR?\nWhich for inference with mathematical models?\n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#comparing-inferential-methods",
    "href": "slides/09-slr-conditions-eval.html#comparing-inferential-methods",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Comparing inferential methods",
    "text": "Comparing inferential methods\n\n\nWhat are the advantages of using simulation-based inference methods? What are the advantages of using inference methods based on mathematical models?\nUnder what scenario(s) would you prefer to use simulation-based methods? Under what scenario(s) would you prefer to use methods based on mathematical models?\n\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#two-statistics",
    "href": "slides/09-slr-conditions-eval.html#two-statistics",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\), Coefficient of Determination : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#r2",
    "href": "slides/09-slr-conditions-eval.html#r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with glance() or summary() using the fitted model:\n\n\nglance(heb_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic       p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.617         0.606  17.5      56.3 0.00000000863     1  -157.  321.  325.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\nglance(heb_fit)$r.squared\n\n[1] 0.6167334"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#interpreting-r2",
    "href": "slides/09-slr-conditions-eval.html#interpreting-r2",
    "title": "SLR: Conditions + Model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\n🗳️ Discussion\n\n\nThe \\(R^2\\) of the model for Number_Organic from Average_Income_K is 61.7%. Which of the following is the correct interpretation of this value?\n\n\nAvg_Income_K correctly predicts 61.7% of Number_Organic in San Anontio HEBs.\n61.7% of the variability in Number_Organic can be explained by Avg_Income_K.\n61.7% of the variability in Avg_Income_K can be explained by Number_Organic.\n61.7% of the time Number_Organic can be predicted by Avg_Income_K."
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#rmse",
    "href": "slides/09-slr-conditions-eval.html#rmse",
    "title": "SLR: Conditions + Model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with glance() using the augmented data:\n\nglance(heb_fit)$sigma\n\n[1] 17.46292\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this when we get to regression with multiple predictors)\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/09-slr-conditions-eval.html#the-augment-function",
    "href": "slides/09-slr-conditions-eval.html#the-augment-function",
    "title": "SLR: Conditions + Model evaluation",
    "section": "The augment function",
    "text": "The augment function\n\nheb_aug &lt;- augment(heb_fit)\n\nhead(heb_aug) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber_Organic\nAvg_Income_K\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n36\n71.186\n53.55498\n-17.554983\n0.0272386\n17.45293\n0.0145449\n-1.0192493\n\n\n4\n34.234\n18.11468\n-14.114677\n0.0925015\n17.53471\n0.0366889\n-0.8484596\n\n\n28\n71.186\n53.55498\n-25.554983\n0.0272386\n17.15160\n0.0308219\n-1.4837325\n\n\n31\n48.760\n32.04642\n-1.046423\n0.0493994\n17.71691\n0.0000981\n-0.0614599\n\n\n78\n78.096\n60.18230\n17.817702\n0.0312671\n17.44374\n0.0173428\n1.0366517\n\n\n14\n40.506\n24.13009\n-10.130092\n0.0711183\n17.62593\n0.0138683\n-0.6018889"
  },
  {
    "objectID": "slides/10-slr-model-eval.html",
    "href": "slides/10-slr-model-eval.html",
    "title": "SLR: Model evaluation",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#computational-set-up",
    "href": "slides/10-slr-model-eval.html#computational-set-up",
    "title": "SLR: Model evaluation",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)   # for plotting using formulas\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# HEB Dataset\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#model-conditions-1",
    "href": "slides/10-slr-model-eval.html#model-conditions-1",
    "title": "SLR: Model evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#augmented-data-frame",
    "href": "slides/10-slr-model-eval.html#augmented-data-frame",
    "title": "SLR: Model evaluation",
    "section": "Augmented data frame",
    "text": "Augmented data frame\n\ndf_fit &lt;- linear_reg() |&gt;\n  fit(price ~ area, data = duke_forest)\n\ndf_aug &lt;- augment(df_fit$fit)\n\nhead(df_aug)\n\n# A tibble: 6 × 8\n    price  area  .fitted  .resid   .hat  .sigma  .cooksd .std.resid\n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 1520000  6040 1079931. 440069. 0.133  162605. 0.604         2.80 \n2 1030000  4475  830340. 199660. 0.0435 168386. 0.0333        1.21 \n3  420000  1745  394951.  25049. 0.0226 169664. 0.000260      0.150\n4  680000  2091  450132. 229868. 0.0157 168011. 0.0150        1.37 \n5  428500  1772  399257.  29243. 0.0220 169657. 0.000345      0.175\n6  456000  1950  427645.  28355. 0.0182 169659. 0.000266      0.170"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#two-statistics",
    "href": "slides/10-slr-model-eval.html#two-statistics",
    "title": "SLR: Model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\), Coefficient of Determination : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(y, \\hat{y})^2\n\\]\n\nAlso called PRE (Percent Reduction in Error) because:\n\\[\nR^2 = \\frac{SSModel}{SSTotal}\n\\]\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#r2",
    "href": "slides/10-slr-model-eval.html#r2",
    "title": "SLR: Model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() from yardstick package using the augmented data:\n\n\nlibrary(yardstick)\nheb_aug &lt;- augment(heb_fit)\n\nrsq(heb_aug, truth = Number_Organic, estimate = .fitted) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.6167334"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#interpreting-r2",
    "href": "slides/10-slr-model-eval.html#interpreting-r2",
    "title": "SLR: Model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\n🗳️ Discussion\n\n\nThe \\(R^2\\) of the model for Number_Organic from Average_Income_K is 61.7%. Which of the following is the correct interpretation of this value?\n\n\nAvg_Income_K correctly predicts 61.7% of Number_Organic in San Antontio HEBs.\n61.7% of the variability in Number_Organic can be explained by Avg_Income_K.\n61.7% of the variability in Avg_Income_K can be explained by Number_Organic.\n61.7% of the time Number_Organic can be predicted by Avg_Income_K."
  },
  {
    "objectID": "slides/10-slr-model-eval.html#alternative-approach-for-r2",
    "href": "slides/10-slr-model-eval.html#alternative-approach-for-r2",
    "title": "SLR: Model evaluation",
    "section": "Alternative approach for \\(R^2\\)",
    "text": "Alternative approach for \\(R^2\\)\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(df_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared   sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.445         0.439 168798.      77.0 6.29e-14     1 -1318. 2641. 2649.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n. . .\n\nglance(df_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#rmse",
    "href": "slides/10-slr-model-eval.html#rmse",
    "title": "SLR: Model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rsq() from yardstick package using the augmented data:\n\n\nrmse(heb_aug, truth = Number_Organic, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        17.0\n\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this and ANOVA when we get to regression with multiple predictors)\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#obtaining-r2-and-rmse",
    "href": "slides/10-slr-model-eval.html#obtaining-r2-and-rmse",
    "title": "SLR: Model evaluation",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\n\nUse rsq() and rmse(), respectively\n\nrsq(df_aug, truth = price, estimate = .fitted)\nrmse(df_aug, truth = price, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#computational-set-up",
    "href": "slides/10-slr-transformations-outliers.html#computational-set-up",
    "title": "SLR: Transformations + Outliers",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(ggformula)   # for creating plots using formulas\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(moderndive)  # for house_price dataset\nlibrary(fivethirtyeight) # for fandango dataset\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#data-house_prices",
    "href": "slides/10-slr-transformations-outliers.html#data-house_prices",
    "title": "SLR: Transformations + Outliers",
    "section": "Data: house_prices",
    "text": "Data: house_prices\n\n\nContains house sale prices for King County, which includes Seattle, from homes sold between May 2014 and May 2015\nObtained from Kaggle.com\n\n\n\nglimpse(house_prices)\n\nRows: 21,613\nColumns: 21\n$ id            &lt;chr&gt; \"7129300520\", \"6414100192\", \"5631500400\", \"2487200875\", …\n$ date          &lt;date&gt; 2014-10-13, 2014-12-09, 2015-02-25, 2014-12-09, 2015-02…\n$ price         &lt;dbl&gt; 221900, 538000, 180000, 604000, 510000, 1225000, 257500,…\n$ bedrooms      &lt;int&gt; 3, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 5, 4, 3, 4, 2,…\n$ bathrooms     &lt;dbl&gt; 1.00, 2.25, 1.00, 3.00, 2.00, 4.50, 2.25, 1.50, 1.00, 2.…\n$ sqft_living   &lt;int&gt; 1180, 2570, 770, 1960, 1680, 5420, 1715, 1060, 1780, 189…\n$ sqft_lot      &lt;int&gt; 5650, 7242, 10000, 5000, 8080, 101930, 6819, 9711, 7470,…\n$ floors        &lt;dbl&gt; 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1…\n$ waterfront    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ view          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,…\n$ condition     &lt;fct&gt; 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4,…\n$ grade         &lt;fct&gt; 7, 7, 6, 7, 8, 11, 7, 7, 7, 7, 8, 7, 7, 7, 7, 9, 7, 7, 7…\n$ sqft_above    &lt;int&gt; 1180, 2170, 770, 1050, 1680, 3890, 1715, 1060, 1050, 189…\n$ sqft_basement &lt;int&gt; 0, 400, 0, 910, 0, 1530, 0, 0, 730, 0, 1700, 300, 0, 0, …\n$ yr_built      &lt;int&gt; 1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 20…\n$ yr_renovated  &lt;int&gt; 0, 1991, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ zipcode       &lt;fct&gt; 98178, 98125, 98028, 98136, 98074, 98053, 98003, 98198, …\n$ lat           &lt;dbl&gt; 47.5112, 47.7210, 47.7379, 47.5208, 47.6168, 47.6561, 47…\n$ long          &lt;dbl&gt; -122.257, -122.319, -122.233, -122.393, -122.045, -122.0…\n$ sqft_living15 &lt;int&gt; 1340, 1690, 2720, 1360, 1800, 4760, 2238, 1650, 1780, 23…\n$ sqft_lot15    &lt;int&gt; 5650, 7639, 8062, 5000, 7503, 101930, 6819, 9711, 8113, …"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#variables",
    "href": "slides/10-slr-transformations-outliers.html#variables",
    "title": "SLR: Transformations + Outliers",
    "section": "Variables",
    "text": "Variables\n\nOutcome\n\nprice: the sale price\n\nPredictor\n\nsqft_living: the square footage of the home"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#recap-fit-the-model",
    "href": "slides/10-slr-transformations-outliers.html#recap-fit-the-model",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\nhp_fit &lt;- lm(price ~ sqft_living, data = house_prices)\ntidy(hp_fit) |&gt;  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-43580.74\n4402.69\n-9.90\n0\n\n\nsqft_living\n280.62\n1.94\n144.92\n0\n\n\n\n\n\n\nModel: \\(\\text{price} = -43580.74 + 280.62\\times\\text{sqft_living}\\)\nInterpretation: If the square footage of the house increases by 1, the price increases by and average of $280.62 and a (theoretical) house with 0 square feet with cost $-43,580.74."
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#recap-fit-the-model-1",
    "href": "slides/10-slr-transformations-outliers.html#recap-fit-the-model-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\n\nCode\ngf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#recap-model-conditions",
    "href": "slides/10-slr-transformations-outliers.html#recap-model-conditions",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Model conditions",
    "text": "Recap: Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other\n\n\n\nHow should we check these assumptions?"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#recap-residual-histogram",
    "href": "slides/10-slr-transformations-outliers.html#recap-residual-histogram",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Residual Histogram",
    "text": "Recap: Residual Histogram\n\n\nCode\nhp_aug &lt;- augment(hp_fit)\n\ngf_histogram(~.resid, data = hp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Residual Histogram\")"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#recap-qq-plot-of-residuals",
    "href": "slides/10-slr-transformations-outliers.html#recap-qq-plot-of-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: QQ-Plot of Residuals",
    "text": "Recap: QQ-Plot of Residuals\n\n\nCode\ngf_qq(~.resid, data = hp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#recap-residuals-vs.-fitted-values",
    "href": "slides/10-slr-transformations-outliers.html#recap-residuals-vs.-fitted-values",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Residuals vs. Fitted Values",
    "text": "Recap: Residuals vs. Fitted Values\n\n\nCode\ngf_point(.resid ~ .fitted, data = hp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#are-model-conditions-satisfied",
    "href": "slides/10-slr-transformations-outliers.html#are-model-conditions-satisfied",
    "title": "SLR: Transformations + Outliers",
    "section": "Are model conditions satisfied?",
    "text": "Are model conditions satisfied?\n\nLinearity: ❓\nConstant variance: ❌\nNormality: ❌\nIndependence: ✅"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#what-to-do-when-regression-conditions-are-violated",
    "href": "slides/10-slr-transformations-outliers.html#what-to-do-when-regression-conditions-are-violated",
    "title": "SLR: Transformations + Outliers",
    "section": "What to do when regression conditions are violated",
    "text": "What to do when regression conditions are violated\nExamples:\n\nLack of normality in residuals\nPatterns in residuals\nHeteroscedasticity (nonconstant variance)\nOutliers: influential points, large residuals"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#data-transformations",
    "href": "slides/10-slr-transformations-outliers.html#data-transformations",
    "title": "SLR: Transformations + Outliers",
    "section": "Data Transformations",
    "text": "Data Transformations\nCan be used to:\n\nAddress nonlinear patterns\nStabilize variance\nRemove skewness from resiudals\nMinimize effects of outliers"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#common-transformations",
    "href": "slides/10-slr-transformations-outliers.html#common-transformations",
    "title": "SLR: Transformations + Outliers",
    "section": "Common Transformations",
    "text": "Common Transformations\nFor either the response \\(Y\\) or predictor \\(X\\):\n\nLogarithm \\(Z \\to \\log(Z)\\)\n\nNote: “log” means “log base \\(e\\)”\n\nSquare Root \\(Z \\to \\sqrt{Z}\\)\nExponential \\(Z \\to e^Z\\)\nPower functions \\(Z \\to Z^2, Z^3, Z^4, \\ldots\\)\nReciprocal \\(Z \\to 1/Z\\)"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-planets",
    "href": "slides/10-slr-transformations-outliers.html#example-planets",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nplanets &lt;- read_csv(\"data/planets.csv\")\n\n\n\\(Y =\\) length of the “year” for planets\n\\(X =\\) distance from the Sun"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-planets-1",
    "href": "slides/10-slr-transformations-outliers.html#example-planets-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Distance ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-planets-2",
    "href": "slides/10-slr-transformations-outliers.html#example-planets-2",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(log(Distance) ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-planets-3",
    "href": "slides/10-slr-transformations-outliers.html#example-planets-3",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(Distance ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-planets-4",
    "href": "slides/10-slr-transformations-outliers.html#example-planets-4",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Distance) ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-mammal-species",
    "href": "slides/10-slr-transformations-outliers.html#example-mammal-species",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nSpeciesArea &lt;- read_csv(\"data/SpeciesArea.csv\")\n\n\n\\(Y =\\) number of mammal species on an island\n\\(X =\\) area of the island"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-mammal-species-1",
    "href": "slides/10-slr-transformations-outliers.html#example-mammal-species-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Species ~ Area, data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-mammal-species-2",
    "href": "slides/10-slr-transformations-outliers.html#example-mammal-species-2",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Species) ~ log(Area), data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#why-a-log-transformation",
    "href": "slides/10-slr-transformations-outliers.html#why-a-log-transformation",
    "title": "SLR: Transformations + Outliers",
    "section": "Why a Log Transformation?",
    "text": "Why a Log Transformation?\n\nSome relationship are multiplicative (not linear)\n\nExample: Area of a circle\n\\[\n\\begin{aligned}\nA &= \\pi r^2 \\text{ (not linear)}\\\\\n\\log(A) &= \\log(\\pi r^2)\n= \\log(\\pi) + 2\\log(r)\\\\\n\\log(A) &= \\beta_0 + \\beta_1\\times \\log(r)\\\\\n\\implies & \\log(A) \\text{ is a linear function of } \\log(r)\n\\end{aligned}\n\\]\nLook for:\n\nStrongly right-skewed distributions\nCurvative in scatterplot\nIncreasing variability in scatterplot"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#back-to-house_sales",
    "href": "slides/10-slr-transformations-outliers.html#back-to-house_sales",
    "title": "SLR: Transformations + Outliers",
    "section": "Back to house_sales",
    "text": "Back to house_sales\n\n\nCode\np1 &lt;- gf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np2 &lt;- gf_point(log(price) ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np3 &lt;- gf_point(price ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np4 &lt;- gf_point(log(price) ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\n(p1 + p2)/ (p3 + p4)"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#fitting-transformed-models",
    "href": "slides/10-slr-transformations-outliers.html#fitting-transformed-models",
    "title": "SLR: Transformations + Outliers",
    "section": "Fitting Transformed Models",
    "text": "Fitting Transformed Models\n\n\n\nlogprice_model &lt;- lm(log(price) ~ sqft_living, data = house_prices)\ntidy(logprice_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.2184641\n0.0063741\n1916.8830\n0\n\n\nsqft_living\n0.0003987\n0.0000028\n142.2326\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &= 12.22  + 3.99\\times 10^{-4}\\times X\\\\\nY &= e^{12.22 + 3.99\\times 10^{-4}\\times X}\\\\\n&= 202805\\times e^{3.99\\times 10^{-4}\\times X}\n\\end{aligned}\n\\]\n\n\nloglog_model &lt;- lm(log(price) ~ log(sqft_living), data = house_prices)\ntidy(loglog_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.729916\n0.0470620\n143.0011\n0\n\n\nlog(sqft_living)\n0.836771\n0.0062233\n134.4587\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &=6.73 + 0.837\\times \\log(X)\\\\\n\\log(Y) &= \\log(e^{6.73})  + \\log(X^{0.837})\\\\\nY &= 873.15\\times X^{0.837}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#residuals-histograms",
    "href": "slides/10-slr-transformations-outliers.html#residuals-histograms",
    "title": "SLR: Transformations + Outliers",
    "section": "Residuals Histograms",
    "text": "Residuals Histograms\n\n\nCode\nlp_aug &lt;- augment(logprice_model)\nll_aug &lt;- augment(loglog_model)\n\np1 &lt;- gf_histogram(~.resid, data = lp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log Price Residuals\")\n\np2 &lt;- gf_histogram(~.resid, data = ll_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log-Log Residuals\")\n\n(p1 + p2)"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#qq-plots-of-residuals",
    "href": "slides/10-slr-transformations-outliers.html#qq-plots-of-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "QQ-Plots of Residuals",
    "text": "QQ-Plots of Residuals\n\n\nCode\np1 &lt;- gf_qq(~.resid, data = lp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log Price QQ\")\n\np2 &lt;- gf_qq(~.resid, data = ll_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log-Log QQ\")\n\np1 + p2"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#residuals-vs.-fitted-values",
    "href": "slides/10-slr-transformations-outliers.html#residuals-vs.-fitted-values",
    "title": "SLR: Transformations + Outliers",
    "section": "Residuals vs. Fitted Values",
    "text": "Residuals vs. Fitted Values\n\n\nCode\np1 &lt;- gf_point(.resid ~ .fitted, data = lp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log Price Model\"\n  )\n\np2 &lt;- gf_point(.resid ~ .fitted, data = ll_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log-Log Model\"\n  )\n\np1 + p2"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#types-of-unusual-points-in-slr",
    "href": "slides/10-slr-transformations-outliers.html#types-of-unusual-points-in-slr",
    "title": "SLR: Transformations + Outliers",
    "section": "Types of “Unusual” Points in SLR",
    "text": "Types of “Unusual” Points in SLR\n\nOutlier: a data point that is far from the regression line\nInfluential point: a data point that has a large effect on the regression fit\n\n\n\n\nHow do we measure “far”?\nHow do we measure “effect on the fit”?"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#detecting-unusual-cases-overview",
    "href": "slides/10-slr-transformations-outliers.html#detecting-unusual-cases-overview",
    "title": "SLR: Transformations + Outliers",
    "section": "Detecting Unusual Cases: Overview",
    "text": "Detecting Unusual Cases: Overview\n\nCompute residuals\n\n“raw”, standardized, studentized\n\nPlots of residuals\n\nboxplot, scatterplot, normal plot\n\nLeverage\n\nunusual values for the predictors"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-movie-scores",
    "href": "slides/10-slr-transformations-outliers.html#example-movie-scores",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie scores",
    "text": "Example: Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#data-prep",
    "href": "slides/10-slr-transformations-outliers.html#data-prep",
    "title": "SLR: Transformations + Outliers",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-movie-scores-1",
    "href": "slides/10-slr-transformations-outliers.html#example-movie-scores-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\n\nCode\nmovie_scores |&gt; \n  gf_point(audience ~ critics) |&gt; \n  gf_lm() |&gt; \n  gf_labs(x = \"Critics Score\", \n       y = \"Audience Score\")"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#boxplot-of-residuals",
    "href": "slides/10-slr-transformations-outliers.html#boxplot-of-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "Boxplot of Residuals",
    "text": "Boxplot of Residuals\n\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\nmovie_fit_aug &lt;- augment(movie_fit)\n\ngf_boxplot(.resid~\"\", data = movie_fit_aug, \n           fill = \"salmon\", ylab = \"Residuals\", xlab = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nDots (outliers) indicate data points more than 1.5 IQRs above (or below) quartiles"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#standardized-residuals",
    "href": "slides/10-slr-transformations-outliers.html#standardized-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\n\nRecall: Z-scores\nFact: If \\(X\\) has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then \\((X-\\mu)/\\sigma\\) has mean 0 and stanard deviation 1\nFor residuals: mean 0 and standard deviation \\(\\hat{\\sigma}_\\epsilon\\)\nStandardized residuals: \\(\\frac{y_i-\\hat{y}_i}{\\hat{\\sigma}_\\epsilon}\\)\n\nLook for values beyond \\(\\pm 2\\) or \\(\\pm 3\\)"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-movie-scores-2",
    "href": "slides/10-slr-transformations-outliers.html#example-movie-scores-2",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\nmovie_fit_aug |&gt;  # Augmented data\n  gf_point(.std.resid ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Standardized Residual\")"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#externally-studentized-residuals",
    "href": "slides/10-slr-transformations-outliers.html#externally-studentized-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "(Externally) Studentized Residuals",
    "text": "(Externally) Studentized Residuals\n\nConcern: An unusual value may exert great influence on the fit\n\nIts residual might be underestimated because the model “moves” a lot to fit it\nThe standard error may also be inflated due to the outlier error\n\nStudentize: Fit the model without that case, then find new \\(\\hat{\\sigma}_\\epsilon\\)"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#example-movie-scores-3",
    "href": "slides/10-slr-transformations-outliers.html#example-movie-scores-3",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\nmovie_fit_aug |&gt;  # Augmented data\n  mutate(studentized_residual = rstudent(movie_fit)) |&gt; \n  gf_point(.std.resid ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Studentized Residual\")"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#what-to-do-with-an-outlier",
    "href": "slides/10-slr-transformations-outliers.html#what-to-do-with-an-outlier",
    "title": "SLR: Transformations + Outliers",
    "section": "What to do with an outlier?",
    "text": "What to do with an outlier?\n\nLook into it\nIf something is unusual about it and you can make a case that it is not a good representation of the population you can throw it out\nIf not and the value is just unusual, keep it"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#influence-vs.-leverage",
    "href": "slides/10-slr-transformations-outliers.html#influence-vs.-leverage",
    "title": "SLR: Transformations + Outliers",
    "section": "Influence vs. Leverage",
    "text": "Influence vs. Leverage\n\nHigh Influence Point: point that DOES impact the regression line\nHigh Leverage Point: point with “potential” to impact regression line because \\(X\\)-value is unusual"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#high-leverage-low-influence",
    "href": "slides/10-slr-transformations-outliers.html#high-leverage-low-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "High Leverage, Low Influence",
    "text": "High Leverage, Low Influence"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#high-leverage-high-influence",
    "href": "slides/10-slr-transformations-outliers.html#high-leverage-high-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "High Leverage, High Influence",
    "text": "High Leverage, High Influence"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#low-leverage-low-influence",
    "href": "slides/10-slr-transformations-outliers.html#low-leverage-low-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "Low Leverage, Low Influence",
    "text": "Low Leverage, Low Influence"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#low-leverage-high-influence",
    "href": "slides/10-slr-transformations-outliers.html#low-leverage-high-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence"
  },
  {
    "objectID": "slides/10-slr-transformations-outliers.html#low-leverage-high-influence-1",
    "href": "slides/10-slr-transformations-outliers.html#low-leverage-high-influence-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#regression-model-revisited",
    "href": "slides/10-slr-model-eval.html#regression-model-revisited",
    "title": "SLR: Model evaluation",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.719\n9.298\n-1.583\n0.122\n\n\nAvg_Income_K\n0.959\n0.128\n7.505\n0.000"
  },
  {
    "objectID": "slides/09-slr-conditions.html#computational-set-up",
    "href": "slides/09-slr-conditions.html#computational-set-up",
    "title": "SLR: Conditions",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)   # for plotting using formulas\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# HEB Dataset\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/09-slr-conditions.html#regression-model-revisited",
    "href": "slides/09-slr-conditions.html#regression-model-revisited",
    "title": "SLR: Conditions",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.719\n9.298\n-1.583\n0.122\n\n\nAvg_Income_K\n0.959\n0.128\n7.505\n0.000"
  },
  {
    "objectID": "slides/09-slr-conditions.html#mathematical-representation-visualized",
    "href": "slides/09-slr-conditions.html#mathematical-representation-visualized",
    "title": "SLR: Conditions",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/09-slr-conditions.html#model-conditions-1",
    "href": "slides/09-slr-conditions.html#model-conditions-1",
    "title": "SLR: Conditions",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/09-slr-conditions.html#linearity",
    "href": "slides/09-slr-conditions.html#linearity",
    "title": "SLR: Conditions",
    "section": "Linearity",
    "text": "Linearity\n\nIf the linear model, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\), adequately describes the relationship between \\(X\\) and \\(Y\\), then the residuals should reflect random (chance) error\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nLinearity satisfied if there is no distinguishable pattern in the residuals plot, i.e. the residuals should be randomly scattered\nA non-random pattern (e.g. a parabola) suggests a linear model does not adequately describe the relationship between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "slides/09-slr-conditions.html#linearity-1",
    "href": "slides/09-slr-conditions.html#linearity-1",
    "title": "SLR: Conditions",
    "section": "Linearity",
    "text": "Linearity\n✅ The residuals vs. fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/09-slr-conditions.html#the-augment-function",
    "href": "slides/09-slr-conditions.html#the-augment-function",
    "title": "SLR: Conditions",
    "section": "The augment function",
    "text": "The augment function\n\nheb_aug &lt;- augment(heb_fit)\n\nhead(heb_aug) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber_Organic\nAvg_Income_K\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n36\n71.186\n53.55498\n-17.554983\n0.0272386\n17.45293\n0.0145449\n-1.0192493\n\n\n4\n34.234\n18.11468\n-14.114677\n0.0925015\n17.53471\n0.0366889\n-0.8484596\n\n\n28\n71.186\n53.55498\n-25.554983\n0.0272386\n17.15160\n0.0308219\n-1.4837325\n\n\n31\n48.760\n32.04642\n-1.046423\n0.0493994\n17.71691\n0.0000981\n-0.0614599\n\n\n78\n78.096\n60.18230\n17.817702\n0.0312671\n17.44374\n0.0173428\n1.0366517\n\n\n14\n40.506\n24.13009\n-10.130092\n0.0711183\n17.62593\n0.0138683\n-0.6018889"
  },
  {
    "objectID": "slides/09-slr-conditions.html#residuals-vs.-fitted-values-code",
    "href": "slides/09-slr-conditions.html#residuals-vs.-fitted-values-code",
    "title": "SLR: Conditions",
    "section": "Residuals vs. fitted values (code)",
    "text": "Residuals vs. fitted values (code)\n\nheb_aug &lt;- augment(heb_fit)\n\ngf_point(.resid ~ .fitted, data = heb_aug) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/09-slr-conditions.html#non-linear-relationships",
    "href": "slides/09-slr-conditions.html#non-linear-relationships",
    "title": "SLR: Conditions",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships"
  },
  {
    "objectID": "slides/09-slr-conditions.html#constant-variance",
    "href": "slides/09-slr-conditions.html#constant-variance",
    "title": "SLR: Conditions",
    "section": "Constant variance",
    "text": "Constant variance\n\nIf the spread of the distribution of \\(Y\\) is equal for all values of \\(X\\) then the spread of the residuals should be approximately equal for each value of \\(X\\)\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nConstant variance satisfied if the vertical spread of the residuals is approximately equal as you move from left to right (i.e. there is no “fan” pattern)\nA fan pattern suggests the constant variance assumption is not satisfied and transformation or some other remedy is required (more on this later in the semester)\nCAREFUL: Inconsistent distribution of \\(X\\)s can make it seem as if there is non-constant variance"
  },
  {
    "objectID": "slides/09-slr-conditions.html#constant-variance-1",
    "href": "slides/09-slr-conditions.html#constant-variance-1",
    "title": "SLR: Conditions",
    "section": "Constant variance",
    "text": "Constant variance\n✅ The vertical spread of the residuals is relatively constant across the plot"
  },
  {
    "objectID": "slides/09-slr-conditions.html#non-constant-variance",
    "href": "slides/09-slr-conditions.html#non-constant-variance",
    "title": "SLR: Conditions",
    "section": "Non-constant variance",
    "text": "Non-constant variance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThink: Is my error/variance proportional to the thing I’m predicting?"
  },
  {
    "objectID": "slides/09-slr-conditions.html#normality",
    "href": "slides/09-slr-conditions.html#normality",
    "title": "SLR: Conditions",
    "section": "Normality",
    "text": "Normality\n\nThe linear model assumes that the distribution of \\(Y\\) is Normal for every value of \\(X\\)\nThis is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied\nNormality satisfied if a histogram of the residuals is approximately normal\n\nCan also check that the points on a normal QQ-plot falls along a diagonal line\n\nMost inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/09-slr-conditions.html#normality-1",
    "href": "slides/09-slr-conditions.html#normality-1",
    "title": "SLR: Conditions",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "slides/09-slr-conditions.html#check-normality-using-a-qq-plot",
    "href": "slides/09-slr-conditions.html#check-normality-using-a-qq-plot",
    "title": "SLR: Conditions",
    "section": "Check normality using a QQ-plot",
    "text": "Check normality using a QQ-plot\n\n\n\n\nCode\ngf_histogram(~.resid, data = heb_aug,\n             bins=7, color = \"white\") |&gt; \n  gf_labs(\n    x = \"Residual\",\n    y = \"Count\",\n    title = \"Histogram of residuals\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngf_qq(~.resid, data = heb_aug) |&gt; \n  gf_qqline() |&gt;  \n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")\n\n\n\n\n\n\n\n\n\n\n\n\nAssess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot).\nIf so, the residuals are normally distributed."
  },
  {
    "objectID": "slides/09-slr-conditions.html#normality-2",
    "href": "slides/09-slr-conditions.html#normality-2",
    "title": "SLR: Conditions",
    "section": "Normality",
    "text": "Normality\n\n❌ The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied.\n✅ The sample size \\(n =  37 &gt; 30\\), so the sample size is large enough to relax this condition and proceed with inference."
  },
  {
    "objectID": "slides/09-slr-conditions.html#independence",
    "href": "slides/09-slr-conditions.html#independence",
    "title": "SLR: Conditions",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nTwo common violations of the independence assumption:\n\nSerial Effect: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation)\nCluster Effect: If there are subgroups represented in the data that are not accounted for in the model (e.g., type of supermarket), you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup"
  },
  {
    "objectID": "slides/09-slr-conditions.html#independence-1",
    "href": "slides/09-slr-conditions.html#independence-1",
    "title": "SLR: Conditions",
    "section": "Independence",
    "text": "Independence\nRecall the description of the data:\n\nAverage household income (per zip code) and number of organic vegetable offerings in San Antonio, TX\nData from HEB website, compiled by high school student Linda Saucedo, Fall 2019\n\n\n❌ Based on the information we have, it’s unclear if the data are independent. In fact, I’d guess that they are likely geographically correlated."
  },
  {
    "objectID": "slides/09-slr-conditions.html#recap",
    "href": "slides/09-slr-conditions.html#recap",
    "title": "SLR: Conditions",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\n\nLinearity\nConstant variance\n\n\n\n\n\nNormality\nIndependence\n\n\n\n\n\n\n\nWhich of these conditions are required for fitting a SLR (and not doing any inference)?\nWhich for simulation-based inference for the slope for an SLR?\nWhich for inference with mathematical models?\n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/09-slr-conditions.html#comparing-inferential-methods",
    "href": "slides/09-slr-conditions.html#comparing-inferential-methods",
    "title": "SLR: Conditions",
    "section": "Comparing inferential methods",
    "text": "Comparing inferential methods\n\n\nWhat are the advantages of using simulation-based inference methods? What are the advantages of using inference methods based on mathematical models?\nUnder what scenario(s) would you prefer to use simulation-based methods? Under what scenario(s) would you prefer to use methods based on mathematical models?\n\n\n\n\n\n−+\n02:00\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "prepare/model-conditions-prep.html",
    "href": "prepare/model-conditions-prep.html",
    "title": "September 13 lecture",
    "section": "",
    "text": "Stat2: Topic 1.3 Assessing Conditions"
  },
  {
    "objectID": "prepare/model-conditions-prep.html#assigned-reading",
    "href": "prepare/model-conditions-prep.html#assigned-reading",
    "title": "September 13 lecture",
    "section": "",
    "text": "Stat2: Topic 1.3 Assessing Conditions"
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html",
    "href": "slides/10-slr-model-eval-old.html",
    "title": "SLR: Model evaluation",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html#computational-set-up",
    "href": "slides/10-slr-model-eval-old.html#computational-set-up",
    "title": "SLR: Model evaluation",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(tidymodels)  # for modeling\nlibrary(openintro)   # for the duke_forest dataset\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html#model-conditions-1",
    "href": "slides/10-slr-model-eval-old.html#model-conditions-1",
    "title": "SLR: Model evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html#augmented-data-frame",
    "href": "slides/10-slr-model-eval-old.html#augmented-data-frame",
    "title": "SLR: Model evaluation",
    "section": "Augmented data frame",
    "text": "Augmented data frame\n\ndf_fit &lt;- linear_reg() |&gt;\n  fit(price ~ area, data = duke_forest)\n\ndf_aug &lt;- augment(df_fit$fit)\n\nhead(df_aug)\n\n# A tibble: 6 × 8\n    price  area  .fitted  .resid   .hat  .sigma  .cooksd .std.resid\n    &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1 1520000  6040 1079931. 440069. 0.133  162605. 0.604         2.80 \n2 1030000  4475  830340. 199660. 0.0435 168386. 0.0333        1.21 \n3  420000  1745  394951.  25049. 0.0226 169664. 0.000260      0.150\n4  680000  2091  450132. 229868. 0.0157 168011. 0.0150        1.37 \n5  428500  1772  399257.  29243. 0.0220 169657. 0.000345      0.175\n6  456000  1950  427645.  28355. 0.0182 169659. 0.000266      0.170"
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html#two-statistics",
    "href": "slides/10-slr-model-eval-old.html#two-statistics",
    "title": "SLR: Model evaluation",
    "section": "Two statistics",
    "text": "Two statistics\n\n\nR-squared, \\(R^2\\) : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)\n\\[\nR^2 = Cor(x,y)^2 = Cor(y, \\hat{y})^2\n\\]\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\n\n. . .\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html#r2",
    "href": "slides/10-slr-model-eval-old.html#r2",
    "title": "SLR: Model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() using the augmented data:\n\n\nrsq(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard       0.445"
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html#interpreting-r2",
    "href": "slides/10-slr-model-eval-old.html#interpreting-r2",
    "title": "SLR: Model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\n🗳️ Discussion\n\n\nThe \\(R^2\\) of the model for price from area of houses in Duke Forest is 44.5%. Which of the following is the correct interpretation of this value?\n\n\nArea correctly predicts 44.5% of price for houses in Duke Forest.\n44.5% of the variability in price for houses in Duke Forest can be explained by area.\n44.5% of the variability in area for houses in Duke Forest can be explained by price\n44.5% of the time price for houses in Duke Forest can be predicted by area."
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html#alternative-approach-for-r2",
    "href": "slides/10-slr-model-eval-old.html#alternative-approach-for-r2",
    "title": "SLR: Model evaluation",
    "section": "Alternative approach for \\(R^2\\)",
    "text": "Alternative approach for \\(R^2\\)\nAlternatively, use glance() to construct a single row summary of the model fit, including \\(R^2\\):\n\nglance(df_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared   sigma statistic  p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt;   &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.445         0.439 168798.      77.0 6.29e-14     1 -1318. 2641. 2649.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\n\n. . .\n\nglance(df_fit)$r.squared\n\n[1] 0.4451945"
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html#rmse",
    "href": "slides/10-slr-model-eval-old.html#rmse",
    "title": "SLR: Model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rmse() using the augmented data:\n\nrmse(df_aug, truth = price, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard     167067.\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this when we get to regression with multiple predictors)"
  },
  {
    "objectID": "slides/10-slr-model-eval-old.html#obtaining-r2-and-rmse",
    "href": "slides/10-slr-model-eval-old.html#obtaining-r2-and-rmse",
    "title": "SLR: Model evaluation",
    "section": "Obtaining \\(R^2\\) and RMSE",
    "text": "Obtaining \\(R^2\\) and RMSE\n\n\nUse rsq() and rmse(), respectively\n\nrsq(df_aug, truth = price, estimate = .fitted)\nrmse(df_aug, truth = price, estimate = .fitted)\n\nFirst argument: data frame containing truth and estimate columns\nSecond argument: name of the column containing truth (observed outcome)\nThird argument: name of the column containing estimate (predicted outcome)"
  },
  {
    "objectID": "slides/01-welcome.html",
    "href": "slides/01-welcome.html",
    "title": "Welcome to MAT 212!",
    "section": "",
    "text": "Education and career journey\n\nGrew up outside New York City\nBS in Math & Statistics from Rice University (Houston, TX)\nBusiness Analyst at Capital One (Plano, TX)\nMS and PhD in Statistics & Operations Research from UNC-Chapel Hill\nPostdoc in Population Genetics at University of Chicago\nAssistant Professor of Math at St. Norbert College (Green Bay, WI)\n\nWork focuses on statistics education, queueing theory, and population genetics\nBig sports fan: NY Knicks, Giants, Rangers, Yankess, UNC Tarheels\nDad of three cute dogs: Allie, Miriam, Tony\n\n\n\n\n\n\n\n\n\nNavigate to Poll Everywhere and answer the survey questions. Note that your answers will not be visible by anyone but myself."
  },
  {
    "objectID": "slides/01-welcome.html#footnotes",
    "href": "slides/01-welcome.html#footnotes",
    "title": "Welcome to MAT 212!",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThese guiding principles are based on Course Policies related to ChatGPT and other AI Tools developed by Joel Gladd, Ph.D.↩︎↩︎"
  },
  {
    "objectID": "slides/07-slr-sim-testing.html#concept-check",
    "href": "slides/07-slr-sim-testing.html#concept-check",
    "title": "SLR: Randomization test for the slope",
    "section": "Concept Check",
    "text": "Concept Check\n\nYou professor is interested in calculating the average amount of time CofI students spend doing homework.\n\nIf he collects a set of data and asks 100 students to compute 95% confidence intervals from that data, how many of those would you expect to contain the true average?\nIf, instead, he has each of those 100 students collect their own data and compute 95% confidence intervals from their own data, how many would you expect to contain the true average?"
  },
  {
    "objectID": "slides/06-slr-bootstrap.html",
    "href": "slides/06-slr-bootstrap.html",
    "title": "SLR: Simulation-based inference",
    "section": "",
    "text": "Goal: Use simple linear regression to model the relationship between temperature and daily bike rentals in the winter season\n\n\n\n\n\n\n\n\n\n\n\n\nStatistical Model:\n\\[count = \\beta_0 +\\beta_1 ~ temp\\_orig + \\epsilon, \\hspace{5mm} \\epsilon \\sim N(0, \\sigma_{\\epsilon}^2)\\]\n. . .\n\nwinter_fit &lt;- lm(count ~ temp_orig, data = winter)\n\ntidy(winter_fit) |&gt; kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-111.038\n238.312\n-0.466\n0.642\n\n\ntemp_orig\n222.416\n18.459\n12.049\n0.000\n\n\n\n\n\n\n\n\nUse the output to write out the estimated regression equation.\n\\[\n\\hat{count} =  -111.038 + 222.416 ~temp\\_orig\n\\]\n\n\nLaTex:\n\\$\\$\\hat{count} = -111.038 + 222.416 ~ temp\\_orig\\$\\$\n\n\n\n\n\n\nYour turn!\n\n\n\n\nInterpret the slope in the context of the data.\nWhy is there no error term in the regression equation?"
  },
  {
    "objectID": "ae/ae-05-conditions.html",
    "href": "ae/ae-05-conditions.html",
    "title": "AE 05: Model Conditions",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-05”\nGo to the Canvas and locate your AE 05 assignment to get started.\nUpload the ae-05.qmd and ae-05.pdf files into the folder you just created. The .qmd and PDF responses are due in Canvas no later than Monday, September 16 at 11:59pm.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-05-conditions.html#data",
    "href": "ae/ae-05-conditions.html#data",
    "title": "AE 05: Model Conditions",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is a subset from the Spotify Songs Tidy Tuesday data set. The data were originally obtained from Spotify using the spotifyr R package.\nIt contains numerous characteristics for each song. You can see the full list of variables and definitions here. This analysis will focus specifically on the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\n\n\nspotify &lt;- read_csv(\"data/spotify-popular.csv\")\n\nAre high energy songs more positive? To answer this question, we’ll analyze data on some of the most popular songs on Spotify, i.e. those with track_popularity &gt;= 80. We’ll use linear regression to fit a model to predict a song’s positiveness (valence) based on its energy level (energy).\nBelow are plots as part of the exploratory data analysis.\n\np1 &lt;- gf_histogram(~valence, data = spotify) |&gt;  \n  gf_labs(title = \"Distribution of Valence\", \n       subtitle = \" for Popular songs on Spotify\", \n       x = \"Valence\")\n\np2 &lt;- gf_histogram(~energy, data = spotify) |&gt; \n  gf_labs(title = \"Distribution of Energy\", \n       subtitle = \"for Popular songs on Spotify\", \n       x = \"Energy\")\np1 + p2 # The patchwork package will arrange your plots for you\n\n\n\n\n\n\n\n\n\ngf_point(valence ~ energy, data = spotify) |&gt; \n  gf_labs(title = \"Valence vs. Energy\", \n       subtitle = \"Popular songs on Spotify\", \n       x = \"Energy\", \n       y = \"Valence\")"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-1",
    "href": "ae/ae-05-conditions.html#exercise-1",
    "title": "AE 05: Model Conditions",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model using the energy of a song to predict its valence, i.e. positiveness. Include the 90% confidence interval for the coefficients, and display the output using 3 digits.\n\n## add code"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-2",
    "href": "ae/ae-05-conditions.html#exercise-2",
    "title": "AE 05: Model Conditions",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet’s check the model conditions before doing any inference. Fill in the code below to use the augment() function to create a new data frame containing the residuals and fitted values (among other information)\n\n\n\n\n\n\nImportant\n\n\n\nNote: Remove #|eval: false from the code chunk after you have filled in the code.\n\n\n\nspotify_aug &lt;- augment(_____)"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-3",
    "href": "ae/ae-05-conditions.html#exercise-3",
    "title": "AE 05: Model Conditions",
    "section": "Exercise 3",
    "text": "Exercise 3\nMake a plot of the residual vs. fitted values.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-4",
    "href": "ae/ae-05-conditions.html#exercise-4",
    "title": "AE 05: Model Conditions",
    "section": "Exercise 4",
    "text": "Exercise 4\nFill in the code to make a histogram of the residuals and a normal QQ-plot.\n\nresid_hist &lt;- gf_histogram(~____, data = ____)) |&gt; \n  gf_labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_qq &lt;- gf_qq(~____, data = ____)  |&gt;\n  gf_qqline() |&gt; \n  gf_labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_hist + resid_qq"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-5",
    "href": "ae/ae-05-conditions.html#exercise-5",
    "title": "AE 05: Model Conditions",
    "section": "Exercise 5",
    "text": "Exercise 5\nAssess the four model conditions. Use the plots from the previous exercises to help make the assessment.\n\nLinearity\nConstant variance\nNormality\nIndependence"
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-6",
    "href": "ae/ae-05-conditions.html#exercise-6",
    "title": "AE 05: Model Conditions",
    "section": "Exercise 6",
    "text": "Exercise 6\nCalculate \\(R^2\\) and interpret this value in the context of the data."
  },
  {
    "objectID": "ae/ae-05-conditions.html#exercise-7",
    "href": "ae/ae-05-conditions.html#exercise-7",
    "title": "AE 05: Model Conditions",
    "section": "Exercise 7",
    "text": "Exercise 7\nCalculate RMSE and interpret this value in the context of the data.\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-06 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "prepare/model-eval-prep.html",
    "href": "prepare/model-eval-prep.html",
    "title": "September 16 lecture",
    "section": "",
    "text": "Stat2: Topic 2.2 Partitioning Variability - ANOVA\nStat2: Topic 2.3 Regression and Correlation"
  },
  {
    "objectID": "prepare/model-eval-prep.html#assigned-reading",
    "href": "prepare/model-eval-prep.html#assigned-reading",
    "title": "September 16 lecture",
    "section": "",
    "text": "Stat2: Topic 2.2 Partitioning Variability - ANOVA\nStat2: Topic 2.3 Regression and Correlation"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#recall-correlation-coefficient",
    "href": "slides/10-slr-model-eval.html#recall-correlation-coefficient",
    "title": "SLR: Model evaluation",
    "section": "Recall: Correlation Coefficient",
    "text": "Recall: Correlation Coefficient\n\nThe correlation coefficient, \\(r\\), is a number between -1 and +1 that measures how strong the linear relationship between two variables \\(x\\) and \\(y\\) is.\n\n\\[\nr = \\frac{\\sum(x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n= \\frac{\\sum(x_i - \\bar{x})(y_i-\\bar{y})}{s_xs_y}\n\\]"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#sum-of-squared-erros",
    "href": "slides/10-slr-model-eval.html#sum-of-squared-erros",
    "title": "SLR: Model evaluation",
    "section": "Sum of Squared Erros",
    "text": "Sum of Squared Erros"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#partitioning-variability",
    "href": "slides/10-slr-model-eval.html#partitioning-variability",
    "title": "SLR: Model evaluation",
    "section": "Partitioning Variability",
    "text": "Partitioning Variability\nLet’s think about variation:\n\n\nDATA = MODEL + ERROR\n\\(\\substack{\\text{Variation} \\\\ \\text{in Y}} = \\substack{\\text{Variation explained} \\\\ \\text{by model}} + \\substack{\\text{Variation not explained} \\\\ \\text{by model}}\\)"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#recall-variance",
    "href": "slides/10-slr-model-eval.html#recall-variance",
    "title": "SLR: Model evaluation",
    "section": "Recall Variance:",
    "text": "Recall Variance:\n\nVariance: A measure of how variable a variable is.\nIntuition: The average squared distance from the mean (deviation) for each observation\n\n\\[\ns_y^2 = \\frac{\\sum(y_i-\\bar{y})^2}{n-1}\n\\]"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#partitioning-variability-1",
    "href": "slides/10-slr-model-eval.html#partitioning-variability-1",
    "title": "SLR: Model evaluation",
    "section": "Partitioning Variability",
    "text": "Partitioning Variability\n\n\n\\(y_i - \\bar{y} = (\\hat{y}_i - \\bar{y}) + (y_i-\\hat{y}_i)\\)\nSquare and sum: \\(\\sum(y_i-\\bar{y})^2 = \\sum(\\hat{y} - \\bar{y})^2 + \\sum(y-\\hat{y})^2\\)\n\\(\\substack{\\text{Sum of squares} \\\\ \\text{Total}} = \\substack{\\text{Sum of squares} \\\\ \\text{model}} + \\substack{\\text{Sum of squares} \\\\ \\text{error}}\\)\n\\(SSTotal = SSModel + SSE\\)\n\\(SST = SSM + SSE\\)"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#partitioning-variability-2",
    "href": "slides/10-slr-model-eval.html#partitioning-variability-2",
    "title": "SLR: Model evaluation",
    "section": "Partitioning Variability",
    "text": "Partitioning Variability\n\n\n\\(y_i - \\bar{y} = (\\hat{y}_i - \\bar{y}) + (y_i-\\hat{y}_i)\\)\nSquare and sum: \\(\\sum(y_i-\\bar{y})^2 = (\\hat{y} - \\bar{y})^2 + (y-\\hat{y})^2\\)\n\\(\\substack{\\text{Sum of squares} \\\\ \\text{Total}} = \\substack{\\text{Sum of squares} \\\\ \\text{model}} + \\substack{\\text{Sum of squares} \\\\ \\text{error}}\\)\n\\(SSTotal = SSModel + SSE\\)\n\\(SST = SSM + SSE\\)"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#anova-in-r",
    "href": "slides/10-slr-model-eval.html#anova-in-r",
    "title": "SLR: Model evaluation",
    "section": "ANOVA in R",
    "text": "ANOVA in R\n\nanova(heb_fit) |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nAvg_Income_K\n1\n17175.06\n17175.0595\n56.32026\n0\n\n\nResiduals\n35\n10673.37\n304.9535\nNA\nNA\n\n\n\n\n\n\n\nMore on this later in the semester"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#activity",
    "href": "slides/10-slr-model-eval.html#activity",
    "title": "SLR: Model evaluation",
    "section": "Activity",
    "text": "Activity\n\nIn groups, at the board, design a simulation-based procedure for producing a p-value for the following hypothesis test.\n\n\\(H_0: R^2 = 0\\)\n\\(H_A: R^2 \\neq 0\\)"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#partitioning-variability-anova",
    "href": "slides/10-slr-model-eval.html#partitioning-variability-anova",
    "title": "SLR: Model evaluation",
    "section": "Partitioning Variability (ANOVA)",
    "text": "Partitioning Variability (ANOVA)\n\n\n\\(y_i - \\bar{y} = (\\hat{y}_i - \\bar{y}) + (y_i-\\hat{y}_i)\\)\nSquare and sum: \\(\\sum(y_i-\\bar{y})^2 = \\sum(\\hat{y} - \\bar{y})^2 + \\sum(y-\\hat{y})^2\\)\n\\(\\substack{\\text{Sum of squares} \\\\ \\text{Total}} = \\substack{\\text{Sum of squares} \\\\ \\text{model}} + \\substack{\\text{Sum of squares} \\\\ \\text{error}}\\)\n\\(SSTotal = SSModel + SSE\\)\n\\(SST = SSM + SSE\\)"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#anova",
    "href": "slides/10-slr-model-eval.html#anova",
    "title": "SLR: Model evaluation",
    "section": "ANOVA",
    "text": "ANOVA\n\n\\(MSModel = \\frac{SSModel}{1}\\)\n\\(MSE = \\frac{SSE}{n-2}\\)\nTest Statistic: \\(F = \\frac{MSModel}{MSE}\\)\nNull Hypothesis: $F\n\n\nanova(heb_fit) |&gt; tidy()\n\n# A tibble: 2 × 6\n  term            df  sumsq meansq statistic  p.value\n  &lt;chr&gt;        &lt;int&gt;  &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 Avg_Income_K     1 17175. 17175.      56.3  8.63e-9\n2 Residuals       35 10673.   305.      NA   NA      \n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#two-statistics-r2",
    "href": "slides/10-slr-model-eval.html#two-statistics-r2",
    "title": "SLR: Model evaluation",
    "section": "Two statistics: \\(R^2\\)",
    "text": "Two statistics: \\(R^2\\)\n\n\nR-squared, \\(R^2\\), Coefficient of Determination : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor) \\[\nR^2 = Cor(y, \\hat{y})^2\n\\]\n\nAlso called **PRE (Percent Reduction in Error)** because:\n\\[\nR^2 = \\frac{SSModel}{SSTotal}\n\\]"
  },
  {
    "objectID": "slides/10-slr-model-eval.html#two-statistics-rmse",
    "href": "slides/10-slr-model-eval.html#two-statistics-rmse",
    "title": "SLR: Model evaluation",
    "section": "Two statistics: RMSE",
    "text": "Two statistics: RMSE\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome)\n\\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\nSometimes people just case about numerator (SSE) or version without the square-root (MSE)\nSometimes the denominator may have \\(n-1\\) instead\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/09-slr-conditions.html#hypothesis-test-conclusion-in-context",
    "href": "slides/09-slr-conditions.html#hypothesis-test-conclusion-in-context",
    "title": "SLR: Conditions",
    "section": "Hypothesis test: Conclusion, in context",
    "text": "Hypothesis test: Conclusion, in context\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\n\nThe data provide convincing evidence that the population slope \\(\\beta_1\\) is different from 0.\nThe data provide convincing evidence of a linear relationship between average household income and the number of organic vegetable options available."
  },
  {
    "objectID": "slides/09-slr-conditions.html#confidence-interval-for-the-slope",
    "href": "slides/09-slr-conditions.html#confidence-interval-for-the-slope",
    "title": "SLR: Conditions",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/09-slr-conditions.html#confidence-interval-critical-value",
    "href": "slides/09-slr-conditions.html#confidence-interval-critical-value",
    "title": "SLR: Conditions",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(heb) - 2)\n\n[1] 2.030108\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(heb) - 2)\n\n[1] 1.689572\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(heb) - 2)\n\n[1] 2.723806"
  },
  {
    "objectID": "slides/09-slr-conditions.html#ci-for-the-slope-calculation",
    "href": "slides/09-slr-conditions.html#ci-for-the-slope-calculation",
    "title": "SLR: Conditions",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\n\\[\\hat{\\beta}_1 = 0.96 \\hspace{15mm} t^* = 2.03 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 0.13\\]\n\n\\[\n0.96 \\pm 2.03 \\times 0.13 = (0.70, 1.22)\n\\]"
  },
  {
    "objectID": "slides/09-slr-conditions.html#ci-for-the-slope-computation",
    "href": "slides/09-slr-conditions.html#ci-for-the-slope-computation",
    "title": "SLR: Conditions",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(heb_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n-33.59\n4.16\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n0.70\n1.22"
  },
  {
    "objectID": "slides/09-slr-conditions.html#intervals-for-predictions-1",
    "href": "slides/09-slr-conditions.html#intervals-for-predictions-1",
    "title": "SLR: Conditions",
    "section": "Intervals for predictions",
    "text": "Intervals for predictions\n\nQuestion: “What is the predicted number of organic vegetable options in a neighborhood with an average income of $70k?”\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead"
  },
  {
    "objectID": "slides/09-slr-conditions.html#two-types-of-predictions",
    "href": "slides/09-slr-conditions.html#two-types-of-predictions",
    "title": "SLR: Conditions",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: “What is the average predicted number of organic vegetable options in a neighborhood with an average income of $70k?”\nPrediction for an individual observation: “What is the predicted number of organic vegetable options at a single HEB in a neighborhood with an average income of $70k?”\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/09-slr-conditions.html#uncertainty-in-predictions",
    "href": "slides/09-slr-conditions.html#uncertainty-in-predictions",
    "title": "SLR: Conditions",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}}\\]\n\nPrediction interval for an individual observation: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}}\\]"
  },
  {
    "objectID": "slides/09-slr-conditions.html#standard-errors",
    "href": "slides/09-slr-conditions.html#standard-errors",
    "title": "SLR: Conditions",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/09-slr-conditions.html#standard-errors-1",
    "href": "slides/09-slr-conditions.html#standard-errors-1",
    "title": "SLR: Conditions",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/09-slr-conditions.html#confidence-interval",
    "href": "slides/09-slr-conditions.html#confidence-interval",
    "title": "SLR: Conditions",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcome:\n\nnew_neighborhood &lt;- tibble(Avg_Income_K = 70)\n\npredict(heb_fit, newdata = new_neighborhood, interval = \"confidence\", level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n52.4175\n46.58558\n58.24942\n\n\n\n\n\n\n\nWe are 95% confident that the mean number of organic vegetable options offered by HEB in a neighborhood with an average income of $70k is between 46.59 and 58.25."
  },
  {
    "objectID": "slides/09-slr-conditions.html#prediction-interval",
    "href": "slides/09-slr-conditions.html#prediction-interval",
    "title": "SLR: Conditions",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction interval for an individual outcome:\n\npredict(heb_fit, newdata = new_neighborhood, interval = \"prediction\", level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n52.4175\n16.48941\n88.34559\n\n\n\n\n\n\nWe are 95% confident that the number of organic vegetable options offered by HEB in a neighborhood with an average income of $70k is between 16.49 and 88.35."
  },
  {
    "objectID": "slides/09-slr-conditions.html#comparing-intervals",
    "href": "slides/09-slr-conditions.html#comparing-intervals",
    "title": "SLR: Conditions",
    "section": "Comparing intervals",
    "text": "Comparing intervals"
  },
  {
    "objectID": "slides/09-slr-conditions.html#extrapolation",
    "href": "slides/09-slr-conditions.html#extrapolation",
    "title": "SLR: Conditions",
    "section": "Extrapolation",
    "text": "Extrapolation\nUsing the model to predict for values outside the range of the original data is extrapolation.\n\n\n\n\nCalculate the prediction interval for the number of organic options in an extremely wealthy neighborhood with an average household income of $500k.\n\nNo, thanks!"
  },
  {
    "objectID": "slides/09-slr-conditions.html#extrapolation-1",
    "href": "slides/09-slr-conditions.html#extrapolation-1",
    "title": "SLR: Conditions",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nWhy do we want to avoid extrapolation?"
  },
  {
    "objectID": "slides/10-slr-conditions.html#computational-set-up",
    "href": "slides/10-slr-conditions.html#computational-set-up",
    "title": "SLR: Conditions",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)   # for plotting using formulas\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# HEB Dataset\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/10-slr-conditions.html#regression-model-revisited",
    "href": "slides/10-slr-conditions.html#regression-model-revisited",
    "title": "SLR: Conditions",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.719\n9.298\n-1.583\n0.122\n\n\nAvg_Income_K\n0.959\n0.128\n7.505\n0.000"
  },
  {
    "objectID": "slides/10-slr-conditions.html#mathematical-representation-visualized",
    "href": "slides/10-slr-conditions.html#mathematical-representation-visualized",
    "title": "SLR: Conditions",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/10-slr-conditions.html#model-conditions-1",
    "href": "slides/10-slr-conditions.html#model-conditions-1",
    "title": "SLR: Conditions",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/10-slr-conditions.html#linearity",
    "href": "slides/10-slr-conditions.html#linearity",
    "title": "SLR: Conditions",
    "section": "Linearity",
    "text": "Linearity\n\nIf the linear model, \\(\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i\\), adequately describes the relationship between \\(X\\) and \\(Y\\), then the residuals should reflect random (chance) error\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nLinearity satisfied if there is no distinguishable pattern in the residuals plot, i.e. the residuals should be randomly scattered\nA non-random pattern (e.g. a parabola) suggests a linear model does not adequately describe the relationship between \\(X\\) and \\(Y\\)"
  },
  {
    "objectID": "slides/10-slr-conditions.html#linearity-1",
    "href": "slides/10-slr-conditions.html#linearity-1",
    "title": "SLR: Conditions",
    "section": "Linearity",
    "text": "Linearity\n✅ The residuals vs. fitted values plot should show a random scatter of residuals (no distinguishable pattern or structure)"
  },
  {
    "objectID": "slides/10-slr-conditions.html#the-augment-function",
    "href": "slides/10-slr-conditions.html#the-augment-function",
    "title": "SLR: Conditions",
    "section": "The augment function",
    "text": "The augment function\n\nheb_aug &lt;- augment(heb_fit)\n\nhead(heb_aug) |&gt; kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNumber_Organic\nAvg_Income_K\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n36\n71.186\n53.55498\n-17.554983\n0.0272386\n17.45293\n0.0145449\n-1.0192493\n\n\n4\n34.234\n18.11468\n-14.114677\n0.0925015\n17.53471\n0.0366889\n-0.8484596\n\n\n28\n71.186\n53.55498\n-25.554983\n0.0272386\n17.15160\n0.0308219\n-1.4837325\n\n\n31\n48.760\n32.04642\n-1.046423\n0.0493994\n17.71691\n0.0000981\n-0.0614599\n\n\n78\n78.096\n60.18230\n17.817702\n0.0312671\n17.44374\n0.0173428\n1.0366517\n\n\n14\n40.506\n24.13009\n-10.130092\n0.0711183\n17.62593\n0.0138683\n-0.6018889"
  },
  {
    "objectID": "slides/10-slr-conditions.html#residuals-vs.-fitted-values-code",
    "href": "slides/10-slr-conditions.html#residuals-vs.-fitted-values-code",
    "title": "SLR: Conditions",
    "section": "Residuals vs. fitted values (code)",
    "text": "Residuals vs. fitted values (code)\n\nheb_aug &lt;- augment(heb_fit)\n\ngf_point(.resid ~ .fitted, data = heb_aug) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/10-slr-conditions.html#non-linear-relationships",
    "href": "slides/10-slr-conditions.html#non-linear-relationships",
    "title": "SLR: Conditions",
    "section": "Non-linear relationships",
    "text": "Non-linear relationships"
  },
  {
    "objectID": "slides/10-slr-conditions.html#constant-variance",
    "href": "slides/10-slr-conditions.html#constant-variance",
    "title": "SLR: Conditions",
    "section": "Constant variance",
    "text": "Constant variance\n\nIf the spread of the distribution of \\(Y\\) is equal for all values of \\(X\\) then the spread of the residuals should be approximately equal for each value of \\(X\\)\nTo assess this, we can look at a plot of the residuals vs. the fitted values\nConstant variance satisfied if the vertical spread of the residuals is approximately equal as you move from left to right (i.e. there is no “fan” pattern)\nA fan pattern suggests the constant variance assumption is not satisfied and transformation or some other remedy is required (more on this later in the semester)\nCAREFUL: Inconsistent distribution of \\(X\\)s can make it seem as if there is non-constant variance"
  },
  {
    "objectID": "slides/10-slr-conditions.html#constant-variance-1",
    "href": "slides/10-slr-conditions.html#constant-variance-1",
    "title": "SLR: Conditions",
    "section": "Constant variance",
    "text": "Constant variance\n✅ The vertical spread of the residuals is relatively constant across the plot"
  },
  {
    "objectID": "slides/10-slr-conditions.html#non-constant-variance",
    "href": "slides/10-slr-conditions.html#non-constant-variance",
    "title": "SLR: Conditions",
    "section": "Non-constant variance",
    "text": "Non-constant variance\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThink: Is my error/variance proportional to the thing I’m predicting?"
  },
  {
    "objectID": "slides/10-slr-conditions.html#normality",
    "href": "slides/10-slr-conditions.html#normality",
    "title": "SLR: Conditions",
    "section": "Normality",
    "text": "Normality\n\nThe linear model assumes that the distribution of \\(Y\\) is Normal for every value of \\(X\\)\nThis is impossible to check in practice, so we will look at the overall distribution of the residuals to assess if the normality assumption is satisfied\nNormality satisfied if a histogram of the residuals is approximately normal\n\nCan also check that the points on a normal QQ-plot falls along a diagonal line\n\nMost inferential methods for regression are robust to some departures from normality, so we can proceed with inference if the sample size is sufficiently large, \\(n &gt; 30\\)"
  },
  {
    "objectID": "slides/10-slr-conditions.html#normality-1",
    "href": "slides/10-slr-conditions.html#normality-1",
    "title": "SLR: Conditions",
    "section": "Normality",
    "text": "Normality"
  },
  {
    "objectID": "slides/10-slr-conditions.html#check-normality-using-a-qq-plot",
    "href": "slides/10-slr-conditions.html#check-normality-using-a-qq-plot",
    "title": "SLR: Conditions",
    "section": "Check normality using a QQ-plot",
    "text": "Check normality using a QQ-plot\n\n\n\n\nCode\ngf_histogram(~.resid, data = heb_aug,\n             bins=7, color = \"white\") |&gt; \n  gf_labs(\n    x = \"Residual\",\n    y = \"Count\",\n    title = \"Histogram of residuals\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ngf_qq(~.resid, data = heb_aug) |&gt; \n  gf_qqline() |&gt;  \n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")\n\n\n\n\n\n\n\n\n\n\n\n\nAssess whether residuals lie along the diagonal line of the Quantile-quantile plot (QQ-plot).\nIf so, the residuals are normally distributed."
  },
  {
    "objectID": "slides/10-slr-conditions.html#normality-2",
    "href": "slides/10-slr-conditions.html#normality-2",
    "title": "SLR: Conditions",
    "section": "Normality",
    "text": "Normality\n\n❌ The residuals do not appear to follow a normal distribution, because the points do not lie on the diagonal line, so normality is not satisfied.\n✅ The sample size \\(n =  37 &gt; 30\\), so the sample size is large enough to relax this condition and proceed with inference."
  },
  {
    "objectID": "slides/10-slr-conditions.html#independence",
    "href": "slides/10-slr-conditions.html#independence",
    "title": "SLR: Conditions",
    "section": "Independence",
    "text": "Independence\n\nWe can often check the independence assumption based on the context of the data and how the observations were collected\nTwo common violations of the independence assumption:\n\nSerial Effect: If the data were collected over time, plot the residuals in time order to see if there is a pattern (serial correlation)\nCluster Effect: If there are subgroups represented in the data that are not accounted for in the model (e.g., type of supermarket), you can color the points in the residual plots by group to see if the model systematically over or under predicts for a particular subgroup"
  },
  {
    "objectID": "slides/10-slr-conditions.html#independence-1",
    "href": "slides/10-slr-conditions.html#independence-1",
    "title": "SLR: Conditions",
    "section": "Independence",
    "text": "Independence\nRecall the description of the data:\n\nAverage household income (per zip code) and number of organic vegetable offerings in San Antonio, TX\nData from HEB website, compiled by high school student Linda Saucedo, Fall 2019\n\n\n❌ Based on the information we have, it’s unclear if the data are independent. In fact, I’d guess that they are likely geographically correlated."
  },
  {
    "objectID": "slides/10-slr-conditions.html#recap",
    "href": "slides/10-slr-conditions.html#recap",
    "title": "SLR: Conditions",
    "section": "Recap",
    "text": "Recap\nUsed residual plots to check conditions for SLR:\n\n\n\n\nLinearity\nConstant variance\n\n\n\n\n\nNormality\nIndependence\n\n\n\n\n\n\n\nWhich of these conditions are required for fitting a SLR (and not doing any inference)?\nWhich for simulation-based inference for the slope for an SLR?\nWhich for inference with mathematical models?\n\n\n\n\n\n−+\n03:00\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/10-slr-conditions.html#comparing-inferential-methods",
    "href": "slides/10-slr-conditions.html#comparing-inferential-methods",
    "title": "SLR: Conditions",
    "section": "Comparing inferential methods",
    "text": "Comparing inferential methods\n\n\nWhat are the advantages of using simulation-based inference methods? What are the advantages of using inference methods based on mathematical models?\nUnder what scenario(s) would you prefer to use simulation-based methods? Under what scenario(s) would you prefer to use methods based on mathematical models?\n\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#computational-set-up",
    "href": "slides/09-slr-math-models-2.html#computational-set-up",
    "title": "SLR: Math Models Continued",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)   # for plotting using formulas\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# HEB Dataset\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#regression-model-revisited",
    "href": "slides/09-slr-math-models-2.html#regression-model-revisited",
    "title": "SLR: Math Models Continued",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.719\n9.298\n-1.583\n0.122\n\n\nAvg_Income_K\n0.959\n0.128\n7.505\n0.000"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#mathematical-representation-visualized",
    "href": "slides/09-slr-math-models-2.html#mathematical-representation-visualized",
    "title": "SLR: Math Models Continued",
    "section": "Mathematical representation, visualized",
    "text": "Mathematical representation, visualized\n\\[\nY|X \\sim N(\\beta_0 + \\beta_1 X, \\sigma_\\epsilon^2)\n\\]\n\nImage source: Introduction to the Practice of Statistics (5th ed)"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#confidence-interval-for-the-slope",
    "href": "slides/09-slr-math-models-2.html#confidence-interval-for-the-slope",
    "title": "SLR: Math Models Continued",
    "section": "Confidence interval for the slope",
    "text": "Confidence interval for the slope\n\\[\n\\text{Estimate} \\pm \\text{ (critical value) } \\times \\text{SE}\n\\]\n\n\\[\n\\hat{\\beta}_1 \\pm t^* \\times SE_{\\hat{\\beta}_1}\n\\]\nwhere \\(t^*\\) is calculated from a \\(t\\) distribution with \\(n-2\\) degrees of freedom"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#confidence-interval-critical-value",
    "href": "slides/09-slr-math-models-2.html#confidence-interval-critical-value",
    "title": "SLR: Math Models Continued",
    "section": "Confidence interval: Critical value",
    "text": "Confidence interval: Critical value\n\n\n\n\n# confidence level: 95%\nqt(0.975, df = nrow(heb) - 2)\n\n[1] 2.030108\n\n\n\n\n\n# confidence level: 90%\nqt(0.95, df = nrow(heb) - 2)\n\n[1] 1.689572\n\n\n\n\n\n# confidence level: 99%\nqt(0.995, df = nrow(heb) - 2)\n\n[1] 2.723806"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#ci-for-the-slope-calculation",
    "href": "slides/09-slr-math-models-2.html#ci-for-the-slope-calculation",
    "title": "SLR: Math Models Continued",
    "section": "95% CI for the slope: Calculation",
    "text": "95% CI for the slope: Calculation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n\n\n\n\n\n\n\n\\[\\hat{\\beta}_1 = 0.96 \\hspace{15mm} t^* = 2.03 \\hspace{15mm} SE_{\\hat{\\beta}_1} = 0.13\\]\n\n\\[\n0.96 \\pm 2.03 \\times 0.13 = (0.70, 1.22)\n\\]"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#ci-for-the-slope-computation",
    "href": "slides/09-slr-math-models-2.html#ci-for-the-slope-computation",
    "title": "SLR: Math Models Continued",
    "section": "95% CI for the slope: Computation",
    "text": "95% CI for the slope: Computation\n\ntidy(heb_fit, conf.int = TRUE, conf.level = 0.95) |&gt; \n  kable(digits = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-14.72\n9.30\n-1.58\n0.12\n-33.59\n4.16\n\n\nAvg_Income_K\n0.96\n0.13\n7.50\n0.00\n0.70\n1.22"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#intervals-for-predictions-1",
    "href": "slides/09-slr-math-models-2.html#intervals-for-predictions-1",
    "title": "SLR: Math Models Continued",
    "section": "Intervals for predictions",
    "text": "Intervals for predictions\n\nQuestion: “What is the predicted number of organic vegetable options in a neighborhood with an average income of $70k?”\nWe said reporting a single estimate for the slope is not wise, and we should report a plausible range instead\nSimilarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#two-types-of-predictions",
    "href": "slides/09-slr-math-models-2.html#two-types-of-predictions",
    "title": "SLR: Math Models Continued",
    "section": "Two types of predictions",
    "text": "Two types of predictions\n\nPrediction for the mean: “What is the average predicted number of organic vegetable options in a neighborhood with an average income of $70k?”\nPrediction for an individual observation: “What is the predicted number of organic vegetable options at a single HEB in a neighborhood with an average income of $70k?”\n\n\n\nWhich would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#uncertainty-in-predictions",
    "href": "slides/09-slr-math-models-2.html#uncertainty-in-predictions",
    "title": "SLR: Math Models Continued",
    "section": "Uncertainty in predictions",
    "text": "Uncertainty in predictions\nConfidence interval for the mean outcome: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE}_{\\hat{\\boldsymbol{\\mu}}}}}\\]\n\nPrediction interval for an individual observation: \\[\\large{\\hat{y} \\pm t_{n-2}^* \\times \\color{purple}{\\mathbf{SE_{\\hat{y}}}}}\\]"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#standard-errors",
    "href": "slides/09-slr-math-models-2.html#standard-errors",
    "title": "SLR: Math Models Continued",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\n\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{1 + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#standard-errors-1",
    "href": "slides/09-slr-math-models-2.html#standard-errors-1",
    "title": "SLR: Math Models Continued",
    "section": "Standard errors",
    "text": "Standard errors\nStandard error of the mean outcome: \\[SE_{\\hat{\\mu}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]\nStandard error of an individual outcome: \\[SE_{\\hat{y}} = \\hat{\\sigma}_\\epsilon\\sqrt{\\mathbf{\\color{purple}{\\Large{1}}} + \\frac{1}{n} + \\frac{(x-\\bar{x})^2}{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2}}\\]"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#confidence-interval",
    "href": "slides/09-slr-math-models-2.html#confidence-interval",
    "title": "SLR: Math Models Continued",
    "section": "Confidence interval",
    "text": "Confidence interval\nThe 95% confidence interval for the mean outcome:\n\nnew_neighborhood &lt;- tibble(Avg_Income_K = 70)\n\npredict(heb_fit, newdata = new_neighborhood, interval = \"confidence\", level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n52.4175\n46.58558\n58.24942\n\n\n\n\n\n\n\nWe are 95% confident that the mean number of organic vegetable options offered by HEB in a neighborhood with an average income of $70k is between 46.59 and 58.25."
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#prediction-interval",
    "href": "slides/09-slr-math-models-2.html#prediction-interval",
    "title": "SLR: Math Models Continued",
    "section": "Prediction interval",
    "text": "Prediction interval\nThe 95% prediction interval for an individual outcome:\n\npredict(heb_fit, newdata = new_neighborhood, interval = \"prediction\", level = 0.95) |&gt;\n  kable()\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n52.4175\n16.48941\n88.34559\n\n\n\n\n\n\nWe are 95% confident that the number of organic vegetable options offered by HEB in a neighborhood with an average income of $70k is between 16.49 and 88.35."
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#comparing-intervals",
    "href": "slides/09-slr-math-models-2.html#comparing-intervals",
    "title": "SLR: Math Models Continued",
    "section": "Comparing intervals",
    "text": "Comparing intervals"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#extrapolation",
    "href": "slides/09-slr-math-models-2.html#extrapolation",
    "title": "SLR: Math Models Continued",
    "section": "Extrapolation",
    "text": "Extrapolation\nUsing the model to predict for values outside the range of the original data is extrapolation.\n\n\n\n\nCalculate the prediction interval for the number of organic options in an extremely wealthy neighborhood with an average household income of $500k.\n\nNo, thanks!"
  },
  {
    "objectID": "slides/09-slr-math-models-2.html#extrapolation-1",
    "href": "slides/09-slr-math-models-2.html#extrapolation-1",
    "title": "SLR: Math Models Continued",
    "section": "Extrapolation",
    "text": "Extrapolation\n\nWhy do we want to avoid extrapolation?\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "ae/ae-05-math-models.html",
    "href": "ae/ae-05-math-models.html",
    "title": "AE 05: Mathematical Models",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-05”\nGo to the Canvas and locate your AE 05 assignment to get started.\nUpload the ae-05.qmd and spotify-popular.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas no later than Saturday, September 14 at 11:59pm.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-05-math-models.html#data",
    "href": "ae/ae-05-math-models.html#data",
    "title": "AE 05: Mathematical Models",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is a subset from the Spotify Songs Tidy Tuesday data set. The data were originally obtained from Spotify using the spotifyr R package.\nIt contains numerous characteristics for each song. You can see the full list of variables and definitions here. This analysis will focus specifically on the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\n\n\nspotify &lt;- read_csv(\"data/spotify-popular.csv\")\n\nAre high energy songs more positive? To answer this question, we’ll analyze data on some of the most popular songs on Spotify, i.e. those with track_popularity &gt;= 80. We’ll use linear regression to fit a model to predict a song’s positiveness (valence) based on its energy level (energy).\nBelow are plots as part of the exploratory data analysis.\n\np1 &lt;- gf_histogram(~valence, data = spotify) |&gt;  \n  gf_labs(title = \"Distribution of Valence\", \n       subtitle = \" for Popular songs on Spotify\", \n       x = \"Valence\")\n\np2 &lt;- gf_histogram(~energy, data = spotify) |&gt; \n  gf_labs(title = \"Distribution of Energy\", \n       subtitle = \"for Popular songs on Spotify\", \n       x = \"Energy\")\np1 + p2 # The patchwork package will arrange your plots for you\n\n\n\n\n\n\n\n\n\ngf_point(valence ~ energy, data = spotify) |&gt; \n  gf_labs(title = \"Valence vs. Energy\", \n       subtitle = \"Popular songs on Spotify\", \n       x = \"Energy\", \n       y = \"Valence\")"
  },
  {
    "objectID": "ae/ae-05-math-models.html#exercise-1",
    "href": "ae/ae-05-math-models.html#exercise-1",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model using the energy of a song to predict its valence, i.e. positiveness. Include the 90% confidence interval for the coefficients, and display the output using 3 digits.\n\n## add code"
  },
  {
    "objectID": "ae/ae-05-math-models.html#exercise-2",
    "href": "ae/ae-05-math-models.html#exercise-2",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 2",
    "text": "Exercise 2\nIn words interpret the estimate and confidence interval for the slope in the previous exercise."
  },
  {
    "objectID": "ae/ae-05-math-models.html#exercise-3",
    "href": "ae/ae-05-math-models.html#exercise-3",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 3",
    "text": "Exercise 3\nInterpret the p-value from Exercise 1."
  },
  {
    "objectID": "ae/forStudents/ae-06/ae-06-conditions.html",
    "href": "ae/forStudents/ae-06/ae-06-conditions.html",
    "title": "AE 05: Mathematical Models",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-05”\nGo to the Canvas and locate your AE 05 assignment to get started.\nUpload the ae-05.qmd and spotify-popular.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas no later than Monday, September 16 at 11:59pm.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/forStudents/ae-06/ae-06-conditions.html#data",
    "href": "ae/forStudents/ae-06/ae-06-conditions.html#data",
    "title": "AE 05: Mathematical Models",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is a subset from the Spotify Songs Tidy Tuesday data set. The data were originally obtained from Spotify using the spotifyr R package.\nIt contains numerous characteristics for each song. You can see the full list of variables and definitions here. This analysis will focus specifically on the following variables:\n\n\n\n\n\n\n\n\nvariable\nclass\ndescription\n\n\n\n\ntrack_id\ncharacter\nSong unique ID\n\n\ntrack_name\ncharacter\nSong Name\n\n\ntrack_artist\ncharacter\nSong Artist\n\n\ntrack_popularity\ndouble\nSong Popularity (0-100) where higher is better\n\n\nenergy\ndouble\nEnergy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy.\n\n\nvalence\ndouble\nA measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).\n\n\n\n\nspotify &lt;- read_csv(\"spotify-popular.csv\")\n\nAre high energy songs more positive? To answer this question, we’ll analyze data on some of the most popular songs on Spotify, i.e. those with track_popularity &gt;= 80. We’ll use linear regression to fit a model to predict a song’s positiveness (valence) based on its energy level (energy).\nBelow are plots as part of the exploratory data analysis.\n\np1 &lt;- gf_histogram(~valence, data = spotify) |&gt;  \n  gf_labs(title = \"Distribution of Valence\", \n       subtitle = \" for Popular songs on Spotify\", \n       x = \"Valence\")\n\np2 &lt;- gf_histogram(~energy, data = spotify) |&gt; \n  gf_labs(title = \"Distribution of Energy\", \n       subtitle = \"for Popular songs on Spotify\", \n       x = \"Energy\")\np1 + p2 # The patchwork package will arrange your plots for you\n\n\n\n\n\n\n\n\n\ngf_point(valence ~ energy, data = spotify) |&gt; \n  gf_labs(title = \"Valence vs. Energy\", \n       subtitle = \"Popular songs on Spotify\", \n       x = \"Energy\", \n       y = \"Valence\")"
  },
  {
    "objectID": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-1",
    "href": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-1",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a model using the energy of a song to predict its valence, i.e. positiveness. Include the 90% confidence interval for the coefficients, and display the output using 3 digits.\n\n## add code"
  },
  {
    "objectID": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-2",
    "href": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-2",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet’s check the model conditions before doing any inference. Fill in the code below to use the augment() function to create a new data frame containing the residuals and fitted values (among other information)\n\n\n\n\n\n\nImportant\n\n\n\nNote: Remove #|eval: false from the code chunk after you have filled in the code.\n\n\n\nspotify_aug &lt;- augment(_____)"
  },
  {
    "objectID": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-3",
    "href": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-3",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 3",
    "text": "Exercise 3\nMake a plot of the residual vs. fitted values.\n\n# add code here"
  },
  {
    "objectID": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-4",
    "href": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-4",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 4",
    "text": "Exercise 4\nFill in the code to make a histogram of the residuals and a normal QQ-plot.\n\nresid_hist &lt;- gf_histogram(~____, data = ____)) |&gt; \n  gf_labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_qq &lt;- gf_qq(~____, data = ____)  |&gt;\n  gf_qqline() |&gt; \n  gf_labs(x = \"_____\", \n       y = \"_____\", \n       title = \"____\")\n\nresid_hist + resid_qq"
  },
  {
    "objectID": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-5",
    "href": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-5",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 5",
    "text": "Exercise 5\nAssess the four model conditions. Use the plots from the previous exercises to help make the assessment.\n\nLinearity\nConstant variance\nNormality\nIndependence"
  },
  {
    "objectID": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-6",
    "href": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-6",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 6",
    "text": "Exercise 6\nCalculate \\(R^2\\) and interpret this value in the context of the data."
  },
  {
    "objectID": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-7",
    "href": "ae/forStudents/ae-06/ae-06-conditions.html#exercise-7",
    "title": "AE 05: Mathematical Models",
    "section": "Exercise 7",
    "text": "Exercise 7\nCalculate RMSE and interpret this value in the context of the data.\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-06 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "ae/ae-05-math-models.html#exericse-4",
    "href": "ae/ae-05-math-models.html#exericse-4",
    "title": "AE 05: Mathematical Models",
    "section": "Exericse 4",
    "text": "Exericse 4\nPredict what the average valence for a song with an energy score 0.5 is. Report and interpret a 90% confidence interval for the average valence."
  },
  {
    "objectID": "ae/ae-05-math-models.html#exericse-5",
    "href": "ae/ae-05-math-models.html#exericse-5",
    "title": "AE 05: Mathematical Models",
    "section": "Exericse 5",
    "text": "Exericse 5\nReport and interpret a 90% confidence interval for a single song with energy score 0.8.\n\n\n\n\n\n\nImportant\n\n\n\nTo submit the AE:\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your qmd and pdf files to the Canvas assignment."
  },
  {
    "objectID": "labs/ForStudents/hw-02/hw-02.html",
    "href": "labs/ForStudents/hw-02/hw-02.html",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "",
    "text": "Important\n\n\n\nDue:\n\nWednesday, September"
  },
  {
    "objectID": "labs/ForStudents/hw-02/hw-02.html#learning-goals",
    "href": "labs/ForStudents/hw-02/hw-02.html#learning-goals",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the lab you will…\n\nBe able to fit a simple linear regression model using R.\nBe able to interpret the slope and intercept for the model.\nBe able to use simulation-based inference to draw conclusions about the slope.\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "labs/ForStudents/hw-02/hw-02.html#exercise-10",
    "href": "labs/ForStudents/hw-02/hw-02.html#exercise-10",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nThere is a statistically significant linear relationship between average air temperature and average ice duration on Lake Monona \\((\\beta_1 \\neq 0)\\).\n\nDoes the confidence interval you calculated in the previous exercise support or refute this claim? Briefly explain.\n\nRender, commit, and push your final changes to your remote lab repo on GitHub. Click the checkbox next to each file in the Git pane to stage the updates you’ve made, write an informative commit message, and push. After you push the changes, the Git pane in RStudio should be empty."
  },
  {
    "objectID": "labs/ForStudents/hw-02/hw-02.html#footnotes",
    "href": "labs/ForStudents/hw-02/hw-02.html#footnotes",
    "title": "HW 02: Ice duration and air temperature in Madison, WI",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://lter.github.io/lterdatasampler/index.html↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎"
  },
  {
    "objectID": "ae/ae-06-conditions.html#introduction",
    "href": "ae/ae-06-conditions.html#introduction",
    "title": "AE 06: Model Conditions",
    "section": "Introduction",
    "text": "Introduction\nThis is a continuation of AE-05. The Data section below is the same as in that exercise. Feel free to skip it if you feel you remember everything about the data set or simply use it as a reference when needed."
  },
  {
    "objectID": "ae/ae-06-conditions.html#exercise-0",
    "href": "ae/ae-06-conditions.html#exercise-0",
    "title": "AE 06: Model Conditions",
    "section": "Exercise 0",
    "text": "Exercise 0\nFit a model using the energy of a song to predict its valence.\n\n## add code"
  },
  {
    "objectID": "hw/hw-03.html#learning-goals",
    "href": "hw/hw-03.html#learning-goals",
    "title": "Homework 03: Coffee grades",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the homework you will…\n\nbe able to use mathematical models to conduct inference for the slope\nbe able to assess conditions for simple linear regression"
  },
  {
    "objectID": "hw/hw-03.html#grading-50-pts",
    "href": "hw/hw-03.html#grading-50-pts",
    "title": "Homework 03: Coffee grades",
    "section": "Grading (50 pts)",
    "text": "Grading (50 pts)\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n4\n\n\nEx 2\n3\n\n\nEx 3\n4\n\n\nEx 4\n3\n\n\nEx 5\n8\n\n\nEx 6\n5\n\n\nEx 7\n8\n\n\nEx 8\n6\n\n\nEx 9\n4\n\n\nEx 10\n2\n\n\nWorkflow & formatting\n31"
  },
  {
    "objectID": "hw/hw-03-old.html",
    "href": "hw/hw-03-old.html",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "",
    "text": "In this analysis you will use multiple linear regression to fit and evaluate models using characteristics of LEGO sets to understand variability in the price."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-1",
    "href": "hw/hw-03-old.html#exercise-1",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 1",
    "text": "Exercise 1\nIn this analysis, we dropped observations that have missing values for some of the relevant variables. What is a disadvantage of dropping observations that have missing values, instead of using a method to impute, i.e., fill in, the missing data? How might dropping these observations impact the generalizability of conclusions? What is the disadvantage of replacing the NAs in Minifigures to 0?"
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-2",
    "href": "hw/hw-03-old.html#exercise-2",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 2",
    "text": "Exercise 2\nVisualize the distributions of the predictor variables Pieces, Size, Year, and Pages. Neatly arrange the plots using the patchwork package."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-3",
    "href": "hw/hw-03-old.html#exercise-3",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat are some feature engineering steps you might use to prepare the variables in the previous exercise for the model? Describe the steps and the function you would use. The list should incorporate at least three different step_ functions.\n\n\n\n\n\n\nTip\n\n\n\nUse the recipes reference page for a list of step_ functions."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-4",
    "href": "hw/hw-03-old.html#exercise-4",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 4",
    "text": "Exercise 4\nThe distribution of Theme is shown below. The bars are ordered by the frequency they occur in the data set.\n\nlegos |&gt;\n  count(Theme) |&gt;\nggplot(aes(x = fct_reorder(Theme, n), y = n)) +\n  geom_col() + \n    labs(title = \"Lego Set Theme\", \n         x = \"Theme\", \n         y = \"Number of LEGO sets\") + \n  coord_flip()\n\n\n\n\n\n\n\n\nWhat is one reason we should avoid putting the variable Theme in a model as is?"
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-5",
    "href": "hw/hw-03-old.html#exercise-5",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 5",
    "text": "Exercise 5\nWe will use v-fold cross validation to compare two models. We’ll start by preparing the data, creating the folds, and defining the model specification that will be used for both models.\n\nSplit the data into training (75%) and testing (25%) sets. Use a seed of 6.\nSplit the training data into 12 folds. Set the seed to 6 again.\nSpecify the model."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-6",
    "href": "hw/hw-03-old.html#exercise-6",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 6",
    "text": "Exercise 6\nUse the training data to create a recipe for the first model. The model uses the variables Size, Theme, and Pages to predict Amazon_Price. Conduct the following feature engineering steps:\n\nUse step_other() to collapse Theme into fewer categories. Define the threshold such that any levels of Theme with fewer than 20 observations is defined as “Other”.\nMean-center Pages.\nMake dummy variables for all categorical predictors.\nRemove any predictors with zero variance.\n\nThen create the workflow that brings together this recipe and the model specification from the previous exercise."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-7",
    "href": "hw/hw-03-old.html#exercise-7",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 7",
    "text": "Exercise 7\nConduct 12-fold cross validation using the workflow from the previous exercise. Calculate and display mean RMSE across the 12 folds.\n\n\n\n\n\n\nNote\n\n\n\nWe will just use RMSE to compare models for this assignment; however, in practice, it is best to take into account multiple model fit statistics to get a more holistic evaluation and comparison of the models."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-8",
    "href": "hw/hw-03-old.html#exercise-8",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 8",
    "text": "Exercise 8\nNow let’s consider a new model that includes all the variables used in model from Exercise 6 along with Year, Pieces and Minifigures.\n\nUse the training data to create a recipe that uses all the feature engineering steps in Exercise 6 with the addition of the following steps:\n\nCreate a new variable called since2018 that calculates the number of years since 2018.\nRemove Year as a potential predictor.\nMean-center Pieces.\n\nCreate the workflow that brings together this recipe and the model specification from Exercise 5.\nConduct 12-fold cross validation using this model workflow. Calculate and display mean RMSE across the 12 folds."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-9",
    "href": "hw/hw-03-old.html#exercise-9",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 9",
    "text": "Exercise 9\nCompare the cross validation results from Exercises 7 and 8. Which model do you select based on RMSE? Briefly explain your choice."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-10",
    "href": "hw/hw-03-old.html#exercise-10",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 10",
    "text": "Exercise 10\nRefit the selected model on the entire training data. Neatly display the model using 3 digits.\nThen, calculate VIF for the model and use it to comment on whether there are potential issues with multicollinearity.\n\n\n\n\n\n\nTip\n\n\n\nWhen we fit a model using recipe and workflow, we need to extract the model object before using augment or vif functions. Fill in the name of the selected model in both blanks in the code below to extract the model object and calculate VIF\n\nlegos_fit_model &lt;- extract_fit_parsnip(______)\nvif(legos_fit_model$fit)"
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-11",
    "href": "hw/hw-03-old.html#exercise-11",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 11",
    "text": "Exercise 11\nCalculate RMSE on the training data and on the testing data. Use it to comment on how well the model performs on new data and whether there are signs of model overfit."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-12",
    "href": "hw/hw-03-old.html#exercise-12",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 12",
    "text": "Exercise 12\nThough we do not check the model conditions in this assignment, complete this exercise assuming the model conditions are met.\nDescribe the effect of Theme on the price of LEGO sets, including an indication of which levels are statistically significant. Use a threshold of 0.05 to determine significance.\n\nBefore submitting, make sure you render your document and commit (with a meaningful commit message) and push all updates."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-13",
    "href": "hw/hw-03-old.html#exercise-13",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 13",
    "text": "Exercise 13\nThe World Bank collects “world development indicators” about the past and current development of countries. These data are made available on the World Bank’s website. It can be used to understand the relationships between these various factors and trends over time.\n\nFor this analysis, we focus on indicators from 2011 on 165 countries. The variables of interest are:\n\ngdp.per.capita: gross domestic product divided by midyear population. GDP is the sum of gross value added by all resident producers in the economy plus any product taxes and minus any subsidies not included in the value of the products. It is calculated without making deductions for depreciation of fabricated assets or for depletion and degradation of natural resources. Data are in current U.S. dollars.\nsanit.access.factor: Population access to sanitation facilities (Low, High)\nedu.expend: Government expenditure on education, total (% of government expenditure)\nlife.expect: Life expectancy at birth (in years)\n\nYou fit a model using sanitation access, education expenditures, and life expectancy to understand variability in GDP. The model takes the form\n\\[\n\\begin{aligned}\\widehat{\\log(GDP)} = \\hat{\\beta}_0 &+ \\hat{\\beta}_1 ~ sanit.access.factor + \\hat{\\beta}_2 ~ edu.expend + \\hat{\\beta}_3 ~life.expect \\\\ &+ \\hat{\\beta}_4 ~ sanit.access.factor \\times life.expect\\end{aligned}\n\\]\nThe output from R is shown below.\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.491\n1.638\n2.742\n0.007\n\n\nsanit.access.factorhigh\n-6.993\n1.971\n-3.548\n0.001\n\n\nedu.expend\n0.097\n0.038\n2.550\n0.012\n\n\nlife.expect\n0.030\n0.029\n1.061\n0.291\n\n\nsanit.access.factorhigh:life.expect\n0.122\n0.032\n3.853\n0.000\n\n\n\n\n\n\nInterpret the coefficient of edu.expend in the context of the data.\nInterpret the coefficient of life.expect for countries with high sanitation access in the context of the data.\n\n\n\n\n\n\nImportant\n\n\n\nWrite all interpretations in terms of the original units, not the log-transformed units."
  },
  {
    "objectID": "hw/hw-03-old.html#exercise-14",
    "href": "hw/hw-03-old.html#exercise-14",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Exercise 14",
    "text": "Exercise 14\nBelow are plots from the exploratory data analysis of the relationships between the predictor variables. Based on these plots, what appears to be a potential issue with the model from Exercise 13? Briefly explain your response."
  },
  {
    "objectID": "hw/hw-03-old.html#footnotes",
    "href": "hw/hw-03-old.html#footnotes",
    "title": "HW 03: Multiple linear regression, Part 2",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎↩︎"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#model-conditions-1",
    "href": "slides/11-slr-model-eval.html#model-conditions-1",
    "title": "SLR: Model evaluation",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#computational-set-up",
    "href": "slides/11-slr-model-eval.html#computational-set-up",
    "title": "SLR: Model evaluation",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(ggformula)   # for plotting using formulas\nlibrary(broom)       # for formatting model output\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# HEB Dataset\nheb &lt;- read_csv(\"data/HEBIncome.csv\") |&gt; \n  mutate(Avg_Income_K = Avg_Household_Income/1000)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#regression-model-revisited",
    "href": "slides/11-slr-model-eval.html#regression-model-revisited",
    "title": "SLR: Model evaluation",
    "section": "Regression model, revisited",
    "text": "Regression model, revisited\n\nheb_fit &lt;- lm(Number_Organic ~ Avg_Income_K, data = heb)\n\ntidy(heb_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-14.719\n9.298\n-1.583\n0.122\n\n\nAvg_Income_K\n0.959\n0.128\n7.505\n0.000"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#partitioning-variability",
    "href": "slides/11-slr-model-eval.html#partitioning-variability",
    "title": "SLR: Model evaluation",
    "section": "Partitioning Variability",
    "text": "Partitioning Variability\nLet’s think about variation:\n\n\nDATA = MODEL + ERROR\n\\(\\substack{\\text{Variation} \\\\ \\text{in Y}} = \\substack{\\text{Variation explained} \\\\ \\text{by model}} + \\substack{\\text{Variation not explained} \\\\ \\text{by model}}\\)"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#partitioning-variability-anova",
    "href": "slides/11-slr-model-eval.html#partitioning-variability-anova",
    "title": "SLR: Model evaluation",
    "section": "Partitioning Variability (ANOVA)",
    "text": "Partitioning Variability (ANOVA)\n\n\n\\(y_i - \\bar{y} = (\\hat{y}_i - \\bar{y}) + (y_i-\\hat{y}_i)\\)\nSquare and sum: \\(\\sum(y_i-\\bar{y})^2 = \\sum(\\hat{y} - \\bar{y})^2 + \\sum(y-\\hat{y})^2\\)\n\\(\\substack{\\text{Sum of squares} \\\\ \\text{Total}} = \\substack{\\text{Sum of squares} \\\\ \\text{model}} + \\substack{\\text{Sum of squares} \\\\ \\text{error}}\\)\n\\(SSTotal = SSModel + SSE\\)\n\\(SST = SSM + SSE\\)"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#anova-in-r",
    "href": "slides/11-slr-model-eval.html#anova-in-r",
    "title": "SLR: Model evaluation",
    "section": "ANOVA in R",
    "text": "ANOVA in R\n\nheb_fit |&gt; \n  anova() |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nAvg_Income_K\n1\n17175.06\n17175.0595\n56.32026\n0\n\n\nResiduals\n35\n10673.37\n304.9535\nNA\nNA\n\n\n\n\n\n\n\nMore on this later in the semester"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#recall-correlation-coefficient",
    "href": "slides/11-slr-model-eval.html#recall-correlation-coefficient",
    "title": "SLR: Model evaluation",
    "section": "Recall: Correlation Coefficient",
    "text": "Recall: Correlation Coefficient\n\nThe correlation coefficient, \\(r\\), is a number between -1 and +1 that measures how strong the linear relationship between two variables \\(x\\) and \\(y\\) is.\n\n\\[\nr = \\frac{\\sum(x_i - \\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum(x_i-\\bar{x})^2\\sum(y_i-\\bar{y})^2}}\n= \\frac{\\sum(x_i - \\bar{x})(y_i-\\bar{y})}{s_xs_y}\n\\]"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#two-statistics-r2",
    "href": "slides/11-slr-model-eval.html#two-statistics-r2",
    "title": "SLR: Model evaluation",
    "section": "Two statistics: \\(R^2\\)",
    "text": "Two statistics: \\(R^2\\)\n\n\nR-squared, \\(R^2\\), Coefficient of Determination : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor) \\[\nR^2 = Cor(y, \\hat{y})^2\n\\]\n\nAlso called PRE (Percent Reduction in Error) because: \\[\nR^2 = \\frac{SSModel}{SSTotal}\n\\]"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#two-statistics-rmse",
    "href": "slides/11-slr-model-eval.html#two-statistics-rmse",
    "title": "SLR: Model evaluation",
    "section": "Two statistics: RMSE",
    "text": "Two statistics: RMSE\n\nRoot mean square error, RMSE: A measure of the average error (average difference between observed and predicted values of the outcome) \\[\nRMSE = \\sqrt{\\frac{\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2}{n}}\n\\]\n\nSometimes people just case about numerator (SSE) or version without the square-root (MSE)\nSometimes the denominator may have \\(n-1\\) instead\n\n\n\n\nWhat indicates a good model fit? Higher or lower \\(R^2\\)? Higher or lower RMSE?"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#r2",
    "href": "slides/11-slr-model-eval.html#r2",
    "title": "SLR: Model evaluation",
    "section": "\\(R^2\\)",
    "text": "\\(R^2\\)\n\n\nRanges between 0 (terrible predictor) and 1 (perfect predictor)\nHas no units\nCalculate with rsq() from yardstick package using the augmented data:\n\n\nlibrary(yardstick)\nheb_aug &lt;- augment(heb_fit)\n\nrsq(heb_aug, truth = Number_Organic, estimate = .fitted) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.6167334"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#interpreting-r2",
    "href": "slides/11-slr-model-eval.html#interpreting-r2",
    "title": "SLR: Model evaluation",
    "section": "Interpreting \\(R^2\\)",
    "text": "Interpreting \\(R^2\\)\n\n🗳️ Discussion\n\n\nThe \\(R^2\\) of the model for Number_Organic from Average_Income_K is 61.7%. Which of the following is the correct interpretation of this value?\n\n\nAvg_Income_K correctly predicts 61.7% of Number_Organic in San Antontio HEBs.\n61.7% of the variability in Number_Organic can be explained by Avg_Income_K.\n61.7% of the variability in Avg_Income_K can be explained by Number_Organic.\n61.7% of the time Number_Organic can be predicted by Avg_Income_K."
  },
  {
    "objectID": "slides/11-slr-model-eval.html#activity",
    "href": "slides/11-slr-model-eval.html#activity",
    "title": "SLR: Model evaluation",
    "section": "Activity",
    "text": "Activity\n\nIn groups, at the board, design a simulation-based procedure for producing a p-value for the following hypothesis test.\n\n\\(H_0: R^2 = 0\\)\n\\(H_A: R^2 \\neq 0\\)"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#rmse",
    "href": "slides/11-slr-model-eval.html#rmse",
    "title": "SLR: Model evaluation",
    "section": "RMSE",
    "text": "RMSE\n\n\nRanges between 0 (perfect predictor) and infinity (terrible predictor)\nSame units as the response variable\nCalculate with rmse() from yardstick package using the augmented data:\n\n\nrmse(heb_aug, truth = Number_Organic, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rmse    standard        17.0\n\n\n\nThe value of RMSE is not very meaningful on its own, but it’s useful for comparing across models (more on this and ANOVA when we get to regression with multiple predictors)\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/11-slr-model-eval.html#warm-up-comparing-inferential-methods",
    "href": "slides/11-slr-model-eval.html#warm-up-comparing-inferential-methods",
    "title": "SLR: Model evaluation",
    "section": "Warm-Up: Comparing inferential methods",
    "text": "Warm-Up: Comparing inferential methods\n\n\nWhat are the advantages of using simulation-based inference methods? What are the advantages of using inference methods based on mathematical models?\nUnder what scenario(s) would you prefer to use simulation-based methods? Under what scenario(s) would you prefer to use methods based on mathematical models?\n\n\n\n\n\n−+\n02:00"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#computational-set-up",
    "href": "slides/12-slr-transformations-outliers.html#computational-set-up",
    "title": "SLR: Transformations + Outliers",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(ggformula)   # for creating plots using formulas\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(moderndive)  # for house_price dataset\nlibrary(fivethirtyeight) # for fandango dataset\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#data-house_prices",
    "href": "slides/12-slr-transformations-outliers.html#data-house_prices",
    "title": "SLR: Transformations + Outliers",
    "section": "Data: house_prices",
    "text": "Data: house_prices\n\n\nContains house sale prices for King County, which includes Seattle, from homes sold between May 2014 and May 2015\nObtained from Kaggle.com\n\n\n\nglimpse(house_prices)\n\nRows: 21,613\nColumns: 21\n$ id            &lt;chr&gt; \"7129300520\", \"6414100192\", \"5631500400\", \"2487200875\", …\n$ date          &lt;date&gt; 2014-10-13, 2014-12-09, 2015-02-25, 2014-12-09, 2015-02…\n$ price         &lt;dbl&gt; 221900, 538000, 180000, 604000, 510000, 1225000, 257500,…\n$ bedrooms      &lt;int&gt; 3, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 5, 4, 3, 4, 2,…\n$ bathrooms     &lt;dbl&gt; 1.00, 2.25, 1.00, 3.00, 2.00, 4.50, 2.25, 1.50, 1.00, 2.…\n$ sqft_living   &lt;int&gt; 1180, 2570, 770, 1960, 1680, 5420, 1715, 1060, 1780, 189…\n$ sqft_lot      &lt;int&gt; 5650, 7242, 10000, 5000, 8080, 101930, 6819, 9711, 7470,…\n$ floors        &lt;dbl&gt; 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1…\n$ waterfront    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ view          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,…\n$ condition     &lt;fct&gt; 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4,…\n$ grade         &lt;fct&gt; 7, 7, 6, 7, 8, 11, 7, 7, 7, 7, 8, 7, 7, 7, 7, 9, 7, 7, 7…\n$ sqft_above    &lt;int&gt; 1180, 2170, 770, 1050, 1680, 3890, 1715, 1060, 1050, 189…\n$ sqft_basement &lt;int&gt; 0, 400, 0, 910, 0, 1530, 0, 0, 730, 0, 1700, 300, 0, 0, …\n$ yr_built      &lt;int&gt; 1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 20…\n$ yr_renovated  &lt;int&gt; 0, 1991, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ zipcode       &lt;fct&gt; 98178, 98125, 98028, 98136, 98074, 98053, 98003, 98198, …\n$ lat           &lt;dbl&gt; 47.5112, 47.7210, 47.7379, 47.5208, 47.6168, 47.6561, 47…\n$ long          &lt;dbl&gt; -122.257, -122.319, -122.233, -122.393, -122.045, -122.0…\n$ sqft_living15 &lt;int&gt; 1340, 1690, 2720, 1360, 1800, 4760, 2238, 1650, 1780, 23…\n$ sqft_lot15    &lt;int&gt; 5650, 7639, 8062, 5000, 7503, 101930, 6819, 9711, 8113, …"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#variables",
    "href": "slides/12-slr-transformations-outliers.html#variables",
    "title": "SLR: Transformations + Outliers",
    "section": "Variables",
    "text": "Variables\n\nOutcome\n\nprice: the sale price in dollars\n\nPredictor\n\nsqft_living: the square footage of the home"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#recap-fit-the-model",
    "href": "slides/12-slr-transformations-outliers.html#recap-fit-the-model",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\nhp_fit &lt;- lm(price ~ sqft_living, data = house_prices)\ntidy(hp_fit) |&gt;  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-43580.74\n4402.69\n-9.90\n0\n\n\nsqft_living\n280.62\n1.94\n144.92\n0\n\n\n\n\n\n\n\nWrite down the model:\n\nModel: \\(\\hat{\\text{price}} = -43580.74 + 280.62\\times\\text{sqft_living}\\)\n\nInterpret the slope and intercept in the context of this problem:\n\nInterpretation: If the square footage of the house increases by 1, the price increases by and average of $280.62 and a (theoretical) house with 0 square feet with cost -$43,580.74."
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#recap-fit-the-model-1",
    "href": "slides/12-slr-transformations-outliers.html#recap-fit-the-model-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\n\nCode\ngf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#recap-model-conditions",
    "href": "slides/12-slr-transformations-outliers.html#recap-model-conditions",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Model conditions",
    "text": "Recap: Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other\n\n\n\nHow should we check these assumptions?"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#recap-residual-histogram",
    "href": "slides/12-slr-transformations-outliers.html#recap-residual-histogram",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Residual Histogram",
    "text": "Recap: Residual Histogram\n\n\nCode\nhp_aug &lt;- augment(hp_fit)\n\ngf_histogram(~.resid, data = hp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Residual Histogram\")"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#recap-qq-plot-of-residuals",
    "href": "slides/12-slr-transformations-outliers.html#recap-qq-plot-of-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: QQ-Plot of Residuals",
    "text": "Recap: QQ-Plot of Residuals\n\n\nCode\ngf_qq(~.resid, data = hp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#recap-residuals-vs.-fitted-values",
    "href": "slides/12-slr-transformations-outliers.html#recap-residuals-vs.-fitted-values",
    "title": "SLR: Transformations + Outliers",
    "section": "Recap: Residuals vs. Fitted Values",
    "text": "Recap: Residuals vs. Fitted Values\n\n\nCode\ngf_point(.resid ~ .fitted, data = hp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#are-model-conditions-satisfied",
    "href": "slides/12-slr-transformations-outliers.html#are-model-conditions-satisfied",
    "title": "SLR: Transformations + Outliers",
    "section": "Are model conditions satisfied?",
    "text": "Are model conditions satisfied?\n\nLinearity: ❓\nConstant variance: ❌\nNormality: ❌\nIndependence: ✅"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#what-to-do-when-regression-conditions-are-violated",
    "href": "slides/12-slr-transformations-outliers.html#what-to-do-when-regression-conditions-are-violated",
    "title": "SLR: Transformations + Outliers",
    "section": "What to do when regression conditions are violated",
    "text": "What to do when regression conditions are violated\nExamples:\n\nLack of normality in residuals\nPatterns in residuals\nHeteroscedasticity (nonconstant variance)\nOutliers: influential points, large residuals"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#data-transformations",
    "href": "slides/12-slr-transformations-outliers.html#data-transformations",
    "title": "SLR: Transformations + Outliers",
    "section": "Data Transformations",
    "text": "Data Transformations\nCan be used to:\n\nAddress nonlinear patterns\nStabilize variance\nRemove skewness from residuals\nMinimize effects of outliers"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#common-transformations",
    "href": "slides/12-slr-transformations-outliers.html#common-transformations",
    "title": "SLR: Transformations + Outliers",
    "section": "Common Transformations",
    "text": "Common Transformations\nFor either the response \\(Y\\) or predictor \\(X\\):\n\nLogarithm \\(Z \\to \\log(Z)\\)\n\nNote: “log” means “log base \\(e\\)”\n\nSquare Root \\(Z \\to \\sqrt{Z}\\)\nExponential \\(Z \\to e^Z\\)\nPower functions \\(Z \\to Z^2, Z^3, Z^4, \\ldots\\)\nReciprocal \\(Z \\to 1/Z\\)"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-planets",
    "href": "slides/12-slr-transformations-outliers.html#example-planets",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nplanets &lt;- read_csv(\"data/planets.csv\")\n\n\n\\(Y =\\) length of the “year” for planets\n\\(X =\\) distance from the Sun"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-planets-1",
    "href": "slides/12-slr-transformations-outliers.html#example-planets-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Distance ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-planets-2",
    "href": "slides/12-slr-transformations-outliers.html#example-planets-2",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(log(Distance) ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-planets-3",
    "href": "slides/12-slr-transformations-outliers.html#example-planets-3",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(Distance ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-planets-4",
    "href": "slides/12-slr-transformations-outliers.html#example-planets-4",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Distance) ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-mammal-species",
    "href": "slides/12-slr-transformations-outliers.html#example-mammal-species",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nSpeciesArea &lt;- read_csv(\"data/SpeciesArea.csv\")\n\n\n\\(Y =\\) number of mammal species on an island\n\\(X =\\) area of the island"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-mammal-species-1",
    "href": "slides/12-slr-transformations-outliers.html#example-mammal-species-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Species ~ Area, data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-mammal-species-2",
    "href": "slides/12-slr-transformations-outliers.html#example-mammal-species-2",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Species) ~ log(Area), data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#why-a-log-transformation",
    "href": "slides/12-slr-transformations-outliers.html#why-a-log-transformation",
    "title": "SLR: Transformations + Outliers",
    "section": "Why a Log Transformation?",
    "text": "Why a Log Transformation?\n\nSome relationship are multiplicative (not linear)\n\nExample: Area of a circle\n\\[\n\\begin{aligned}\nA &= \\pi r^2 \\text{ (not linear)}\\\\\n\\log(A) &= \\log(\\pi r^2)\n= \\log(\\pi) + 2\\log(r)\\\\\n\\log(A) &= \\beta_0 + \\beta_1\\times \\log(r)\\\\\n\\implies & \\log(A) \\text{ is a linear function of } \\log(r)\n\\end{aligned}\n\\]\nLook for:\n\nStrongly right-skewed distributions\nCurvative in scatterplot\nIncreasing variability in scatterplot"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#back-to-house_sales",
    "href": "slides/12-slr-transformations-outliers.html#back-to-house_sales",
    "title": "SLR: Transformations + Outliers",
    "section": "Back to house_sales",
    "text": "Back to house_sales\n\n\nCode\np1 &lt;- gf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np2 &lt;- gf_point(log(price) ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np3 &lt;- gf_point(price ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np4 &lt;- gf_point(log(price) ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\n(p1 + p2)/ (p3 + p4)"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#fitting-transformed-models",
    "href": "slides/12-slr-transformations-outliers.html#fitting-transformed-models",
    "title": "SLR: Transformations + Outliers",
    "section": "Fitting Transformed Models",
    "text": "Fitting Transformed Models\n\n\n\nlogprice_model &lt;- lm(log(price) ~ sqft_living, data = house_prices)\ntidy(logprice_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.2184641\n0.0063741\n1916.8830\n0\n\n\nsqft_living\n0.0003987\n0.0000028\n142.2326\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &= 12.22  + 3.99\\times 10^{-4}\\times X\\\\\nY &= e^{12.22 + 3.99\\times 10^{-4}\\times X}\\\\\n&= 202805\\times e^{3.99\\times 10^{-4}\\times X}\n\\end{aligned}\n\\]\n\n\nloglog_model &lt;- lm(log(price) ~ log(sqft_living), data = house_prices)\ntidy(loglog_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.729916\n0.0470620\n143.0011\n0\n\n\nlog(sqft_living)\n0.836771\n0.0062233\n134.4587\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &=6.73 + 0.837\\times \\log(X)\\\\\n\\log(Y) &= \\log(e^{6.73})  + \\log(X^{0.837})\\\\\nY &= 873.15\\times X^{0.837}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#residuals-histograms",
    "href": "slides/12-slr-transformations-outliers.html#residuals-histograms",
    "title": "SLR: Transformations + Outliers",
    "section": "Residuals Histograms",
    "text": "Residuals Histograms\n\n\nCode\nlp_aug &lt;- augment(logprice_model)\nll_aug &lt;- augment(loglog_model)\n\np1 &lt;- gf_histogram(~.resid, data = lp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log Price Residuals\")\n\np2 &lt;- gf_histogram(~.resid, data = ll_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log-Log Residuals\")\n\n(p1 + p2)"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#qq-plots-of-residuals",
    "href": "slides/12-slr-transformations-outliers.html#qq-plots-of-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "QQ-Plots of Residuals",
    "text": "QQ-Plots of Residuals\n\n\nCode\np1 &lt;- gf_qq(~.resid, data = lp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log Price QQ\")\n\np2 &lt;- gf_qq(~.resid, data = ll_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log-Log QQ\")\n\np1 + p2"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#residuals-vs.-fitted-values",
    "href": "slides/12-slr-transformations-outliers.html#residuals-vs.-fitted-values",
    "title": "SLR: Transformations + Outliers",
    "section": "Residuals vs. Fitted Values",
    "text": "Residuals vs. Fitted Values\n\n\nCode\np1 &lt;- gf_point(.resid ~ .fitted, data = lp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log Price Model\"\n  )\n\np2 &lt;- gf_point(.resid ~ .fitted, data = ll_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log-Log Model\"\n  )\n\np1 + p2"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#types-of-unusual-points-in-slr",
    "href": "slides/12-slr-transformations-outliers.html#types-of-unusual-points-in-slr",
    "title": "SLR: Transformations + Outliers",
    "section": "Types of “Unusual” Points in SLR",
    "text": "Types of “Unusual” Points in SLR\n\nOutlier: a data point that is far from the regression line\nInfluential point: a data point that has a large effect on the regression fit\n\n\n\n\nHow do we measure “far”?\nHow do we measure “effect on the fit”?"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#detecting-unusual-cases-overview",
    "href": "slides/12-slr-transformations-outliers.html#detecting-unusual-cases-overview",
    "title": "SLR: Transformations + Outliers",
    "section": "Detecting Unusual Cases: Overview",
    "text": "Detecting Unusual Cases: Overview\n\nCompute residuals\n\n“raw”, standardized, studentized\n\nPlots of residuals\n\nboxplot, scatterplot, normal plot\n\nLeverage\n\nunusual values for the predictors"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-movie-scores",
    "href": "slides/12-slr-transformations-outliers.html#example-movie-scores",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie scores",
    "text": "Example: Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#data-prep",
    "href": "slides/12-slr-transformations-outliers.html#data-prep",
    "title": "SLR: Transformations + Outliers",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-movie-scores-1",
    "href": "slides/12-slr-transformations-outliers.html#example-movie-scores-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\n\nCode\nmovie_scores |&gt; \n  gf_point(audience ~ critics) |&gt; \n  gf_lm() |&gt; \n  gf_labs(x = \"Critics Score\", \n       y = \"Audience Score\")"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#boxplot-of-residuals",
    "href": "slides/12-slr-transformations-outliers.html#boxplot-of-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "Boxplot of Residuals",
    "text": "Boxplot of Residuals\n\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\nmovie_fit_aug &lt;- augment(movie_fit)\n\ngf_boxplot(.resid~\"\", data = movie_fit_aug, \n           fill = \"salmon\", ylab = \"Residuals\", xlab = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nDots (outliers) indicate data points more than 1.5 IQRs above (or below) quartiles"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#standardized-residuals",
    "href": "slides/12-slr-transformations-outliers.html#standardized-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\n\nRecall: Z-scores\nFact: If \\(X\\) has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then \\((X-\\mu)/\\sigma\\) has mean 0 and stanard deviation 1\nFor residuals: mean 0 and standard deviation \\(\\hat{\\sigma}_\\epsilon\\)\nStandardized residuals: \\(\\frac{y_i-\\hat{y}_i}{\\hat{\\sigma}_\\epsilon}\\)\n\nLook for values beyond \\(\\pm 2\\) or \\(\\pm 3\\)"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-movie-scores-2",
    "href": "slides/12-slr-transformations-outliers.html#example-movie-scores-2",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\nmovie_fit_aug |&gt;  # Augmented data\n  gf_point(.std.resid ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Standardized Residual\")"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#externally-studentized-residuals",
    "href": "slides/12-slr-transformations-outliers.html#externally-studentized-residuals",
    "title": "SLR: Transformations + Outliers",
    "section": "(Externally) Studentized Residuals",
    "text": "(Externally) Studentized Residuals\n\nConcern: An unusual value may exert great influence on the fit\n\nIts residual might be underestimated because the model “moves” a lot to fit it\nThe standard error may also be inflated due to the outlier error\n\nStudentize: Fit the model without that case, then find new \\(\\hat{\\sigma}_\\epsilon\\)"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#example-movie-scores-3",
    "href": "slides/12-slr-transformations-outliers.html#example-movie-scores-3",
    "title": "SLR: Transformations + Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\nmovie_fit_aug |&gt;  # Augmented data\n  mutate(studentized_residual = rstudent(movie_fit)) |&gt; \n  gf_point(.std.resid ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Studentized Residual\")"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#what-to-do-with-an-outlier",
    "href": "slides/12-slr-transformations-outliers.html#what-to-do-with-an-outlier",
    "title": "SLR: Transformations + Outliers",
    "section": "What to do with an outlier?",
    "text": "What to do with an outlier?\n\nLook into it\nIf something is unusual about it and you can make a case that it is not a good representation of the population you can throw it out\nIf not and the value is just unusual, keep it"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#influence-vs.-leverage",
    "href": "slides/12-slr-transformations-outliers.html#influence-vs.-leverage",
    "title": "SLR: Transformations + Outliers",
    "section": "Influence vs. Leverage",
    "text": "Influence vs. Leverage\n\nHigh Influence Point: point that DOES impact the regression line\nHigh Leverage Point: point with “potential” to impact regression line because \\(X\\)-value is unusual"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#high-leverage-low-influence",
    "href": "slides/12-slr-transformations-outliers.html#high-leverage-low-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "High Leverage, Low Influence",
    "text": "High Leverage, Low Influence"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#high-leverage-high-influence",
    "href": "slides/12-slr-transformations-outliers.html#high-leverage-high-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "High Leverage, High Influence",
    "text": "High Leverage, High Influence"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#low-leverage-low-influence",
    "href": "slides/12-slr-transformations-outliers.html#low-leverage-low-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "Low Leverage, Low Influence",
    "text": "Low Leverage, Low Influence"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#low-leverage-high-influence",
    "href": "slides/12-slr-transformations-outliers.html#low-leverage-high-influence",
    "title": "SLR: Transformations + Outliers",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence"
  },
  {
    "objectID": "slides/12-slr-transformations-outliers.html#low-leverage-high-influence-1",
    "href": "slides/12-slr-transformations-outliers.html#low-leverage-high-influence-1",
    "title": "SLR: Transformations + Outliers",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "prepare/transformations-outliers-prep.html",
    "href": "prepare/transformations-outliers-prep.html",
    "title": "September 18 lecture",
    "section": "",
    "text": "Stat2: Topic 1.4 Transformations/Reexpressions\nStat2: Topic 1.5 Outliers and Influential Points"
  },
  {
    "objectID": "prepare/transformations-outliers-prep.html#assigned-reading",
    "href": "prepare/transformations-outliers-prep.html#assigned-reading",
    "title": "September 18 lecture",
    "section": "",
    "text": "Stat2: Topic 1.4 Transformations/Reexpressions\nStat2: Topic 1.5 Outliers and Influential Points"
  },
  {
    "objectID": "slides/12-slr-transformations.html#computational-set-up",
    "href": "slides/12-slr-transformations.html#computational-set-up",
    "title": "SLR: Transformations",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(ggformula)   # for creating plots using formulas\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(moderndive)  # for house_price dataset\nlibrary(fivethirtyeight) # for fandango dataset\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/12-slr-transformations.html#data-house_prices",
    "href": "slides/12-slr-transformations.html#data-house_prices",
    "title": "SLR: Transformations",
    "section": "Data: house_prices",
    "text": "Data: house_prices\n\n\nContains house sale prices for King County, which includes Seattle, from homes sold between May 2014 and May 2015\nObtained from Kaggle.com\n\n\n\nglimpse(house_prices)\n\nRows: 21,613\nColumns: 21\n$ id            &lt;chr&gt; \"7129300520\", \"6414100192\", \"5631500400\", \"2487200875\", …\n$ date          &lt;date&gt; 2014-10-13, 2014-12-09, 2015-02-25, 2014-12-09, 2015-02…\n$ price         &lt;dbl&gt; 221900, 538000, 180000, 604000, 510000, 1225000, 257500,…\n$ bedrooms      &lt;int&gt; 3, 3, 2, 4, 3, 4, 3, 3, 3, 3, 3, 2, 3, 3, 5, 4, 3, 4, 2,…\n$ bathrooms     &lt;dbl&gt; 1.00, 2.25, 1.00, 3.00, 2.00, 4.50, 2.25, 1.50, 1.00, 2.…\n$ sqft_living   &lt;int&gt; 1180, 2570, 770, 1960, 1680, 5420, 1715, 1060, 1780, 189…\n$ sqft_lot      &lt;int&gt; 5650, 7242, 10000, 5000, 8080, 101930, 6819, 9711, 7470,…\n$ floors        &lt;dbl&gt; 1.0, 2.0, 1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1…\n$ waterfront    &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, …\n$ view          &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0,…\n$ condition     &lt;fct&gt; 3, 3, 3, 5, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4,…\n$ grade         &lt;fct&gt; 7, 7, 6, 7, 8, 11, 7, 7, 7, 7, 8, 7, 7, 7, 7, 9, 7, 7, 7…\n$ sqft_above    &lt;int&gt; 1180, 2170, 770, 1050, 1680, 3890, 1715, 1060, 1050, 189…\n$ sqft_basement &lt;int&gt; 0, 400, 0, 910, 0, 1530, 0, 0, 730, 0, 1700, 300, 0, 0, …\n$ yr_built      &lt;int&gt; 1955, 1951, 1933, 1965, 1987, 2001, 1995, 1963, 1960, 20…\n$ yr_renovated  &lt;int&gt; 0, 1991, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ zipcode       &lt;fct&gt; 98178, 98125, 98028, 98136, 98074, 98053, 98003, 98198, …\n$ lat           &lt;dbl&gt; 47.5112, 47.7210, 47.7379, 47.5208, 47.6168, 47.6561, 47…\n$ long          &lt;dbl&gt; -122.257, -122.319, -122.233, -122.393, -122.045, -122.0…\n$ sqft_living15 &lt;int&gt; 1340, 1690, 2720, 1360, 1800, 4760, 2238, 1650, 1780, 23…\n$ sqft_lot15    &lt;int&gt; 5650, 7639, 8062, 5000, 7503, 101930, 6819, 9711, 8113, …"
  },
  {
    "objectID": "slides/12-slr-transformations.html#variables",
    "href": "slides/12-slr-transformations.html#variables",
    "title": "SLR: Transformations",
    "section": "Variables",
    "text": "Variables\n\nOutcome\n\nprice: the sale price in dollars\n\nPredictor\n\nsqft_living: the square footage of the home"
  },
  {
    "objectID": "slides/12-slr-transformations.html#recap-fit-the-model",
    "href": "slides/12-slr-transformations.html#recap-fit-the-model",
    "title": "SLR: Transformations",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\nhp_fit &lt;- lm(price ~ sqft_living, data = house_prices)\ntidy(hp_fit) |&gt;  kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-43580.74\n4402.69\n-9.90\n0\n\n\nsqft_living\n280.62\n1.94\n144.92\n0\n\n\n\n\n\n\n\nWrite down the model:\n\nModel: \\(\\hat{\\text{price}} = -43580.74 + 280.62\\times\\text{sqft_living}\\)\n\nInterpret the slope and intercept in the context of this problem:\n\nInterpretation: If the square footage of the house increases by 1, the price increases by and average of $280.62 and a (theoretical) house with 0 square feet with cost -$43,580.74."
  },
  {
    "objectID": "slides/12-slr-transformations.html#recap-fit-the-model-1",
    "href": "slides/12-slr-transformations.html#recap-fit-the-model-1",
    "title": "SLR: Transformations",
    "section": "Recap: Fit the model",
    "text": "Recap: Fit the model\n\n\nCode\ngf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))"
  },
  {
    "objectID": "slides/12-slr-transformations.html#recap-model-conditions",
    "href": "slides/12-slr-transformations.html#recap-model-conditions",
    "title": "SLR: Transformations",
    "section": "Recap: Model conditions",
    "text": "Recap: Model conditions\n\nLinearity: There is a linear relationship between the outcome and predictor variables\nConstant variance: The variability of the errors is equal for all values of the predictor variable\nNormality: The errors follow a normal distribution\nIndependence: The errors are independent from each other\n\n\n\nHow should we check these assumptions?"
  },
  {
    "objectID": "slides/12-slr-transformations.html#recap-residual-histogram",
    "href": "slides/12-slr-transformations.html#recap-residual-histogram",
    "title": "SLR: Transformations",
    "section": "Recap: Residual Histogram",
    "text": "Recap: Residual Histogram\n\n\nCode\nhp_aug &lt;- augment(hp_fit)\n\ngf_histogram(~.resid, data = hp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Residual Histogram\")"
  },
  {
    "objectID": "slides/12-slr-transformations.html#recap-qq-plot-of-residuals",
    "href": "slides/12-slr-transformations.html#recap-qq-plot-of-residuals",
    "title": "SLR: Transformations",
    "section": "Recap: QQ-Plot of Residuals",
    "text": "Recap: QQ-Plot of Residuals\n\n\nCode\ngf_qq(~.resid, data = hp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Normal QQ-plot of residuals\")"
  },
  {
    "objectID": "slides/12-slr-transformations.html#recap-residuals-vs.-fitted-values",
    "href": "slides/12-slr-transformations.html#recap-residuals-vs.-fitted-values",
    "title": "SLR: Transformations",
    "section": "Recap: Residuals vs. Fitted Values",
    "text": "Recap: Residuals vs. Fitted Values\n\n\nCode\ngf_point(.resid ~ .fitted, data = hp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Residuals vs. fitted values\"\n  )"
  },
  {
    "objectID": "slides/12-slr-transformations.html#are-model-conditions-satisfied",
    "href": "slides/12-slr-transformations.html#are-model-conditions-satisfied",
    "title": "SLR: Transformations",
    "section": "Are model conditions satisfied?",
    "text": "Are model conditions satisfied?\n\nLinearity: ❓\nConstant variance: ❌\nNormality: ❌\nIndependence: ✅"
  },
  {
    "objectID": "slides/12-slr-transformations.html#what-to-do-when-regression-conditions-are-violated",
    "href": "slides/12-slr-transformations.html#what-to-do-when-regression-conditions-are-violated",
    "title": "SLR: Transformations",
    "section": "What to do when regression conditions are violated",
    "text": "What to do when regression conditions are violated\nExamples:\n\nLack of normality in residuals\nPatterns in residuals\nHeteroscedasticity (nonconstant variance)\nOutliers: influential points, large residuals"
  },
  {
    "objectID": "slides/12-slr-transformations.html#data-transformations",
    "href": "slides/12-slr-transformations.html#data-transformations",
    "title": "SLR: Transformations",
    "section": "Data Transformations",
    "text": "Data Transformations\nCan be used to:\n\nAddress nonlinear patterns\nStabilize variance\nRemove skewness from residuals\nMinimize effects of outliers"
  },
  {
    "objectID": "slides/12-slr-transformations.html#common-transformations",
    "href": "slides/12-slr-transformations.html#common-transformations",
    "title": "SLR: Transformations",
    "section": "Common Transformations",
    "text": "Common Transformations\nFor either the response \\(Y\\) or predictor \\(X\\):\n\nLogarithm \\(Z \\to \\log(Z)\\)\n\nNote: “log” means “log base \\(e\\)”\n\nSquare Root \\(Z \\to \\sqrt{Z}\\)\nExponential \\(Z \\to e^Z\\)\nPower functions \\(Z \\to Z^2, Z^3, Z^4, \\ldots\\)\nReciprocal \\(Z \\to 1/Z\\)"
  },
  {
    "objectID": "slides/12-slr-transformations.html#example-planets",
    "href": "slides/12-slr-transformations.html#example-planets",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nplanets &lt;- read_csv(\"data/planets.csv\")\n\n\n\\(Y =\\) length of the “year” for planets\n\\(X =\\) distance from the Sun"
  },
  {
    "objectID": "slides/12-slr-transformations.html#example-planets-1",
    "href": "slides/12-slr-transformations.html#example-planets-1",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Distance ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations.html#example-planets-2",
    "href": "slides/12-slr-transformations.html#example-planets-2",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(log(Distance) ~ Year, data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations.html#example-planets-3",
    "href": "slides/12-slr-transformations.html#example-planets-3",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(Distance ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations.html#example-planets-4",
    "href": "slides/12-slr-transformations.html#example-planets-4",
    "title": "SLR: Transformations",
    "section": "Example: Planets",
    "text": "Example: Planets\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Distance) ~ log(Year), data = planets) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations.html#example-mammal-species",
    "href": "slides/12-slr-transformations.html#example-mammal-species",
    "title": "SLR: Transformations",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nSpeciesArea &lt;- read_csv(\"data/SpeciesArea.csv\")\n\n\n\\(Y =\\) number of mammal species on an island\n\\(X =\\) area of the island"
  },
  {
    "objectID": "slides/12-slr-transformations.html#example-mammal-species-1",
    "href": "slides/12-slr-transformations.html#example-mammal-species-1",
    "title": "SLR: Transformations",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(Y = \\beta_0 + \\beta_1\\times X\\)\n\n\n\nCode\ngf_point(Species ~ Area, data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations.html#example-mammal-species-2",
    "href": "slides/12-slr-transformations.html#example-mammal-species-2",
    "title": "SLR: Transformations",
    "section": "Example: Mammal Species",
    "text": "Example: Mammal Species\n\nModel: \\(\\log(Y) = \\beta_0 + \\beta_1\\times \\log(X)\\)\n\n\n\nCode\ngf_point(log(Species) ~ log(Area), data = SpeciesArea) |&gt; \n  gf_lm()"
  },
  {
    "objectID": "slides/12-slr-transformations.html#why-a-log-transformation",
    "href": "slides/12-slr-transformations.html#why-a-log-transformation",
    "title": "SLR: Transformations",
    "section": "Why a Log Transformation?",
    "text": "Why a Log Transformation?\n\nSome relationship are multiplicative (not linear)\n\nExample: Area of a circle\n\\[\n\\begin{aligned}\nA &= \\pi r^2 \\text{ (not linear)}\\\\\n\\log(A) &= \\log(\\pi r^2)\n= \\log(\\pi) + 2\\log(r)\\\\\n\\log(A) &= \\beta_0 + \\beta_1\\times \\log(r)\\\\\n\\implies & \\log(A) \\text{ is a linear function of } \\log(r)\n\\end{aligned}\n\\]\nLook for:\n\nStrongly right-skewed distributions\nCurvative in scatterplot\nIncreasing variability in scatterplot"
  },
  {
    "objectID": "slides/12-slr-transformations.html#back-to-house_sales",
    "href": "slides/12-slr-transformations.html#back-to-house_sales",
    "title": "SLR: Transformations",
    "section": "Back to house_sales",
    "text": "Back to house_sales\n\n\nCode\np1 &lt;- gf_point(price ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np2 &lt;- gf_point(log(price) ~ sqft_living, data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"Square Footage\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np3 &lt;- gf_point(price ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"Sale Price\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\np4 &lt;- gf_point(log(price) ~ log(sqft_living), data = house_prices,\n         alpha = 0.25, size = 0.01) |&gt; \n  gf_smooth(method = \"lm\", color = \"red\") |&gt; \n  gf_labs(x = \"log(Square Footage)\", \n       y = \"log(Sale Price)\")  |&gt; \n  gf_refine(scale_y_continuous(labels = label_dollar()),\n  scale_x_continuous(labels = label_number()))\n\n(p1 + p2)/ (p3 + p4)"
  },
  {
    "objectID": "slides/12-slr-transformations.html#fitting-transformed-models",
    "href": "slides/12-slr-transformations.html#fitting-transformed-models",
    "title": "SLR: Transformations",
    "section": "Fitting Transformed Models",
    "text": "Fitting Transformed Models\n\n\n\nlogprice_model &lt;- lm(log(price) ~ sqft_living, data = house_prices)\ntidy(logprice_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.2184641\n0.0063741\n1916.8830\n0\n\n\nsqft_living\n0.0003987\n0.0000028\n142.2326\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &= 12.22  + 3.99\\times 10^{-4}\\times X\\\\\nY &= e^{12.22 + 3.99\\times 10^{-4}\\times X}\\\\\n&= 202805\\times e^{3.99\\times 10^{-4}\\times X}\n\\end{aligned}\n\\]\n\n\nloglog_model &lt;- lm(log(price) ~ log(sqft_living), data = house_prices)\ntidy(loglog_model) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n6.729916\n0.0470620\n143.0011\n0\n\n\nlog(sqft_living)\n0.836771\n0.0062233\n134.4587\n0\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\log(Y) &=6.73 + 0.837\\times \\log(X)\\\\\n\\log(Y) &= \\log(e^{6.73})  + \\log(X^{0.837})\\\\\nY &= 873.15\\times X^{0.837}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/12-slr-transformations.html#residuals-histograms",
    "href": "slides/12-slr-transformations.html#residuals-histograms",
    "title": "SLR: Transformations",
    "section": "Residuals Histograms",
    "text": "Residuals Histograms\n\n\nCode\nlp_aug &lt;- augment(logprice_model)\nll_aug &lt;- augment(loglog_model)\n\np1 &lt;- gf_histogram(~.resid, data = lp_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log Price Residuals\")\n\np2 &lt;- gf_histogram(~.resid, data = ll_aug, bins = 100) |&gt; \n  gf_labs(x = \"Residual\", \n       y = \"Count\", \n       title = \"Log-Log Residuals\")\n\n(p1 + p2)"
  },
  {
    "objectID": "slides/12-slr-transformations.html#qq-plots-of-residuals",
    "href": "slides/12-slr-transformations.html#qq-plots-of-residuals",
    "title": "SLR: Transformations",
    "section": "QQ-Plots of Residuals",
    "text": "QQ-Plots of Residuals\n\n\nCode\np1 &lt;- gf_qq(~.resid, data = lp_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log Price QQ\")\n\np2 &lt;- gf_qq(~.resid, data = ll_aug) |&gt; \n  gf_qqline() |&gt;\n  gf_labs(x = \"Theoretical quantile\", \n       y = \"Observed quantile\", \n       title = \"Log-Log QQ\")\n\np1 + p2"
  },
  {
    "objectID": "slides/12-slr-transformations.html#residuals-vs.-fitted-values",
    "href": "slides/12-slr-transformations.html#residuals-vs.-fitted-values",
    "title": "SLR: Transformations",
    "section": "Residuals vs. Fitted Values",
    "text": "Residuals vs. Fitted Values\n\n\nCode\np1 &lt;- gf_point(.resid ~ .fitted, data = lp_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log Price Model\"\n  )\n\np2 &lt;- gf_point(.resid ~ .fitted, data = ll_aug, \n         alpha = 0.25, size = 0.01) |&gt; \n  gf_hline(yintercept = 0, linetype = \"dashed\") |&gt; \n  gf_labs(\n    x = \"Fitted value\", y = \"Residual\",\n    title = \"Log-Log Model\"\n  )\n\np1 + p2"
  },
  {
    "objectID": "slides/13-slr-outliers.html#computational-set-up",
    "href": "slides/13-slr-outliers.html#computational-set-up",
    "title": "SLR: Outliers",
    "section": "Computational set up",
    "text": "Computational set up\n\n# load packages\nlibrary(tidyverse)   # for data wrangling and visualization\nlibrary(broom)       # for formatting model output\nlibrary(ggformula)   # for creating plots using formulas\nlibrary(scales)      # for pretty axis labels\nlibrary(knitr)       # for pretty tables\nlibrary(moderndive)  # for house_price dataset\nlibrary(fivethirtyeight) # for fandango dataset\nlibrary(kableExtra)  # also for pretty tables\nlibrary(patchwork)   # arrange plots\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/13-slr-outliers.html#types-of-unusual-points-in-slr",
    "href": "slides/13-slr-outliers.html#types-of-unusual-points-in-slr",
    "title": "SLR: Outliers",
    "section": "Types of “Unusual” Points in SLR",
    "text": "Types of “Unusual” Points in SLR\n\nOutlier: a data point that is far from the regression line\nInfluential point: a data point that has a large effect on the regression fit\n\n\n\n\nHow do we measure “far”?\nHow do we measure “effect on the fit”?"
  },
  {
    "objectID": "slides/13-slr-outliers.html#detecting-unusual-cases-overview",
    "href": "slides/13-slr-outliers.html#detecting-unusual-cases-overview",
    "title": "SLR: Outliers",
    "section": "Detecting Unusual Cases: Overview",
    "text": "Detecting Unusual Cases: Overview\n\nCompute residuals\n\n“raw”, standardized, studentized\n\nPlots of residuals\n\nboxplot, scatterplot, normal plot\n\nLeverage\n\nunusual values for the predictors"
  },
  {
    "objectID": "slides/13-slr-outliers.html#example-movie-scores",
    "href": "slides/13-slr-outliers.html#example-movie-scores",
    "title": "SLR: Outliers",
    "section": "Example: Movie scores",
    "text": "Example: Movie scores\n\n\n\nData behind the FiveThirtyEight story Be Suspicious Of Online Movie Ratings, Especially Fandango’s\nIn the fivethirtyeight package: fandango\nContains every film released in 2014 and 2015 that has at least 30 fan reviews on Fandango, an IMDb score, Rotten Tomatoes critic and user ratings, and Metacritic critic and user scores"
  },
  {
    "objectID": "slides/13-slr-outliers.html#data-prep",
    "href": "slides/13-slr-outliers.html#data-prep",
    "title": "SLR: Outliers",
    "section": "Data prep",
    "text": "Data prep\n\nRename Rotten Tomatoes columns as critics and audience\nRename the dataset as movie_scores\n\n\nmovie_scores &lt;- fandango |&gt;\n  rename(critics = rottentomatoes, \n         audience = rottentomatoes_user)"
  },
  {
    "objectID": "slides/13-slr-outliers.html#example-movie-scores-1",
    "href": "slides/13-slr-outliers.html#example-movie-scores-1",
    "title": "SLR: Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\n\nCode\nmovie_scores |&gt; \n  gf_point(audience ~ critics) |&gt; \n  gf_lm() |&gt; \n  gf_labs(x = \"Critics Score\", \n       y = \"Audience Score\")"
  },
  {
    "objectID": "slides/13-slr-outliers.html#boxplot-of-residuals",
    "href": "slides/13-slr-outliers.html#boxplot-of-residuals",
    "title": "SLR: Outliers",
    "section": "Boxplot of Residuals",
    "text": "Boxplot of Residuals\n\n\n\nmovie_fit &lt;- lm(audience ~ critics, data = movie_scores)\nmovie_fit_aug &lt;- augment(movie_fit)\n\ngf_boxplot(.resid ~ \"\", data = movie_fit_aug, \n           fill = \"salmon\", ylab = \"Residuals\", xlab = \"\")\n\n\n\n\n\n\n\n\n\n\n\n\nDots (outliers) indicate data points more than 1.5 IQRs above (or below) quartiles"
  },
  {
    "objectID": "slides/13-slr-outliers.html#standardized-residuals",
    "href": "slides/13-slr-outliers.html#standardized-residuals",
    "title": "SLR: Outliers",
    "section": "Standardized Residuals",
    "text": "Standardized Residuals\n\n\nRecall: Z-scores\nFact: If \\(X\\) has mean \\(\\mu\\) and standard deviation \\(\\sigma\\), then \\((X-\\mu)/\\sigma\\) has mean 0 and standard deviation 1\nFor residuals: mean 0 and standard deviation \\(\\hat{\\sigma}_\\epsilon\\)\nStandardized residuals: \\(\\frac{y_i-\\hat{y}_i}{\\hat{\\sigma}_\\epsilon}\\)\n\nLook for values beyond \\(\\pm 2\\) or \\(\\pm 3\\)"
  },
  {
    "objectID": "slides/13-slr-outliers.html#example-movie-scores-2",
    "href": "slides/13-slr-outliers.html#example-movie-scores-2",
    "title": "SLR: Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\n\nCode\np1 &lt;- movie_fit_aug |&gt;  # Augmented data\n  gf_boxplot(\"\" ~ .std.resid, \n           xlab = \"Standardized Residual\")\n\np2 &lt;- movie_fit_aug |&gt;  # Augmented data\n  gf_point(.std.resid ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Standardized Residual\")\n\np1 + p2"
  },
  {
    "objectID": "slides/13-slr-outliers.html#externally-studentized-residuals",
    "href": "slides/13-slr-outliers.html#externally-studentized-residuals",
    "title": "SLR: Outliers",
    "section": "(Externally) Studentized Residuals",
    "text": "(Externally) Studentized Residuals\n\nConcern: An unusual value may exert great influence on the fit\n\nIts residual might be underestimated because the model “moves” a lot to fit it\nThe standard error may also be inflated due to the outlier error\n\nStudentize: Fit the model without that case, then find new \\(\\hat{\\sigma}_\\epsilon\\)"
  },
  {
    "objectID": "slides/13-slr-outliers.html#example-movie-scores-3",
    "href": "slides/13-slr-outliers.html#example-movie-scores-3",
    "title": "SLR: Outliers",
    "section": "Example: Movie Scores",
    "text": "Example: Movie Scores\n\nmovie_fit_aug |&gt;  # Augmented data\n  mutate(studentized_residual = rstudent(movie_fit)) |&gt; \n  gf_point(studentized_residual ~ .fitted, \n           xlab = \"Predicted\", ylab = \"Studentized Residual\")"
  },
  {
    "objectID": "slides/13-slr-outliers.html#what-to-do-with-an-outlier",
    "href": "slides/13-slr-outliers.html#what-to-do-with-an-outlier",
    "title": "SLR: Outliers",
    "section": "What to do with an outlier?",
    "text": "What to do with an outlier?\n\nLook into it\nIf something is unusual about it and you can make a case that it is not a good representation of the population you can throw it out\nIf not and the value is just unusual, keep it"
  },
  {
    "objectID": "slides/13-slr-outliers.html#influence-vs.-leverage",
    "href": "slides/13-slr-outliers.html#influence-vs.-leverage",
    "title": "SLR: Outliers",
    "section": "Influence vs. Leverage",
    "text": "Influence vs. Leverage\n\nHigh Influence Point: point that DOES impact the regression line\nHigh Leverage Point: point with “potential” to impact regression line because \\(X\\)-value is unusual"
  },
  {
    "objectID": "slides/13-slr-outliers.html#high-leverage-low-influence",
    "href": "slides/13-slr-outliers.html#high-leverage-low-influence",
    "title": "SLR: Outliers",
    "section": "High Leverage, Low Influence",
    "text": "High Leverage, Low Influence"
  },
  {
    "objectID": "slides/13-slr-outliers.html#high-leverage-high-influence",
    "href": "slides/13-slr-outliers.html#high-leverage-high-influence",
    "title": "SLR: Outliers",
    "section": "High Leverage, High Influence",
    "text": "High Leverage, High Influence"
  },
  {
    "objectID": "slides/13-slr-outliers.html#low-leverage-low-influence",
    "href": "slides/13-slr-outliers.html#low-leverage-low-influence",
    "title": "SLR: Outliers",
    "section": "Low Leverage, Low Influence",
    "text": "Low Leverage, Low Influence"
  },
  {
    "objectID": "slides/13-slr-outliers.html#low-leverage-high-influence",
    "href": "slides/13-slr-outliers.html#low-leverage-high-influence",
    "title": "SLR: Outliers",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence"
  },
  {
    "objectID": "slides/13-slr-outliers.html#low-leverage-high-influence-1",
    "href": "slides/13-slr-outliers.html#low-leverage-high-influence-1",
    "title": "SLR: Outliers",
    "section": "Low Leverage, High Influence",
    "text": "Low Leverage, High Influence"
  },
  {
    "objectID": "slides/13-slr-outliers.html#recap-augment-function",
    "href": "slides/13-slr-outliers.html#recap-augment-function",
    "title": "SLR: Outliers",
    "section": "Recap: Augment function",
    "text": "Recap: Augment function\n\nmovie_fit_aug |&gt; \n  head() |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\naudience\ncritics\n.fitted\n.resid\n.hat\n.sigma\n.cooksd\n.std.resid\n\n\n\n\n86\n74\n70.69768\n15.302321\n0.0081597\n12.51615\n0.0061774\n1.2254688\n\n\n80\n85\n76.40313\n3.596866\n0.0112688\n12.57830\n0.0004743\n0.2885034\n\n\n90\n80\n73.80975\n16.190255\n0.0096283\n12.50817\n0.0081839\n1.2975389\n\n\n84\n18\n41.65173\n42.348272\n0.0207618\n12.06226\n0.1234982\n3.4131653\n\n\n28\n14\n39.57702\n-11.577018\n0.0234805\n12.54373\n0.0104964\n-0.9343768\n\n\n62\n63\n64.99222\n-2.992225\n0.0068844\n12.57943\n0.0001988\n-0.2394750"
  },
  {
    "objectID": "prepare/transformations-prep.html",
    "href": "prepare/transformations-prep.html",
    "title": "September 18 lecture",
    "section": "",
    "text": "Stat2: Topic 1.4 Transformations/Reexpressions\nStat2: Topic 1.5 Outliers and Influential Points"
  },
  {
    "objectID": "prepare/transformations-prep.html#assigned-reading",
    "href": "prepare/transformations-prep.html#assigned-reading",
    "title": "September 18 lecture",
    "section": "",
    "text": "Stat2: Topic 1.4 Transformations/Reexpressions\nStat2: Topic 1.5 Outliers and Influential Points"
  },
  {
    "objectID": "prepare/outliers-prep.html",
    "href": "prepare/outliers-prep.html",
    "title": "September 18 lecture",
    "section": "",
    "text": "Stat2: Topic 1.4 Transformations/Reexpressions\nStat2: Topic 1.5 Outliers and Influential Points"
  },
  {
    "objectID": "prepare/outliers-prep.html#assigned-reading",
    "href": "prepare/outliers-prep.html#assigned-reading",
    "title": "September 18 lecture",
    "section": "",
    "text": "Stat2: Topic 1.4 Transformations/Reexpressions\nStat2: Topic 1.5 Outliers and Influential Points"
  },
  {
    "objectID": "ae/ae-08-transformations.html",
    "href": "ae/ae-08-transformations.html",
    "title": "AE 08: Transformations",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-08”.\nGo to the Canvas and locate your AE 08 assignment to get started.\nUpload the ae-08.qmd files into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(yardstick)\nlibrary(Stat2Data)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-08-transformations.html#data",
    "href": "ae/ae-08-transformations.html#data",
    "title": "AE 08: Transformations",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is from the Stat2Data R package which is the companion package for this course’s textbook. The data was originally generated by the American Medical Association and concerns the availability of health care in counties in the United States. You can find information here by searching for the County Health Resources dataset.\n\ndata(\"CountyHealth\") # Loads the data from the package\n\nIt is relatively easy to count the number of hospitals a county has, whereas counting the number of doctors is much more difficult. We’d like to build a linear model to predict the number of doctors, contained the variable MDs, from the number of hospitals, Hospitals."
  },
  {
    "objectID": "ae/ae-08-transformations.html#exercise-1",
    "href": "ae/ae-08-transformations.html#exercise-1",
    "title": "AE 08: Transformations",
    "section": "Exercise 1",
    "text": "Exercise 1\nUse a visualization and to argue why a simple linear regression model would not be appropriate for the data if we do not make any transformations."
  },
  {
    "objectID": "ae/ae-08-transformations.html#exercise-2",
    "href": "ae/ae-08-transformations.html#exercise-2",
    "title": "AE 08: Transformations",
    "section": "Exercise 2",
    "text": "Exercise 2\nUse visualizations to find the best model that involves at least one log transformation."
  },
  {
    "objectID": "ae/ae-08-transformations.html#exercise-3",
    "href": "ae/ae-08-transformations.html#exercise-3",
    "title": "AE 08: Transformations",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit that model and write the equation that describes it."
  },
  {
    "objectID": "ae/ae-08-transformations.html#exercise-4-time-permitting",
    "href": "ae/ae-08-transformations.html#exercise-4-time-permitting",
    "title": "AE 08: Transformations",
    "section": "Exercise 4 (Time Permitting)",
    "text": "Exercise 4 (Time Permitting)\nIs the model you created in exercise 2 or exercise 3 a “better” model. Use the residuals and evaluation metrics to make your argument. Hint: you want to look at the residuals of the transformed models but look at the evaluation metrics when you transform your predictions back to the original scale."
  },
  {
    "objectID": "ae/ae-08-transformations.html#to-submit-the-ae",
    "href": "ae/ae-08-transformations.html#to-submit-the-ae",
    "title": "AE 08: Transformations",
    "section": "To submit the AE:",
    "text": "To submit the AE:\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "slides/12-slr-transformations.html#a-note-on-evaluation",
    "href": "slides/12-slr-transformations.html#a-note-on-evaluation",
    "title": "SLR: Transformations",
    "section": "A note on evaluation",
    "text": "A note on evaluation\nIf you are computing your evaluation metrics (e.g. \\(R^2\\) or RMSE), you should transform your predictions BACK to their original scale, especially if you’re trying to choose the best model\n\n\nWhy do we need to undo the transformation for evaluation metrics by not residuals plots?\nWhy don’t we need to worry about the predictors?"
  },
  {
    "objectID": "ae/ae-08-transformations.html#exercise-4",
    "href": "ae/ae-08-transformations.html#exercise-4",
    "title": "AE 08: Transformations",
    "section": "Exercise 4",
    "text": "Exercise 4\nUse visualizations to find the best model the involves at least one square root transformation. Hint: you can create square root transformations in the same way you can make log transformations by replacing log with sqrt."
  },
  {
    "objectID": "ae/ae-08-transformations.html#exercise-5",
    "href": "ae/ae-08-transformations.html#exercise-5",
    "title": "AE 08: Transformations",
    "section": "Exercise 5",
    "text": "Exercise 5\nFit that model and write the equation that describes it."
  },
  {
    "objectID": "ae/ae-08-transformations.html#exercise-6-time-permitting",
    "href": "ae/ae-08-transformations.html#exercise-6-time-permitting",
    "title": "AE 08: Transformations",
    "section": "Exercise 6 (Time Permitting)",
    "text": "Exercise 6 (Time Permitting)\nIs the model you created in exercise 3 or exercise 5 a “better” model? Use residuals and evaluation metrics to make your argument. Hint: you want to look at the residuals of the transformed models but look at the evaluation metrics when you transform your predictions back to the original scale."
  },
  {
    "objectID": "ae/forStudents/ae-08/ae-08-transformations.html",
    "href": "ae/forStudents/ae-08/ae-08-transformations.html",
    "title": "AE 08: Transformations",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-08”.\nGo to the Canvas and locate your AE 08 assignment to get started.\nUpload the ae-08.qmd files into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(yardstick)\nlibrary(Stat2Data)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/forStudents/ae-08/ae-08-transformations.html#data",
    "href": "ae/forStudents/ae-08/ae-08-transformations.html#data",
    "title": "AE 08: Transformations",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is from the Stat2Data R package which is the companion package for this course’s textbook. The data was originally generated by the American Medical Association and concerns the availability of health care in counties in the United States. You can find information here by searching for the County Health Resources dataset.\n\ndata(\"CountyHealth\") # Loads the data from the package\n\nIt is relatively easy to count the number of hospitals a county has, whereas counting the number of doctors is much more difficult. We’d like to build a linear model to predict the number of doctors, contained the variable MDs, from the number of hospitals, Hospitals."
  },
  {
    "objectID": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-1",
    "href": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-1",
    "title": "AE 08: Transformations",
    "section": "Exercise 1",
    "text": "Exercise 1\nUse a visualization and to argue why a simple linear regression model would not be appropriate for the data if we do not make any transformations."
  },
  {
    "objectID": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-2",
    "href": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-2",
    "title": "AE 08: Transformations",
    "section": "Exercise 2",
    "text": "Exercise 2\nUse visualizations to find the best model that involves at least one log transformation."
  },
  {
    "objectID": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-3",
    "href": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-3",
    "title": "AE 08: Transformations",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit that model and write the equation that describes it."
  },
  {
    "objectID": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-4",
    "href": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-4",
    "title": "AE 08: Transformations",
    "section": "Exercise 4",
    "text": "Exercise 4\nUse visualizations to find the best model the involves at least one square root transformation. Hint: you can create square root transformations in the same way you can make log transformations by replacing log with sqrt."
  },
  {
    "objectID": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-5",
    "href": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-5",
    "title": "AE 08: Transformations",
    "section": "Exercise 5",
    "text": "Exercise 5\nFit that model and write the equation that describes it."
  },
  {
    "objectID": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-6-time-permitting",
    "href": "ae/forStudents/ae-08/ae-08-transformations.html#exercise-6-time-permitting",
    "title": "AE 08: Transformations",
    "section": "Exercise 6 (Time Permitting)",
    "text": "Exercise 6 (Time Permitting)\nIs the model you created in exercise 3 or exercise 5 a “better” model? Use residuals and evaluation metrics to make your argument. Hint: you want to look at the residuals of the transformed models but look at the evaluation metrics when you transform your predictions back to the original scale."
  },
  {
    "objectID": "ae/forStudents/ae-08/ae-08-transformations.html#to-submit-the-ae",
    "href": "ae/forStudents/ae-08/ae-08-transformations.html#to-submit-the-ae",
    "title": "AE 08: Transformations",
    "section": "To submit the AE:",
    "text": "To submit the AE:\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-09-outliers.html",
    "href": "ae/ae-09-outliers.html",
    "title": "AE 09: Outliers",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-09”.\nGo to the Canvas and locate your AE-09 assignment to get started.\nUpload the ae-09.qmd file into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment.\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(yardstick)\nlibrary(Stat2Data)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(knitr)\nlibrary(patchwork) #arrange plots in a grid"
  },
  {
    "objectID": "ae/ae-09-outliers.html#data",
    "href": "ae/ae-09-outliers.html#data",
    "title": "AE 09: Outliers",
    "section": "Data",
    "text": "Data\nThe data set for this assignment is from the Stat2Data R package which is the companion package for this course’s textbook. The data contains data from all MLB games played on August, 11, 2017. On this day there were no extra-innings games or rain delays. You can find information here by searching for the Baseball Game TImes of One Day in 2017 dataset.\n\ndata(\"BaseballTimes2017\") # Loads the data from the package\n\nWe are interested in predicting Time the time in minutes to play the game, from either Runs, the number of runs scores by the two teams combined, or Pitchers, the number of pitchers sued total for the two teams."
  },
  {
    "objectID": "ae/ae-09-outliers.html#exercise-1",
    "href": "ae/ae-09-outliers.html#exercise-1",
    "title": "AE 09: Outliers",
    "section": "Exercise 1",
    "text": "Exercise 1\nArgue that Runs is a better predictor of Time than Pitchers."
  },
  {
    "objectID": "ae/ae-09-outliers.html#exercise-2",
    "href": "ae/ae-09-outliers.html#exercise-2",
    "title": "AE 09: Outliers",
    "section": "Exercise 2",
    "text": "Exercise 2\nArgue whether you think the CIN-MIL game would be considered a high leverage and/or high influence point."
  },
  {
    "objectID": "ae/ae-09-outliers.html#exercise-3",
    "href": "ae/ae-09-outliers.html#exercise-3",
    "title": "AE 09: Outliers",
    "section": "Exercise 3",
    "text": "Exercise 3\nRemove the CIN-MIL game from the data set. Which model is better now?"
  },
  {
    "objectID": "ae/ae-09-outliers.html#exercise-4",
    "href": "ae/ae-09-outliers.html#exercise-4",
    "title": "AE 08: Transformations",
    "section": "Exercise 4",
    "text": "Exercise 4\nUse visualizations to find the best model the involves at least one square root transformation. Hint: you can create square root transformations in the same way you can make log transformations by replacing log with sqrt."
  },
  {
    "objectID": "ae/ae-09-outliers.html#exercise-5",
    "href": "ae/ae-09-outliers.html#exercise-5",
    "title": "AE 08: Transformations",
    "section": "Exercise 5",
    "text": "Exercise 5\nFit that model and write the equation that describes it."
  },
  {
    "objectID": "ae/ae-09-outliers.html#exercise-6-time-permitting",
    "href": "ae/ae-09-outliers.html#exercise-6-time-permitting",
    "title": "AE 08: Transformations",
    "section": "Exercise 6 (Time Permitting)",
    "text": "Exercise 6 (Time Permitting)\nIs the model you created in exercise 3 or exercise 5 a “better” model? Use residuals and evaluation metrics to make your argument. Hint: you want to look at the residuals of the transformed models but look at the evaluation metrics when you transform your predictions back to the original scale."
  },
  {
    "objectID": "ae/ae-09-outliers.html#to-submit-the-ae",
    "href": "ae/ae-09-outliers.html#to-submit-the-ae",
    "title": "AE 09: Outliers",
    "section": "To submit the AE:",
    "text": "To submit the AE:\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "hw/hw-04.html#exercise-5-1",
    "href": "hw/hw-04.html#exercise-5-1",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 5",
    "text": "Exercise 5\nNext, let’s consider how well the model fits the relationship between the early career pay and percent with a STEM degree.\n\nCalculate \\(R^2\\) and interpret it in the context of the data.\nCalculate RMSE and interpret it in the context of the data.\n\n\nTeam Member 3: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the responses to exercise 4 - 5.\nTeam Member 4: It’s your turn! Type the team’s response to exercise 6."
  },
  {
    "objectID": "hw/hw-04.html#exercise-7-1",
    "href": "hw/hw-04.html#exercise-7-1",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 7",
    "text": "Exercise 7\nFit another model which predicts the typical early career pay using only the percentage of alumni that believe their career will make the world a better place.\n\nNeatly display this model with 3 digits.\nWhich model do you think is better - this model or the model fit in Exercise 1? Explain your response showing any analysis used to make the decision.\n\n\nTeam Member 1: Render, commit and push your changes to GitHub with an appropriate commit message again. Make sure to commit and push all changed files so that your Git pane is cleared up afterwards and the rest of the team can see the completed homework.\n\n\nAll other team members: Pull to get the updated documents from GitHub. Click on the .qmd file, and you should see the team’s completed homework!"
  },
  {
    "objectID": "hw/hw-04-old.html",
    "href": "hw/hw-04-old.html",
    "title": "HW 04: Logistic regression",
    "section": "",
    "text": "Important\n\n\n\nDue Wednesday, November 15 at 11:59pm"
  },
  {
    "objectID": "hw/hw-04-old.html#learning-goals",
    "href": "hw/hw-04-old.html#learning-goals",
    "title": "HW 04: Logistic regression",
    "section": "Learning goals",
    "text": "Learning goals\nBy the end of the assignment you will be able to…\n\nUse logistic regression to explore the relationship between a binary response variable and multiple predictor variables\nConduct exploratory data analysis for logistic regression\nInterpret coefficients of logistic regression model\nUse statistics to help choose the best fit model\nUse the logistic regression model for prediction and classification"
  },
  {
    "objectID": "hw/hw-04-old.html#getting-started",
    "href": "hw/hw-04-old.html#getting-started",
    "title": "HW 04: Logistic regression",
    "section": "Getting started",
    "text": "Getting started\nThe repo for this assignment is available on GitHub at github.com/sta210-fa23 and starts with the prefix hw-04. See Lab 01 for more detailed instructions on getting started."
  },
  {
    "objectID": "hw/hw-04-old.html#packages",
    "href": "hw/hw-04-old.html#packages",
    "title": "HW 04: Logistic regression",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this assignment.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-1",
    "href": "hw/hw-04-old.html#exercise-1",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nWhy do you think the authors chose to only include data from people who were eligible to vote for at least four election cycles?"
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-2",
    "href": "hw/hw-04-old.html#exercise-2",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nLet’s prepare the data for analysis and modeling.\n\nCreate a new variable called frequent_voter that takes the value 1 if the voter_category is “always” and 0 otherwise.\nMake a table of the distribution of frequent_voter.\nWhat percentage of the respondents in the data say they voted “in all or all-but-one of the elections they were eligible in”?"
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-3",
    "href": "hw/hw-04-old.html#exercise-3",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe variable Q30 contains the respondent’s political party identification. Make a new variable, party_id, that simplifies Q30 into three categories: “Democrat”, “Republican”, “Independent/Neither”, The category “Independent/Neither” will also include respondents who did not answer the question. Make party_id a factor and relevel it so that it is consistent with the ordering of the responses in Question 30 of the survey.\n\nMake a plot of the distribution of party_id.\nWhich category of party_id occurs most frequently in this data set?"
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-4",
    "href": "hw/hw-04-old.html#exercise-4",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nIn the FiveThirtyEight article, the authors include visualizations of the relationship between the voter category and demographic variables such as race, age, education, etc.\n\nMake a segmented barplot (also known as a stacked barplot) displaying the distribution of frequent_voter for each category of party_id. Make the plot such that the percentages (instead of counts) are displayed.\nUse the plot to describe the relationship between these two variables.\n\n\n\n\n\n\n\nTip\n\n\n\nSee the plots of demographic information by voting history in the FiveThirtyEight article for examples of segmented bar plots."
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-5",
    "href": "hw/hw-04-old.html#exercise-5",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nLet’s start by fitting a model using the demographic factors - ppage, educ, race, gender, income_cat - to predict the odds a person is a frequent voter.\n\nSplit the data into training (75%) and testing sets (25%). Use a seed of 29.\nFit the model on the training data. Display the model using 3 digits.\nConsider the relationship between ppage and one’s voting behavior. Interpret the coefficient of ppage in the context of the data in terms of the odds a person is a frequent voter."
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-6",
    "href": "hw/hw-04-old.html#exercise-6",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nShould party identification be added to the model? Use a drop-in-deviance test to determine if party identification should be added to the model fit in the previous exercise. Include the hypotheses in mathematical notation, the output from the test, and the conclusion in the context of the data."
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-7",
    "href": "hw/hw-04-old.html#exercise-7",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nDisplay the model chosen from the previous exercise using 3 digits.\nThen use the model selected to write a short paragraph (2 - 5 sentences) describing the effect (or lack of effect) of political party on the odds a person is a frequent voter. The paragraph should include an indication of which levels (if any) are statistically significant along with specifics about the differences in the odds between the political parties, as appropriate."
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-8",
    "href": "hw/hw-04-old.html#exercise-8",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nIn the article, the authors write\n\n“Nonvoters were more likely to have lower incomes; to be young; to have lower levels of education; and to say they don’t belong to either political party, which are all traits that square with what we know about people less likely to engage with the political system.”\n\nConsider the model you selected in Exercise 6. Is it consistent with this statement? Briefly explain why or why not."
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-9",
    "href": "hw/hw-04-old.html#exercise-9",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 9",
    "text": "Exercise 9\nUse the testing data to produce the ROC curve and calculate the area under curve (AUC) for the model selected in Exercise 6. Write 1 - 2 sentences describing how well the model fits the data."
  },
  {
    "objectID": "hw/hw-04-old.html#exercise-10",
    "href": "hw/hw-04-old.html#exercise-10",
    "title": "HW 04: Logistic regression",
    "section": "Exercise 10",
    "text": "Exercise 10\nYou have been tasked by a local political organization to identify adults in the community who are frequent voters. These adults will receive targeted political mailings that will be different from the mailings sent to adults who are not frequent voters. You will use the model selected in Exercise 6 to identify the frequent voters.\nMake a confusion matrix based on the cut-off probability of 0.25. Use the confusion matrix to calculate the following:\n\nSensitivity\nSpecificity\nFalse negative rate\nFalse positive rate"
  },
  {
    "objectID": "hw/hw-04-old.html#footnotes",
    "href": "hw/hw-04-old.html#footnotes",
    "title": "HW 04: Logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎↩︎"
  },
  {
    "objectID": "prepare/project-intro-prep.html",
    "href": "prepare/project-intro-prep.html",
    "title": "September 23rd lecture",
    "section": "",
    "text": "Read through project description:\n\nProject Instructions\nTips + resources\nRubric"
  },
  {
    "objectID": "prepare/project-intro-prep.html#assigned-reading",
    "href": "prepare/project-intro-prep.html#assigned-reading",
    "title": "September 23rd lecture",
    "section": "",
    "text": "Read through project description:\n\nProject Instructions\nTips + resources\nRubric"
  },
  {
    "objectID": "slides/14-mlr.html#computational-setup",
    "href": "slides/14-mlr.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(ISLR2)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/14-mlr.html#data-peer-to-peer-lender",
    "href": "slides/14-mlr.html#data-peer-to-peer-lender",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\nRows: 50\nColumns: 18\n$ state                   &lt;fct&gt; NJ, CA, SC, CA, OH, IN, NY, MO, FL, FL, MD, HI…\n$ emp_length              &lt;dbl&gt; 3, 10, NA, 0, 4, 6, 2, 10, 6, 3, 8, 10, 10, 2,…\n$ term                    &lt;dbl&gt; 60, 36, 36, 36, 60, 36, 36, 36, 60, 60, 36, 36…\n$ homeownership           &lt;fct&gt; rent, rent, mortgage, rent, mortgage, mortgage…\n$ annual_income           &lt;dbl&gt; 59000, 60000, 75000, 75000, 254000, 67000, 288…\n$ verified_income         &lt;fct&gt; Not Verified, Not Verified, Verified, Not Veri…\n$ debt_to_income          &lt;dbl&gt; 0.55752542, 1.30568333, 1.05628000, 0.57434667…\n$ total_credit_limit      &lt;int&gt; 95131, 51929, 301373, 59890, 422619, 349825, 1…\n$ total_credit_utilized   &lt;int&gt; 32894, 78341, 79221, 43076, 60490, 72162, 2872…\n$ num_cc_carrying_balance &lt;int&gt; 8, 2, 14, 10, 2, 4, 1, 3, 10, 4, 3, 4, 3, 2, 3…\n$ loan_purpose            &lt;fct&gt; debt_consolidation, credit_card, debt_consolid…\n$ loan_amount             &lt;int&gt; 22000, 6000, 25000, 6000, 25000, 6400, 3000, 1…\n$ grade                   &lt;fct&gt; B, B, E, B, B, B, D, A, A, C, D, A, A, A, A, E…\n$ interest_rate           &lt;dbl&gt; 10.90, 9.92, 26.30, 9.92, 9.43, 9.92, 17.09, 6…\n$ public_record_bankrupt  &lt;int&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…\n$ loan_status             &lt;fct&gt; Current, Current, Current, Current, Current, C…\n$ has_second_income       &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ total_income            &lt;dbl&gt; 59000, 60000, 75000, 75000, 254000, 67000, 288…"
  },
  {
    "objectID": "slides/14-mlr.html#variables",
    "href": "slides/14-mlr.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nFeatures (another name for predictors)\n\nIncome: Annual income (in 1000’s of US dollars)\nRating: Credit Rating\n\nOutcome\n\nLimit: Credit limit\n\n\nComplete Exercises 0-1. Please don’t look ahead in the slides."
  },
  {
    "objectID": "slides/14-mlr.html#outcome-interest_rate",
    "href": "slides/14-mlr.html#outcome-interest_rate",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\nCode\nloan50 |&gt; \n  gf_density(~interest_rate, fill = \"steelblue\") |&gt; \n  gf_labs(title = \"Distribution of interest rate\")|&gt; \n  gf_refine(scale_x_continuous(labels = percent_format(scale = 1)))\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n5.31\n7.96\n9.93\n13.715\n26.3\n11.5672\n5.052115\n50\n0"
  },
  {
    "objectID": "slides/14-mlr.html#predictors",
    "href": "slides/14-mlr.html#predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Predictors",
    "text": "Predictors\n\n\nCode\np1 &lt;- Credit |&gt; \n  gf_density(~Limit, fill = \"steelblue\") |&gt; \n  gf_labs(title = \"Distribution of credit limit\",\n          x = \"Credit Limit\")|&gt; \n    gf_refine(scale_x_continuous(labels = dollar_format()))\n\np2 &lt;- Credit |&gt; \n  gf_histogram(~Rating, binwidth = 50) |&gt; \n  gf_labs(title = \"\",\n       x = \"Credit Rating\")\n\np3 &lt;- Credit |&gt; \n  gf_histogram(~Income, binwidth = 10) |&gt; \n  gf_labs(title = \"\",\n       x = \"Annual income (in $1,000s)\") |&gt; \n  gf_refine(scale_x_continuous(labels = dollar_format()))\n\np1 / (p2 + p3)"
  },
  {
    "objectID": "slides/14-mlr.html#data-manipulation-1-rescale-income",
    "href": "slides/14-mlr.html#data-manipulation-1-rescale-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_k = annual_income / 1000)\n\ngf_histogram(~annual_income_k, data = loan50, binwidth = 20) |&gt; \n  gf_labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/14-mlr.html#outcome-vs.-predictors",
    "href": "slides/14-mlr.html#outcome-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors\n\n\nCode\np4 &lt;- Credit |&gt; \n  gf_point(Limit ~ Rating, color = \"steelblue\") |&gt; \n  gf_labs(\n    y = \"Credit Limit\",\n    x = \"Credit Rating\"\n  ) |&gt; \n  gf_refine(scale_y_continuous(labels = dollar_format()))\n\n\np5 &lt;- Credit |&gt; \n  gf_point(Limit ~ Income, color = \"steelblue\") |&gt; \n  gf_labs(\n    y = \"Credit Limit\",\n    x = \"Annual income (in $1,000s)\"\n  ) |&gt; \n  gf_refine(scale_x_continuous(labels = dollar_format()),\n                               scale_y_continuous(labels = percent_format(scale = 1)))\n\np4 / p5"
  },
  {
    "objectID": "slides/14-mlr.html#single-vs.-multiple-predictors",
    "href": "slides/14-mlr.html#single-vs.-multiple-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Single vs. multiple predictors",
    "text": "Single vs. multiple predictors\nSo far we’ve used a single predictor variable to understand variation in a quantitative response variable\n\nNow we want to use multiple predictor variables to understand variation in a quantitative response variable"
  },
  {
    "objectID": "slides/14-mlr.html#multiple-linear-regression-mlr",
    "href": "slides/14-mlr.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{Rating} + \\hat{\\beta}_2 \\text{Income}\n\\end{aligned}\n\\]\nSimilar to simple linear regression, this model assumes that at each combination of the predictor variables, the values Limit follow a Normal distribution."
  },
  {
    "objectID": "slides/14-mlr.html#multiple-linear-regression-1",
    "href": "slides/14-mlr.html#multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nRecall: The simple linear regression model assumes\n\\[\nY|X\\sim N(\\beta_0 + \\beta_1 X, \\sigma_{\\epsilon}^2)\n\\]\n\nSimilarly: The multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/14-mlr.html#multiple-linear-regression-2",
    "href": "slides/14-mlr.html#multiple-linear-regression-2",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/14-mlr.html#model-equation",
    "href": "slides/14-mlr.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{Limit}} =  -532.471 &+14.771 \\times \\text{Rating}\\\\\n& -0.557 \\times \\text{Income}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/14-mlr.html#interpreting-hatbeta_j",
    "href": "slides/14-mlr.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(y\\) when \\(x_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n\n\nComplete Exercises 5-6."
  },
  {
    "objectID": "slides/14-mlr.html#prediction",
    "href": "slides/14-mlr.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted credit limit for an borrower with an credit rating of ratio of 700, and who has an annual income of $59,000?\n\n\n\n-532.471 + 14.771 * 700 + - .577 * 59\n\n[1] 9773.186\n\n\n\nThe predicted credit limit for an borrower with an credit rating of ratio of 700, and who has an annual income of $59,000 is $9773.19."
  },
  {
    "objectID": "slides/14-mlr.html#prediction-revisited",
    "href": "slides/14-mlr.html#prediction-revisited",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction, revisited",
    "text": "Prediction, revisited\nJust like with simple linear regression, we can use the predict function in R to calculate the appropriate intervals for our predicted values:\n\n\n       fit      lwr      upr\n1 9840.213 9476.983 10203.44\n\n\n\n\n\n\n\n\nNote\n\n\nDifference in predicted value due to rounding the coefficients on the previous slide."
  },
  {
    "objectID": "slides/14-mlr.html#confidence-interval-for-hatmu_y",
    "href": "slides/14-mlr.html#confidence-interval-for-hatmu_y",
    "title": "Multiple linear regression (MLR)",
    "section": "Confidence interval for \\(\\hat{\\mu}_y\\)",
    "text": "Confidence interval for \\(\\hat{\\mu}_y\\)\n\nCalculate a 90% confidence interval for the estimated mean interest rate for borrowers with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000.\n\n\n\npredict(int_fit, new_borrower, interval = \"confidence\", level = 0.90)\n\n        fit       lwr       upr\n1 -491.3498 -557.3297 -425.3699"
  },
  {
    "objectID": "slides/14-mlr.html#prediction-interval-for-haty",
    "href": "slides/14-mlr.html#prediction-interval-for-haty",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 90% confidence interval for the predicted credit limit for an individual borrower an credit rating of ratio of 700, and who has an annual income of $59,000?\n\n\n\npredict(lim_fit, new_borrower, interval = \"prediction\", level = 0.90)\n\n       fit      lwr      upr\n1 9840.213 9535.599 10144.83\n\n\n\nWhen would you use \"confidence\"? Would the interval be wider or narrower?"
  },
  {
    "objectID": "slides/14-mlr.html#cautions",
    "href": "slides/14-mlr.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/14-mlr.html#interpreting-results",
    "href": "slides/14-mlr.html#interpreting-results",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-532.471\n24.173\n-22.028\n0.000\n-579.994\n-484.948\n\n\nRating\n14.771\n0.096\n153.124\n0.000\n14.582\n14.961\n\n\nIncome\n0.557\n0.423\n1.316\n0.189\n-0.275\n1.390\n\n\n\n\n\n\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/14-mlr.html#indicator-variables-1",
    "href": "slides/14-mlr.html#indicator-variables-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category"
  },
  {
    "objectID": "slides/14-mlr.html#data-manipulation-2-create-indicator-variables-for-verified_income",
    "href": "slides/14-mlr.html#data-manipulation-2-create-indicator-variables-for-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 2: Create indicator variables for verified_income",
    "text": "Data manipulation 2: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n\n\n\n\n\n\nverified_income\nnot_verified\nsource_verified\nverified\n\n\n\n\nNot Verified\n1\n0\n0\n\n\nVerified\n0\n0\n1\n\n\nSource Verified\n0\n1\n0"
  },
  {
    "objectID": "slides/14-mlr.html#indicators-in-the-model",
    "href": "slides/14-mlr.html#indicators-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe reference level is the category that doesn’t have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding.\nlm will do this for you.\n\n\n\nloan50 |&gt;\n  select(verified_income, source_verified, verified) |&gt;\n  slice(1, 3, 6) |&gt; \n  kable()\n\n\n\n\nverified_income\nsource_verified\nverified\n\n\n\n\nNot Verified\n0\n0\n\n\nVerified\n0\n1\n\n\nSource Verified\n1\n0"
  },
  {
    "objectID": "slides/14-mlr.html#interpreting-verified_income",
    "href": "slides/14-mlr.html#interpreting-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income\n0.000\n0.000\n-1.804\n0.078\n0.000\n0.000\n\n\n\n\n\n\n\n\n\n\nThe reference level is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\n\n\n\n\n\nInterpret the coefficient of Verified in the context of the data."
  },
  {
    "objectID": "slides/14-mlr.html#interaction-terms-1",
    "href": "slides/14-mlr.html#interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model.\nWe want a model of the form:\n\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{Rating} + \\hat{\\beta}_2 \\text{Income} +\n\\hat{\\beta}_3\\text{Rating}\\times\\text{Income}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/14-mlr.html#interest-rate-vs.-annual-income",
    "href": "slides/14-mlr.html#interest-rate-vs.-annual-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification.\n\nlim_fit_aug &lt;- lim_fit |&gt; \n  augment() |&gt; \n  mutate(interaction = Income*Rating)\n\nlim_fit_aug |&gt; head()\n\n# A tibble: 6 × 10\n  Limit Rating Income .fitted .resid    .hat .sigma   .cooksd .std.resid\n  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1  3606    283   14.9   3656.  -50.1 0.00467   182. 0.000119     -0.275 \n2  6645    483  106.    6661.  -16.1 0.0119    182. 0.0000316    -0.0887\n3  7075    514  105.    7118.  -43.2 0.0102    182. 0.000196     -0.238 \n4  9504    681  149.    9610. -106.  0.0245    182. 0.00289      -0.587 \n5  4897    357   55.9   4772.  125.  0.00307   182. 0.000485      0.687 \n6  8047    569   80.2   7917.  130.  0.00737   182. 0.00127       0.716 \n# ℹ 1 more variable: interaction &lt;dbl&gt;\n\n\n\n# Interaction terms\n\n## Interaction terms\n\n-   Sometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\n-   This is an **interaction effect**.\n-   To account for this, we can include **interaction terms** in the model.\n\n## Interest rate vs. annual income\n\nThe lines are not parallel indicating there is an **interaction effect**. The slope of annual income differs based on the income verification.\n\n::: {.cell}\n::: {.cell-output-display}\n![](14-mlr_files/figure-revealjs/unnamed-chunk-14-1.png){width=960}\n:::\n:::\n\n## Interaction term in model {.smaller}\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlim_fit &lt;- lm(Limit ~ Rating  + Income  * Income,\n      data = Credit)\n:::"
  },
  {
    "objectID": "slides/14-mlr.html#interaction-terms-3",
    "href": "slides/14-mlr.html#interaction-terms-3",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/14-mlr.html#interest-rate-vs.-annual-income-1",
    "href": "slides/14-mlr.html#interest-rate-vs.-annual-income-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/14-mlr.html#interaction-term-in-model",
    "href": "slides/14-mlr.html#interaction-term-in-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_int_fit &lt;- lm(Limit ~ Rating  + Income  * Income,\n      data = Credit)"
  },
  {
    "objectID": "slides/14-mlr.html#interpreting-interaction-terms",
    "href": "slides/14-mlr.html#interpreting-interaction-terms",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the credit limit depends on the borrowers credit rating"
  },
  {
    "objectID": "slides/14-mlr.html#data-manipulation-3-create-interaction-variables",
    "href": "slides/14-mlr.html#data-manipulation-3-create-interaction-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 3: Create interaction variables",
    "text": "Data manipulation 3: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * Income_cent is an implicit data manipulation step as well (R will do this for you)\n\n\nRows: 400\nColumns: 3\n$ `(Intercept)` &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ Rating        &lt;dbl&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 1…\n$ Income        &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.99…"
  },
  {
    "objectID": "slides/14-mlr.html#recap",
    "href": "slides/14-mlr.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nIntroduced multiple linear regression\nInterpreted coefficients in the multiple linear regression model\nCalculated predictions and associated intervals for multiple linear regression models\nUsed interaction terms\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/14-mlr.html#looking-backward",
    "href": "slides/14-mlr.html#looking-backward",
    "title": "Multiple linear regression (MLR)",
    "section": "Looking backward",
    "text": "Looking backward\nData manipulation, with dplyr (from tidyverse):"
  },
  {
    "objectID": "prepare/mlr-prep.html",
    "href": "prepare/mlr-prep.html",
    "title": "September 27 lecture",
    "section": "",
    "text": "Stat2: Topic 3.1 Multiple Linear Regression Model\nStat2: Topic 4.5 Coding Categoricadl Predictors"
  },
  {
    "objectID": "prepare/mlr-prep.html#assigned-reading",
    "href": "prepare/mlr-prep.html#assigned-reading",
    "title": "September 27 lecture",
    "section": "",
    "text": "Stat2: Topic 3.1 Multiple Linear Regression Model\nStat2: Topic 4.5 Coding Categoricadl Predictors"
  },
  {
    "objectID": "hw/hw-05.html#introduction",
    "href": "hw/hw-05.html#introduction",
    "title": "HW 05: Candy Competition",
    "section": "Introduction",
    "text": "Introduction\nIn today’s lab you will analyze data about candy that was collected from an online experiment conducted at FiveThirtyEight.\n\nLearning goals\nBy the end of the lab you will be able to\n\ndescribe the components of a recipe\nfit a model using recipes\ncompare models\ncontinue developing a collaborative workflow with your teammates"
  },
  {
    "objectID": "hw/hw-05.html#workflow-using-git-and-github-as-a-team",
    "href": "hw/hw-05.html#workflow-using-git-and-github-as-a-team",
    "title": "HW 05: Candy Competition",
    "section": "Workflow: Using Git and GitHub as a team",
    "text": "Workflow: Using Git and GitHub as a team\n\n\n\n\n\n\nImportant\n\n\n\nThere are no Team Member markers in this lab; however, you should use a similar workflow as in Lab 04. Only one person should type in the group’s .qmd file at a time. Once that person has finished typing the group’s responses, they should render, commit, and push the changes to GitHub. All other teammates can pull to see the updates in RStudio.\nEvery teammate must have at least one commit in the lab. Everyone is expected to contribute to discussion even when they are not typing."
  },
  {
    "objectID": "hw/hw-05.html#data-candy",
    "href": "hw/hw-05.html#data-candy",
    "title": "HW 05: Candy Competition",
    "section": "Data: Candy",
    "text": "Data: Candy\nThe data from this lab comes from the the article FiveThirtyEight The Ultimate Halloween Candy Power Ranking by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy matchups (e.g. Reeses vs. Skittles). Click here to check out some of the match ups.\nThe data set contains the characteristics and win percentage from 85 candies in the experiment. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nchocolate\nDoes it contain chocolate?\n\n\nfruity\nIs it fruit flavored?\n\n\ncaramel\nIs there caramel in the candy?\n\n\npeanutalmondy\nDoes it contain peanuts, peanut butter or almonds?\n\n\nnougat\nDoes it contain nougat?\n\n\ncrispedricewafer\nDoes it contain crisped rice, wafers, or a cookie component?\n\n\nhard\nIs it a hard candy?\n\n\nbar\nIs it a candy bar?\n\n\npluribus\nIs it one of many candies in a bag or box?\n\n\nsugarpercent\nThe percentile of sugar it falls under within the data set. Values 0 - 1.\n\n\npricepercent\nThe unit price percentile compared to the rest of the set. Values 0 - 1.\n\n\nwinpercent\nThe overall win percentage according to 269,000 matchups. Values 0 - 100.\n\n\n\nUse the code below to get a glimpse of the candy_rankings data frame in the fivethirtyeight R package.\n\nglimpse(candy_rankings)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter…\n$ chocolate        &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31…\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "hw/hw-05.html#exercises",
    "href": "hw/hw-05.html#exercises",
    "title": "HW 05: Candy Competition",
    "section": "Exercises",
    "text": "Exercises\nThe goal of this analysis is to use multiple linear regression to understand the factors that make a good candy.\n\nExercise 1\n\nUse ggplot to make a graph of your choice exploring the relationship between at least 3 variables in the data set.\nWrite two observations from your graph.\n\n\n\nExercise 2\nSplit the data into training (80%) and testing sets (20%). Call the training set candy_train and the testing set candy_test. Use a seed of 2.\n\n\nExercise 3\nBelow is a recipe for a model that uses the characteristics of candy to understand variability in the win percentage. The lines of the recipe code are labeled Line 1 - Line 9. Describe what each line of code does. The explanation should be written comprehensively and specifically enough that someone could replicate the data manipulation steps based on your description.\nFor example, if a line of code is step_center(X), a comprehensive and specific explanation something similar to the following: “This line of code mean centers the variable \\(X\\) by subtracting \\(\\bar{X}\\) from each value of \\(X\\) in the training data.”\n\n\n\n\n\n\nTip\n\n\n\nUse the Recipes Function Reference page as a resource to learn more about the step_ functions.\n\n\n\n#Line 1\ncandy_rec &lt;- recipe(winpercent ~ ., data = candy_train) |&gt; \n#Line 2\n  update_role(competitorname, new_role = \"ID\") |&gt; \n# Line 3\n  step_cut(sugarpercent, breaks = c(0, 0.25, 0.5, 0.75, 1)) |&gt; \n#Line 4\n  step_mutate(pricepercent = pricepercent * 100) |&gt; \n#Line 5\n  step_dummy(all_nominal_predictors()) |&gt; \n#Line 6\n  step_interact(terms =~ pricepercent:chocolate) |&gt; \n#Line 7\n  step_interact(terms =~ peanutyalmondy:chocolate) |&gt; \n#Line 8\n  step_rm(fruity, caramel, hard, pluribus, bar, nougat, crispedricewafer) |&gt; \n#Line 9\n  step_zv(all_predictors()) \n\n\n\nExercise 4\nFill in the code to use prep and bake for a preview of what will happen when the recipe in Exercise 3 is applied.\n\ncandy_rec |&gt;\n  prep() |&gt;\n  bake(_____) |&gt;\n  glimpse()\n\nHow many terms (not including the intercept) will be in the model produced by this recipe?\n\n\nExercise 5\nFill in the code to specify model, build the model workflow using the recipe in Exercise 2, and fit the model to the training data. Then, neatly display the model using 3 digits.\n\n#specify the model\ncandy_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n#build model workflow\ncandy_workflow &lt;- workflow() |&gt;\n  add_model(_____) |&gt;\n  add_recipe(_____) \n\n# fit the model \ncandy_fit &lt;- candy_workflow |&gt;\n  fit(data = _____)\n\n\n\nExercise 6\nInterpret the following in the context of the data:\n\nIntercept\nCoefficient of sugarpercent_X.0.75.1.\nCoefficient of pricepercent_x_chocolateTRUE\nEffect of pricepercent for chocolate candy\n\n\n\nExercise 7\nLet’s consider another model. Use the recipe workflow to fit a new model that meets the following criteria:\n\nIncludes variables chocolate, pricepercent, crispedricewafer, peanutyalmondy, sugarpercent\nUpdate pricepercent so it ranges from 0 to 100 (instead of 0 to 1)\nMakes sugarpercent a factor where the levels equal the four quartiles: 0 - 0.25, 0.25 - 0.50, 0.50 - 0.75, 0.75 - 1\nIncludes the interaction between pricepercent and peanutyalmondy\n\nNeatly display the model using 3 digits.\n\n\n\n\n\n\nTip\n\n\n\nSee the Function Reference page on recipes.tidymodels.org to find the appropriate recipe functions.\n\n\n\n\nExercise 8\n\nConsider the model from Exercise 5 “Model 1” and the model fit in Exercise 7 “Model 2”. Use the glance() function to calculate \\(R^2\\) for both of these models.\nWhich model would you choose based on \\(R^2\\)? Briefly explain your choice.\n\n\n\nExercise 9\nWe will use RMSE to evalulate the predictive performance of each model on the testing data.\n\nUse the code below to calculate predicted values and RMSE for Model 1 on the testing data. Then get the RMSE for Model 2 on the testing data.\n\n\npredict_test1 &lt;- predict(candy_fit, candy_test) |&gt; \n  bind_cols(candy_test)\n\nrmse1 &lt;- rmse(predict_test1, truth = winpercent, estimate = .pred)\n\n\nWhich model would you choose based on RMSE on the testing data? Briefly explain your choice.\n\n\n\nExercise 10\n\nUse the model you selected in Exercise 9 to describe what generally makes a good candy, i.e., one with a high win percentage."
  },
  {
    "objectID": "hw/hw-05.html#submission",
    "href": "hw/hw-05.html#submission",
    "title": "HW 05: Candy Competition",
    "section": "Submission",
    "text": "Submission\n\n\n\n\n\n\nWarning\n\n\n\nBefore you wrap up the assignment, make sure all documents are updated on your GitHub repo. We will be checking these to make sure you have been practicing how to commit and push changes.\nRemember – you must turn in a PDF file to the Gradescope page before the submission deadline for full credit.\n\n\nOne team member submit the assignment:\n\nGo to http://www.gradescope.com and log in using your NetID credentials.\nClick on your STA 210 course.\nClick on the assignment, and you’ll be prompted to submit it.\nSelect all team members’ names, so they receive credit on the assignment. Click here for video on adding team members to assignment on Gradescope.\nMark the pages associated with each exercise. All of the pages of your lab should be associated with at least one question (i.e., should be “checked”).\nSelect the first page of your PDF submission to be associated with the “Workflow & formatting” section."
  },
  {
    "objectID": "hw/hw-05.html#grading",
    "href": "hw/hw-05.html#grading",
    "title": "HW 05: County Health",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n3\n\n\nEx 2\n3\n\n\nEx 3\n5\n\n\nEx 4\n8\n\n\nEx 5\n4\n\n\nEx 6\n3\n\n\nEx 7\n5\n\n\nEx 8\n6\n\n\nEx 9\n4\n\n\nEx 10\n4\n\n\nWorkflow & formatting\n51"
  },
  {
    "objectID": "hw/hw-05-old.html",
    "href": "hw/hw-05-old.html",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "",
    "text": "Important\n\n\n\nDue Wednesday, November 29 at 11:59pm"
  },
  {
    "objectID": "hw/hw-05-old.html#learning-goals",
    "href": "hw/hw-05-old.html#learning-goals",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Learning goals",
    "text": "Learning goals\nIn this assignment, you will…\n\nFit and interpret multinomial logistic regression models.\nEvaluate conditions for multinomial logistic regression\nContinue developing a workflow for reproducible data analysis."
  },
  {
    "objectID": "hw/hw-05-old.html#getting-started",
    "href": "hw/hw-05-old.html#getting-started",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Getting started",
    "text": "Getting started\nThe repo for this assignment is available on GitHub at github.com/sta210-fa23 and starts with the prefix hw-05. See Lab 01 for more detailed instructions on getting started."
  },
  {
    "objectID": "hw/hw-05-old.html#packages",
    "href": "hw/hw-05-old.html#packages",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Packages",
    "text": "Packages\nThe following packages will be used for this assignment.\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(patchwork)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-05-old.html#the-data",
    "href": "hw/hw-05-old.html#the-data",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "The Data",
    "text": "The Data\nFor this assignment, we will analyze data from the eye witness identification experiment in Carlson and Carlson (2014). In this experiment, participants were asked to watch a video of a mock crime (from the first person perspective), spend a few minutes completing a random task, and then identify the perpetrator of the mock crime from a line up shown on the screen. Every lineup in this analysis included the true perpetrator from the video. After viewing the line-up , each participant could make one of the following decisions (id):\n\ncorrect: correctly identified the true perpetrator\nfoil: incorrectly identified the “foil”, i.e. a person who looks very similar to the perpetrator\nreject: incorrectly concluded the true perpetrator is not in the lineup\n\nThe main objective of the analysis is to understand how different conditions of the mock crime and suspect lineup affect the decision made by the participant. We will consider the following conditions to describe the decisions:\n\nlineup: How potential suspects are shown to the participants\n\nSimultaneous Lineup: Participants were shown photos of all 6 potential suspects at the same time and were required to make a single decision (identify someone from the lineup or reject the lineup).\nSequential 5 Lineup: Photos of the 6 suspects were shown one at a time. The participant was required to make a decision (choose or don’t choose) as each photo was shown. Once a decision was made, participants were not allowed to reexamine a photo. If the participant made an identification, the remaining photos were not shown. In each of these lineups the true perpetrator was always the 5th photo in the lineup.\n\nweapon: Whether or not a weapon was present in the video of the mock crime.\nfeature: Whether or not the perpetrator had a distinctive marking on his face. In this experiment, the distinctive feature was a large “N” sticker on one cheek. (The letter “N” was chosen to represent the first author’s alma mater - University of Nebraska.)\n\nThe data may be found in eyewitness.csv in the data folder.\n\new &lt;- read_csv(\"data/eyewitness.csv\")\new &lt;- ew |&gt;\n  mutate(id = as_factor(id))"
  },
  {
    "objectID": "hw/hw-05-old.html#exercise-1",
    "href": "hw/hw-05-old.html#exercise-1",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nLet’s begin by doing some exploratory data analysis. The univariate plots for each of the predictor variables and the response variable are shown below.\n\n\n\n\n\n\n\n\n\nComplete the exploratory data analysis by creating the plots and/or summary statistics to examine the relationship between the response variable (id) and each of the explanatory variables (lineup, weapon, and feature)."
  },
  {
    "objectID": "hw/hw-05-old.html#exercise-2",
    "href": "hw/hw-05-old.html#exercise-2",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nUsing the plots/tables from Question 1:\n\nWhat is one thing you learn about the data from the univariate plots?\nBased on the bivariate plots, do any of the predictors appear to have a significant effect on the id? Briefly explain."
  },
  {
    "objectID": "hw/hw-05-old.html#exercise-3",
    "href": "hw/hw-05-old.html#exercise-3",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nBriefly explain why you should use a multinomial logistic regression model to predict id using lineup, weapon and feature."
  },
  {
    "objectID": "hw/hw-05-old.html#exercise-4",
    "href": "hw/hw-05-old.html#exercise-4",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit the multinomial logistic model that only includes main effects. Display the model output.\n\nWhat is the baseline category for the response variable?\nInterpret the intercepts for each part of the model in terms of the odds.\nInterpret the coefficients of lineup for each part of the model in terms of the odds."
  },
  {
    "objectID": "hw/hw-05-old.html#exercise-5",
    "href": "hw/hw-05-old.html#exercise-5",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nYou want to consider all possible interaction effects for the model.\n\nUse the appropriate test to determine if there is at least one significant interaction effect.\n\n\n\nBased on your test, is there evidence of any significant interaction effects? Briefly explain.\n\n\n\n\n\n\n\nImportant\n\n\n\nRegardless of your answer to Question 5, use the model that includes the interaction terms for the remainder of the assignment."
  },
  {
    "objectID": "hw/hw-05-old.html#exercise-6",
    "href": "hw/hw-05-old.html#exercise-6",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nAccording to the model,\n\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the log-odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nIf there was no weapon but the perpetrator had a distinctive feature in the mock crime, how do the odds of reject vs. a correct ID change when there is a simultaneous lineup vs. a sequential lineup?\nWhich group of participants (i.e. which set of experimental conditions) is described by the intercept?"
  },
  {
    "objectID": "hw/hw-05-old.html#exercise-7",
    "href": "hw/hw-05-old.html#exercise-7",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nAre the conditions for inference met? List the conditions, and, if relevant, create visualizations to check the conditions and evaluate whether each condition is met. Include an assessment about each condition and a brief explanation about your conclusion."
  },
  {
    "objectID": "hw/hw-05-old.html#exercise-8",
    "href": "hw/hw-05-old.html#exercise-8",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nUse the model to predict the decision made by each participant. Make a table of the predicted vs.the actual decisions.\n\nBriefly describe how the predicted decision is determined for each participant.\nWhat is the misclassification rate?"
  },
  {
    "objectID": "hw/hw-05-old.html#footnotes",
    "href": "hw/hw-05-old.html#footnotes",
    "title": "HW 05: Multinomial Logistic Regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nCarlson, C. A., & Carlson, M. A. (2014). An evaluation of lineup presentation, weapon presence, and a distinctive feature using ROC analysis. Journal of Applied Research in Memory and Cognition, 3(2), 45-53↩︎\nThe “Workflow & formatting” grade is to assess the reproducible workflow and document format. This includes having at least 3 informative commit messages, a neatly organized document with readable code and your name and the date in the YAML.↩︎↩︎"
  },
  {
    "objectID": "hw/hw-05.html#data-county-health",
    "href": "hw/hw-05.html#data-county-health",
    "title": "HW 05: County Health",
    "section": "Data: County Health",
    "text": "Data: County Health\nThe data set for this homework is from the Stat2Data R package which is the companion package for this course’s textbook. It is the same data set that we used in AE-08. The data was originally generated by the American Medical Association and concerns the availability of health care in counties in the United States. You can find information here by searching for the County Health Resources dataset.\n\ndata(\"CountyHealth\") # Loads the data from the package\n\nIt is relatively easy to count the number of hospitals a county has, whereas counting the number of doctors is much more difficult. We’d like to build a linear model to predict the number of doctors, contained in the variable MDs, from the number of hospitals, Hospitals and the number of beds, Beds."
  },
  {
    "objectID": "hw/hw-05.html#exercise-9",
    "href": "hw/hw-05.html#exercise-9",
    "title": "HW 05: County Health",
    "section": "Exercise 9",
    "text": "Exercise 9\nHow much of the variation in MDs is explained by the model you just fit? Which model would you say is the “best”, given what we’ve learned through the first lecture on multiple linear regression."
  },
  {
    "objectID": "hw/hw-05.html#exercise-10",
    "href": "hw/hw-05.html#exercise-10",
    "title": "HW 05: County Health",
    "section": "Exercise 10",
    "text": "Exercise 10\nUsing the “best” model, predict the average number of doctors in a metro area with 1,000 beds and 4 hospitals. Report a 95% prediction interval and interpret your results in context."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html",
    "href": "hw/forStudents/hw-04/hw-04.html",
    "title": "HW 04: Alumni jobs",
    "section": "",
    "text": "Important\n\n\n\nDue: Friday, September 27, 11:59pm\nIn this homework, you’ll use simple linear regression to analyze the relationship between the median early career salary and percent of alumni who perceive their job as making the world a better place for colleges and universities in the United States."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html#exercise-1",
    "href": "hw/forStudents/hw-04/hw-04.html#exercise-1",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a linear model that can be used to predict the typical (median) early career pay based on the percent of STEM degrees. Display the model output using 3 digits for numerical values."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html#exercise-2",
    "href": "hw/forStudents/hw-04/hw-04.html#exercise-2",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 2",
    "text": "Exercise 2\nCalculate the predicted values and residuals from your model and save these results in a data frame. Print the first five rows of the new data frame."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html#exercise-3",
    "href": "hw/forStudents/hw-04/hw-04.html#exercise-3",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 3",
    "text": "Exercise 3\nBefore using the model for prediction, let’s check the model conditions. To figure out how to do so, we type the following prompt into ChatGPT (“an AI powered language model”) “How do you check the model conditions for simple linear regression?”\nThe response from ChatGPT shown below would receive partial credit. For each condition, state whether it is correct or incorrect/incomplete. If it incorrect/incomplete, state what is wrong or missing, then write the correct response. Please put all answers after the ChatGPT text.\n\nIn simple linear regression, you’re trying to establish a linear relationship between two variables: a dependent variable (Y) and an independent variable (X). The model conditions or assumptions for simple linear regression are important to check to ensure that your model is valid and reliable. Here are the key assumptions and how to check them:\n1. Linearity Assumption:\n\nAssumption: The relationship between the independent variable (X) and the dependent variable (Y) is linear.\nHow to Check: Create a scatterplot of Y against X. If the points roughly form a straight line, the linearity assumption is likely met.\n\n2. Independence Assumption:\n\nAssumption: The residuals (the differences between observed Y and predicted Y) are independent of each other.\nHow to Check: Plot the residuals against the independent variable (X) or the predicted values. Look for patterns or trends in the residuals. There should be no clear structure or autocorrelation in the residuals.\n\n3. Homoscedasticity [Constant variance] Assumption:\n\nAssumption: The variance of the residuals is constant across all levels of the independent variable.\nHow to Check: Plot the residuals against the predicted values or the independent variable (X). The spread of the residuals should remain roughly constant across the range of X.\n\n4. Normality Assumption:\n\nAssumption: The residuals follow a normal distribution.\nHow to Check: Create a histogram or a normal probability plot (Q-Q plot) of the residuals. If the residuals are approximately normally distributed, they should follow a bell-shaped curve in the histogram and lie along a straight line in the Q-Q plot.\n\nIt’s important to note that these assumptions may not always hold perfectly in real-world data, and minor violations may not necessarily invalidate your model. However, if you detect significant violations of these assumptions, you may need to consider alternative modeling techniques or transformations of your data to address the issues."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html#exercise-4",
    "href": "hw/forStudents/hw-04/hw-04.html#exercise-4",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 4",
    "text": "Exercise 4\nDetermine whether the model conditions hold.\n\n\n\n\n\n\nImportant\n\n\n\nRegardless of your previous answer, you can proceed assuming all the model conditions are satisfied."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html#exercise-5",
    "href": "hw/forStudents/hw-04/hw-04.html#exercise-5",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 5",
    "text": "Exercise 5\n\n31 percent of the alumni at Johns Hopkins University received a STEM degree. Calculate the predicted early career pay at this university and the appropriate 95% interval. Interpret the interval in the context of the data.\nThe actual early career pay at this university is $67,200. Is this what you expected based on your analysis? Briefly explain your reasoning."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html#exercise-6",
    "href": "hw/forStudents/hw-04/hw-04.html#exercise-6",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 6",
    "text": "Exercise 6\nNext, let’s consider how well the model fits the relationship between the early career pay and percent with a STEM degree.\n\nCalculate \\(R^2\\) and interpret it in the context of the data.\nCalculate RMSE and interpret it in the context of the data."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html#exercise-7",
    "href": "hw/forStudents/hw-04/hw-04.html#exercise-7",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 7",
    "text": "Exercise 7\nDo you think the model is useful for understanding and predicting the typical early career pay for alumni at a university? Briefly explain your reasoning."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html#exercise-8",
    "href": "hw/forStudents/hw-04/hw-04.html#exercise-8",
    "title": "HW 04: Alumni jobs",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit another model which predicts the typical early career pay using only the percentage of alumni that believe their career will make the world a better place.\n\nNeatly display this model with 3 digits.\nWhich model do you think is better - this model or the model fit in Exercise 1? Explain your response showing any analysis used to make the decision."
  },
  {
    "objectID": "hw/forStudents/hw-04/hw-04.html#footnotes",
    "href": "hw/forStudents/hw-04/hw-04.html#footnotes",
    "title": "HW 04: Alumni jobs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow and overall readability.↩︎"
  },
  {
    "objectID": "ae/ae-10-mlr.html",
    "href": "ae/ae-10-mlr.html",
    "title": "AE 10: Multiple Linear Regression",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-10”.\nGo to the Canvas and locate your AE-10 assignment to get started.\nUpload the ae-10.qmd file into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment."
  },
  {
    "objectID": "ae/ae-10-mlr.html#packages-data",
    "href": "ae/ae-10-mlr.html#packages-data",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(ISLR2)\nlibrary(GGally)\nlibrary(yardstick)\n\nThe data for this AE is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers. We will focus on the following variables:\nPredictors\n\nIncome: Annual income (in 1000’s of US dollars)\nRating: Credit Rating\n\nResponse\n\nLimit: Credit limit"
  },
  {
    "objectID": "ae/ae-10-mlr.html#analysis-goal",
    "href": "ae/ae-10-mlr.html#analysis-goal",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this analysis are to fit a linear regression model that has the following predictors:\n\nIncome\nRating\nAn interaction term between the two"
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-1",
    "href": "ae/ae-10-mlr.html#exercise-1",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nUse the function ggpairs from the GGally package (already loaded) to create a matrix of plots and correlations for our three variables of interest. Note that you will have to use select to select the four columns you are interested in. Which variable do you think will be the best predictor of Limit?"
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-2",
    "href": "ae/ae-10-mlr.html#exercise-2",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nFit a linear model with just Income as the predictor and get the p-value associated with it’s coefficient. Is it statistically significant?"
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-3",
    "href": "ae/ae-10-mlr.html#exercise-3",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a linear model with just Rating as the predictor and get the p-value associated with it’s coefficient. Is it statistically significant?"
  },
  {
    "objectID": "ae/ae-10-mlr.html#exericse-4",
    "href": "ae/ae-10-mlr.html#exericse-4",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exericse 4",
    "text": "Exericse 4\nFit a model with all three annual_income, debt_to_income, and verified_income as predictors (no interaction term for now). Are the coefficients for verified_income the same as they were when only verified_income was a predictor? Why do you think this is? Be prepared to discuss your answer with the class."
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-5",
    "href": "ae/ae-10-mlr.html#exercise-5",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nInterpret all coefficients in the model."
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-6-optional",
    "href": "ae/ae-10-mlr.html#exercise-6-optional",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 6 (Optional)",
    "text": "Exercise 6 (Optional)\nThe following code will fit a polynomial of order 2 to your data and then output the \\(R^2\\) value. Run this code repeatedly increase the order of the polynomial. Doese your \\(R^2\\) value ever decrease? If not, why do think this is? Does it imply that our model is ALWAYS getting “better”?\n\npoly_model &lt;- lm(interest_rate ~ poly(debt_to_income, degree = 2, raw = TRUE),\n                 data = loan50)\n\npoly_aug &lt;- augment(poly_model)\n\nrsq(poly_aug, truth = interest_rate, estimate = .fitted)\n\n# A tibble: 1 × 3\n  .metric .estimator .estimate\n  &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;\n1 rsq     standard      0.0770"
  },
  {
    "objectID": "ae/ae-10-mlr.html#to-submit-the-ae",
    "href": "ae/ae-10-mlr.html#to-submit-the-ae",
    "title": "AE 10: Multiple Linear Regression",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "slides/14-mlr.knit.html",
    "href": "slides/14-mlr.knit.html",
    "title": "Multiple linear regression (MLR)",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/14-mlr.knit.html#computational-setup",
    "href": "slides/14-mlr.knit.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "",
    "text": "# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/14-mlr.knit.html#data-peer-to-peer-lender",
    "href": "slides/14-mlr.knit.html#data-peer-to-peer-lender",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n\n\n\nannual_income\ndebt_to_income\nverified_income\ninterest_rate\n\n\n\n\n59000\n0.5575254\nNot Verified\n10.90\n\n\n60000\n1.3056833\nNot Verified\n9.92\n\n\n75000\n1.0562800\nVerified\n26.30\n\n\n75000\n0.5743467\nNot Verified\n9.92\n\n\n254000\n0.2381496\nNot Verified\n9.43\n\n\n67000\n1.0770448\nSource Verified\n9.92\n\n\n28800\n0.0997222\nSource Verified\n17.09\n\n\n80000\n0.3509125\nNot Verified\n6.08\n\n\n34000\n0.6975000\nNot Verified\n7.97\n\n\n80000\n0.1668542\nSource Verified\n12.62"
  },
  {
    "objectID": "slides/14-mlr.knit.html#variables",
    "href": "slides/14-mlr.knit.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nOutcome: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/14-mlr.knit.html#outcome-interest_rate",
    "href": "slides/14-mlr.knit.html#outcome-interest_rate",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome: interest_rate",
    "text": "Outcome: interest_rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n5.31\n7.96\n9.93\n13.715\n26.3\n11.5672\n5.052115\n50\n0"
  },
  {
    "objectID": "slides/14-mlr.knit.html#predictors",
    "href": "slides/14-mlr.knit.html#predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/14-mlr.knit.html#data-manipulation-1-rescale-income",
    "href": "slides/14-mlr.knit.html#data-manipulation-1-rescale-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_k = annual_income / 1000)\n\ngf_histogram(~annual_income_k, data = loan50, binwidth = 20) |&gt; \n  gf_labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/14-mlr.knit.html#outcome-vs.-predictors",
    "href": "slides/14-mlr.knit.html#outcome-vs.-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/14-mlr.knit.html#single-vs.-multiple-predictors",
    "href": "slides/14-mlr.knit.html#single-vs.-multiple-predictors",
    "title": "Multiple linear regression (MLR)",
    "section": "Single vs. multiple predictors",
    "text": "Single vs. multiple predictors\nSo far we’ve used a single predictor variable to understand variation in a quantitative response variable\n. . .\nNow we want to use multiple predictor variables to understand variation in a quantitative response variable"
  },
  {
    "objectID": "slides/14-mlr.knit.html#multiple-linear-regression-mlr",
    "href": "slides/14-mlr.knit.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on the analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{interest_rate}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{debt_to_income} \\\\ & + \\hat{\\beta}_2 \\text{verified_income} \\\\ &+ \\hat{\\beta}_3 \\text{annual_income_k}\n\\end{aligned}\n\\]\nSimilar to simple linear regression, this model assumes that at each combination of the predictor variables, the values interest_rate follow a Normal distribution."
  },
  {
    "objectID": "slides/14-mlr.knit.html#multiple-linear-regression-1",
    "href": "slides/14-mlr.knit.html#multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nRecall: The simple linear regression model assumes\n\\[\nY|X\\sim N(\\beta_0 + \\beta_1 X, \\sigma_{\\epsilon}^2)\n\\]\n. . .\nSimilarly: The multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/14-mlr.knit.html#multiple-linear-regression-2",
    "href": "slides/14-mlr.knit.html#multiple-linear-regression-2",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n. . .\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/14-mlr.knit.html#model-equation",
    "href": "slides/14-mlr.knit.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{interest_rate}} =  10.726 &+0.671 \\times \\text{debt_to_income}\\\\\n&+ 2.211 \\times \\text{source_verified}\\\\  \n&+ 6.880 \\times \\text{verified}\\\\\n& -0.021 \\times \\text{annual_income_k}\n\\end{align}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nWe will talk about why there are two terms in the model for verified_income shortly!"
  },
  {
    "objectID": "slides/14-mlr.knit.html#interpreting-hatbeta_j",
    "href": "slides/14-mlr.knit.html#interpreting-hatbeta_j",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting \\(\\hat{\\beta}_j\\)",
    "text": "Interpreting \\(\\hat{\\beta}_j\\)\n\nThe estimated coefficient \\(\\hat{\\beta}_j\\) is the expected change in the mean of \\(y\\) when \\(x_j\\) increases by one unit, holding the values of all other predictor variables constant.\n\n. . .\n\nExample: The estimated coefficient for debt_to_income is 0.671. This means for each point in an borrower’s debt to income ratio, the interest rate on the loan is expected to be greater by 0.671%, holding annual income and income verification constant."
  },
  {
    "objectID": "slides/14-mlr.knit.html#prediction",
    "href": "slides/14-mlr.knit.html#prediction",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction",
    "text": "Prediction\n\nWhat is the predicted interest rate for an borrower with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000?\n\n\n\n10.726 + 0.671 * 0.558 + 2.211 * 0 + 6.880 * 0 - 0.021 * 59\n\n[1] 9.861418\n\n\n. . .\nThe predicted interest rate for an borrower with with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000 is 9.86%."
  },
  {
    "objectID": "slides/14-mlr.knit.html#prediction-revisited",
    "href": "slides/14-mlr.knit.html#prediction-revisited",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction, revisited",
    "text": "Prediction, revisited\nJust like with simple linear regression, we can use the predict() function in R to calculate the appropriate intervals for our predicted values:\n\nnew_borrower &lt;- tibble(\n  debt_to_income  = 0.558, \n  verified_income = \"Not Verified\", \n  annual_income_k = 59\n)\n\npredict(int_fit, new_borrower)\n\n       1 \n9.890888 \n\n\n\n\n\n\n\n\nNote\n\n\n\nDifference in predicted value due to rounding the coefficients on the previous slide."
  },
  {
    "objectID": "slides/14-mlr.knit.html#confidence-interval-for-hatmu_y",
    "href": "slides/14-mlr.knit.html#confidence-interval-for-hatmu_y",
    "title": "Multiple linear regression (MLR)",
    "section": "Confidence interval for \\(\\hat{\\mu}_y\\)",
    "text": "Confidence interval for \\(\\hat{\\mu}_y\\)\n\nCalculate a 90% confidence interval for the estimated mean interest rate for borrowers with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000.\n\n\n\npredict(int_fit, new_borrower, interval = \"confidence\", level = 0.90)\n\n       fit      lwr      upr\n1 9.890888 8.159246 11.62253"
  },
  {
    "objectID": "slides/14-mlr.knit.html#prediction-interval-for-haty",
    "href": "slides/14-mlr.knit.html#prediction-interval-for-haty",
    "title": "Multiple linear regression (MLR)",
    "section": "Prediction interval for \\(\\hat{y}\\)",
    "text": "Prediction interval for \\(\\hat{y}\\)\n\nCalculate a 90% confidence interval for the predicted interest rate for an individual applicant with an debt-to-income ratio of 0.558, whose income is not verified, and who has an annual income of $59,000.\n\n\n\npredict(int_fit, new_borrower, interval = \"prediction\", level = 0.90)\n\n       fit      lwr      upr\n1 9.890888 2.180521 17.60126"
  },
  {
    "objectID": "slides/14-mlr.knit.html#cautions",
    "href": "slides/14-mlr.knit.html#cautions",
    "title": "Multiple linear regression (MLR)",
    "section": "Cautions",
    "text": "Cautions\n\nDo not extrapolate! Because there are multiple predictor variables, there is the potential to extrapolate in many directions\nThe multiple regression model only shows association, not causality\n\nTo show causality, you must have a carefully designed experiment or carefully account for confounding variables in an observational study"
  },
  {
    "objectID": "slides/14-mlr.knit.html#interpreting-results",
    "href": "slides/14-mlr.knit.html#interpreting-results",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting results",
    "text": "Interpreting results\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_k\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n. . .\n\n\nDescribe the subset of borrowers who are expected to get an interest rate of 10.726% based on our model. Is this interpretation meaningful? Why or why not?"
  },
  {
    "objectID": "slides/14-mlr.knit.html#indicator-variables-1",
    "href": "slides/14-mlr.knit.html#indicator-variables-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category"
  },
  {
    "objectID": "slides/14-mlr.knit.html#data-manipulation-2-create-indicator-variables-for-verified_income",
    "href": "slides/14-mlr.knit.html#data-manipulation-2-create-indicator-variables-for-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 2: Create indicator variables for verified_income",
    "text": "Data manipulation 2: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\n. . .\n\n\n\n\n\nverified_income\nnot_verified\nsource_verified\nverified\n\n\n\n\nNot Verified\n1\n0\n0\n\n\nVerified\n0\n0\n1\n\n\nSource Verified\n0\n1\n0"
  },
  {
    "objectID": "slides/14-mlr.knit.html#indicators-in-the-model",
    "href": "slides/14-mlr.knit.html#indicators-in-the-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe reference level is the category that doesn’t have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding.\nlm will do this for you.\n\n. . .\n\nloan50 |&gt;\n  select(verified_income, source_verified, verified) |&gt;\n  slice(1, 3, 6) |&gt; \n  kable()\n\n\n\n\nverified_income\nsource_verified\nverified\n\n\n\n\nNot Verified\n0\n0\n\n\nVerified\n0\n1\n\n\nSource Verified\n1\n0"
  },
  {
    "objectID": "slides/14-mlr.knit.html#interpreting-verified_income",
    "href": "slides/14-mlr.knit.html#interpreting-verified_income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n10.726\n1.507\n7.116\n0.000\n7.690\n13.762\n\n\ndebt_to_income\n0.671\n0.676\n0.993\n0.326\n-0.690\n2.033\n\n\nverified_incomeSource Verified\n2.211\n1.399\n1.581\n0.121\n-0.606\n5.028\n\n\nverified_incomeVerified\n6.880\n1.801\n3.820\n0.000\n3.253\n10.508\n\n\nannual_income_k\n-0.021\n0.011\n-1.804\n0.078\n-0.043\n0.002\n\n\n\n\n\n\n\n. . .\n\n\nThe reference level is Not verified.\nPeople with source verified income are expected to take a loan with an interest rate that is 2.211% higher, on average, than the rate on loans to those whose income is not verified, holding all else constant.\n\n\n. . .\n\nInterpret the coefficient of Verified in the context of the data."
  },
  {
    "objectID": "slides/14-mlr.knit.html#interaction-terms-1",
    "href": "slides/14-mlr.knit.html#interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/14-mlr.knit.html#interest-rate-vs.-annual-income",
    "href": "slides/14-mlr.knit.html#interest-rate-vs.-annual-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/14-mlr.knit.html#interaction-terms-3",
    "href": "slides/14-mlr.knit.html#interaction-terms-3",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model."
  },
  {
    "objectID": "slides/14-mlr.knit.html#interest-rate-vs.-annual-income-1",
    "href": "slides/14-mlr.knit.html#interest-rate-vs.-annual-income-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification."
  },
  {
    "objectID": "slides/14-mlr.knit.html#interaction-term-in-model",
    "href": "slides/14-mlr.knit.html#interaction-term-in-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction term in model",
    "text": "Interaction term in model\n\nint_int_fit &lt;- lm(interest_rate ~ debt_to_income + verified_income + annual_income_k + verified_income * annual_income_k,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.560\n2.034\n4.700\n0.000\n\n\ndebt_to_income\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n3.577\n2.539\n1.409\n0.166\n\n\nverified_incomeVerified\n9.923\n3.654\n2.716\n0.009\n\n\nannual_income_k\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_k\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_k\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/14-mlr.knit.html#interpreting-interaction-terms",
    "href": "slides/14-mlr.knit.html#interpreting-interaction-terms",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/14-mlr.knit.html#data-manipulation-3-create-interaction-variables",
    "href": "slides/14-mlr.knit.html#data-manipulation-3-create-interaction-variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation 3: Create interaction variables",
    "text": "Data manipulation 3: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_k_cent is an implicit data manipulation step as well (R will do this for you)\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                    &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, …\n$ debt_to_income                                   &lt;dbl&gt; 0.55752542, 1.3056833…\n$ annual_income_k                                  &lt;dbl&gt; 59.0, 60.0, 75.0, 75.…\n$ `verified_incomeNot Verified`                    &lt;dbl&gt; 1, 1, 0, 1, 1, 0, 0, …\n$ `verified_incomeSource Verified`                 &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, …\n$ verified_incomeVerified                          &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, …\n$ `annual_income_k:verified_incomeNot Verified`    &lt;dbl&gt; 59, 60, 0, 75, 254, 0…\n$ `annual_income_k:verified_incomeSource Verified` &lt;dbl&gt; 0.0, 0.0, 0.0, 0.0, 0…\n$ `annual_income_k:verified_incomeVerified`        &lt;dbl&gt; 0, 0, 75, 0, 0, 0, 0,…"
  },
  {
    "objectID": "slides/14-mlr.knit.html#recap",
    "href": "slides/14-mlr.knit.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nIntroduced multiple linear regression\nInterpreted coefficients in the multiple linear regression model\nCalculated predictions and associated intervals for multiple linear regression models\nMean-centered quantitative predictors\nUsed indicator variables for categorical predictors\nUsed interaction terms"
  },
  {
    "objectID": "slides/14-mlr.knit.html#looking-backward",
    "href": "slides/14-mlr.knit.html#looking-backward",
    "title": "Multiple linear regression (MLR)",
    "section": "Looking backward",
    "text": "Looking backward\nData manipulation, with dplyr (from tidyverse):\n\nloan50 |&gt;\n  select(interest_rate, annual_income, debt_to_income, verified_income) |&gt;\n  mutate(\n    # 1. rescale income\n    annual_income_th = annual_income / 1000,\n    # 2. mean-center quantitative predictors\n    debt_inc_cent = debt_to_income - mean(debt_to_income),\n    annual_income_th_cent = annual_income_th - mean(annual_income_th),\n    # 3. create dummy variables for verified_income\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0),\n    # 4. create interaction variables\n    `annual_income_th_cent:verified_incomeSource Verified` = annual_income_th_cent * source_verified,\n    `annual_income_th_cent:verified_incomeVerified` = annual_income_th_cent * verified\n  )"
  },
  {
    "objectID": "ae/ae-09-feature-engineering-old.html",
    "href": "ae/ae-09-feature-engineering-old.html",
    "title": "AE 09: Feature engineering with recipes",
    "section": "",
    "text": "Important\n\n\n\nGo to the course GitHub organization and locate your ae-09 repo to get started.\nRender, commit, and push your responses to GitHub by the end of class. The responses are due in your GitHub repo no later than Thursday, October 12 at 11:59pm."
  },
  {
    "objectID": "ae/ae-09-feature-engineering-old.html#packages-data",
    "href": "ae/ae-09-feature-engineering-old.html#packages-data",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(knitr)\nlibrary(openintro)\n\nThe data for this AE is from the loan50 data set in the openintro R package. We will focus on the following variables:\nPredictors\n\nannual_income: Annual income (in US dollars)\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\nResponse\n\ninterest_rate: Interest rate for the loan (0- 100)"
  },
  {
    "objectID": "ae/ae-09-feature-engineering-old.html#analysis-goal",
    "href": "ae/ae-09-feature-engineering-old.html#analysis-goal",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this analysis are to build a recipe to fit a linear regression model on the training data that has the following features:\n\nannual_income rescaled to thousands of dollars\nMean-centered quantitative variables\nIndicator (dummy) variables for the categorical predictor\nInteraction term between rescaled annual_income and verified_income\n\nand (2) use prep() and bake() to check the recipe"
  },
  {
    "objectID": "ae/ae-09-feature-engineering-old.html#testtrain-split",
    "href": "ae/ae-09-feature-engineering-old.html#testtrain-split",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Test/train split",
    "text": "Test/train split\nFill in the code to split the data into 90% training, 10% testing.\n\n\n\n\n\n\nImportant\n\n\n\nRemove #| eval: false from the code chunk.\n\n\n\nset.seed(123)\n\nloans_split &lt;- initial_split(loan50, prop = _____)\nloan_train &lt;- training(_____)\nloan_test  &lt;- _____(loan_split)"
  },
  {
    "objectID": "ae/ae-09-feature-engineering-old.html#build-a-recipe",
    "href": "ae/ae-09-feature-engineering-old.html#build-a-recipe",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Build a recipe",
    "text": "Build a recipe\n\nUse step_mutate() to create a new variable annual_income_th that is annual_income rescaled to thousands of dollars\nUse step_center() to mean-center quantitative variables\nUse step_dummy() to create indicator variables for the categorical predictor\nUse step_interact() to create interaction between annual_income_th and verified_income\n\n\n\n\n\n\n\nImportant\n\n\n\nRemove #| eval: false from the code chunk.\n\n\n\n# use original variables when specifying recipe\nloan_rec &lt;-  recipe(interest_rate ~ annaul_income + debt_to_income + verified_income, \n                    data = loan_train) |&gt;\n  # add recipe steps\n\n\nloan_rec"
  },
  {
    "objectID": "ae/ae-09-feature-engineering-old.html#check-recipe-using-prep-and-bake",
    "href": "ae/ae-09-feature-engineering-old.html#check-recipe-using-prep-and-bake",
    "title": "AE 09: Feature engineering with recipes",
    "section": "Check recipe using prep() and bake()",
    "text": "Check recipe using prep() and bake()\nRemove #| eval: false from the code chunk\n\n# determine required parameters to be estimated\nloan_rec_trained &lt;- prep(loan_rec)\n\n# apply recipe computations to data\nbake(loan_rec_trained, loan_train) |&gt;\n  glimpse()"
  },
  {
    "objectID": "ae/ae-09-feature-engineering-old.html#to-submit-the-ae",
    "href": "ae/ae-09-feature-engineering-old.html#to-submit-the-ae",
    "title": "AE 09: Feature engineering with recipes",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nPush all your work to your ae-09 repo on GitHub. (You do not submit AEs on Gradescope)."
  },
  {
    "objectID": "slides/14-mlr.html#data-manipulation-rescale-income",
    "href": "slides/14-mlr.html#data-manipulation-rescale-income",
    "title": "Multiple linear regression (MLR)",
    "section": "Data manipulation: Rescale income",
    "text": "Data manipulation: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_k = annual_income / 1000)\n\ngf_histogram(~annual_income_k, data = loan50, binwidth = 20) |&gt; \n  gf_labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/14-mlr.html#trying-something-new",
    "href": "slides/14-mlr.html#trying-something-new",
    "title": "Multiple linear regression (MLR)",
    "section": "Trying something new",
    "text": "Trying something new\n\nOpen but do not start AE-10\nNotify Dr. F when you’re ready to proceed\nThe goal is to integrate the activity into the lecture (keep track of how you like this approach and give Dr. F feedback at the end of class)\nWhile you’re waiting feel free to start on Exercise 0\n\n\n📋 AE 10 - Credit Cards"
  },
  {
    "objectID": "slides/14-mlr.html#data-credit-cards",
    "href": "slides/14-mlr.html#data-credit-cards",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Credit Cards",
    "text": "Data: Credit Cards\nThe data is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers.\n\n\nRows: 400\nColumns: 11\n$ Income    &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 7…\n$ Limit     &lt;dbl&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, …\n$ Rating    &lt;dbl&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, …\n$ Cards     &lt;dbl&gt; 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2, …\n$ Age       &lt;dbl&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, …\n$ Education &lt;dbl&gt; 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 15,…\n$ Own       &lt;fct&gt; No, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, No, Yes, No…\n$ Student   &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, No…\n$ Married   &lt;fct&gt; Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye…\n$ Region    &lt;fct&gt; South, West, West, West, South, South, East, West, South, Ea…\n$ Balance   &lt;dbl&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0,…"
  },
  {
    "objectID": "slides/14-mlr.html#outcome-limit",
    "href": "slides/14-mlr.html#outcome-limit",
    "title": "Multiple linear regression (MLR)",
    "section": "Outcome: Limit",
    "text": "Outcome: Limit\n\n\nCode\nCredit |&gt; \n  gf_density(~Limit, fill = \"steelblue\") |&gt; \n  gf_labs(title = \"Distribution of credit limit\",\n          x = \"Credit Limit\")|&gt; \n  gf_refine(scale_x_continuous(labels = dollar_format()))\n\n\n\n\n\n\n\n\n\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\n855\n3088\n4622.5\n5872.75\n13913\n4735.6\n2308.199\n400\n0"
  },
  {
    "objectID": "slides/14-mlr.html#model-fit-1",
    "href": "slides/14-mlr.html#model-fit-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit",
    "text": "Model fit\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-532.471\n24.173\n-22.028\n0.000\n\n\nRating\n14.771\n0.096\n153.124\n0.000\n\n\nIncome\n0.557\n0.423\n1.316\n0.189"
  },
  {
    "objectID": "slides/14-mlr.html#visualizing-model",
    "href": "slides/14-mlr.html#visualizing-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Model",
    "text": "Visualizing Model"
  },
  {
    "objectID": "slides/14-mlr.html#model-equation-1",
    "href": "slides/14-mlr.html#model-equation-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{Limit}} =  \\beta &+14.771 \\times \\text{Rating}\\\\\n& -0.557 \\times \\text{Income}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/14-mlr.html#visualizing-interaction-model-exagerated",
    "href": "slides/14-mlr.html#visualizing-interaction-model-exagerated",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Interaction Model Exagerated",
    "text": "Visualizing Interaction Model Exagerated"
  },
  {
    "objectID": "slides/14-mlr.html#visualizing-interaction-model",
    "href": "slides/14-mlr.html#visualizing-interaction-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Interaction Model",
    "text": "Visualizing Interaction Model"
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-0",
    "href": "ae/ae-10-mlr.html#exercise-0",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 0",
    "text": "Exercise 0\nWhat is a credit rating and what is a credit limit as it applies to a credit card? The primary credit rating in the US is called a FICO score. Based on the data, do you think that Rating corresponds to the borrower’s FICO score?"
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-1b-option",
    "href": "ae/ae-10-mlr.html#exercise-1b-option",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 1B (Option)",
    "text": "Exercise 1B (Option)\nIf you have extra time, examine the variable individually and Comment on anything that you think is relevant."
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-4",
    "href": "ae/ae-10-mlr.html#exercise-4",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nFit a model with both Income and Rating as predictors. Find a spot on the white board to write down an equation representing the fitted model. How do the coefficients and p-values of Income and Rating compare to those in the two models above? Discuss what you see and the possible reasons you see them."
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-6",
    "href": "ae/ae-10-mlr.html#exercise-6",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nWhat is the predicted credit limit for an single borrower with a credit rating of 700 and an annual income of $59,000? Include a 90% confidence interval. Hint: make a new tibble and use the predict function. How would you interpret this interval in context?"
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-7",
    "href": "ae/ae-10-mlr.html#exercise-7",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 7",
    "text": "Exercise 7\nAdd an interaction term between Rating and Income. Interpret all coefficients in context. What do you notice about the p-values now?"
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-8",
    "href": "ae/ae-10-mlr.html#exercise-8",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 8",
    "text": "Exercise 8\nWhat is the predicted credit limit for an single borrower with a credit rating of 700 and an annual income of $59,000? Include a 90% confidence interval. How does it compare to your answer to Exercise 6."
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-9-optional",
    "href": "ae/ae-10-mlr.html#exercise-9-optional",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 9 (Optional)",
    "text": "Exercise 9 (Optional)\nNote that this data set only considers borrowers who have actually been granted loans. How does this impact the generalizability of our analysis?"
  },
  {
    "objectID": "slides/14-mlr.html#visualizing-interaction-model-exaggerated",
    "href": "slides/14-mlr.html#visualizing-interaction-model-exaggerated",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Interaction Model Exaggerated",
    "text": "Visualizing Interaction Model Exaggerated"
  },
  {
    "objectID": "slides/14-mlr.html#application-exercise",
    "href": "slides/14-mlr.html#application-exercise",
    "title": "Multiple linear regression (MLR)",
    "section": "Application Exercise",
    "text": "Application Exercise\n\nComplete Activity"
  },
  {
    "objectID": "slides/14-mlr.html#model-fit-2",
    "href": "slides/14-mlr.html#model-fit-2",
    "title": "Multiple linear regression (MLR)",
    "section": "Model Fit",
    "text": "Model Fit\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-579.85561\n37.48489\n-15.46905\n0.00000\n\n\nIncome\n1.81240\n0.86962\n2.08414\n0.03779\n\n\nRating\n14.87125\n0.11375\n130.73507\n0.00000\n\n\nIncome:Rating\n-0.00221\n0.00134\n-1.65140\n0.09945\n\n\n\n\n\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n&  -579.85561 + 14.87125~\\text{Rating} + 1.81240~\\text{Income}\\\\\n& \\qquad- 0.00221~\\text{Rating}\\times\\text{Income}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/14-mlr.html#whats-actually-happening",
    "href": "slides/14-mlr.html#whats-actually-happening",
    "title": "Multiple linear regression (MLR)",
    "section": "What’s actually happening:",
    "text": "What’s actually happening:\n\nCredit_int &lt;- Credit |&gt; \n  mutate(Interaction = Income * Rating) \n\nCredit_int |&gt; \n  select(Limit, Income, Rating, Interaction) |&gt; \n  head() |&gt; \n  kable()\n\n\n\n\nLimit\nIncome\nRating\nInteraction\n\n\n\n\n3606\n14.891\n283\n4214.153\n\n\n6645\n106.025\n483\n51210.075\n\n\n7075\n104.593\n514\n53760.802\n\n\n9504\n148.924\n681\n101417.244\n\n\n4897\n55.882\n357\n19949.874\n\n\n8047\n80.180\n569\n45622.420"
  },
  {
    "objectID": "slides/14-mlr.html#whats-actually-happening-1",
    "href": "slides/14-mlr.html#whats-actually-happening-1",
    "title": "Multiple linear regression (MLR)",
    "section": "What’s actually happening:",
    "text": "What’s actually happening:\n\nlm(Limit ~ Income + Rating + Interaction, data = Credit_int) |&gt; \n  tidy() |&gt; \n  kable(digits = 5)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-579.85561\n37.48489\n-15.46905\n0.00000\n\n\nIncome\n1.81240\n0.86962\n2.08414\n0.03779\n\n\nRating\n14.87125\n0.11375\n130.73507\n0.00000\n\n\nInteraction\n-0.00221\n0.00134\n-1.65140\n0.09945"
  },
  {
    "objectID": "slides/14-mlr.html#interpreting-the-interaction-term",
    "href": "slides/14-mlr.html#interpreting-the-interaction-term",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting the interaction term",
    "text": "Interpreting the interaction term\n\nFor a fixed Rating the slope of Income is \\((1.81240 - 0.0021\\times\\text{Rating})\\)\nFor a fixed Income the slope of Rating is \\((14.87125 - 0.0021\\times\\text{Income})\\)"
  },
  {
    "objectID": "ae/ae-10-mlr.html#exercise-1b-optional",
    "href": "ae/ae-10-mlr.html#exercise-1b-optional",
    "title": "AE 10: Multiple Linear Regression",
    "section": "Exercise 1B (Optional)",
    "text": "Exercise 1B (Optional)\nIf you have extra time, examine the variables individually and comment on anything that you think is relevant."
  },
  {
    "objectID": "prepare/mlr-inference-prep.html",
    "href": "prepare/mlr-inference-prep.html",
    "title": "October 4 lecture",
    "section": "",
    "text": "Stat2: Topic 3.2 Assessing a Multiple Regression Model"
  },
  {
    "objectID": "prepare/mlr-inference-prep.html#assigned-reading",
    "href": "prepare/mlr-inference-prep.html#assigned-reading",
    "title": "October 4 lecture",
    "section": "",
    "text": "Stat2: Topic 3.2 Assessing a Multiple Regression Model"
  },
  {
    "objectID": "prepare/mlr-interaction-prep.html",
    "href": "prepare/mlr-interaction-prep.html",
    "title": "October 2 lecture",
    "section": "",
    "text": "Stat2: Topic 3.4 New Predictors from Old"
  },
  {
    "objectID": "prepare/mlr-interaction-prep.html#assigned-reading",
    "href": "prepare/mlr-interaction-prep.html#assigned-reading",
    "title": "October 2 lecture",
    "section": "",
    "text": "Stat2: Topic 3.4 New Predictors from Old"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#trying-something-new",
    "href": "slides/15-mlr-interaction.html#trying-something-new",
    "title": "Multiple linear regression (MLR)",
    "section": "Trying something new",
    "text": "Trying something new\n\nPicking up where we left off\nOpen AE-10\n\n\n📋 AE 10 - Credit Cards"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#computational-setup",
    "href": "slides/15-mlr-interaction.html#computational-setup",
    "title": "Multiple linear regression (MLR)",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(ISLR2)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#data-credit-cards",
    "href": "slides/15-mlr-interaction.html#data-credit-cards",
    "title": "Multiple linear regression (MLR)",
    "section": "Data: Credit Cards",
    "text": "Data: Credit Cards\nThe data is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers.\n\n\nRows: 400\nColumns: 11\n$ Income    &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 7…\n$ Limit     &lt;dbl&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, …\n$ Rating    &lt;dbl&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, …\n$ Cards     &lt;dbl&gt; 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2, …\n$ Age       &lt;dbl&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, …\n$ Education &lt;dbl&gt; 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 15,…\n$ Own       &lt;fct&gt; No, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, No, Yes, No…\n$ Student   &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, No…\n$ Married   &lt;fct&gt; Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye…\n$ Region    &lt;fct&gt; South, West, West, West, South, South, East, West, South, Ea…\n$ Balance   &lt;dbl&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0,…"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#variables",
    "href": "slides/15-mlr-interaction.html#variables",
    "title": "Multiple linear regression (MLR)",
    "section": "Variables",
    "text": "Variables\nFeatures (another name for predictors)\n\nIncome: Annual income (in 1000’s of US dollars)\nRating: Credit Rating\n\nOutcome\n\nLimit: Credit limit"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#multiple-linear-regression",
    "href": "slides/15-mlr-interaction.html#multiple-linear-regression",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes\n\\[\nY|X_1, X_2, \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_{\\epsilon}^2)\n\\]"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#multiple-linear-regression-1",
    "href": "slides/15-mlr-interaction.html#multiple-linear-regression-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nAt any combination of the predictors, the mean value of the response \\(Y\\), is\n\\[\n\\mu_{Y|X_1, \\ldots, X_p} = \\beta_0 + \\beta_1 X_{1} + \\beta_2 X_2 + \\dots + \\beta_p X_p\n\\]\n\nUsing multiple linear regression, we can estimate the mean response for any combination of predictors\n\\[\n\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X_{1} + \\hat{\\beta}_2 X_2 + \\dots + \\hat{\\beta}_p X_{p}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#multiple-linear-regression-mlr",
    "href": "slides/15-mlr-interaction.html#multiple-linear-regression-mlr",
    "title": "Multiple linear regression (MLR)",
    "section": "Multiple linear regression (MLR)",
    "text": "Multiple linear regression (MLR)\nBased on out analysis goals, we will use a multiple linear regression model of the following form\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{Rating} + \\hat{\\beta}_2 \\text{Income}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#model-fit",
    "href": "slides/15-mlr-interaction.html#model-fit",
    "title": "Multiple linear regression (MLR)",
    "section": "Model fit",
    "text": "Model fit\n\nlim_fit &lt;- lm(Limit ~ Rating   + Income,\n      data = Credit)\n\ntidy(lim_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-532.471\n24.173\n-22.028\n0.000\n\n\nRating\n14.771\n0.096\n153.124\n0.000\n\n\nIncome\n0.557\n0.423\n1.316\n0.189"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#model-equation",
    "href": "slides/15-mlr-interaction.html#model-equation",
    "title": "Multiple linear regression (MLR)",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{Limit}} =  -532.471 &+14.771 \\times \\text{Rating}\\\\\n& -0.557 \\times \\text{Income}\n\\end{align}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#visualizing-model",
    "href": "slides/15-mlr-interaction.html#visualizing-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Model",
    "text": "Visualizing Model"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#interaction-terms-1",
    "href": "slides/15-mlr-interaction.html#interaction-terms-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Interaction terms",
    "text": "Interaction terms\n\nSometimes the relationship between a predictor variable and the response depends on the value of another predictor variable.\nThis is an interaction effect.\nTo account for this, we can include interaction terms in the model.\nWe want a model of the form:\n\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n\\hat{\\beta}_0 & + \\hat{\\beta}_1 \\text{Rating} + \\hat{\\beta}_2 \\text{Income} +\n\\hat{\\beta}_3\\text{Rating}\\times\\text{Income}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#interpreting-interaction-terms",
    "href": "slides/15-mlr-interaction.html#interpreting-interaction-terms",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the credit limit depends on the borrowers credit rating"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#visualizing-interaction-model-exaggerated",
    "href": "slides/15-mlr-interaction.html#visualizing-interaction-model-exaggerated",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Interaction Model Exaggerated",
    "text": "Visualizing Interaction Model Exaggerated"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#visualizing-interaction-model",
    "href": "slides/15-mlr-interaction.html#visualizing-interaction-model",
    "title": "Multiple linear regression (MLR)",
    "section": "Visualizing Interaction Model",
    "text": "Visualizing Interaction Model"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#application-exercise",
    "href": "slides/15-mlr-interaction.html#application-exercise",
    "title": "Multiple linear regression (MLR)",
    "section": "Application Exercise",
    "text": "Application Exercise\n\nComplete Activity"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#model-fit-1",
    "href": "slides/15-mlr-interaction.html#model-fit-1",
    "title": "Multiple linear regression (MLR)",
    "section": "Model Fit",
    "text": "Model Fit\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-579.85561\n37.48489\n-15.46905\n0.00000\n\n\nIncome\n1.81240\n0.86962\n2.08414\n0.03779\n\n\nRating\n14.87125\n0.11375\n130.73507\n0.00000\n\n\nIncome:Rating\n-0.00221\n0.00134\n-1.65140\n0.09945\n\n\n\n\n\n\\[\n\\begin{aligned}\\hat{\\text{Limit}} ~ =\n&  -579.85561 + 14.87125~\\text{Rating} + 1.81240~\\text{Income}\\\\\n& \\qquad- 0.00221~\\text{Rating}\\times\\text{Income}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#interpreting-the-interaction-term",
    "href": "slides/15-mlr-interaction.html#interpreting-the-interaction-term",
    "title": "Multiple linear regression (MLR)",
    "section": "Interpreting the interaction term",
    "text": "Interpreting the interaction term\n\nFor a fixed Rating the slope of Income is \\((1.81240 - 0.0021\\times\\text{Rating})\\)\nFor a fixed Income the slope of Rating is \\((14.87125 - 0.0021\\times\\text{Income})\\)"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#whats-actually-happening",
    "href": "slides/15-mlr-interaction.html#whats-actually-happening",
    "title": "Multiple linear regression (MLR)",
    "section": "What’s actually happening:",
    "text": "What’s actually happening:\n\nCredit_int &lt;- Credit |&gt; \n  mutate(Interaction = Income * Rating) \n\nCredit_int |&gt; \n  select(Limit, Income, Rating, Interaction) |&gt; \n  head() |&gt; \n  kable()\n\n\n\n\nLimit\nIncome\nRating\nInteraction\n\n\n\n\n3606\n14.891\n283\n4214.153\n\n\n6645\n106.025\n483\n51210.075\n\n\n7075\n104.593\n514\n53760.802\n\n\n9504\n148.924\n681\n101417.244\n\n\n4897\n55.882\n357\n19949.874\n\n\n8047\n80.180\n569\n45622.420"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#whats-actually-happening-1",
    "href": "slides/15-mlr-interaction.html#whats-actually-happening-1",
    "title": "Multiple linear regression (MLR)",
    "section": "What’s actually happening:",
    "text": "What’s actually happening:\n\nlm(Limit ~ Income + Rating + Interaction, data = Credit_int) |&gt; \n  tidy() |&gt; \n  kable(digits = 5)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-579.85561\n37.48489\n-15.46905\n0.00000\n\n\nIncome\n1.81240\n0.86962\n2.08414\n0.03779\n\n\nRating\n14.87125\n0.11375\n130.73507\n0.00000\n\n\nInteraction\n-0.00221\n0.00134\n-1.65140\n0.09945"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#recap",
    "href": "slides/15-mlr-interaction.html#recap",
    "title": "Multiple linear regression (MLR)",
    "section": "Recap",
    "text": "Recap\n\nIntroduced multiple linear regression\nInterpreted coefficients in the multiple linear regression model\nCalculated predictions and associated intervals for multiple linear regression models\nUsed interaction terms\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#picking-up-where-we-left-off",
    "href": "slides/15-mlr-interaction.html#picking-up-where-we-left-off",
    "title": "Multiple linear regression (MLR)",
    "section": "Picking up where we left off",
    "text": "Picking up where we left off\n\nOpen AE-10\n\n\n📋 AE 10 - Credit Cards"
  },
  {
    "objectID": "slides/15-mlr-interaction.html#announcements",
    "href": "slides/15-mlr-interaction.html#announcements",
    "title": "Multiple linear regression (MLR)",
    "section": "Announcements",
    "text": "Announcements\n\nUpdate eval: false to eval: true at the tops of HW’s 03, 04, 05.\nResubmit PDFs from 03 and 04\nOpen AE-10\n\n\n📋 AE 10 - Credit Cards"
  },
  {
    "objectID": "slides/16-mlr-inference.html#announcements",
    "href": "slides/16-mlr-inference.html#announcements",
    "title": "MLR: Inference and conditions",
    "section": "Announcements",
    "text": "Announcements\n\nMidterm next Friday 10/11 (right before spring break)\nProject proposal also due 10/11 but will accept until 10/14 without penalty"
  },
  {
    "objectID": "slides/16-mlr-inference.html#topics",
    "href": "slides/16-mlr-inference.html#topics",
    "title": "MLR: Inference and conditions",
    "section": "Topics",
    "text": "Topics\n\n\nInference for multiple linear regression\nChecking model conditions"
  },
  {
    "objectID": "slides/16-mlr-inference.html#computational-setup",
    "href": "slides/16-mlr-inference.html#computational-setup",
    "title": "MLR: Inference and conditions",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(ISLR2)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/16-mlr-inference.html#data-rail_trail",
    "href": "slides/16-mlr-inference.html#data-rail_trail",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n\n\n\n\nvolume\nhightemp\navgtemp\nseason\ncloudcover\nprecip\nday_type\n\n\n\n\n501\n83\n66.5\nSummer\n7.6\n0.00\nWeekday\n\n\n419\n73\n61.0\nSummer\n6.3\n0.29\nWeekday\n\n\n397\n74\n63.0\nSpring\n7.5\n0.32\nWeekday\n\n\n385\n95\n78.0\nSummer\n2.6\n0.00\nWeekend\n\n\n200\n44\n48.0\nSpring\n10.0\n0.14\nWeekday\n\n\n375\n69\n61.5\nSpring\n6.6\n0.02\nWeekday\n\n\n417\n66\n52.5\nSpring\n2.4\n0.00\nWeekday\n\n\n629\n66\n52.0\nSpring\n0.0\n0.00\nWeekend\n\n\n533\n80\n67.5\nSummer\n3.8\n0.00\nWeekend\n\n\n547\n79\n62.0\nSummer\n4.1\n0.00\nWeekday\n\n\n\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/16-mlr-inference.html#variables",
    "href": "slides/16-mlr-inference.html#variables",
    "title": "MLR: Inference and conditions",
    "section": "Variables",
    "text": "Variables\nFeatures (another name for predictors)\n\nIncome: Annual income (in 1000’s of US dollars)\nRating: Credit Rating\n\nOutcome\n\nLimit: Credit limit"
  },
  {
    "objectID": "slides/16-mlr-inference.html#review-simple-linear-regression-slr",
    "href": "slides/16-mlr-inference.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference and conditions",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\ngf_point(Limit ~ Rating, data = Credit, alpha = 0.5) |&gt; \n  gf_lm()  |&gt; \n  gf_labs(x = \"Credit Rating\", y = \"Income\") |&gt; \n  gf_refine(scale_y_continuous(labels = dollar_format()),\n            scale_x_continuous(labels = dollar_format()))"
  },
  {
    "objectID": "slides/16-mlr-inference.html#slr-model-summary",
    "href": "slides/16-mlr-inference.html#slr-model-summary",
    "title": "MLR: Inference and conditions",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nincome_slr_fit &lt;- lm(Limit ~ Income, data = Credit)\n\ntidy(income_slr_fit) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2389.86941\n114.828758\n20.81246\n0\n\n\nIncome\n51.87502\n2.003836\n25.88785\n0"
  },
  {
    "objectID": "slides/16-mlr-inference.html#slr-hypothesis-test",
    "href": "slides/16-mlr-inference.html#slr-hypothesis-test",
    "title": "MLR: Inference and conditions",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2389.87\n114.83\n20.81\n0\n\n\nIncome\n51.88\n2.00\n25.89\n0\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t= 25.89\\) . The p-value is calculated using a \\(t\\) distribution with 399 degrees of freedom. The p-value is \\(\\approx 0\\) .\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that income is a helpful predictor for a credit card holder’s credit limit, i.e. there is a linear relationship between income and credit limit."
  },
  {
    "objectID": "slides/16-mlr-inference.html#multiple-linear-regression",
    "href": "slides/16-mlr-inference.html#multiple-linear-regression",
    "title": "MLR: Inference and conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\ncredit_fit &lt;- lm(Limit ~ Rating + Income, data = Credit)\n\ntidy(credit_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-532.47\n24.17\n-22.03\n0.00\n\n\nRating\n14.77\n0.10\n153.12\n0.00\n\n\nIncome\n0.56\n0.42\n1.32\n0.19"
  },
  {
    "objectID": "slides/16-mlr-inference.html#multiple-linear-regression-1",
    "href": "slides/16-mlr-inference.html#multiple-linear-regression-1",
    "title": "MLR: Inference and conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes \\[Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)\\]\n\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)\\), we can rewrite the previous statement as\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i}, \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)\\]"
  },
  {
    "objectID": "slides/16-mlr-inference.html#estimating-sigma_epsilon",
    "href": "slides/16-mlr-inference.html#estimating-sigma_epsilon",
    "title": "MLR: Inference and conditions",
    "section": "Estimating \\(\\sigma_\\epsilon\\)",
    "text": "Estimating \\(\\sigma_\\epsilon\\)\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)\\) the residual is \\[\n\\begin{aligned}\ne_i &= y_{i} - \\hat{y_i}\\\\\n&= y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\n\\end{aligned}\n\\]\n\nThe estimated value of the regression standard error , \\(\\sigma_{\\epsilon}\\), is\n\\[\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}\\]\n\n\nAs with SLR, we use \\(\\hat{\\sigma}_{\\epsilon}\\) to calculate \\(SE_{\\hat{\\beta}_j}\\), the standard error of each coefficient. See Matrix Form of Linear Regression for more detail."
  },
  {
    "objectID": "slides/16-mlr-inference.html#mlr-hypothesis-test-hightemp",
    "href": "slides/16-mlr-inference.html#mlr-hypothesis-test-hightemp",
    "title": "MLR: Inference and conditions",
    "section": "MLR hypothesis test: hightemp",
    "text": "MLR hypothesis test: hightemp\n\nSet hypotheses: \\(H_0: \\beta_{hightemp} = 0\\) vs. \\(H_A: \\beta_{hightemp} \\ne 0\\), given season is in the model\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.43\\). The p-value is calculated using a \\(t\\) distribution with 86 \\((n - p - 1)\\) degrees of freedom. The p-value is \\(\\approx 0\\).\n\n\n\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature for the day is a useful predictor in a model that already contains the season as a predictor for number of daily riders."
  },
  {
    "objectID": "slides/16-mlr-inference.html#the-model-for-season-spring",
    "href": "slides/16-mlr-inference.html#the-model-for-season-spring",
    "title": "MLR: Inference and conditions",
    "section": "The model for season = Spring",
    "text": "The model for season = Spring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 1 - 76.84 \\times 0 \\\\\n&= -120.10 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/16-mlr-inference.html#the-model-for-season-summer",
    "href": "slides/16-mlr-inference.html#the-model-for-season-summer",
    "title": "MLR: Inference and conditions",
    "section": "The model for season = Summer",
    "text": "The model for season = Summer\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 1 \\\\\n&= -202.07 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/16-mlr-inference.html#the-model-for-season-fall",
    "href": "slides/16-mlr-inference.html#the-model-for-season-fall",
    "title": "MLR: Inference and conditions",
    "section": "The model for season = Fall",
    "text": "The model for season = Fall\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 0 \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/16-mlr-inference.html#the-models",
    "href": "slides/16-mlr-inference.html#the-models",
    "title": "MLR: Inference and conditions",
    "section": "The models",
    "text": "The models\nSame slope, different intercepts\n\nseason = Spring: \\(-120.10 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Summer: \\(-202.07 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Fall: \\(-125.23 + 7.54 \\times \\texttt{hightemp}\\)"
  },
  {
    "objectID": "slides/16-mlr-inference.html#interaction-terms",
    "href": "slides/16-mlr-inference.html#interaction-terms",
    "title": "MLR: Inference and conditions",
    "section": "Interaction terms",
    "text": "Interaction terms\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-579.85561\n37.48489\n-15.46905\n0.00000\n\n\nRating\n14.87125\n0.11375\n130.73507\n0.00000\n\n\nIncome\n1.81240\n0.86962\n2.08414\n0.03779\n\n\nRating:Income\n-0.00221\n0.00134\n-1.65140\n0.09945\n\n\n\n\n\n\n\n\nDo the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/16-mlr-inference.html#confidence-interval-for-beta_j-1",
    "href": "slides/16-mlr-inference.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference and conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C\\%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically: We are \\(C\\%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant.\n\n\nComplete Exercise 3."
  },
  {
    "objectID": "slides/16-mlr-inference.html#confidence-interval-for-beta_j-2",
    "href": "slides/16-mlr-inference.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference and conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-532.47\n24.17\n-22.03\n0.00\n-579.99\n-484.95\n\n\nRating\n14.77\n0.10\n153.12\n0.00\n14.58\n14.96\n\n\nIncome\n0.56\n0.42\n1.32\n0.19\n-0.28\n1.39"
  },
  {
    "objectID": "slides/16-mlr-inference.html#ci-for-hightemp",
    "href": "slides/16-mlr-inference.html#ci-for-hightemp",
    "title": "MLR: Inference and conditions",
    "section": "CI for hightemp",
    "text": "CI for hightemp\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\n\n\nWe are 95% confident that for every degree Fahrenheit the day is warmer, the number of riders increases by 5.21 to 9.87, on average, holding season constant."
  },
  {
    "objectID": "slides/16-mlr-inference.html#ci-for-seasonspring",
    "href": "slides/16-mlr-inference.html#ci-for-seasonspring",
    "title": "MLR: Inference and conditions",
    "section": "CI for seasonSpring",
    "text": "CI for seasonSpring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\n\n\nWe are 95% confident that the number of riders on a Spring day is lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant.\n\n\nIs season a significant predictor of the number of riders, after accounting for high temperature?"
  },
  {
    "objectID": "slides/16-mlr-inference.html#large-sample-sizes",
    "href": "slides/16-mlr-inference.html#large-sample-sizes",
    "title": "MLR: Inference and conditions",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/16-mlr-inference.html#small-sample-sizes",
    "href": "slides/16-mlr-inference.html#small-sample-sizes",
    "title": "MLR: Inference and conditions",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON’T immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\n\nComplete Exercise 4\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/16-mlr-inference.html#full-model",
    "href": "slides/16-mlr-inference.html#full-model",
    "title": "MLR: Inference and conditions",
    "section": "Full model",
    "text": "Full model\nIncluding all available predictors\nFit:\n\nrt_full_fit &lt;- lm(volume ~ ., data = rail_trail)\n\n\nSummarize:\n\ntidy(rt_full_fit)\n\n# A tibble: 8 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\n\n\nAugment:\n\nrt_full_aug &lt;- augment(rt_full_fit)"
  },
  {
    "objectID": "slides/16-mlr-inference.html#model-conditions",
    "href": "slides/16-mlr-inference.html#model-conditions",
    "title": "MLR: Inference and conditions",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/16-mlr-inference.html#checking-linearity",
    "href": "slides/16-mlr-inference.html#checking-linearity",
    "title": "MLR: Inference and conditions",
    "section": "Checking Linearity",
    "text": "Checking Linearity\n\nLook at a plot of the residuals vs. predicted values\nLook at a plot of the residuals vs. each predictor\nLinearity is met if there is no discernible pattern in each of these plots"
  },
  {
    "objectID": "slides/16-mlr-inference.html#residuals-vs.-predicted-values",
    "href": "slides/16-mlr-inference.html#residuals-vs.-predicted-values",
    "title": "MLR: Inference and conditions",
    "section": "Residuals vs. predicted values",
    "text": "Residuals vs. predicted values\n\ngf_point(.resid ~ .fitted, data = rt_full_aug, alpha = 0.7) |&gt; \n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") |&gt; \n  gf_labs(x = \"Predicted values\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/16-mlr-inference.html#residuals-vs.-each-predictor",
    "href": "slides/16-mlr-inference.html#residuals-vs.-each-predictor",
    "title": "MLR: Inference and conditions",
    "section": "Residuals vs. each predictor",
    "text": "Residuals vs. each predictor"
  },
  {
    "objectID": "slides/16-mlr-inference.html#checking-linearity-1",
    "href": "slides/16-mlr-inference.html#checking-linearity-1",
    "title": "MLR: Inference and conditions",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plot of the residuals vs. predicted values looked OK\nThe plots of residuals vs. hightemp and avgtemp appear to have a parabolic pattern.\nThe linearity condition does not appear to be satisfied given these plots.\n\n\n\nGiven this conclusion, what might be a next step in the analysis?"
  },
  {
    "objectID": "slides/16-mlr-inference.html#checking-constant-variance",
    "href": "slides/16-mlr-inference.html#checking-constant-variance",
    "title": "MLR: Inference and conditions",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/16-mlr-inference.html#checking-constant-variance-1",
    "href": "slides/16-mlr-inference.html#checking-constant-variance-1",
    "title": "MLR: Inference and conditions",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied.\n\n\n\nWe will talk about to address this later in the notes."
  },
  {
    "objectID": "slides/16-mlr-inference.html#checking-normality",
    "href": "slides/16-mlr-inference.html#checking-normality",
    "title": "MLR: Inference and conditions",
    "section": "Checking normality",
    "text": "Checking normality\n\nThe distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size 90 is sufficiently large to relax this condition if it was not satisfied."
  },
  {
    "objectID": "slides/16-mlr-inference.html#checking-independence",
    "href": "slides/16-mlr-inference.html#checking-independence",
    "title": "MLR: Inference and conditions",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable."
  },
  {
    "objectID": "slides/16-mlr-inference.html#checking-independence-1",
    "href": "slides/16-mlr-inference.html#checking-independence-1",
    "title": "MLR: Inference and conditions",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs. order of data collection:\n\ngf_line(.resid ~ 1:nrow(rt_full_aug), data = rt_full_aug) |&gt; \n  gf_point()  |&gt; \n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\")  |&gt; \n  gf_labs(x = \"Order of data collection\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/16-mlr-inference.html#checking-independence-2",
    "href": "slides/16-mlr-inference.html#checking-independence-2",
    "title": "MLR: Inference and conditions",
    "section": "Checking independence",
    "text": "Checking independence\n\nNo clear pattern in the residuals vs. order of data collection plot.\nIndependence condition appears to be satisfied, as far as we can evaluate it.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/16-mlr-inference.html#what-is-multicollinearity",
    "href": "slides/16-mlr-inference.html#what-is-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "What is multicollinearity",
    "text": "What is multicollinearity\nMulticollinearity is the case when two or more predictor variables are strongly correlated with one another"
  },
  {
    "objectID": "slides/16-mlr-inference.html#example",
    "href": "slides/16-mlr-inference.html#example",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\nLet’s assume the true population regression equation is \\(y = 3 + 4x\\)\n\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/16-mlr-inference.html#example-1",
    "href": "slides/16-mlr-inference.html#example-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the “best” combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)"
  },
  {
    "objectID": "slides/16-mlr-inference.html#why-multicollinearity-is-a-problem",
    "href": "slides/16-mlr-inference.html#why-multicollinearity-is-a-problem",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have perfect collinearities, we are unable to get estimates for the coefficients\n\nWhen we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference\nIt is also difficult to interpret the model coefficients"
  },
  {
    "objectID": "slides/16-mlr-inference.html#detecting-multicollinearity",
    "href": "slides/16-mlr-inference.html#detecting-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when…\n\nThere are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\n\n\n\nOne (or more) predictor variables is an almost perfect linear combination of the others\nThere is a quadratic term in the model without mean-centering the variable first\nThere are interactions between two or more continuous variables\n\nCan reduce this by mean-centering the variables first\n\nThere is a categorical predictor with very few observations in the baseline level"
  },
  {
    "objectID": "slides/16-mlr-inference.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/16-mlr-inference.html#detecting-multicollinearity-in-the-eda",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship\n\nLook at the distribution of categorical predictors and avoid setting the baseline to a category with very few observations"
  },
  {
    "objectID": "slides/16-mlr-inference.html#detecting-multicollinearity-vif",
    "href": "slides/16-mlr-inference.html#detecting-multicollinearity-vif",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/16-mlr-inference.html#detecting-multicollinearity-vif-1",
    "href": "slides/16-mlr-inference.html#detecting-multicollinearity-vif-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\n\nTypically \\(VIF &gt; 5\\) indicates concerning multicollinearity\nVariables with similar values of VIF are typically the ones correlated with each other\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/16-mlr-inference.html#vif-for-rail-trail-model",
    "href": "slides/16-mlr-inference.html#vif-for-rail-trail-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "VIF for rail trail model",
    "text": "VIF for rail trail model\n\nvif(rt_full_fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\n\nhightemp, avgtemp, and seasonSummer are correlated."
  },
  {
    "objectID": "slides/16-mlr-inference.html#solutions-for-multicollinearity",
    "href": "slides/16-mlr-inference.html#solutions-for-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Solutions for Multicollinearity",
    "text": "Solutions for Multicollinearity\n\nDrop some predictors\nCombine some predictors\nDiscount the individual coefficients and t-tests (i.e. predictions are meaningful but coefficients, tests, and confidence intervals are not)\n\n\nLet’s try removing hightemp and avgtemp (separately)"
  },
  {
    "objectID": "slides/16-mlr-inference.html#model-without-hightemp",
    "href": "slides/16-mlr-inference.html#model-without-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 &lt;- lm(volume ~ . - hightemp, data = rail_trail)\n  \nm1 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038"
  },
  {
    "objectID": "slides/16-mlr-inference.html#model-without-avgtemp",
    "href": "slides/16-mlr-inference.html#model-without-avgtemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 &lt;- lm(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/16-mlr-inference.html#choosing-a-model",
    "href": "slides/16-mlr-inference.html#choosing-a-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel without hightemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel without avgtemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\n\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model without avgtemp is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/16-mlr-inference.html#selected-model-for-now",
    "href": "slides/16-mlr-inference.html#selected-model-for-now",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Selected model (for now)",
    "text": "Selected model (for now)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/16-mlr-inference.html#topics-1",
    "href": "slides/16-mlr-inference.html#topics-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Topics",
    "text": "Topics\n\nLog transformation on the response variable\nLog transformation on the predictor variable"
  },
  {
    "objectID": "slides/16-mlr-inference.html#residuals-vs.-fitted-for-the-selected-model",
    "href": "slides/16-mlr-inference.html#residuals-vs.-fitted-for-the-selected-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. fitted for the selected model",
    "text": "Residuals vs. fitted for the selected model\n\nThe constant variance condition is not satisfied. We can transform the response variable to address the violation in this condition."
  },
  {
    "objectID": "slides/16-mlr-inference.html#identifying-a-need-to-transform-y",
    "href": "slides/16-mlr-inference.html#identifying-a-need-to-transform-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Identifying a need to transform \\(Y\\)",
    "text": "Identifying a need to transform \\(Y\\)\n\nTypically, a “fan-shaped” residual plot indicates the need for a transformation of the response variable \\(Y\\)\n\nThere are multiple ways to transform a variable, e.g., \\(\\sqrt{Y}\\), \\(1/Y\\), \\(\\log(Y)\\)\n\\(\\log(Y)\\) the most straightforward to interpret, so we use that transformation when possible\n\n\n\n\nWhen building a model:\n\nChoose a transformation and build the model on the transformed data\nReassess the residual plots\nIf the residuals plots did not sufficiently improve, try a new transformation!"
  },
  {
    "objectID": "slides/16-mlr-inference.html#log-transformation-on-y",
    "href": "slides/16-mlr-inference.html#log-transformation-on-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\n\nIf we apply a log transformation to the response variable, we want to estimate the parameters for the statistical model\n\n\\[\n\\log(Y) = \\beta_0+ \\beta_1 X_1 + \\dots +\\beta_pX_p + \\epsilon, \\hspace{10mm} \\epsilon \\sim N(0,\\sigma^2_\\epsilon)\n\\]\n\nThe regression equation is\n\n\\[\\widehat{\\log(Y)} = \\hat{\\beta}_0+ \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_pX_p\\]"
  },
  {
    "objectID": "slides/16-mlr-inference.html#log-transformation-on-y-1",
    "href": "slides/16-mlr-inference.html#log-transformation-on-y-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\nWe want to interpret the model in terms of the original variable \\(Y\\), not \\(\\log(Y)\\), so we need to write the regression equation in terms of \\(Y\\)\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\nThe predicted value \\(\\hat{Y}\\) is the predicted median of \\(Y\\). Note, when the distribution of \\(Y|X_1, \\ldots, X_p\\) is symmetric, then the median equals the mean. See the slides in the appendix for more detail."
  },
  {
    "objectID": "slides/16-mlr-inference.html#model-interpretation",
    "href": "slides/16-mlr-inference.html#model-interpretation",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n\n\nIntercept: When \\(X_1 = \\dots = X_p =0\\), \\(Y\\) is expected to be \\(\\exp\\{\\hat{\\beta}_0\\}\\)\nSlope: For every one unit increase in \\(X_j\\), the \\(Y\\) is expected to multiply by a factor of \\(\\exp\\{\\hat{\\beta}_j\\}\\), holding all else constant\n\n\nWhy is the interpretation in terms of a multiplicative change?"
  },
  {
    "objectID": "slides/16-mlr-inference.html#model-for-logvolume",
    "href": "slides/16-mlr-inference.html#model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model for \\(log(volume)\\)",
    "text": "Model for \\(log(volume)\\)\n\n#fit model\nlog_rt_fit &lt;- lm(log(volume) ~ hightemp + season + cloudcover + precip + day_type, data = rail_trail)\n\ntidy(log_rt_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327"
  },
  {
    "objectID": "slides/16-mlr-inference.html#interpretation-of-model-for-logvolume",
    "href": "slides/16-mlr-inference.html#interpretation-of-model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Interpretation of model for \\(\\log(volume)\\)",
    "text": "Interpretation of model for \\(\\log(volume)\\)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327\n\n\n\n\n\n\n\nInterpret the intercept in terms of (1) log(volume) and (2) volume.\nInterpret the coefficient of hightemp in terms of (1) log(volume) and (2) volume."
  },
  {
    "objectID": "slides/16-mlr-inference.html#residuals-for-model-with-logvolume",
    "href": "slides/16-mlr-inference.html#residuals-for-model-with-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals for model with \\(\\log(volume)\\)",
    "text": "Residuals for model with \\(\\log(volume)\\)"
  },
  {
    "objectID": "slides/16-mlr-inference.html#compare-residual-plots",
    "href": "slides/16-mlr-inference.html#compare-residual-plots",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Compare residual plots",
    "text": "Compare residual plots"
  },
  {
    "objectID": "slides/16-mlr-inference.html#log-transformation-on-x",
    "href": "slides/16-mlr-inference.html#log-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log Transformation on \\(X\\)",
    "text": "Log Transformation on \\(X\\)\n\nTry a transformation on \\(X\\) if the scatterplot shows some curvature but the variance is constant for all values of \\(X\\)"
  },
  {
    "objectID": "slides/16-mlr-inference.html#respiratory-rate-vs.-age",
    "href": "slides/16-mlr-inference.html#respiratory-rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Respiratory Rate vs. Age",
    "text": "Respiratory Rate vs. Age\n\nA high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a “high” rate, we first want to understand the relationship between a child’s age and their respiratory rate.\nThe data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the Sleuth3 R package and is originally form a 1994 publication “Reference Values for Respiratory Rate in the First 3 Years of Life”.\nVariables:\n\nAge: age in months\nRate: respiratory rate (breaths per minute)"
  },
  {
    "objectID": "slides/16-mlr-inference.html#rate-vs.-age",
    "href": "slides/16-mlr-inference.html#rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Rate vs. Age",
    "text": "Rate vs. Age"
  },
  {
    "objectID": "slides/16-mlr-inference.html#model-with-transformation-on-x",
    "href": "slides/16-mlr-inference.html#model-with-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model with Transformation on \\(X\\)",
    "text": "Model with Transformation on \\(X\\)\nSuppose we have the following regression equation:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\log(X)\\]\n\n\nIntercept: When \\(X = 1\\) \\((\\log(X) = 0)\\), \\(Y\\) is expected to be \\(\\hat{\\beta}_0\\) (i.e. the mean of \\(Y\\) is \\(\\hat{\\beta}_0\\))\nSlope: When \\(X\\) is multiplied by a factor of \\(\\mathbf{C}\\), the mean of \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(C)}\\) units\n\nExample: when \\(X\\) is multiplied by a factor of 2, \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(2)}\\) units"
  },
  {
    "objectID": "slides/16-mlr-inference.html#model-interpretation-1",
    "href": "slides/16-mlr-inference.html#model-interpretation-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n50.135\n0.632\n79.330\n0\n\n\nlog(Age)\n-5.982\n0.263\n-22.781\n0\n\n\n\n\n\n\\[\\hat{\\text{Rate}} = 50.135 - 5.982 \\times \\log\\text{(Age)}\\]\n\n\nInterpret the intercept in the context of the data.\nInterpret the slope in terms of age multiplying by 2 in the context of the data."
  },
  {
    "objectID": "slides/16-mlr-inference.html#learn-more",
    "href": "slides/16-mlr-inference.html#learn-more",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Learn more",
    "text": "Learn more\nSee Log Transformations in Linear Regression for more details about interpreting regression models with log-transformed variables."
  },
  {
    "objectID": "slides/16-mlr-inference.html#why-medianyx-instead-of-mu_yx",
    "href": "slides/16-mlr-inference.html#why-medianyx-instead-of-mu_yx",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\nSuppose we have a set of values\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet’s calculate \\(\\overline{\\log(x)}\\)\n\nlog_x &lt;- log(x)\nmean(log_x)\n\n[1] 2.066476\n\n\n\nLet’s calculate \\(\\log(\\bar{x})\\)\n\nxbar &lt;- mean(x)\nlog(xbar)\n\n[1] 2.228477\n\n\n\n\n\n\n\nNote: \\(\\overline{\\log(x)} \\neq \\log(\\bar{x})\\)"
  },
  {
    "objectID": "slides/16-mlr-inference.html#why-medianyx-instead-of-mu_yx-1",
    "href": "slides/16-mlr-inference.html#why-medianyx-instead-of-mu_yx-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n\n\n\nLet’s calculate \\(\\text{Median}(\\log(x))\\)\n\nlog_x &lt;- log(x)\nmedian(log_x)\n\n[1] 2.079442\n\n\n\nLet’s calculate \\(\\log(\\text{Median}(x))\\)\n\nmedian_x &lt;- median(x)\nlog(median_x)\n\n[1] 2.079442\n\n\n\n\n\n\n\nNote: \\(\\text{Median} (\\log(x)) = \\log(\\text{Median}(x))\\)"
  },
  {
    "objectID": "slides/16-mlr-inference.html#mean-median-and-log",
    "href": "slides/16-mlr-inference.html#mean-median-and-log",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean, Median, and Log",
    "text": "Mean, Median, and Log\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\\[\\overline{\\log(x)} \\neq \\log(\\bar{x})\\]\n\nmean(log_x) == log(xbar)\n\n[1] FALSE\n\n\n\n\\[\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))\\]\n\nmedian(log_x) == log(median_x)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/16-mlr-inference.html#mean-and-median-of-logy",
    "href": "slides/16-mlr-inference.html#mean-and-median-of-logy",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(Y)\\)",
    "text": "Mean and median of \\(\\log(Y)\\)\n\nRecall that \\(Y = \\beta_0 + \\beta_1 X\\) is the mean value of the response at the given value of the predictor \\(X\\). This doesn’t hold when we log-transform the response variable.\nMathematically, the mean of the logged values is not necessarily equal to the log of the mean value. Therefore at a given value of \\(X\\)\n\n\n\\[\n\\begin{aligned}\\exp\\{\\text{Mean}(\\log(Y|X))\\} \\neq \\text{Mean}(Y|X) \\\\[5pt]\n\\Rightarrow \\exp\\{\\beta_0 + \\beta_1 X\\} \\neq \\text{Mean}(Y|X) \\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/16-mlr-inference.html#mean-and-median-of-logy-1",
    "href": "slides/16-mlr-inference.html#mean-and-median-of-logy-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(y)\\)",
    "text": "Mean and median of \\(\\log(y)\\)\n\nHowever, the median of the logged values is equal to the log of the median value. Therefore,\n\n\\[\\exp\\{\\text{Median}(\\log(Y|X))\\} = \\text{Median}(Y|X)\\]\n\n\nIf the distribution of \\(\\log(Y)\\) is symmetric about the regression line, for a given value \\(X\\), we can expect \\(Mean(Y)\\) and \\(Median(Y)\\) to be approximately equal.\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/16-mlr-inference.html#data-credit-cards",
    "href": "slides/16-mlr-inference.html#data-credit-cards",
    "title": "MLR: Inference and conditions",
    "section": "Data: Credit Cards",
    "text": "Data: Credit Cards\nThe data is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers.\n\n\nRows: 400\nColumns: 11\n$ Income    &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 7…\n$ Limit     &lt;dbl&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, …\n$ Rating    &lt;dbl&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, …\n$ Cards     &lt;dbl&gt; 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2, …\n$ Age       &lt;dbl&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, …\n$ Education &lt;dbl&gt; 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 15,…\n$ Own       &lt;fct&gt; No, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, No, Yes, No…\n$ Student   &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, No…\n$ Married   &lt;fct&gt; Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye…\n$ Region    &lt;fct&gt; South, West, West, West, South, South, East, West, South, Ea…\n$ Balance   &lt;dbl&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0,…"
  },
  {
    "objectID": "slides/16-mlr-inference.html#mlr-hypothesis-test-income",
    "href": "slides/16-mlr-inference.html#mlr-hypothesis-test-income",
    "title": "MLR: Inference and conditions",
    "section": "MLR hypothesis test: Income",
    "text": "MLR hypothesis test: Income\n\nSet hypotheses: \\(H_0: \\beta_{Income} = 0\\) vs. \\(H_A: \\beta_{Income} \\ne 0\\), given Rating is in the model\n\n\n\nCalculate test statistic and p-value: The test statistic is \\(t = 1.32\\). The p-value is calculated using a \\(t\\) distribution with \\[(n - p - 1) = 400 - 2 -1 = 398\\] degrees of freedom. The p-value is \\(\\approx 0.19\\).\n\n\n\n\nState the conclusion: The p-value is not small, so we fail to reject \\(H_0\\). The data does not provide convincing evidence that a borrowers income is a useful predictor in a model that already contains credit rating as a predictor for the credit limit of a borrower.\n\n\nComplete Exercises 1-2."
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html",
    "href": "ae/ae-11-mlr-inference.html",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-11”.\nGo to the Canvas and locate your AE-11 assignment to get started.\nUpload the ae-11.qmd file into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment."
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#packages-data",
    "href": "ae/ae-11-mlr-inference.html#packages-data",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(ISLR2)\nlibrary(GGally)\nlibrary(yardstick)\n\nThe data for this AE is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers. We will focus on the following variables:\nPredictors\n\nIncome: Annual income (in 1000’s of US dollars)\nRating: Credit Rating\n\nResponse\n\nLimit: Credit limit"
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#analysis-goal",
    "href": "ae/ae-11-mlr-inference.html#analysis-goal",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this activity are to:\n\nPerform inference for multiple linear regression\nConduct/interpret hypothesis tests\nConstruct/interpret confidence intervals"
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#exercise-0",
    "href": "ae/ae-11-mlr-inference.html#exercise-0",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "Exercise 0",
    "text": "Exercise 0\nFit and display two linear regression models predicting Limit from Income and Rating. In one, include an interaction term between the two variables and in the other, do not."
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#exercise-1",
    "href": "ae/ae-11-mlr-inference.html#exercise-1",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nConsider the model without an interaction term. Perform a hypothesis test on Rating (fill in the blanks where appropriate:\n\nSet hypothesis: \\(H_0: \\beta_{Rating} [fill in]\\) vs. \\(\\beta_{Rating} [fill in]\\). Restate these hypothesis in words: [fill in]\nCalculate test statistics and p-value:  The test statistic is \\([fill in]\\). The p-value is \\([fill in]\\).\nState the conclusion: [fill in]"
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#to-submit-the-ae",
    "href": "ae/ae-11-mlr-inference.html#to-submit-the-ae",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html",
    "href": "slides/10-mlr-inference-old.html",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "",
    "text": "Inference for multiple linear regression\nChecking model conditions\nVariable transformations"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#topics",
    "href": "slides/10-mlr-inference-old.html#topics",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "",
    "text": "Inference for multiple linear regression\nChecking model conditions\nVariable transformations"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#computational-setup",
    "href": "slides/10-mlr-inference-old.html#computational-setup",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n\n\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ recipes      1.1.0     ✔ yardstick    1.3.1\n\n\n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard()        masks purrr::discard()\n✖ dplyr::filter()          masks stats::filter()\n✖ recipes::fixed()         masks stringr::fixed()\n✖ kableExtra::group_rows() masks dplyr::group_rows()\n✖ dplyr::lag()             masks stats::lag()\n✖ yardstick::spec()        masks readr::spec()\n✖ Hmisc::src()             masks dplyr::src()\n✖ recipes::step()          masks stats::step()\n✖ Hmisc::summarize()       masks dplyr::summarize()\n✖ parsnip::translate()     masks Hmisc::translate()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(ggformula)\nlibrary(broom)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#data-rail_trail",
    "href": "slides/10-mlr-inference-old.html#data-rail_trail",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n\nRows: 90 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): season, day_type\ndbl (5): volume, hightemp, avgtemp, cloudcover, precip\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nvolume\nhightemp\navgtemp\nseason\ncloudcover\nprecip\nday_type\n\n\n\n\n501\n83\n66.5\nSummer\n7.6\n0.00\nWeekday\n\n\n419\n73\n61.0\nSummer\n6.3\n0.29\nWeekday\n\n\n397\n74\n63.0\nSpring\n7.5\n0.32\nWeekday\n\n\n385\n95\n78.0\nSummer\n2.6\n0.00\nWeekend\n\n\n200\n44\n48.0\nSpring\n10.0\n0.14\nWeekday\n\n\n375\n69\n61.5\nSpring\n6.6\n0.02\nWeekday\n\n\n417\n66\n52.5\nSpring\n2.4\n0.00\nWeekday\n\n\n629\n66\n52.0\nSpring\n0.0\n0.00\nWeekend\n\n\n533\n80\n67.5\nSummer\n3.8\n0.00\nWeekend\n\n\n547\n79\n62.0\nSummer\n4.1\n0.00\nWeekday\n\n\n\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#variables",
    "href": "slides/10-mlr-inference-old.html#variables",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Variables",
    "text": "Variables\nOutcome:\nvolume estimated number of trail users that day (number of breaks recorded)\n. . .\nPredictors\n\n\nhightemp daily high temperature (in degrees Fahrenheit)\navgtemp average of daily low and daily high temperature (in degrees Fahrenheit)\nseason one of “Fall”, “Spring”, or “Summer”\ncloudcover measure of cloud cover (in oktas)\nprecip measure of precipitation (in inches)\nday_type one of “weekday” or “weekend”"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#review-simple-linear-regression-slr",
    "href": "slides/10-mlr-inference-old.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\ngf_point(volume ~ hightemp, data = rail_trail, alpha = 0.5) |&gt; \n  gf_lm()  |&gt; \n  gf_labs(x = \"High temp (F)\", y = \"Number of riders\")\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#slr-model-summary",
    "href": "slides/10-mlr-inference-old.html#slr-model-summary",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nrt_slr_fit &lt;- lm(volume ~ hightemp, data = rail_trail)\n\ntidy(rt_slr_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.08\n59.40\n-0.29\n0.77\n\n\nhightemp\n5.70\n0.85\n6.72\n0.00"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#slr-hypothesis-test",
    "href": "slides/10-mlr-inference-old.html#slr-hypothesis-test",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-17.08\n59.40\n-0.29\n0.77\n\n\nhightemp\n5.70\n0.85\n6.72\n0.00\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\n. . .\n\nCalculate test statistic and p-value: The test statistic is \\(t= 6.72\\) . The p-value is calculated using a \\(t\\) distribution with 88 degrees of freedom. The p-value is \\(\\approx 0\\) .\n\n. . .\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature is a helpful predictor for the number of daily riders, i.e. there is a linear relationship between high temperature and number of daily riders."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#multiple-linear-regression",
    "href": "slides/10-mlr-inference-old.html#multiple-linear-regression",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\nrt_mlr_main_fit &lt;- lm(volume ~ hightemp + season, data = rail_trail)\n\ntidy(rt_mlr_main_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#multiple-linear-regression-1",
    "href": "slides/10-mlr-inference-old.html#multiple-linear-regression-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes \\[Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)\\]\n. . .\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)\\), we can rewrite the previous statement as\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i} \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)\\]"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#estimating-sigma_epsilon",
    "href": "slides/10-mlr-inference-old.html#estimating-sigma_epsilon",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Estimating \\(\\sigma_\\epsilon\\)",
    "text": "Estimating \\(\\sigma_\\epsilon\\)\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)\\) the residual is \\[\n\\begin{aligned}\ne_i &= y_{i} - \\hat{y_i}\\\\\n&= y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\n\\end{aligned}\n\\]\n. . .\nThe estimated value of the regression standard error , \\(\\sigma_{\\epsilon}\\), is\n\\[\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}\\]\n. . .\nAs with SLR, we use \\(\\hat{\\sigma}_{\\epsilon}\\) to calculate \\(SE_{\\hat{\\beta}_j}\\), the standard error of each coefficient. See Matrix Form of Linear Regression for more detail."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#mlr-hypothesis-test-hightemp",
    "href": "slides/10-mlr-inference-old.html#mlr-hypothesis-test-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "MLR hypothesis test: hightemp",
    "text": "MLR hypothesis test: hightemp\n\nSet hypotheses: \\(H_0: \\beta_{hightemp} = 0\\) vs. \\(H_A: \\beta_{hightemp} \\ne 0\\), given season is in the model\n\n. . .\n\nCalculate test statistic and p-value: The test statistic is \\(t = 6.43\\). The p-value is calculated using a \\(t\\) distribution with 86 \\((n - p - 1)\\) degrees of freedom. The p-value is \\(\\approx 0\\).\n\n. . .\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that high temperature for the day is a useful predictor in a model that already contains the season as a predictor for number of daily riders."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#the-model-for-season-spring",
    "href": "slides/10-mlr-inference-old.html#the-model-for-season-spring",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Spring",
    "text": "The model for season = Spring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n. . .\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 1 - 76.84 \\times 0 \\\\\n&= -120.10 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#the-model-for-season-summer",
    "href": "slides/10-mlr-inference-old.html#the-model-for-season-summer",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Summer",
    "text": "The model for season = Summer\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n. . .\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 1 \\\\\n&= -202.07 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#the-model-for-season-fall",
    "href": "slides/10-mlr-inference-old.html#the-model-for-season-fall",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The model for season = Fall",
    "text": "The model for season = Fall\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n\n\n\n\n\n\n. . .\n\\[\n\\begin{aligned}\n\\widehat{volume} &= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times \\texttt{seasonSpring} - 76.84 \\times \\texttt{seasonSummer} \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp} + 5.13 \\times 0 - 76.84 \\times 0 \\\\\n&= -125.23 + 7.54 \\times \\texttt{hightemp}\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#the-models",
    "href": "slides/10-mlr-inference-old.html#the-models",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "The models",
    "text": "The models\nSame slope, different intercepts\n\nseason = Spring: \\(-120.10 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Summer: \\(-202.07 + 7.54 \\times \\texttt{hightemp}\\)\nseason = Fall: \\(-125.23 + 7.54 \\times \\texttt{hightemp}\\)"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#interaction-terms",
    "href": "slides/10-mlr-inference-old.html#interaction-terms",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Interaction terms",
    "text": "Interaction terms\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-10.53\n166.80\n-0.06\n0.95\n\n\nhightemp\n5.48\n2.95\n1.86\n0.07\n\n\nseasonSpring\n-293.95\n190.33\n-1.54\n0.13\n\n\nseasonSummer\n354.18\n255.08\n1.39\n0.17\n\n\nhightemp:seasonSpring\n4.88\n3.26\n1.50\n0.14\n\n\nhightemp:seasonSummer\n-4.54\n3.75\n-1.21\n0.23\n\n\n\n\n\n\n\n\nDo the data provide evidence of a significant interaction effect? Comment on the significance of the interaction terms."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#confidence-interval-for-beta_j-1",
    "href": "slides/10-mlr-inference-old.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C\\%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically: We are \\(C\\%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#confidence-interval-for-beta_j-2",
    "href": "slides/10-mlr-inference-old.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\ntidy(rt_mlr_main_fit, conf.int = TRUE) |&gt;\n  kable(\"html\", digits = 2) |&gt; kable_styling(font_size = 24)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#ci-for-hightemp",
    "href": "slides/10-mlr-inference-old.html#ci-for-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "CI for hightemp",
    "text": "CI for hightemp\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\n\n\nWe are 95% confident that for every degree Fahrenheit the day is warmer, the number of riders increases by 5.21 to 9.87, on average, holding season constant."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#ci-for-seasonspring",
    "href": "slides/10-mlr-inference-old.html#ci-for-seasonspring",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "CI for seasonSpring",
    "text": "CI for seasonSpring\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-125.23\n71.66\n-1.75\n0.08\n-267.68\n17.22\n\n\nhightemp\n7.54\n1.17\n6.43\n0.00\n5.21\n9.87\n\n\nseasonSpring\n5.13\n34.32\n0.15\n0.88\n-63.10\n73.36\n\n\nseasonSummer\n-76.84\n47.71\n-1.61\n0.11\n-171.68\n18.00\n\n\n\n\n\n\n\n\nWe are 95% confident that the number of riders on a Spring day is lower by 63.1 to higher by 73.4 compared to a Fall day, on average, holding high temperature for the day constant.\n. . .\n\nIs season a significant predictor of the number of riders, after accounting for high temperature?"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#large-sample-sizes",
    "href": "slides/10-mlr-inference-old.html#large-sample-sizes",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#small-sample-sizes",
    "href": "slides/10-mlr-inference-old.html#small-sample-sizes",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON’T immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#full-model",
    "href": "slides/10-mlr-inference-old.html#full-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Full model",
    "text": "Full model\nIncluding all available predictors\nFit:\n\nrt_full_fit &lt;- lm(volume ~ ., data = rail_trail)\n\n. . .\nSummarize:\n\ntidy(rt_full_fit)\n\n# A tibble: 8 × 5\n  term            estimate std.error statistic p.value\n  &lt;chr&gt;              &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)        17.6      76.6      0.230 0.819  \n2 hightemp            7.07      2.42     2.92  0.00450\n3 avgtemp            -2.04      3.14    -0.648 0.519  \n4 seasonSpring       35.9      33.0      1.09  0.280  \n5 seasonSummer       24.2      52.8      0.457 0.649  \n6 cloudcover         -7.25      3.84    -1.89  0.0627 \n7 precip            -95.7      42.6     -2.25  0.0273 \n8 day_typeWeekend    35.9      22.4      1.60  0.113  \n\n\n. . .\nAugment:\n\nrt_full_aug &lt;- augment(rt_full_fit)"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#model-conditions",
    "href": "slides/10-mlr-inference-old.html#model-conditions",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model conditions",
    "text": "Model conditions\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#checking-linearity",
    "href": "slides/10-mlr-inference-old.html#checking-linearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking Linearity",
    "text": "Checking Linearity\n\nLook at a plot of the residuals vs. predicted values\nLook at a plot of the residuals vs. each predictor\nLinearity is met if there is no discernible pattern in each of these plots"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#residuals-vs.-predicted-values",
    "href": "slides/10-mlr-inference-old.html#residuals-vs.-predicted-values",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. predicted values",
    "text": "Residuals vs. predicted values\n\ngf_point(.resid ~ .fitted, data = rt_full_aug, alpha = 0.7) |&gt; \n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") |&gt; \n  gf_labs(x = \"Predicted values\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#residuals-vs.-each-predictor",
    "href": "slides/10-mlr-inference-old.html#residuals-vs.-each-predictor",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. each predictor",
    "text": "Residuals vs. each predictor"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#checking-linearity-1",
    "href": "slides/10-mlr-inference-old.html#checking-linearity-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plot of the residuals vs. predicted values looked OK\nThe plots of residuals vs. hightemp and avgtemp appear to have a parabolic pattern.\nThe linearity condition does not appear to be satisfied given these plots.\n\n. . .\n\nGiven this conclusion, what might be a next step in the analysis?"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#checking-constant-variance",
    "href": "slides/10-mlr-inference-old.html#checking-constant-variance",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#checking-constant-variance-1",
    "href": "slides/10-mlr-inference-old.html#checking-constant-variance-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied.\n\n. . .\n\nWe will talk about to address this later in the notes."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#checking-normality",
    "href": "slides/10-mlr-inference-old.html#checking-normality",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking normality",
    "text": "Checking normality\n\n\n\n\n\n\n\n\n\nThe distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size 90 is sufficiently large to relax this condition if it was not satisfied."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#checking-independence",
    "href": "slides/10-mlr-inference-old.html#checking-independence",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#checking-independence-1",
    "href": "slides/10-mlr-inference-old.html#checking-independence-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs. order of data collection:\n\ngf_line(.resid ~ 1:nrow(rt_full_aug), data = rt_full_aug) |&gt; \n  gf_point()  |&gt; \n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\")  |&gt; \n  gf_labs(x = \"Order of data collection\", y = \"Residuals\")"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#checking-independence-2",
    "href": "slides/10-mlr-inference-old.html#checking-independence-2",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Checking independence",
    "text": "Checking independence\n\nNo clear pattern in the residuals vs. order of data collection plot.\nIndependence condition appears to be satisfied, as far as we can evaluate it."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#what-is-multicollinearity",
    "href": "slides/10-mlr-inference-old.html#what-is-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "What is multicollinearity",
    "text": "What is multicollinearity\nMulticollinearity is the case when two or more predictor variables are strongly correlated with one another"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#example",
    "href": "slides/10-mlr-inference-old.html#example",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\nLet’s assume the true population regression equation is \\(y = 3 + 4x\\)\n. . .\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#example-1",
    "href": "slides/10-mlr-inference-old.html#example-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the “best” combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#why-multicollinearity-is-a-problem",
    "href": "slides/10-mlr-inference-old.html#why-multicollinearity-is-a-problem",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have perfect collinearities, we are unable to get estimates for the coefficients\n\nWhen we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference\nIt is also difficult to interpret the model coefficients"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#detecting-multicollinearity",
    "href": "slides/10-mlr-inference-old.html#detecting-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when…\n\nThere are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\n\n\n\nOne (or more) predictor variables is an almost perfect linear combination of the others\nThere is a quadratic term in the model without mean-centering the variable first\nThere are interactions between two or more continuous variables\n\nCan reduce this by mean-centering the variables first\n\nThere is a categorical predictor with very few observations in the baseline level"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/10-mlr-inference-old.html#detecting-multicollinearity-in-the-eda",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship\n\nLook at the distribution of categorical predictors and avoid setting the baseline to a category with very few observations"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#detecting-multicollinearity-vif",
    "href": "slides/10-mlr-inference-old.html#detecting-multicollinearity-vif",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#detecting-multicollinearity-vif-1",
    "href": "slides/10-mlr-inference-old.html#detecting-multicollinearity-vif-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\n\nTypically \\(VIF &gt; 5\\) indicates concerning multicollinearity\nVariables with similar values of VIF are typically the ones correlated with each other\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#vif-for-rail-trail-model",
    "href": "slides/10-mlr-inference-old.html#vif-for-rail-trail-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "VIF for rail trail model",
    "text": "VIF for rail trail model\n\nvif(rt_full_fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\n. . .\nhightemp, avgtemp, and seasonSummer are correlated."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#solutions-for-multicollinearity",
    "href": "slides/10-mlr-inference-old.html#solutions-for-multicollinearity",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Solutions for Multicollinearity",
    "text": "Solutions for Multicollinearity\n\nDrop some predictors\nCombine some predictors\nDiscount the individual coefficients and t-tests (i.e. predictions are meaningful but coefficients, tests, and confidence intervals are not)\n\n. . .\nLet’s try removing hightemp and avgtemp (separately)"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#model-without-hightemp",
    "href": "slides/10-mlr-inference-old.html#model-without-hightemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 &lt;- lm(volume ~ . - hightemp, data = rail_trail)\n  \nm1 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#model-without-avgtemp",
    "href": "slides/10-mlr-inference-old.html#model-without-avgtemp",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 &lt;- lm(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#choosing-a-model",
    "href": "slides/10-mlr-inference-old.html#choosing-a-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel without hightemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel without avgtemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\n. . .\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model without avgtemp is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#selected-model-for-now",
    "href": "slides/10-mlr-inference-old.html#selected-model-for-now",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Selected model (for now)",
    "text": "Selected model (for now)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#topics-1",
    "href": "slides/10-mlr-inference-old.html#topics-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Topics",
    "text": "Topics\n\nLog transformation on the response variable\nLog transformation on the predictor variable"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#residuals-vs.-fitted-for-the-selected-model",
    "href": "slides/10-mlr-inference-old.html#residuals-vs.-fitted-for-the-selected-model",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals vs. fitted for the selected model",
    "text": "Residuals vs. fitted for the selected model\n\n\n\n\n\n\n\n\n\nThe constant variance condition is not satisfied. We can transform the response variable to address the violation in this condition."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#identifying-a-need-to-transform-y",
    "href": "slides/10-mlr-inference-old.html#identifying-a-need-to-transform-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Identifying a need to transform \\(Y\\)",
    "text": "Identifying a need to transform \\(Y\\)\n\nTypically, a “fan-shaped” residual plot indicates the need for a transformation of the response variable \\(Y\\)\n\nThere are multiple ways to transform a variable, e.g., \\(\\sqrt{Y}\\), \\(1/Y\\), \\(\\log(Y)\\)\n\\(\\log(Y)\\) the most straightforward to interpret, so we use that transformation when possible\n\n\n. . .\n\nWhen building a model:\n\nChoose a transformation and build the model on the transformed data\nReassess the residual plots\nIf the residuals plots did not sufficiently improve, try a new transformation!"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#log-transformation-on-y",
    "href": "slides/10-mlr-inference-old.html#log-transformation-on-y",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\n\nIf we apply a log transformation to the response variable, we want to estimate the parameters for the statistical model\n\n\\[\n\\log(Y) = \\beta_0+ \\beta_1 X_1 + \\dots +\\beta_pX_p + \\epsilon, \\hspace{10mm} \\epsilon \\sim N(0,\\sigma^2_\\epsilon)\n\\]\n\nThe regression equation is\n\n\\[\\widehat{\\log(Y)} = \\hat{\\beta}_0+ \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_pX_p\\]"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#log-transformation-on-y-1",
    "href": "slides/10-mlr-inference-old.html#log-transformation-on-y-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log transformation on \\(Y\\)",
    "text": "Log transformation on \\(Y\\)\nWe want to interpret the model in terms of the original variable \\(Y\\), not \\(\\log(Y)\\), so we need to write the regression equation in terms of \\(Y\\)\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\n\nThe predicted value \\(\\hat{Y}\\) is the predicted median of \\(Y\\). Note, when the distribution of \\(Y|X_1, \\ldots, X_p\\) is symmetric, then the median equals the mean. See the slides in the appendix for more detail."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#model-interpretation",
    "href": "slides/10-mlr-inference-old.html#model-interpretation",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\\[\\begin{align}\\hat{Y} &= \\exp\\{\\hat{\\beta}_0 + \\hat{\\beta}_1 X + \\dots + \\hat{\\beta}_PX_P\\}\\\\ &= \\exp\\{\\hat{\\beta}_0\\}\\exp\\{\\hat{\\beta}_1X\\}\\dots\\exp\\{\\hat{\\beta}_pX_p\\}\\end{align}\\]\n. . .\n\nIntercept: When \\(X_1 = \\dots = X_p =0\\), \\(Y\\) is expected to be \\(\\exp\\{\\hat{\\beta}_0\\}\\)\nSlope: For every one unit increase in \\(X_j\\), the \\(Y\\) is expected to multiply by a factor of \\(\\exp\\{\\hat{\\beta}_j\\}\\), holding all else constant\n\n\nWhy is the interpretation in terms of a multiplicative change?"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#model-for-logvolume",
    "href": "slides/10-mlr-inference-old.html#model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model for \\(log(volume)\\)",
    "text": "Model for \\(log(volume)\\)\n\n#fit model\nlog_rt_fit &lt;- lm(log(volume) ~ hightemp + season + cloudcover + precip + day_type, data = rail_trail)\n\ntidy(log_rt_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#interpretation-of-model-for-logvolume",
    "href": "slides/10-mlr-inference-old.html#interpretation-of-model-for-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Interpretation of model for \\(\\log(volume)\\)",
    "text": "Interpretation of model for \\(\\log(volume)\\)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n4.738\n0.219\n21.667\n0.000\n\n\nhightemp\n0.018\n0.003\n5.452\n0.000\n\n\nseasonSpring\n0.026\n0.094\n0.283\n0.778\n\n\nseasonSummer\n-0.047\n0.139\n-0.338\n0.736\n\n\ncloudcover\n-0.025\n0.010\n-2.452\n0.016\n\n\nprecip\n-0.294\n0.123\n-2.397\n0.019\n\n\nday_typeWeekend\n0.064\n0.065\n0.987\n0.327\n\n\n\n\n\n\n\nInterpret the intercept in terms of (1) log(volume) and (2) volume.\nInterpret the coefficient of hightemp in terms of (1) log(volume) and (2) volume."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#residuals-for-model-with-logvolume",
    "href": "slides/10-mlr-inference-old.html#residuals-for-model-with-logvolume",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Residuals for model with \\(\\log(volume)\\)",
    "text": "Residuals for model with \\(\\log(volume)\\)"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#compare-residual-plots",
    "href": "slides/10-mlr-inference-old.html#compare-residual-plots",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Compare residual plots",
    "text": "Compare residual plots"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#log-transformation-on-x",
    "href": "slides/10-mlr-inference-old.html#log-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Log Transformation on \\(X\\)",
    "text": "Log Transformation on \\(X\\)\n\n\n\n\n\n\n\n\n\nTry a transformation on \\(X\\) if the scatterplot shows some curvature but the variance is constant for all values of \\(X\\)"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#respiratory-rate-vs.-age",
    "href": "slides/10-mlr-inference-old.html#respiratory-rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Respiratory Rate vs. Age",
    "text": "Respiratory Rate vs. Age\n\nA high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a “high” rate, we first want to understand the relationship between a child’s age and their respiratory rate.\nThe data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the Sleuth3 R package and is originally form a 1994 publication “Reference Values for Respiratory Rate in the First 3 Years of Life”.\nVariables:\n\nAge: age in months\nRate: respiratory rate (breaths per minute)"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#rate-vs.-age",
    "href": "slides/10-mlr-inference-old.html#rate-vs.-age",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Rate vs. Age",
    "text": "Rate vs. Age\n\n\n`geom_smooth()` using method = 'loess' and formula = 'y ~ x'"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#model-with-transformation-on-x",
    "href": "slides/10-mlr-inference-old.html#model-with-transformation-on-x",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model with Transformation on \\(X\\)",
    "text": "Model with Transformation on \\(X\\)\nSuppose we have the following regression equation:\n\\[\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\log(X)\\]\n. . .\n\nIntercept: When \\(X = 1\\) \\((\\log(X) = 0)\\), \\(Y\\) is expected to be \\(\\hat{\\beta}_0\\) (i.e. the mean of \\(Y\\) is \\(\\hat{\\beta}_0\\))\nSlope: When \\(X\\) is multiplied by a factor of \\(\\mathbf{C}\\), the mean of \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(C)}\\) units\n\nExample: when \\(X\\) is multiplied by a factor of 2, \\(Y\\) is expected to increase by \\(\\boldsymbol{\\hat{\\beta}_1}\\mathbf{\\log(2)}\\) units"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#model-interpretation-1",
    "href": "slides/10-mlr-inference-old.html#model-interpretation-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Model interpretation",
    "text": "Model interpretation\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n50.135\n0.632\n79.330\n0\n\n\nlog(Age)\n-5.982\n0.263\n-22.781\n0\n\n\n\n\n\n\\[\\hat{\\text{Rate}} = 50.135 - 5.982 \\times \\log\\text{(Age)}\\]\n\n\nInterpret the intercept in the context of the data.\nInterpret the slope in terms of age multiplying by 2 in the context of the data."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#learn-more",
    "href": "slides/10-mlr-inference-old.html#learn-more",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Learn more",
    "text": "Learn more\nSee Log Transformations in Linear Regression for more details about interpreting regression models with log-transformed variables."
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#why-medianyx-instead-of-mu_yx",
    "href": "slides/10-mlr-inference-old.html#why-medianyx-instead-of-mu_yx",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\nSuppose we have a set of values\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n. . .\n\n\nLet’s calculate \\(\\overline{\\log(x)}\\)\n\nlog_x &lt;- log(x)\nmean(log_x)\n\n[1] 2.066476\n\n\n\nLet’s calculate \\(\\log(\\bar{x})\\)\n\nxbar &lt;- mean(x)\nlog(xbar)\n\n[1] 2.228477\n\n\n\n\n\n. . .\nNote: \\(\\overline{\\log(x)} \\neq \\log(\\bar{x})\\)"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#why-medianyx-instead-of-mu_yx-1",
    "href": "slides/10-mlr-inference-old.html#why-medianyx-instead-of-mu_yx-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)",
    "text": "Why \\(Median(Y|X)\\) instead of \\(\\mu_{Y|X}\\)\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\n. . .\n\n\nLet’s calculate \\(\\text{Median}(\\log(x))\\)\n\nlog_x &lt;- log(x)\nmedian(log_x)\n\n[1] 2.079442\n\n\n\nLet’s calculate \\(\\log(\\text{Median}(x))\\)\n\nmedian_x &lt;- median(x)\nlog(median_x)\n\n[1] 2.079442\n\n\n\n\n\n. . .\nNote: \\(\\text{Median} (\\log(x)) = \\log(\\text{Median}(x))\\)"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#mean-median-and-log",
    "href": "slides/10-mlr-inference-old.html#mean-median-and-log",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean, Median, and Log",
    "text": "Mean, Median, and Log\n\nx &lt;- c(3, 5, 6, 8, 10, 14, 19)\n\n\\[\\overline{\\log(x)} \\neq \\log(\\bar{x})\\]\n\nmean(log_x) == log(xbar)\n\n[1] FALSE\n\n\n. . .\n\\[\\text{Median}(\\log(x)) = \\log(\\text{Median}(x))\\]\n\nmedian(log_x) == log(median_x)\n\n[1] TRUE"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#mean-and-median-of-logy",
    "href": "slides/10-mlr-inference-old.html#mean-and-median-of-logy",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(Y)\\)",
    "text": "Mean and median of \\(\\log(Y)\\)\n\nRecall that \\(Y = \\beta_0 + \\beta_1 X\\) is the mean value of the response at the given value of the predictor \\(X\\). This doesn’t hold when we log-transform the response variable.\nMathematically, the mean of the logged values is not necessarily equal to the log of the mean value. Therefore at a given value of \\(X\\)\n\n. . .\n\\[\n\\begin{aligned}\\exp\\{\\text{Mean}(\\log(Y|X))\\} \\neq \\text{Mean}(Y|X) \\\\[5pt]\n\\Rightarrow \\exp\\{\\beta_0 + \\beta_1 X\\} \\neq \\text{Mean}(Y|X) \\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/10-mlr-inference-old.html#mean-and-median-of-logy-1",
    "href": "slides/10-mlr-inference-old.html#mean-and-median-of-logy-1",
    "title": "MLR: Inference, conditions, and transformations",
    "section": "Mean and median of \\(\\log(y)\\)",
    "text": "Mean and median of \\(\\log(y)\\)\n\nHowever, the median of the logged values is equal to the log of the median value. Therefore,\n\n\\[\\exp\\{\\text{Median}(\\log(Y|X))\\} = \\text{Median}(Y|X)\\]\n. . .\n\nIf the distribution of \\(\\log(Y)\\) is symmetric about the regression line, for a given value \\(X\\), we can expect \\(Mean(Y)\\) and \\(Median(Y)\\) to be approximately equal."
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#exercise-2",
    "href": "ae/ae-11-mlr-inference.html#exercise-2",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nConsider the model with an interaction term. Interpret the p-value associated with Income and the p-value associated with the interaction term."
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#exercise",
    "href": "ae/ae-11-mlr-inference.html#exercise",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "Exercise",
    "text": "Exercise\nWhat does it mean for two things to be independent in statistics (feel free to use google)? Do we think our p-values are independent?"
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#exercise-3",
    "href": "ae/ae-11-mlr-inference.html#exercise-3",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nGenerate 95% confidence intervals for the model without an interaction term. Hint: use the tidy function with the argument conf.int = TRUE. Interpret the confidence interval for Income and explain why the combination of p-value and confidence interval makes sense."
  },
  {
    "objectID": "ae/ae-11-mlr-inference.html#exercise-4",
    "href": "ae/ae-11-mlr-inference.html#exercise-4",
    "title": "AE 11: Inference for Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat does it mean for two things to be independent in statistics (feel free to use google)? Do we think our p-values/confidence intervals are independent across variables?"
  },
  {
    "objectID": "slides/17-mlr-conditions.html",
    "href": "slides/17-mlr-conditions.html",
    "title": "MLR: Inference and conditions",
    "section": "",
    "text": "Midterm next Friday 10/11 (right before spring break)\nProject proposal also due 10/11 but will accept until 10/14 without penalty"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#announcements",
    "href": "slides/17-mlr-conditions.html#announcements",
    "title": "MLR: Inference and conditions",
    "section": "",
    "text": "Midterm next Friday 10/11 (right before spring break)\nProject proposal also due 10/11 but will accept until 10/14 without penalty"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#topics",
    "href": "slides/17-mlr-conditions.html#topics",
    "title": "MLR: Inference and conditions",
    "section": "Topics",
    "text": "Topics\n\n\nInference for multiple linear regression\nChecking model conditions"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#computational-setup",
    "href": "slides/17-mlr-conditions.html#computational-setup",
    "title": "MLR: Inference and conditions",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ISLR2)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:mosaic':\n\n    rescale\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#data-credit-cards",
    "href": "slides/17-mlr-conditions.html#data-credit-cards",
    "title": "MLR: Inference and conditions",
    "section": "Data: Credit Cards",
    "text": "Data: Credit Cards\nThe data is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers.\n\n\nRows: 400\nColumns: 11\n$ Income    &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 7…\n$ Limit     &lt;dbl&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, …\n$ Rating    &lt;dbl&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, …\n$ Cards     &lt;dbl&gt; 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2, …\n$ Age       &lt;dbl&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, …\n$ Education &lt;dbl&gt; 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 15,…\n$ Own       &lt;fct&gt; No, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, No, Yes, No…\n$ Student   &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, No…\n$ Married   &lt;fct&gt; Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye…\n$ Region    &lt;fct&gt; South, West, West, West, South, South, East, West, South, Ea…\n$ Balance   &lt;dbl&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0,…"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#variables",
    "href": "slides/17-mlr-conditions.html#variables",
    "title": "MLR: Inference and conditions",
    "section": "Variables",
    "text": "Variables\nFeatures (another name for predictors)\n\nIncome: Annual income (in 1000’s of US dollars)\nRating: Credit Rating\n\nOutcome\n\nLimit: Credit limit"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#review-simple-linear-regression-slr",
    "href": "slides/17-mlr-conditions.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference and conditions",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\ngf_point(Limit ~ Rating, data = Credit, alpha = 0.5) |&gt; \n  gf_lm()  |&gt; \n  gf_labs(x = \"Credit Rating\", y = \"Income\") |&gt; \n  gf_refine(scale_y_continuous(labels = dollar_format()),\n            scale_x_continuous(labels = dollar_format()))\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "slides/17-mlr-conditions.html#slr-model-summary",
    "href": "slides/17-mlr-conditions.html#slr-model-summary",
    "title": "MLR: Inference and conditions",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nincome_slr_fit &lt;- lm(Limit ~ Income, data = Credit)\n\ntidy(income_slr_fit) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2389.86941\n114.828758\n20.81246\n0\n\n\nIncome\n51.87502\n2.003836\n25.88785\n0"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#slr-hypothesis-test",
    "href": "slides/17-mlr-conditions.html#slr-hypothesis-test",
    "title": "MLR: Inference and conditions",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2389.87\n114.83\n20.81\n0\n\n\nIncome\n51.88\n2.00\n25.89\n0\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\n. . .\n\nCalculate test statistic and p-value: The test statistic is \\(t= 25.89\\) . The p-value is calculated using a \\(t\\) distribution with 399 degrees of freedom. The p-value is \\(\\approx 0\\) .\n\n. . .\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that income is a helpful predictor for a credit card holder’s credit limit, i.e. there is a linear relationship between income and credit limit."
  },
  {
    "objectID": "slides/17-mlr-conditions.html#multiple-linear-regression",
    "href": "slides/17-mlr-conditions.html#multiple-linear-regression",
    "title": "MLR: Inference and conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\ncredit_fit &lt;- lm(Limit ~ Rating + Income, data = Credit)\n\ntidy(credit_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-532.47\n24.17\n-22.03\n0.00\n\n\nRating\n14.77\n0.10\n153.12\n0.00\n\n\nIncome\n0.56\n0.42\n1.32\n0.19"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#multiple-linear-regression-1",
    "href": "slides/17-mlr-conditions.html#multiple-linear-regression-1",
    "title": "MLR: Inference and conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes \\[Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)\\]\n. . .\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)\\), we can rewrite the previous statement as\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i}, \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)\\]"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#estimating-sigma_epsilon",
    "href": "slides/17-mlr-conditions.html#estimating-sigma_epsilon",
    "title": "MLR: Inference and conditions",
    "section": "Estimating \\(\\sigma_\\epsilon\\)",
    "text": "Estimating \\(\\sigma_\\epsilon\\)\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)\\) the residual is \\[\n\\begin{aligned}\ne_i &= y_{i} - \\hat{y_i}\\\\\n&= y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\n\\end{aligned}\n\\]\n. . .\nThe estimated value of the regression standard error , \\(\\sigma_{\\epsilon}\\), is\n\\[\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}\\]\n. . .\nAs with SLR, we use \\(\\hat{\\sigma}_{\\epsilon}\\) to calculate \\(SE_{\\hat{\\beta}_j}\\), the standard error of each coefficient. See Matrix Form of Linear Regression for more detail."
  },
  {
    "objectID": "slides/17-mlr-conditions.html#mlr-hypothesis-test-income",
    "href": "slides/17-mlr-conditions.html#mlr-hypothesis-test-income",
    "title": "MLR: Inference and conditions",
    "section": "MLR hypothesis test: Income",
    "text": "MLR hypothesis test: Income\n\nSet hypotheses: \\(H_0: \\beta_{Income} = 0\\) vs. \\(H_A: \\beta_{Income} \\ne 0\\), given Rating is in the model\n\n. . .\n\nCalculate test statistic and p-value: The test statistic is \\(t = 1.32\\). The p-value is calculated using a \\(t\\) distribution with \\[(n - p - 1) = 400 - 2 -1 = 398\\] degrees of freedom. The p-value is \\(\\approx 0.19\\).\n\n. . .\n\nState the conclusion: The p-value is not small, so we fail to reject \\(H_0\\). The data does not provide convincing evidence that a borrowers income is a useful predictor in a model that already contains credit rating as a predictor for the credit limit of a borrower.\n\n\nComplete Exercises 1-2."
  },
  {
    "objectID": "slides/17-mlr-conditions.html#confidence-interval-for-beta_j-1",
    "href": "slides/17-mlr-conditions.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference and conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C\\%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically: We are \\(C\\%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant.\n\n\nComplete Exercise 3."
  },
  {
    "objectID": "slides/17-mlr-conditions.html#confidence-interval-for-beta_j-2",
    "href": "slides/17-mlr-conditions.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference and conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-532.47\n24.17\n-22.03\n0.00\n-579.99\n-484.95\n\n\nRating\n14.77\n0.10\n153.12\n0.00\n14.58\n14.96\n\n\nIncome\n0.56\n0.42\n1.32\n0.19\n-0.28\n1.39"
  },
  {
    "objectID": "slides/17-mlr-conditions.html#large-sample-sizes",
    "href": "slides/17-mlr-conditions.html#large-sample-sizes",
    "title": "MLR: Inference and conditions",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/17-mlr-conditions.html#small-sample-sizes",
    "href": "slides/17-mlr-conditions.html#small-sample-sizes",
    "title": "MLR: Inference and conditions",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON’T immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\nComplete exercise 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "hw/hw-06.html",
    "href": "hw/hw-06.html",
    "title": "Homework 06: Candy Competition",
    "section": "",
    "text": "Important\n\n\n\nDue: Friday, October 20, 11:59pm"
  },
  {
    "objectID": "hw/hw-06.html#introduction",
    "href": "hw/hw-06.html#introduction",
    "title": "Homework 06: Candy Competition",
    "section": "Introduction",
    "text": "Introduction\nIn today’s homework you will analyze data about candy that was collected from an online experiment conducted at FiveThirtyEight.\n\nLearning goals\nBy the end of the homework you will be able to\n\nFit a linear model with multiple predictors and an interaction term\nFit a linear model with categorical predictors\nConduct inference on multiple linear models"
  },
  {
    "objectID": "hw/hw-06.html#getting-started",
    "href": "hw/hw-06.html#getting-started",
    "title": "Homework 06: Candy Competition",
    "section": "Getting started",
    "text": "Getting started\n\nGo to RStudio and login with your College of Idaho Email and Password.\nMake a subfolder in your hw directory to store this homework.\nLog into Canvas, navigate to Homework 6 and upload the hw-06.qmd file into the folder your just made."
  },
  {
    "objectID": "hw/hw-06.html#packages",
    "href": "hw/hw-06.html#packages",
    "title": "Homework 06: Candy Competition",
    "section": "Packages",
    "text": "Packages\nThe following packages are used in the lab.\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(ggformula)\nlibrary(fivethirtyeight)\nlibrary(knitr)\nlibrary(yardstick)\n\n# add other packages as needed"
  },
  {
    "objectID": "hw/hw-06.html#data-candy",
    "href": "hw/hw-06.html#data-candy",
    "title": "Homework 06: Candy Competition",
    "section": "Data: Candy",
    "text": "Data: Candy\nThe data from this lab comes from the the article FiveThirtyEight The Ultimate Halloween Candy Power Ranking by Walt Hickey. To collect data, Hickey and collaborators at FiveThirtyEight set up an experiment people could vote on a series of randomly generated candy matchups (e.g. Reeses vs. Skittles). Click here to check out some of the match ups.\nThe data set contains the characteristics and win percentage from 85 candies in the experiment. The variables are\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nchocolate\nDoes it contain chocolate?\n\n\nfruity\nIs it fruit flavored?\n\n\ncaramel\nIs there caramel in the candy?\n\n\npeanutalmondy\nDoes it contain peanuts, peanut butter or almonds?\n\n\nnougat\nDoes it contain nougat?\n\n\ncrispedricewafer\nDoes it contain crisped rice, wafers, or a cookie component?\n\n\nhard\nIs it a hard candy?\n\n\nbar\nIs it a candy bar?\n\n\npluribus\nIs it one of many candies in a bag or box?\n\n\nsugarpercent\nThe percentile of sugar it falls under within the data set. Values 0 - 1.\n\n\npricepercent\nThe unit price percentile compared to the rest of the set. Values 0 - 1.\n\n\nwinpercent\nThe overall win percentage according to 269,000 matchups. Values 0 - 100.\n\n\n\nUse the code below to get a glimpse of the candy_rankings data frame in the fivethirtyeight R package.\n\nglimpse(candy_rankings)\n\nRows: 85\nColumns: 13\n$ competitorname   &lt;chr&gt; \"100 Grand\", \"3 Musketeers\", \"One dime\", \"One quarter…\n$ chocolate        &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ fruity           &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, TRUE, FALSE, FALSE, FALSE…\n$ caramel          &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ peanutyalmondy   &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, TRUE, …\n$ nougat           &lt;lgl&gt; FALSE, TRUE, FALSE, FALSE, FALSE, FALSE, TRUE, FALSE,…\n$ crispedricewafer &lt;lgl&gt; TRUE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE…\n$ hard             &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALS…\n$ bar              &lt;lgl&gt; TRUE, TRUE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, F…\n$ pluribus         &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, TRUE…\n$ sugarpercent     &lt;dbl&gt; 0.732, 0.604, 0.011, 0.011, 0.906, 0.465, 0.604, 0.31…\n$ pricepercent     &lt;dbl&gt; 0.860, 0.511, 0.116, 0.511, 0.511, 0.767, 0.767, 0.51…\n$ winpercent       &lt;dbl&gt; 66.97173, 67.60294, 32.26109, 46.11650, 52.34146, 50.…"
  },
  {
    "objectID": "hw/hw-06.html#exercises",
    "href": "hw/hw-06.html#exercises",
    "title": "Homework 06: Candy Competition",
    "section": "Exercises",
    "text": "Exercises\nThe goal of this analysis is to use multiple linear regression to understand the factors that make a good candy."
  },
  {
    "objectID": "hw/hw-06.html#exercise-2",
    "href": "hw/hw-06.html#exercise-2",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nOur response variable in this homework will be winpercentage. Choose two additional variables, one quantitative and one categorical. Generate a SINGLE plot that visualizes all three variables. Hint: remember that you can tie any aesthetic in your plot (e.g. color) to a variable be writing aesthatic = ~variable_name.\nWrite two observations from your plot."
  },
  {
    "objectID": "hw/hw-06.html#exercise-4",
    "href": "hw/hw-06.html#exercise-4",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 4",
    "text": "Exercise 4\nInterpret the following in the context of the data:\n\nIntercept\nCoefficient of your quantitative variable\nCoefficient(s) of your categorical variables\nCoefficient(s) of your interaction term."
  },
  {
    "objectID": "hw/hw-06.html#exercise-5",
    "href": "hw/hw-06.html#exercise-5",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 5",
    "text": "Exercise 5\nChoose one of the coefficients from your model and write out all the steps in the hypothesis test that corresponds to it’s p-value as in this slide."
  },
  {
    "objectID": "hw/hw-06.html#exercise-6",
    "href": "hw/hw-06.html#exercise-6",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 6",
    "text": "Exercise 6\nInterpret the other p-values in your model in the context of the data. You do not need to write out the full hypothesis testing framework as you did in Exercise 5."
  },
  {
    "objectID": "hw/hw-06.html#exercise-7",
    "href": "hw/hw-06.html#exercise-7",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 7",
    "text": "Exercise 7\nGenerate 95% confidence intervals for all of the slopes in your model and interpret them in the context of the data. Are these intervals independent of one another?"
  },
  {
    "objectID": "hw/hw-06.html#grading",
    "href": "hw/hw-06.html#grading",
    "title": "Homework 06: Candy Competition",
    "section": "Grading",
    "text": "Grading\nTotal points available: 50 points.\n\n\n\nComponent\nPoints\n\n\n\n\nEx 1\n2\n\n\nEx 2\n3\n\n\nEx 3\n5\n\n\nEx 4\n8\n\n\nEx 5\n6\n\n\nEx 6\n5\n\n\nEx 7\n5\n\n\nEx 8\n4\n\n\nEx 9\n4\n\n\nEx 10\n4\n\n\nWorkflow & formatting\n41"
  },
  {
    "objectID": "hw/hw-06.html#footnotes",
    "href": "hw/hw-06.html#footnotes",
    "title": "Homework 06: Candy Competition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe “Workflow & formatting” grade is to assess the reproducible workflow, clarity, and professionalism.↩︎"
  },
  {
    "objectID": "hw/hw-06.html#exercise-1",
    "href": "hw/hw-06.html#exercise-1",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 1",
    "text": "Exercise 1\nNotice that the values of pricepercent and sugarpercent are proportions. Change the scale so that they are percentages."
  },
  {
    "objectID": "hw/hw-06.html#exercise-3",
    "href": "hw/hw-06.html#exercise-3",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a linear model including both variables you chose above and include an interaction term between the two."
  },
  {
    "objectID": "hw/hw-06.html#exercise-8",
    "href": "hw/hw-06.html#exercise-8",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 8",
    "text": "Exercise 8\nFit a linear model predicting winpercent using chocolate and pricepercent as predictors. Include a interaction term between pricepercent and chocolate. Describe the effect of pricepercent for chocolate candy in the context of the data."
  },
  {
    "objectID": "hw/hw-06.html#exercise-9",
    "href": "hw/hw-06.html#exercise-9",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nConsider the model from Exercise 3 “Model 1” and the model fit in Exercise 8 “Model 2”. If you happened to have chosen pricepercent and chocolate in Exercise 3, please choose two new variables and to complete this problem with.\nWhich model would you choose based on \\(R^2\\)? Briefly explain your choice.\nWhich model would you choose based on \\(RMSE\\)? Briefly explain your choice."
  },
  {
    "objectID": "hw/hw-06.html#exercise-10",
    "href": "hw/hw-06.html#exercise-10",
    "title": "Homework 06: Candy Competition",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nUse the model you selected in Exercise 9 to describe what generally makes a good candy, i.e., one with a high win percentage."
  },
  {
    "objectID": "prepare/categorical-predictors-prep.html",
    "href": "prepare/categorical-predictors-prep.html",
    "title": "October 7 lecture",
    "section": "",
    "text": "Stat2: Topic 3.4 Comparing two regression lines"
  },
  {
    "objectID": "prepare/categorical-predictors-prep.html#assigned-reading",
    "href": "prepare/categorical-predictors-prep.html#assigned-reading",
    "title": "October 7 lecture",
    "section": "",
    "text": "Stat2: Topic 3.4 Comparing two regression lines"
  },
  {
    "objectID": "slides/18-mlr-conditions.html",
    "href": "slides/18-mlr-conditions.html",
    "title": "MLR: Inference and conditions",
    "section": "",
    "text": "Midterm next Friday 10/11 (right before spring break)\nProject proposal also due 10/11 but will accept until 10/14 without penalty"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#announcements",
    "href": "slides/18-mlr-conditions.html#announcements",
    "title": "MLR: Inference and conditions",
    "section": "",
    "text": "Midterm next Friday 10/11 (right before spring break)\nProject proposal also due 10/11 but will accept until 10/14 without penalty"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#topics",
    "href": "slides/18-mlr-conditions.html#topics",
    "title": "MLR: Inference and conditions",
    "section": "Topics",
    "text": "Topics\n\n\nInference for multiple linear regression\nChecking model conditions"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#computational-setup",
    "href": "slides/18-mlr-conditions.html#computational-setup",
    "title": "MLR: Inference and conditions",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\nAttaching package: 'mosaic'\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(ISLR2)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:mosaic':\n\n    rescale\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#data-credit-cards",
    "href": "slides/18-mlr-conditions.html#data-credit-cards",
    "title": "MLR: Inference and conditions",
    "section": "Data: Credit Cards",
    "text": "Data: Credit Cards\nThe data is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers.\n\n\nRows: 400\nColumns: 11\n$ Income    &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 7…\n$ Limit     &lt;dbl&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, …\n$ Rating    &lt;dbl&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, …\n$ Cards     &lt;dbl&gt; 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2, …\n$ Age       &lt;dbl&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, …\n$ Education &lt;dbl&gt; 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 15,…\n$ Own       &lt;fct&gt; No, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, No, Yes, No…\n$ Student   &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, No…\n$ Married   &lt;fct&gt; Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye…\n$ Region    &lt;fct&gt; South, West, West, West, South, South, East, West, South, Ea…\n$ Balance   &lt;dbl&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0,…"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#variables",
    "href": "slides/18-mlr-conditions.html#variables",
    "title": "MLR: Inference and conditions",
    "section": "Variables",
    "text": "Variables\nFeatures (another name for predictors)\n\nIncome: Annual income (in 1000’s of US dollars)\nRating: Credit Rating\n\nOutcome\n\nLimit: Credit limit"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#review-simple-linear-regression-slr",
    "href": "slides/18-mlr-conditions.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference and conditions",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\ngf_point(Limit ~ Rating, data = Credit, alpha = 0.5) |&gt; \n  gf_lm()  |&gt; \n  gf_labs(x = \"Credit Rating\", y = \"Income\") |&gt; \n  gf_refine(scale_y_continuous(labels = dollar_format()),\n            scale_x_continuous(labels = dollar_format()))\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#slr-model-summary",
    "href": "slides/18-mlr-conditions.html#slr-model-summary",
    "title": "MLR: Inference and conditions",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nincome_slr_fit &lt;- lm(Limit ~ Income, data = Credit)\n\ntidy(income_slr_fit) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2389.86941\n114.828758\n20.81246\n0\n\n\nIncome\n51.87502\n2.003836\n25.88785\n0"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#slr-hypothesis-test",
    "href": "slides/18-mlr-conditions.html#slr-hypothesis-test",
    "title": "MLR: Inference and conditions",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2389.87\n114.83\n20.81\n0\n\n\nIncome\n51.88\n2.00\n25.89\n0\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\n. . .\n\nCalculate test statistic and p-value: The test statistic is \\(t= 25.89\\) . The p-value is calculated using a \\(t\\) distribution with 399 degrees of freedom. The p-value is \\(\\approx 0\\) .\n\n. . .\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that income is a helpful predictor for a credit card holder’s credit limit, i.e. there is a linear relationship between income and credit limit."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#multiple-linear-regression",
    "href": "slides/18-mlr-conditions.html#multiple-linear-regression",
    "title": "MLR: Inference and conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\ncredit_fit &lt;- lm(Limit ~ Rating + Income, data = Credit)\n\ntidy(credit_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-532.47\n24.17\n-22.03\n0.00\n\n\nRating\n14.77\n0.10\n153.12\n0.00\n\n\nIncome\n0.56\n0.42\n1.32\n0.19"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#multiple-linear-regression-1",
    "href": "slides/18-mlr-conditions.html#multiple-linear-regression-1",
    "title": "MLR: Inference and conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes \\[Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)\\]\n. . .\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)\\), we can rewrite the previous statement as\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i}, \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)\\]"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#estimating-sigma_epsilon",
    "href": "slides/18-mlr-conditions.html#estimating-sigma_epsilon",
    "title": "MLR: Inference and conditions",
    "section": "Estimating \\(\\sigma_\\epsilon\\)",
    "text": "Estimating \\(\\sigma_\\epsilon\\)\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)\\) the residual is \\[\n\\begin{aligned}\ne_i &= y_{i} - \\hat{y_i}\\\\\n&= y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\n\\end{aligned}\n\\]\n. . .\nThe estimated value of the regression standard error , \\(\\sigma_{\\epsilon}\\), is\n\\[\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}\\]\n. . .\nAs with SLR, we use \\(\\hat{\\sigma}_{\\epsilon}\\) to calculate \\(SE_{\\hat{\\beta}_j}\\), the standard error of each coefficient. See Matrix Form of Linear Regression for more detail."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#mlr-hypothesis-test-income",
    "href": "slides/18-mlr-conditions.html#mlr-hypothesis-test-income",
    "title": "MLR: Inference and conditions",
    "section": "MLR hypothesis test: Income",
    "text": "MLR hypothesis test: Income\n\nSet hypotheses: \\(H_0: \\beta_{Income} = 0\\) vs. \\(H_A: \\beta_{Income} \\ne 0\\), given Rating is in the model\n\n. . .\n\nCalculate test statistic and p-value: The test statistic is \\(t = 1.32\\). The p-value is calculated using a \\(t\\) distribution with \\[(n - p - 1) = 400 - 2 -1 = 398\\] degrees of freedom. The p-value is \\(\\approx 0.19\\).\n\n. . .\n\nState the conclusion: The p-value is not small, so we fail to reject \\(H_0\\). The data does not provide convincing evidence that a borrowers income is a useful predictor in a model that already contains credit rating as a predictor for the credit limit of a borrower.\n\n\nComplete Exercises 1-2."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#confidence-interval-for-beta_j-1",
    "href": "slides/18-mlr-conditions.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference and conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C\\%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically: We are \\(C\\%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant.\n\n\nComplete Exercise 3."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#confidence-interval-for-beta_j-2",
    "href": "slides/18-mlr-conditions.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference and conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-532.47\n24.17\n-22.03\n0.00\n-579.99\n-484.95\n\n\nRating\n14.77\n0.10\n153.12\n0.00\n14.58\n14.96\n\n\nIncome\n0.56\n0.42\n1.32\n0.19\n-0.28\n1.39"
  },
  {
    "objectID": "slides/18-mlr-conditions.html#large-sample-sizes",
    "href": "slides/18-mlr-conditions.html#large-sample-sizes",
    "title": "MLR: Inference and conditions",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/18-mlr-conditions.html#small-sample-sizes",
    "href": "slides/18-mlr-conditions.html#small-sample-sizes",
    "title": "MLR: Inference and conditions",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON’T immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\nComplete exercise 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "slides/17-categorical-predictors.html",
    "href": "slides/17-categorical-predictors.html",
    "title": "Feature engineering",
    "section": "",
    "text": "Group labs resume this week\nProf. Tackett will not have office hours on Friday\n\nEmail to schedule an appointment if you need to meet\nAll other office hours on regular schedule"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#announcements",
    "href": "slides/17-categorical-predictors.html#announcements",
    "title": "Feature engineering",
    "section": "",
    "text": "Group labs resume this week\nProf. Tackett will not have office hours on Friday\n\nEmail to schedule an appointment if you need to meet\nAll other office hours on regular schedule"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#statistician-of-the-day-rafael-irizarry",
    "href": "slides/17-categorical-predictors.html#statistician-of-the-day-rafael-irizarry",
    "title": "Feature engineering",
    "section": "Statistician of the day: Rafael Irizarry",
    "text": "Statistician of the day: Rafael Irizarry\n\n\nDr. Rafael Irizarry is a Professor of Biostatistics at the Harvard T.H. Chan School of Public Health and Professor of Biostatistics and Computational Biology at the Dana-Farber Cancer Institute. He earned a Bachelor of Science degree in Mathematics from the University of Puerto Rico at Rio Piedras and a PhD from the University of California, Berkeley in Statistics. Dr. Irizarry’s work is highly cited, and he has been given many prestigious awards including the Presidents’ Award given by the Committee of Presidents of Statistical Societies.\n\n\n\n\n\n\n\n\nSource: hardin47.github.io/CURV/scholars/irizarry"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#work-on-impacts-of-hurricane-maria",
    "href": "slides/17-categorical-predictors.html#work-on-impacts-of-hurricane-maria",
    "title": "Feature engineering",
    "section": "Work on impacts of Hurricane Maria",
    "text": "Work on impacts of Hurricane Maria\n\nPart of a team that used stratified sampling to survey residents in Puerto Rico about the impacts of the 2017 Hurricane Maria\nEstimated percent of population who lost access to services, such as electricity and water, and the association with remoteness\nUsed confidence intervals to estimate deaths that were directly and indirectly attributable to the hurricane\n\nTheir estimate was more than 70 times the official count\n\n\nArticle: Kishore, N., Marqués, D., Mahmud, A., Kiang, M. V., Rodriguez, I., Fuller, A., ... & Buckee, C. O. (2018). Mortality in Puerto Rico after Hurricane Maria. New England journal of medicine, 379(2), 162-170.\nGitHub repo: github.com/c2-d2/pr_mort_official"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#topics",
    "href": "slides/17-categorical-predictors.html#topics",
    "title": "Categorical Predictors",
    "section": "Topics",
    "text": "Topics\n\nUnderstanding categorical predictors\nUnderstand how categorical predictors interact with quantitative predictors"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#computational-setup",
    "href": "slides/17-categorical-predictors.html#computational-setup",
    "title": "Categorical Predictors",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(openintro)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#data-peer-to-peer-lender",
    "href": "slides/17-categorical-predictors.html#data-peer-to-peer-lender",
    "title": "Categorical Predictors",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 3\n   annual_income verified_income interest_rate\n           &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000 Not Verified            10.9 \n 2         60000 Not Verified             9.92\n 3         75000 Verified                26.3 \n 4         75000 Not Verified             9.92\n 5        254000 Not Verified             9.43\n 6         67000 Source Verified          9.92\n 7         28800 Source Verified         17.1 \n 8         80000 Not Verified             6.08\n 9         34000 Not Verified             7.97\n10         80000 Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#variables",
    "href": "slides/17-categorical-predictors.html#variables",
    "title": "Categorical Predictors",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nResponse: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#response-interest_rate",
    "href": "slides/17-categorical-predictors.html#response-interest_rate",
    "title": "Categorical Predictors",
    "section": "Response: interest_rate",
    "text": "Response: interest_rate\n\n\n\n\n\n\nmin\nmedian\nmax\niqr\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#predictors",
    "href": "slides/17-categorical-predictors.html#predictors",
    "title": "Categorical Predictors",
    "section": "Predictors",
    "text": "Predictors\n\n\nCode\np1 &lt;- loan50 |&gt; \n  gf_bar(verified_income~.) |&gt; \n  gf_labs(title = \"Verified Income\", \n       y = \"\")\n\np2 &lt;- loan50 |&gt; \n  gf_histogram(~annual_income, binwidth = 20000) |&gt; \n  gf_labs(title = \"\",\n       x = \"Annual income\")\n\np1 / p2"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#data-manipulation-1-rescale-income",
    "href": "slides/17-categorical-predictors.html#data-manipulation-1-rescale-income",
    "title": "Feature engineering",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/17-categorical-predictors.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "Feature engineering",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/17-categorical-predictors.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "Feature engineering",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#interest-rate-vs.-annual-income",
    "href": "slides/17-categorical-predictors.html#interest-rate-vs.-annual-income",
    "title": "Categorical Predictors",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/17-categorical-predictors.html#data-manipulation-4-create-interaction-variables",
    "title": "Feature engineering",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          &lt;dbl&gt; 1, 1, 1, 1, 1, …\n$ debt_inc_cent                                          &lt;dbl&gt; -0.16511719, 0.…\n$ annual_income_th_cent                                  &lt;dbl&gt; -27.17, -26.17,…\n$ `verified_incomeNot Verified`                          &lt;dbl&gt; 1, 1, 0, 1, 1, …\n$ `verified_incomeSource Verified`                       &lt;dbl&gt; 0, 0, 0, 0, 0, …\n$ verified_incomeVerified                                &lt;dbl&gt; 0, 0, 1, 0, 0, …\n$ `annual_income_th_cent:verified_incomeNot Verified`    &lt;dbl&gt; -27.17, -26.17,…\n$ `annual_income_th_cent:verified_incomeSource Verified` &lt;dbl&gt; 0.00, 0.00, 0.0…\n$ `annual_income_th_cent:verified_incomeVerified`        &lt;dbl&gt; 0.00, 0.00, -11…"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#interaction-term-in-the-model",
    "href": "slides/17-categorical-predictors.html#interaction-term-in-the-model",
    "title": "Feature engineering",
    "section": "Interaction term in the model",
    "text": "Interaction term in the model\n\nint_cent_int_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(interest_rate ~ debt_inc_cent  +  verified_income + \n        annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.484\n0.989\n9.586\n0.000\n\n\ndebt_inc_cent\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n2.157\n1.418\n1.522\n0.135\n\n\nverified_incomeVerified\n7.181\n1.870\n3.840\n0.000\n\n\nannual_income_th_cent\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th_cent\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th_cent\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#interpreting-interaction-terms",
    "href": "slides/17-categorical-predictors.html#interpreting-interaction-terms",
    "title": "Categorical Predictors",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n10.303\n1.897\n5.432\n0.000\n\n\nannual_income_k\n-0.009\n0.019\n-0.471\n0.640\n\n\nverified_incomeSource Verified\n3.423\n2.534\n1.351\n0.184\n\n\nverified_incomeVerified\n9.788\n3.652\n2.680\n0.010\n\n\nannual_income_k:verified_incomeSource Verified\n-0.015\n0.026\n-0.591\n0.558\n\n\nannual_income_k:verified_incomeVerified\n-0.031\n0.033\n-0.961\n0.342\n\n\n\n\n\n\nSlope of annual_income_k depends on verified_income level\nNo difference: fit three separate linear models on the data corresponding to each level of verified_income"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#understanding-the-model",
    "href": "slides/17-categorical-predictors.html#understanding-the-model",
    "title": "Categorical Predictors",
    "section": "Understanding the model",
    "text": "Understanding the model\n\\[\n\\begin{aligned}\n\\hat{interest\\_rate} &= 910.303 - 0.009 \\times annual\\_income\\_k \\\\ &+ 3.423 \\times SourceVerified + 9.788 \\times Verified \\\\ &- 0.015 \\times annual\\_income\\_k \\times SourceVerified\\\\ &- 0.031 \\times annual\\_income\\_k \\times Verified\n\\end{aligned}\n\\]\n\nComplete Exercise 7\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#the-office",
    "href": "slides/17-categorical-predictors.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#data",
    "href": "slides/17-categorical-predictors.html#data",
    "title": "Feature engineering",
    "section": "Data",
    "text": "Data\nThe data come from data.world, by way of TidyTuesday\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\n\nRows: 188 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): title\ndbl  (4): season, episode, imdb_rating, total_votes\ndate (1): air_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\noffice_ratings\n\n# A tibble: 188 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ℹ 178 more rows"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#imdb-ratings",
    "href": "slides/17-categorical-predictors.html#imdb-ratings",
    "title": "Feature engineering",
    "section": "IMDB ratings",
    "text": "IMDB ratings"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#imdb-ratings-vs.-number-of-votes",
    "href": "slides/17-categorical-predictors.html#imdb-ratings-vs.-number-of-votes",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. number of votes",
    "text": "IMDB ratings vs. number of votes"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#outliers",
    "href": "slides/17-categorical-predictors.html#outliers",
    "title": "Feature engineering",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#imdb-ratings-vs.-air-date",
    "href": "slides/17-categorical-predictors.html#imdb-ratings-vs.-air-date",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. air date",
    "text": "IMDB ratings vs. air date"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#imdb-ratings-vs.-seasons",
    "href": "slides/17-categorical-predictors.html#imdb-ratings-vs.-seasons",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. seasons",
    "text": "IMDB ratings vs. seasons"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#spending-our-data",
    "href": "slides/17-categorical-predictors.html#spending-our-data",
    "title": "Feature engineering",
    "section": "Spending our data",
    "text": "Spending our data\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we’ve done so far)"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#splitting-the-data",
    "href": "slides/17-categorical-predictors.html#splitting-the-data",
    "title": "Feature engineering",
    "section": "Splitting the data",
    "text": "Splitting the data\n\nTake a random sample of X% of the data and set aside (testing data)\n\nTypically 10 - 20%\n\nFit a model on the remaining Y% of the data (training data)\n\nTypically 80 - 90%\n\nUse the coefficients from the model fit on training data to make predictions and evaluate performance on the testing data"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#train-test",
    "href": "slides/17-categorical-predictors.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings, prop = 0.75) # prop = 0.75 by default\n\n. . .\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\n. . .\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#training-data",
    "href": "slides/17-categorical-predictors.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 × 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ℹ 131 more rows"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#feature-engineering-1",
    "href": "slides/17-categorical-predictors.html#feature-engineering-1",
    "title": "Feature engineering",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple (parsimonious) models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance and improve interpretability)"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#feature-engineering-with-dplyr",
    "href": "slides/17-categorical-predictors.html#feature-engineering-with-dplyr",
    "title": "Feature engineering",
    "section": "Feature engineering with dplyr",
    "text": "Feature engineering with dplyr\n\noffice_train |&gt;\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )\n\n# A tibble: 141 × 8\n  season episode title            imdb_rating total_votes air_date   month  wday\n  &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 8           18 Last Day in Flo…         7.8        1429 2012-03-08     3     5\n2 9           14 Vandalism                7.6        1402 2013-01-31     1     5\n3 2            8 Performance Rev…         8.2        2416 2005-11-15    11     3\n4 9            5 Here Comes Treb…         7.1        1515 2012-10-25    10     5\n5 3           22 Beach Games              9.1        2783 2007-05-10     5     5\n6 7            1 Nepotism                 8.4        1897 2010-09-23     9     5\n# ℹ 135 more rows\n\n\n. . .\n\nCan you identify any potential problems with this approach?"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#modeling-workflow",
    "href": "slides/17-categorical-predictors.html#modeling-workflow",
    "title": "Feature engineering",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\n\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#initiate-a-recipe",
    "href": "slides/17-categorical-predictors.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloging names and types of variables\n  )\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#step-1-alter-roles",
    "href": "slides/17-categorical-predictors.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn’t a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec |&gt;\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#step-2-add-features",
    "href": "slides/17-categorical-predictors.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec |&gt;\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#step-3-add-more-features",
    "href": "slides/17-categorical-predictors.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec |&gt;\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#step-4-convert-numbers-to-factors",
    "href": "slides/17-categorical-predictors.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec |&gt;\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#step-5-make-dummy-variables",
    "href": "slides/17-categorical-predictors.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec |&gt;\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#step-6-remove-zero-variance-predictors",
    "href": "slides/17-categorical-predictors.html#step-6-remove-zero-variance-predictors",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance predictors",
    "text": "Step 6: Remove zero variance predictors\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec |&gt;\n  step_zv(all_predictors())\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#putting-it-all-together",
    "href": "slides/17-categorical-predictors.html#putting-it-all-together",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  # make title's role ID\n  update_role(title, new_role = \"ID\") |&gt;\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) |&gt;\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) |&gt;\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) |&gt;\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#putting-it-all-together-1",
    "href": "slides/17-categorical-predictors.html#putting-it-all-together-1",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#next-step",
    "href": "slides/17-categorical-predictors.html#next-step",
    "title": "Feature engineering",
    "section": "Next step…",
    "text": "Next step…\nWe will complete the workflow to fit a model predicting IMDB ratings that includes the following predictors:\n\nepisode\ntotal_votes\nindicator variables for season\nindicator variables for day of week aired (created using air_date)\nindicator variables for month aired (created using air_date)\n\n. . .\n\nWhat feature will not end up in the final model? Why is it not included?"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#working-with-recipes",
    "href": "slides/17-categorical-predictors.html#working-with-recipes",
    "title": "Feature engineering",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don’t get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n\n\n\n\n\n\nNote\n\n\n\nThis is optional, we’ll show the results for demonstrative purposes. It doesn’t need to be part of your modeling pipeline, but it can be assuring to see the effects of the recipe steps as you build the recipe."
  },
  {
    "objectID": "slides/17-categorical-predictors.html#recap",
    "href": "slides/17-categorical-predictors.html#recap",
    "title": "Feature engineering",
    "section": "Recap",
    "text": "Recap\n\n\nReview: Training and testing splits\nFeature engineering with recipes"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html",
    "href": "slides/11-feature-engineering-old.html",
    "title": "Feature engineering",
    "section": "",
    "text": "Group labs resume this week\nProf. Tackett will not have office hours on Friday\n\nEmail to schedule an appointment if you need to meet\nAll other office hours on regular schedule"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#announcements",
    "href": "slides/11-feature-engineering-old.html#announcements",
    "title": "Feature engineering",
    "section": "",
    "text": "Group labs resume this week\nProf. Tackett will not have office hours on Friday\n\nEmail to schedule an appointment if you need to meet\nAll other office hours on regular schedule"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#statistician-of-the-day-rafael-irizarry",
    "href": "slides/11-feature-engineering-old.html#statistician-of-the-day-rafael-irizarry",
    "title": "Feature engineering",
    "section": "Statistician of the day: Rafael Irizarry",
    "text": "Statistician of the day: Rafael Irizarry\n\n\nDr. Rafael Irizarry is a Professor of Biostatistics at the Harvard T.H. Chan School of Public Health and Professor of Biostatistics and Computational Biology at the Dana-Farber Cancer Institute. He earned a Bachelor of Science degree in Mathematics from the University of Puerto Rico at Rio Piedras and a PhD from the University of California, Berkeley in Statistics. Dr. Irizarry’s work is highly cited, and he has been given many prestigious awards including the Presidents’ Award given by the Committee of Presidents of Statistical Societies.\n\n\n\n\n\n\n\n\nSource: hardin47.github.io/CURV/scholars/irizarry"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#work-on-impacts-of-hurricane-maria",
    "href": "slides/11-feature-engineering-old.html#work-on-impacts-of-hurricane-maria",
    "title": "Feature engineering",
    "section": "Work on impacts of Hurricane Maria",
    "text": "Work on impacts of Hurricane Maria\n\nPart of a team that used stratified sampling to survey residents in Puerto Rico about the impacts of the 2017 Hurricane Maria\nEstimated percent of population who lost access to services, such as electricity and water, and the association with remoteness\nUsed confidence intervals to estimate deaths that were directly and indirectly attributable to the hurricane\n\nTheir estimate was more than 70 times the official count\n\n\nArticle: Kishore, N., Marqués, D., Mahmud, A., Kiang, M. V., Rodriguez, I., Fuller, A., ... & Buckee, C. O. (2018). Mortality in Puerto Rico after Hurricane Maria. New England journal of medicine, 379(2), 162-170.\nGitHub repo: github.com/c2-d2/pr_mort_official"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#topics",
    "href": "slides/11-feature-engineering-old.html#topics",
    "title": "Feature engineering",
    "section": "Topics",
    "text": "Topics\n\nUnderstanding categorical predictors and interaction terms\nFeature engineering"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#computational-setup",
    "href": "slides/11-feature-engineering-old.html#computational-setup",
    "title": "Feature engineering",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Dig deeper into tidy modeling with R at https://www.tmwr.org\n\nlibrary(openintro)\n\nLoading required package: airports\nLoading required package: cherryblossom\nLoading required package: usdata\n\nAttaching package: 'openintro'\n\nThe following object is masked from 'package:modeldata':\n\n    ames\n\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(colorblindr)\n\nLoading required package: colorspace\n\nlibrary(gghighlight)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 16))"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#data-peer-to-peer-lender",
    "href": "slides/11-feature-engineering-old.html#data-peer-to-peer-lender",
    "title": "Feature engineering",
    "section": "Data: Peer-to-peer lender",
    "text": "Data: Peer-to-peer lender\nToday’s data is a sample of 50 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package.\n\n\n# A tibble: 50 × 4\n   annual_income debt_to_income verified_income interest_rate\n           &lt;dbl&gt;          &lt;dbl&gt; &lt;fct&gt;                   &lt;dbl&gt;\n 1         59000         0.558  Not Verified            10.9 \n 2         60000         1.31   Not Verified             9.92\n 3         75000         1.06   Verified                26.3 \n 4         75000         0.574  Not Verified             9.92\n 5        254000         0.238  Not Verified             9.43\n 6         67000         1.08   Source Verified          9.92\n 7         28800         0.0997 Source Verified         17.1 \n 8         80000         0.351  Not Verified             6.08\n 9         34000         0.698  Not Verified             7.97\n10         80000         0.167  Source Verified         12.6 \n# ℹ 40 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#variables",
    "href": "slides/11-feature-engineering-old.html#variables",
    "title": "Feature engineering",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nannual_income: Annual income\ndebt_to_income: Debt-to-income ratio, i.e. the percentage of a borrower’s total debt divided by their total income\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\n\nResponse: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#response-interest_rate",
    "href": "slides/11-feature-engineering-old.html#response-interest_rate",
    "title": "Feature engineering",
    "section": "Response: interest_rate",
    "text": "Response: interest_rate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmin\nmedian\nmax\niqr\n\n\n\n\n5.31\n9.93\n26.3\n5.755"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#predictors",
    "href": "slides/11-feature-engineering-old.html#predictors",
    "title": "Feature engineering",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#data-manipulation-1-rescale-income",
    "href": "slides/11-feature-engineering-old.html#data-manipulation-1-rescale-income",
    "title": "Feature engineering",
    "section": "Data manipulation 1: Rescale income",
    "text": "Data manipulation 1: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_th = annual_income / 1000)\n\nggplot(loan50, aes(x = annual_income_th)) +\n  geom_histogram(binwidth = 20) +\n  labs(title = \"Annual income (in $1000s)\", \n       x = \"\")"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#data-manipulation-2-mean-center-numeric-predictors",
    "href": "slides/11-feature-engineering-old.html#data-manipulation-2-mean-center-numeric-predictors",
    "title": "Feature engineering",
    "section": "Data manipulation 2: Mean-center numeric predictors",
    "text": "Data manipulation 2: Mean-center numeric predictors\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    debt_inc_cent = debt_to_income - mean(debt_to_income), \n    annual_income_th_cent = annual_income_th - mean(annual_income_th)\n    )"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "href": "slides/11-feature-engineering-old.html#data-manipulation-3-create-indicator-variables-for-verified_income",
    "title": "Feature engineering",
    "section": "Data manipulation 3: Create indicator variables for verified_income",
    "text": "Data manipulation 3: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#interest-rate-vs.-annual-income",
    "href": "slides/11-feature-engineering-old.html#interest-rate-vs.-annual-income",
    "title": "Feature engineering",
    "section": "Interest rate vs. annual income",
    "text": "Interest rate vs. annual income\nThe lines are not parallel indicating there is an interaction effect. The slope of annual income differs based on the income verification.\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#data-manipulation-4-create-interaction-variables",
    "href": "slides/11-feature-engineering-old.html#data-manipulation-4-create-interaction-variables",
    "title": "Feature engineering",
    "section": "Data manipulation 4: Create interaction variables",
    "text": "Data manipulation 4: Create interaction variables\nDefining the interaction variable in the model formula as verified_income * annual_income_th_cent is an implicit data manipulation step as well\n\n\nRows: 50\nColumns: 9\n$ `(Intercept)`                                          &lt;dbl&gt; 1, 1, 1, 1, 1, …\n$ debt_inc_cent                                          &lt;dbl&gt; -0.16511719, 0.…\n$ annual_income_th_cent                                  &lt;dbl&gt; -27.17, -26.17,…\n$ `verified_incomeNot Verified`                          &lt;dbl&gt; 1, 1, 0, 1, 1, …\n$ `verified_incomeSource Verified`                       &lt;dbl&gt; 0, 0, 0, 0, 0, …\n$ verified_incomeVerified                                &lt;dbl&gt; 0, 0, 1, 0, 0, …\n$ `annual_income_th_cent:verified_incomeNot Verified`    &lt;dbl&gt; -27.17, -26.17,…\n$ `annual_income_th_cent:verified_incomeSource Verified` &lt;dbl&gt; 0.00, 0.00, 0.0…\n$ `annual_income_th_cent:verified_incomeVerified`        &lt;dbl&gt; 0.00, 0.00, -11…"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#interaction-term-in-the-model",
    "href": "slides/11-feature-engineering-old.html#interaction-term-in-the-model",
    "title": "Feature engineering",
    "section": "Interaction term in the model",
    "text": "Interaction term in the model\n\nint_cent_int_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(interest_rate ~ debt_inc_cent  +  verified_income + \n        annual_income_th_cent + verified_income * annual_income_th_cent,\n      data = loan50)\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.484\n0.989\n9.586\n0.000\n\n\ndebt_inc_cent\n0.691\n0.685\n1.009\n0.319\n\n\nverified_incomeSource Verified\n2.157\n1.418\n1.522\n0.135\n\n\nverified_incomeVerified\n7.181\n1.870\n3.840\n0.000\n\n\nannual_income_th_cent\n-0.007\n0.020\n-0.341\n0.735\n\n\nverified_incomeSource Verified:annual_income_th_cent\n-0.016\n0.026\n-0.643\n0.523\n\n\nverified_incomeVerified:annual_income_th_cent\n-0.032\n0.033\n-0.979\n0.333"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#interpreting-interaction-terms",
    "href": "slides/11-feature-engineering-old.html#interpreting-interaction-terms",
    "title": "Feature engineering",
    "section": "Interpreting interaction terms",
    "text": "Interpreting interaction terms\n\nWhat the interaction means: The effect of annual income on the interest rate differs by -0.016 when the income is source verified compared to when it is not verified, holding all else constant.\nInterpreting annual_income for source verified: If the income is source verified, we expect the interest rate to decrease by 0.023% (-0.007 + -0.016) for each additional thousand dollars in annual income, holding all else constant."
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#understanding-the-model",
    "href": "slides/11-feature-engineering-old.html#understanding-the-model",
    "title": "Feature engineering",
    "section": "Understanding the model",
    "text": "Understanding the model\n\\[\n\\begin{aligned}\n\\hat{interest\\_rate} &= 9.484 + 0.691 \\times debt\\_inc\\_cent\\\\ &- 0.007 \\times annual\\_income\\_th\\_cent \\\\ &+ 2.157 \\times SourceVerified + 7.181 \\times Verified \\\\ &- 0.016 \\times annual\\_inc\\_th\\_cent \\times SourceVerified\\\\ &- 0.032 \\times annual\\_inc\\_th\\_cent \\times Verified\n\\end{aligned}\n\\]\n\n\nWhat is \\(p\\), the number of predictor terms in the model?\nWrite the equation of the model to predict interest rate for applicants with Not Verified income.\nWrite the equation of the model to predict interest rate for applicants with Verified income."
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#the-office",
    "href": "slides/11-feature-engineering-old.html#the-office",
    "title": "Feature engineering",
    "section": "The Office",
    "text": "The Office"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#data",
    "href": "slides/11-feature-engineering-old.html#data",
    "title": "Feature engineering",
    "section": "Data",
    "text": "Data\nThe data come from data.world, by way of TidyTuesday\n\noffice_ratings &lt;- read_csv(\"data/office_ratings.csv\")\n\nRows: 188 Columns: 6\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): title\ndbl  (4): season, episode, imdb_rating, total_votes\ndate (1): air_date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\noffice_ratings\n\n# A tibble: 188 × 6\n   season episode title             imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                   &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      1       1 Pilot                     7.6        3706 2005-03-24\n 2      1       2 Diversity Day             8.3        3566 2005-03-29\n 3      1       3 Health Care               7.9        2983 2005-04-05\n 4      1       4 The Alliance              8.1        2886 2005-04-12\n 5      1       5 Basketball                8.4        3179 2005-04-19\n 6      1       6 Hot Girl                  7.8        2852 2005-04-26\n 7      2       1 The Dundies               8.7        3213 2005-09-20\n 8      2       2 Sexual Harassment         8.2        2736 2005-09-27\n 9      2       3 Office Olympics           8.4        2742 2005-10-04\n10      2       4 The Fire                  8.4        2713 2005-10-11\n# ℹ 178 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#imdb-ratings",
    "href": "slides/11-feature-engineering-old.html#imdb-ratings",
    "title": "Feature engineering",
    "section": "IMDB ratings",
    "text": "IMDB ratings"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#imdb-ratings-vs.-number-of-votes",
    "href": "slides/11-feature-engineering-old.html#imdb-ratings-vs.-number-of-votes",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. number of votes",
    "text": "IMDB ratings vs. number of votes"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#outliers",
    "href": "slides/11-feature-engineering-old.html#outliers",
    "title": "Feature engineering",
    "section": "Outliers",
    "text": "Outliers"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#imdb-ratings-vs.-air-date",
    "href": "slides/11-feature-engineering-old.html#imdb-ratings-vs.-air-date",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. air date",
    "text": "IMDB ratings vs. air date"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#imdb-ratings-vs.-seasons",
    "href": "slides/11-feature-engineering-old.html#imdb-ratings-vs.-seasons",
    "title": "Feature engineering",
    "section": "IMDB ratings vs. seasons",
    "text": "IMDB ratings vs. seasons"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#spending-our-data",
    "href": "slides/11-feature-engineering-old.html#spending-our-data",
    "title": "Feature engineering",
    "section": "Spending our data",
    "text": "Spending our data\n\nThere are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.\nDoing all of this on the entire data we have available leaves us with no other data to assess our choices\nWe can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we’ve done so far)"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#splitting-the-data",
    "href": "slides/11-feature-engineering-old.html#splitting-the-data",
    "title": "Feature engineering",
    "section": "Splitting the data",
    "text": "Splitting the data\n\nTake a random sample of X% of the data and set aside (testing data)\n\nTypically 10 - 20%\n\nFit a model on the remaining Y% of the data (training data)\n\nTypically 80 - 90%\n\nUse the coefficients from the model fit on training data to make predictions and evaluate performance on the testing data"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#train-test",
    "href": "slides/11-feature-engineering-old.html#train-test",
    "title": "Feature engineering",
    "section": "Train / test",
    "text": "Train / test\nStep 1: Create an initial split:\n\nset.seed(123)\noffice_split &lt;- initial_split(office_ratings, prop = 0.75) # prop = 0.75 by default\n\n. . .\n\nStep 2: Save training data\n\noffice_train &lt;- training(office_split)\ndim(office_train)\n\n[1] 141   6\n\n\n. . .\n\nStep 3: Save testing data\n\noffice_test  &lt;- testing(office_split)\ndim(office_test)\n\n[1] 47  6"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#training-data",
    "href": "slides/11-feature-engineering-old.html#training-data",
    "title": "Feature engineering",
    "section": "Training data",
    "text": "Training data\n\noffice_train\n\n# A tibble: 141 × 6\n   season episode title               imdb_rating total_votes air_date  \n    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;                     &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;    \n 1      8      18 Last Day in Florida         7.8        1429 2012-03-08\n 2      9      14 Vandalism                   7.6        1402 2013-01-31\n 3      2       8 Performance Review          8.2        2416 2005-11-15\n 4      9       5 Here Comes Treble           7.1        1515 2012-10-25\n 5      3      22 Beach Games                 9.1        2783 2007-05-10\n 6      7       1 Nepotism                    8.4        1897 2010-09-23\n 7      3      15 Phyllis' Wedding            8.3        2283 2007-02-08\n 8      9      21 Livin' the Dream            8.9        2041 2013-05-02\n 9      9      18 Promos                      8          1445 2013-04-04\n10      8      12 Pool Party                  8          1612 2012-01-19\n# ℹ 131 more rows"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#feature-engineering-1",
    "href": "slides/11-feature-engineering-old.html#feature-engineering-1",
    "title": "Feature engineering",
    "section": "Feature engineering",
    "text": "Feature engineering\n\nWe prefer simple (parsimonious) models when possible, but parsimony does not mean sacrificing accuracy (or predictive performance) in the interest of simplicity\nVariables that go into the model and how they are represented are just as critical to success of the model\nFeature engineering allows us to get creative with our predictors in an effort to make them more useful for our model (to increase its predictive performance and improve interpretability)"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#feature-engineering-with-dplyr",
    "href": "slides/11-feature-engineering-old.html#feature-engineering-with-dplyr",
    "title": "Feature engineering",
    "section": "Feature engineering with dplyr",
    "text": "Feature engineering with dplyr\n\noffice_train |&gt;\n  mutate(\n    season = as_factor(season),\n    month = lubridate::month(air_date),\n    wday = lubridate::wday(air_date)\n  )\n\n# A tibble: 141 × 8\n  season episode title            imdb_rating total_votes air_date   month  wday\n  &lt;fct&gt;    &lt;dbl&gt; &lt;chr&gt;                  &lt;dbl&gt;       &lt;dbl&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 8           18 Last Day in Flo…         7.8        1429 2012-03-08     3     5\n2 9           14 Vandalism                7.6        1402 2013-01-31     1     5\n3 2            8 Performance Rev…         8.2        2416 2005-11-15    11     3\n4 9            5 Here Comes Treb…         7.1        1515 2012-10-25    10     5\n5 3           22 Beach Games              9.1        2783 2007-05-10     5     5\n6 7            1 Nepotism                 8.4        1897 2010-09-23     9     5\n# ℹ 135 more rows\n\n\n. . .\n\nCan you identify any potential problems with this approach?"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#modeling-workflow",
    "href": "slides/11-feature-engineering-old.html#modeling-workflow",
    "title": "Feature engineering",
    "section": "Modeling workflow",
    "text": "Modeling workflow\n\n\n\nCreate a recipe for feature engineering steps to be applied to the training data\nFit the model to the training data after these steps have been applied\nUsing the model estimates from the training data, predict outcomes for the test data\nEvaluate the performance of the model on the test data"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#initiate-a-recipe",
    "href": "slides/11-feature-engineering-old.html#initiate-a-recipe",
    "title": "Feature engineering",
    "section": "Initiate a recipe",
    "text": "Initiate a recipe\n\noffice_rec &lt;- recipe(\n  imdb_rating ~ .,    # formula\n  data = office_train # data for cataloging names and types of variables\n  )\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 5"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#step-1-alter-roles",
    "href": "slides/11-feature-engineering-old.html#step-1-alter-roles",
    "title": "Feature engineering",
    "section": "Step 1: Alter roles",
    "text": "Step 1: Alter roles\ntitle isn’t a predictor, but we might want to keep it around as an ID\n\noffice_rec &lt;- office_rec |&gt;\n  update_role(title, new_role = \"ID\")\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#step-2-add-features",
    "href": "slides/11-feature-engineering-old.html#step-2-add-features",
    "title": "Feature engineering",
    "section": "Step 2: Add features",
    "text": "Step 2: Add features\nNew features for day of week and month\n\noffice_rec &lt;- office_rec |&gt;\n  step_date(air_date, features = c(\"dow\", \"month\"))\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#step-3-add-more-features",
    "href": "slides/11-feature-engineering-old.html#step-3-add-more-features",
    "title": "Feature engineering",
    "section": "Step 3: Add more features",
    "text": "Step 3: Add more features\nIdentify holidays in air_date, then remove air_date\n\noffice_rec &lt;- office_rec |&gt;\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  )\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#step-4-convert-numbers-to-factors",
    "href": "slides/11-feature-engineering-old.html#step-4-convert-numbers-to-factors",
    "title": "Feature engineering",
    "section": "Step 4: Convert numbers to factors",
    "text": "Step 4: Convert numbers to factors\nConvert season to factor\n\noffice_rec &lt;- office_rec |&gt;\n  step_num2factor(season, levels = as.character(1:9))\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#step-5-make-dummy-variables",
    "href": "slides/11-feature-engineering-old.html#step-5-make-dummy-variables",
    "title": "Feature engineering",
    "section": "Step 5: Make dummy variables",
    "text": "Step 5: Make dummy variables\nConvert all nominal (categorical) predictors to factors\n\noffice_rec &lt;- office_rec |&gt;\n  step_dummy(all_nominal_predictors())\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#step-6-remove-zero-variance-predictors",
    "href": "slides/11-feature-engineering-old.html#step-6-remove-zero-variance-predictors",
    "title": "Feature engineering",
    "section": "Step 6: Remove zero variance predictors",
    "text": "Step 6: Remove zero variance predictors\nRemove all predictors that contain only a single value\n\noffice_rec &lt;- office_rec |&gt;\n  step_zv(all_predictors())\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#putting-it-all-together",
    "href": "slides/11-feature-engineering-old.html#putting-it-all-together",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec &lt;- recipe(imdb_rating ~ ., data = office_train) |&gt;\n  # make title's role ID\n  update_role(title, new_role = \"ID\") |&gt;\n  # extract day of week and month of air_date\n  step_date(air_date, features = c(\"dow\", \"month\")) |&gt;\n  # identify holidays and add indicators\n  step_holiday(\n    air_date, \n    holidays = c(\"USThanksgivingDay\", \"USChristmasDay\", \"USNewYearsDay\", \"USIndependenceDay\"), \n    keep_original_cols = FALSE\n  ) |&gt;\n  # turn season into factor\n  step_num2factor(season, levels = as.character(1:9)) |&gt;\n  # make dummy variables\n  step_dummy(all_nominal_predictors()) |&gt;\n  # remove zero variance predictors\n  step_zv(all_predictors())"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#putting-it-all-together-1",
    "href": "slides/11-feature-engineering-old.html#putting-it-all-together-1",
    "title": "Feature engineering",
    "section": "Putting it all together",
    "text": "Putting it all together\n\noffice_rec\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 4\nID:        1\n\n\n\n\n\n── Operations \n\n\n• Date features from: air_date\n\n\n• Holiday features from: air_date\n\n\n• Factor variables from: season\n\n\n• Dummy variables from: all_nominal_predictors()\n\n\n• Zero variance filter on: all_predictors()"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#next-step",
    "href": "slides/11-feature-engineering-old.html#next-step",
    "title": "Feature engineering",
    "section": "Next step…",
    "text": "Next step…\nWe will complete the workflow to fit a model predicting IMDB ratings that includes the following predictors:\n\nepisode\ntotal_votes\nindicator variables for season\nindicator variables for day of week aired (created using air_date)\nindicator variables for month aired (created using air_date)\n\n. . .\n\nWhat feature will not end up in the final model? Why is it not included?"
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#working-with-recipes",
    "href": "slides/11-feature-engineering-old.html#working-with-recipes",
    "title": "Feature engineering",
    "section": "Working with recipes",
    "text": "Working with recipes\n\nWhen building recipes you in a pipeline, you don’t get to see the effect of the recipe on your data, which can be unsettling\nYou can take a peek at what will happen when you ultimately apply the recipe to your data at the time of fitting the model\nThis requires two functions: prep() to train the recipe and bake() to apply it to your data\n\n\n\n\n\n\n\nNote\n\n\n\nThis is optional, we’ll show the results for demonstrative purposes. It doesn’t need to be part of your modeling pipeline, but it can be assuring to see the effects of the recipe steps as you build the recipe."
  },
  {
    "objectID": "slides/11-feature-engineering-old.html#recap",
    "href": "slides/11-feature-engineering-old.html#recap",
    "title": "Feature engineering",
    "section": "Recap",
    "text": "Recap\n\n\nReview: Training and testing splits\nFeature engineering with recipes"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html",
    "href": "ae/ae-12-categorical-predictors.html",
    "title": "AE 12: Categorical Predictors",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-12”.\nGo to the Canvas and locate your AE-12 assignment to get started.\nUpload the ae-12.qmd file into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment."
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#packages-data",
    "href": "ae/ae-12-categorical-predictors.html#packages-data",
    "title": "AE 12: Categorical Predictors",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(mosaic)\nlibrary(broom)\nlibrary(knitr)\nlibrary(openintro)\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_k = annual_income / 1000)\n\nThe data for this AE is is a sample of 10,000 loans made through a peer-to-peer lending club. The data is in the loan50 data frame in the openintro R package."
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#variables",
    "href": "ae/ae-12-categorical-predictors.html#variables",
    "title": "AE 12: Categorical Predictors",
    "section": "Variables",
    "text": "Variables\n\nannual_income_k: Annual income in $1,000’s\nverified_income: Whether borrower’s income source and amount have been verified (Not Verified, Source Verified, Verified)\n\nResponse: interest_rate: Interest rate for the loan"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#analysis-goal",
    "href": "ae/ae-12-categorical-predictors.html#analysis-goal",
    "title": "AE 12: Categorical Predictors",
    "section": "Analysis goal",
    "text": "Analysis goal\n\nPredict interest_rate using the categorical variable verified_income as a predictor\nInclude other quantitative variables and understand how they interact with verified_income"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#exercise-0-optional",
    "href": "ae/ae-12-categorical-predictors.html#exercise-0-optional",
    "title": "AE 12: Categorical Predictors",
    "section": "Exercise 0 (Optional)",
    "text": "Exercise 0 (Optional)\nWhat is a debt-to-income ratio? Feel free to use Google."
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#exercise-1",
    "href": "ae/ae-12-categorical-predictors.html#exercise-1",
    "title": "AE 12: Categorical Predictors",
    "section": "Exercise 1",
    "text": "Exercise 1\nGenerate side-by-side boxplots of interest_rate vs. verified_income. Does it appear that there is a relationship between the two variables?"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#exercise-2",
    "href": "ae/ae-12-categorical-predictors.html#exercise-2",
    "title": "AE 12: Categorical Predictors",
    "section": "Exercise 2",
    "text": "Exercise 2\nBased on the output of the code below, what do you think would be the best predictions for the interest rate of a borrow with Not Verified, Source Verified, and Verified income, respectively.\n\nfavstats(interest_rate ~ verified_income, data = loan50) |&gt; \n  kable()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nverified_income\nmin\nQ1\nmedian\nQ3\nmax\nmean\nsd\nn\nmissing\n\n\n\n\n\nNA\nNA\nNA\nNA\nNA\nNaN\nNA\n0\n0\n\n\nNot Verified\n5.31\n7.9600\n9.44\n9.9300\n18.45\n9.541429\n2.984269\n21\n0\n\n\nSource Verified\n6.08\n7.8075\n10.91\n16.2875\n19.42\n11.765500\n4.270998\n20\n0\n\n\nVerified\n5.32\n11.9800\n14.08\n21.4500\n26.30\n15.853333\n7.694652\n9\n0"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#exercise-3",
    "href": "ae/ae-12-categorical-predictors.html#exercise-3",
    "title": "AE 12: Categorical Predictors",
    "section": "Exercise 3",
    "text": "Exercise 3\nFit a linear model predicting interest_rate from verified_income. What is the reference level for verified_income?"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#exercise-4",
    "href": "ae/ae-12-categorical-predictors.html#exercise-4",
    "title": "AE 12: Categorical Predictors",
    "section": "Exercise 4",
    "text": "Exercise 4\nWITHOUT WRITING ANY CODE except for addition, subtraction, multiplication, and addition, what would the model predict the average interest_rate for each of the three levels of verified_income? How do these answers compare to your answers from Exercise 2?"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#to-submit-the-ae",
    "href": "ae/ae-12-categorical-predictors.html#to-submit-the-ae",
    "title": "AE 12: Categorical Predictors",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "slides/17-categorical-predictors.html#announcements-getting-started",
    "href": "slides/17-categorical-predictors.html#announcements-getting-started",
    "title": "Categorical Predictors",
    "section": "Announcements & Getting Started",
    "text": "Announcements & Getting Started\n\nExam Review Wednesday\nExam Friday\nProject proposals due Friday!\n\nAccepted without penalty through Monday 10/14 at 11:59pm\n\nDon’t forget about Oral R Quiz!\n\nHappy to administer over Teams during the break if you’d like…\n\n\n\n📋 AE 12 - P2P Loans\n\nOpen up AE 12\nComplete Exercise 0 if you have time."
  },
  {
    "objectID": "slides/17-categorical-predictors.html#data-manipulation-rescale-income",
    "href": "slides/17-categorical-predictors.html#data-manipulation-rescale-income",
    "title": "Categorical Predictors",
    "section": "Data manipulation: Rescale income",
    "text": "Data manipulation: Rescale income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(annual_income_k = annual_income / 1000)\n\nloan50 |&gt; \n  gf_histogram(~annual_income_k, binwidth = 20) |&gt; \n  gf_labs(title = \"Annual income (in $1000s)\", x = \"\")"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#indicator-variables",
    "href": "slides/17-categorical-predictors.html#indicator-variables",
    "title": "Categorical Predictors",
    "section": "Indicator variables",
    "text": "Indicator variables\n\nSuppose there is a categorical variable with \\(K\\) categories (levels)\nWe can make \\(K\\) indicator variables - one indicator for each category\nAn indicator variable takes values 1 or 0\n\n1 if the observation belongs to that category\n0 if the observation does not belong to that category"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#data-manipulation-create-indicator-variables-for-verified_income",
    "href": "slides/17-categorical-predictors.html#data-manipulation-create-indicator-variables-for-verified_income",
    "title": "Categorical Predictors",
    "section": "Data manipulation: Create indicator variables for verified_income",
    "text": "Data manipulation: Create indicator variables for verified_income\n\nloan50 &lt;- loan50 |&gt;\n  mutate(\n    not_verified = if_else(verified_income == \"Not Verified\", 1, 0),\n    source_verified = if_else(verified_income == \"Source Verified\", 1, 0),\n    verified = if_else(verified_income == \"Verified\", 1, 0)\n  )\n\nloan50 |&gt;\n  select(verified_income, not_verified, source_verified, verified) |&gt;\n  slice(1, 3, 6)\n\n# A tibble: 3 × 4\n  verified_income not_verified source_verified verified\n  &lt;fct&gt;                  &lt;dbl&gt;           &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified               1               0        0\n2 Verified                   0               0        1\n3 Source Verified            0               1        0"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#indicators-in-the-model",
    "href": "slides/17-categorical-predictors.html#indicators-in-the-model",
    "title": "Categorical Predictors",
    "section": "Indicators in the model",
    "text": "Indicators in the model\n\nWe will use \\(K-1\\) of the indicator variables in the model.\nThe reference level or baseline is the category that doesn’t have a term in the model.\nThe coefficients of the indicator variables in the model are interpreted as the expected change in the response compared to the baseline, holding all other variables constant.\nThis approach is also called dummy coding and R will do this for you\n\n\nloan50 |&gt;\n  select(verified_income, source_verified, verified) |&gt;\n  slice(1, 3, 6)\n\n# A tibble: 3 × 3\n  verified_income source_verified verified\n  &lt;fct&gt;                     &lt;dbl&gt;    &lt;dbl&gt;\n1 Not Verified                  0        0\n2 Verified                      0        1\n3 Source Verified               1        0"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#application-exercise",
    "href": "slides/17-categorical-predictors.html#application-exercise",
    "title": "Categorical Predictors",
    "section": "Application Exercise",
    "text": "Application Exercise\n\nComplete Exercises 3 & 4"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#interpreting-verified_income",
    "href": "slides/17-categorical-predictors.html#interpreting-verified_income",
    "title": "Categorical Predictors",
    "section": "Interpreting verified_income",
    "text": "Interpreting verified_income\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n9.541\n1.006\n9.487\n0.000\n\n\nverified_incomeSource Verified\n2.224\n1.440\n1.544\n0.129\n\n\nverified_incomeVerified\n6.312\n1.836\n3.437\n0.001\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhere do we see each of the estimates in the plot?\nWhere do we see the values we’d predict in the plot?\nAre verified_income and interest_rate correlated?\n\n\n\n\n\n−+\n03:00"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#model-equation",
    "href": "slides/17-categorical-predictors.html#model-equation",
    "title": "Categorical Predictors",
    "section": "Model equation",
    "text": "Model equation\n\\[\n\\begin{align}\\hat{\\text{interest_rate}} =  9.541 &+ 2.224 \\times \\text{source_verified}\\\\  \n&+ 6.312 \\times \\text{verified}\n\\end{align}\n\\]"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#exercise-5",
    "href": "ae/ae-12-categorical-predictors.html#exercise-5",
    "title": "AE 12: Categorical Predictors",
    "section": "Exercise 5",
    "text": "Exercise 5\nThe linear model below predicts interest_rate from annual_income_k. Add verified_income as a predictor to this model. Do not include an interaction term. Be prepared to discuss how and why the plot changes when you add in verified_income.\n\nex5_model &lt;- lm(interest_rate ~ annual_income_k, data = loan50) \nex5_model |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.994265\n1.2851395\n10.111171\n0.0000000\n\n\nannual_income_k\n-0.016561\n0.0124397\n-1.331308\n0.1893763\n\n\n\n\nplotModel(ex5_model) # nifty function from the mosaic package"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#exercise-6",
    "href": "ae/ae-12-categorical-predictors.html#exercise-6",
    "title": "AE 12: Categorical Predictors",
    "section": "Exercise 6",
    "text": "Exercise 6\nHow do you think the plot above will change if you add in an interaction term between verified_income and interest_rate? AFTER thinking about it, add in an interaction term between verified_income and annual_income_k.\n\nex6_model &lt;- lm(interest_rate ~ annual_income_k, data = loan50) \nex6_model |&gt; \n  tidy() |&gt; \n  kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n12.994265\n1.2851395\n10.111171\n0.0000000\n\n\nannual_income_k\n-0.016561\n0.0124397\n-1.331308\n0.1893763\n\n\n\n\nplotModel(ex6_model) # nifty function from the mosaic package"
  },
  {
    "objectID": "ae/ae-12-categorical-predictors.html#exercise-7",
    "href": "ae/ae-12-categorical-predictors.html#exercise-7",
    "title": "AE 12: Categorical Predictors",
    "section": "Exercise 7",
    "text": "Exercise 7\nBased on the model above (and the equation on the slides):\n\nWrite the equation of the model to predict interest rate for applicants with Not Verified income.\nWrite the equation of the model to predict interest rate for applicants with Verified income.\nOur degrees of freedom will be \\(n-p-1\\). What is \\(p\\) in this case? Hint: it isn’t 2."
  },
  {
    "objectID": "slides/17-categorical-predictors.html#interest-rate-vs.-annual-income-parallel-slopes",
    "href": "slides/17-categorical-predictors.html#interest-rate-vs.-annual-income-parallel-slopes",
    "title": "Categorical Predictors",
    "section": "Interest rate vs. annual income: parallel slopes",
    "text": "Interest rate vs. annual income: parallel slopes"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#interpretation",
    "href": "slides/17-categorical-predictors.html#interpretation",
    "title": "Categorical Predictors",
    "section": "Interpretation",
    "text": "Interpretation\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n11.388\n1.352\n8.423\n0.000\n\n\nannual_income_k\n-0.022\n0.011\n-1.974\n0.054\n\n\nverified_incomeSource Verified\n2.171\n1.398\n1.553\n0.127\n\n\nverified_incomeVerified\n6.792\n1.799\n3.776\n0.000\n\n\n\n\n\n\nSlope of annual_income_k is -0.022 regardless of verified_income level\nChange in verified_income implies a shift in the intercept\nIntercept for Not Verified is 11.388\nFor Source Verified shift intercept up 2.171 - (i.e. intercept \\(= 11.388 + 2.171 = 13.599\\))\nFor Verified shift intercept up 6.792 from Not Verified - (i.e. intercept \\(= 11.388 + 6.792 = 18.180\\))"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#interest-rate-vs.-annual-income-interaction-term",
    "href": "slides/17-categorical-predictors.html#interest-rate-vs.-annual-income-interaction-term",
    "title": "Categorical Predictors",
    "section": "Interest rate vs. annual income: interaction term",
    "text": "Interest rate vs. annual income: interaction term"
  },
  {
    "objectID": "slides/17-categorical-predictors.html#parallel-slopes-interpretation",
    "href": "slides/17-categorical-predictors.html#parallel-slopes-interpretation",
    "title": "Categorical Predictors",
    "section": "Parallel slopes interpretation",
    "text": "Parallel slopes interpretation\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n11.388\n1.352\n8.423\n0.000\n\n\nannual_income_k\n-0.022\n0.011\n-1.974\n0.054\n\n\nverified_incomeSource Verified\n2.171\n1.398\n1.553\n0.127\n\n\nverified_incomeVerified\n6.792\n1.799\n3.776\n0.000\n\n\n\n\n\n\nSlope of annual_income_k is -0.022 regardless of verified_income level\nChange in verified_income corresponds to a shift in the intercept\n\nIntercept for Not Verified is 11.388\nFor Source Verified shift intercept up 2.171\n\n(i.e. intercept \\(= 11.388 + 2.171 = 13.599\\))\n\nFor Verified shift intercept up 6.792 from Not Verified\n\n(i.e. intercept \\(= 11.388 + 6.792 = 18.180\\))"
  },
  {
    "objectID": "ae/ae-08-exam-01-review.html#exercise-1-1",
    "href": "ae/ae-08-exam-01-review.html#exercise-1-1",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 1",
    "text": "Exercise 1\n\n\n\n\n\n\nWrite the statistical model that we will be trying to estimate. Use greek letters and include an error term."
  },
  {
    "objectID": "slides/18-exam-01-review.html#announcements",
    "href": "slides/18-exam-01-review.html#announcements",
    "title": "Exam 01 review",
    "section": "Announcements",
    "text": "Announcements\n\nExam Friday!\nProposal due Friday!\nAccepted until Monday\nDon’t forget about Oral R Quiz!\n\nHappy to administer over Teams during the break if you’d like…"
  },
  {
    "objectID": "slides/18-exam-01-review.html#exam-01",
    "href": "slides/18-exam-01-review.html#exam-01",
    "title": "Exam 01 review",
    "section": "Exam 01",
    "text": "Exam 01\n\n\n\n15% of final course grade\nExploratory data analysis\nFitting and interpreting simple linear regression models\nStatistical models and regression equations\nSimulation-based inference\n\n\n\nMathematical models for inference\nPrediction\nModel conditions\nModel evaluation\nMultiple linear regression (including basics of categorical response variables)"
  },
  {
    "objectID": "slides/18-exam-01-review.html#outline-of-in-class-portion",
    "href": "slides/18-exam-01-review.html#outline-of-in-class-portion",
    "title": "Exam 01 review",
    "section": "Outline of in-class portion",
    "text": "Outline of in-class portion\n\nClosed-book, closed-note.\n10 questions, some with multiple parts\nQuestion types:\n\nMultiple choice\nShort answer (no more than 3 sentences)\nTrue/ False.\n\nIf false, write 1 - 2 sentence justification about why it is false.\n\nEvaluate a response.\n\nLast page (front and back) will have analysis output.\nJust need a pencil. No calculator permitted on exam."
  },
  {
    "objectID": "ae/exam-01-review.html",
    "href": "ae/exam-01-review.html",
    "title": "Exam 01 Review/Practice",
    "section": "",
    "text": "Important\n\n\n\nNote that you are not required to turn anything in for this assignment and you should complete this on a sheet of paper."
  },
  {
    "objectID": "ae/exam-01-review.html#packages",
    "href": "ae/exam-01-review.html#packages",
    "title": "Exam 01 Review/Practice",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(yardstick)\nlibrary(ggformula)\nlibrary(knitr)\nlibrary(infer)\nlibrary(patchwork)"
  },
  {
    "objectID": "ae/exam-01-review.html#restaurant-tips",
    "href": "ae/exam-01-review.html#restaurant-tips",
    "title": "Exam 01 Review/Practice",
    "section": "Restaurant tips",
    "text": "Restaurant tips\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nMeal: which meal this was (Lunch, Late Lunch, Dinner)\nParty: number of people in the party\n\nView the data set to see the remaining variables.\n\ntips &lt;- read_csv(\"data/tip-data.csv\")\n\nglimpse(tips)\n\nRows: 169\nColumns: 13\n$ Day              &lt;chr&gt; \"Saturday\", \"Saturday\", \"Tuesday\", \"Tuesday\", \"Tuesda…\n$ Meal             &lt;chr&gt; \"Dinner\", \"Dinner\", \"Dinner\", \"Dinner\", \"Dinner\", \"Di…\n$ Payment          &lt;chr&gt; \"Credit\", \"Credit\", \"Credit\", \"Credit\", \"Cash\", \"Cred…\n$ Party            &lt;dbl&gt; 1, 1, 1, 3, 2, 2, 4, 3, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2,…\n$ Age              &lt;chr&gt; \"Yadult\", \"Yadult\", \"SenCit\", \"Middle\", \"SenCit\", \"Mi…\n$ GiftCard         &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Comps            &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\"…\n$ Alcohol          &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bday             &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\", \"No\",…\n$ Bill             &lt;dbl&gt; 17.01, 14.23, 20.97, 20.87, 34.66, 25.15, 48.17, 22.6…\n$ `W/Tip`          &lt;dbl&gt; 20.00, 16.23, 25.97, 24.87, 45.00, 30.00, 53.17, 26.6…\n$ Tip              &lt;dbl&gt; 2.99, 2.00, 5.00, 4.00, 10.34, 4.85, 5.00, 4.00, 5.00…\n$ `Tip Percentage` &lt;dbl&gt; 0.1757790, 0.1405481, 0.2384359, 0.1916627, 0.2983266…"
  },
  {
    "objectID": "ae/exam-01-review.html#exploratory-data-analysis",
    "href": "ae/exam-01-review.html#exploratory-data-analysis",
    "title": "Exam 01 Review/Practice",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\np1 &lt;- tips |&gt; \n  gf_histogram(~Tip, color = \"white\", binwidth = 2) |&gt; \n  gf_labs(x = \"Tips ($)\",\n       title = \"Tips at local restaurant\")\n\np2 &lt;-  tips |&gt; \n  gf_histogram(~Party, color = \"white\") |&gt; \n  gf_labs(x = \"Party\",\n       title = \"Number of diners in party\") |&gt; \n  gf_lims(x=c(0, 7))\n\np3 &lt;- tips |&gt;\n  gf_bar(~ Meal) |&gt; \n  gf_labs(x = \"Meal\",\n       title = \"Number of meal types\")\n\np4 &lt;- tips |&gt;\n  gf_jitter(Tip ~ Party) |&gt; \n  gf_labs(x = \"Number of diners in party\", \n       y = \"Tips ($)\",\n       title = \"Tips vs. Party\")\n\np5 &lt;- tips |&gt;\n  gf_jitter(Tip ~ Meal, height = 0, width = 0.25) |&gt; \n  gf_labs(x = \"Meal\", \n       y = \"Tips ($)\",\n       title = \"Tips vs. Meal\")\n\n(p1 + p2) / p3\n\n\n\n\n\n\n\n(p4 + p5)\n\n\n\n\n\n\n\n\nThe goal is to fit a model that uses the number of diners in the party and the meal to understand variability in the tips. For Exercise 2-8, assume we are only using Party and not Meal to predict Tip."
  },
  {
    "objectID": "ae/exam-01-review.html#exercise-1",
    "href": "ae/exam-01-review.html#exercise-1",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nWhat is another type of plot we could use to look at the relationship between Meal and Tips?\nWhy might we want to use a jitter plot instead of a scatter plot to display the relationship between Party and Tips?"
  },
  {
    "objectID": "ae/exam-01-review.html#exercise-2",
    "href": "ae/exam-01-review.html#exercise-2",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 2",
    "text": "Exercise 2\nWrite the statistical model that we will be trying to estimate. Use Greek letters and include an error term."
  },
  {
    "objectID": "ae/exam-01-review.html#exercise-3",
    "href": "ae/exam-01-review.html#exercise-3",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 3",
    "text": "Exercise 3\nThe regression output with 90% confidence intervals for the coefficients is shown below.\n\ntips_fit &lt;- lm(Tip ~ Party, data = tips)\n\ntips_fit |&gt; \n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.383\n0.321\n1.195\n0.234\n\n\nParty\n1.957\n0.118\n16.553\n0.000\n\n\n\n\n\n\nWrite the regression equation and interpret the slope in the context of the data.\nWhat would your model predict is the Tip for a party of size 3?\nOne family for four walks into your restaurant. Consider the following two predictions and intervals for the Tip left by this family. Which one is appropriate for this scenario? If you chose the wider one, explain why it is larger. If you chose the narrower one, explain why it is smaller.\n\n\nfamily &lt;- tibble(Party = 4)\npredict(tips_fit, newdata = family, interval = \"confidence\") |&gt; kable(digits = 3)\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n8.21\n7.712\n8.709\n\n\n\n\npredict(tips_fit, newdata = family, interval = \"prediction\") |&gt; kable(digits = 3)\n\n\n\n\nfit\nlwr\nupr\n\n\n\n\n8.21\n4.068\n12.353"
  },
  {
    "objectID": "ae/exam-01-review.html#exercise-4",
    "href": "ae/exam-01-review.html#exercise-4",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nglance(tips_fit) |&gt; kable(digits = 3)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.621\n0.619\n2.083\n273.996\n0\n1\n-362.794\n731.588\n740.978\n724.472\n167\n169\n\n\n\n\n\nWhat is the regression error for the model. Write the definition of this value in the context of the data."
  },
  {
    "objectID": "ae/exam-01-review.html#exercise-5",
    "href": "ae/exam-01-review.html#exercise-5",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 5",
    "text": "Exercise 5\n\ntips_aug &lt;- augment(tips_fit)\n\nrsq(tips_aug, truth = Tip, estimate = .fitted) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrsq\nstandard\n0.6213116\n\n\n\n\nrmse(tips_aug, truth = Tip, estimate = .fitted) |&gt; kable()\n\n\n\n\n.metric\n.estimator\n.estimate\n\n\n\n\nrmse\nstandard\n2.070463\n\n\n\n\n\n\nDefine \\(R^2\\) and interpret it in the context of the data.\nDefine RMSE and interpret it in the context of the data."
  },
  {
    "objectID": "ae/exam-01-review.html#exercise-6",
    "href": "ae/exam-01-review.html#exercise-6",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 6",
    "text": "Exercise 6\nThe following code can be used to create a bootstrap distribution for the slope (and the intercept, though we’ll focus primarily on the slope in our inference) for the coefficient of Party in our linear model. Use the plot below to (visually) construct a 90% confidence interval for the slope:\n\nDescribe why you chose the values you chose for your interval.\nInterpret the interval in the context of the data.\nHow would increasing the number of repetitions change the size of the confidence interval?\nHow would increasing the sample size change the size of the confidence interval?\nHow would increasing the confidence level change the size of the condidence interval?\n\n\nset.seed(1234)\n\nboot_dist &lt;- tips |&gt;\n  specify(Tip ~ Party) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  fit()\n\nboot_dist |&gt;\n  filter(term == \"Party\") |&gt; \n  gf_histogram(~estimate)"
  },
  {
    "objectID": "ae/exam-01-review.html#exercise-7",
    "href": "ae/exam-01-review.html#exercise-7",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 7",
    "text": "Exercise 7\nSet-up a hypothesis test for the slope of Party. Make sure to include:\n\nBoth hypotheses in mathematical notation and words.\nThe test statistic.\nThe distribution of the test statistic.\nThe p-value.\nThe result of your test at a significance level of \\(\\alpha= 0.05\\).\n\nYou may want to refer to the output in Exercise 3."
  },
  {
    "objectID": "ae/exam-01-review.html#exercise-8",
    "href": "ae/exam-01-review.html#exercise-8",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 8",
    "text": "Exercise 8\nList the conditions necessary for conducting inference. Include how you would test each one and how you would determine if they were satisfied.\nExtra credit: Based on the context of this problem, you should expect that the constant variance assumption is likely to be violated… why? Think about how people tip."
  },
  {
    "objectID": "ae/exam-01-review.html#exercise-9",
    "href": "ae/exam-01-review.html#exercise-9",
    "title": "Exam 01 Review/Practice",
    "section": "Exercise 9",
    "text": "Exercise 9\nConsider two models:\nModel 1\n\nlm(Tip ~ Party + Meal, data = tips) |&gt;\n  tidy() |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.142\n0.341\n3.345\n0.001\n\n\nParty\n1.852\n0.116\n15.984\n0.000\n\n\nMealLate Night\n-1.842\n0.386\n-4.774\n0.000\n\n\nMealLunch\n-0.582\n0.402\n-1.446\n0.150\n\n\n\n\n\nModel 2\n\nlm(Tip ~ Party + Meal + Party*Meal, data = tips) |&gt;\n  tidy() |&gt; \n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n1.209\n0.453\n2.672\n0.008\n\n\nParty\n1.824\n0.169\n10.795\n0.000\n\n\nMealLate Night\n-2.136\n0.682\n-3.133\n0.002\n\n\nMealLunch\n-0.419\n0.867\n-0.484\n0.629\n\n\nParty:MealLate Night\n0.159\n0.283\n0.560\n0.576\n\n\nParty:MealLunch\n-0.052\n0.283\n-0.184\n0.854\n\n\n\n\n\n\nFor model 1, interpret the slope and p-value of Party in the context of the problem.\nFor model 1, what is the reference level of Meal.\nWhich model has the lower RMSE? How do you know that this is true without being able to see it?\nConsider the following two plots of the data. Roughly sketch model 1 on the left and model 2 on the right. You will receive full credit if (a) positive slopes are positive and negative slopes are negative, (b) steeper slopes are steeper, and (c) higher intercepts are higher.\n\n\np1 &lt;- tips |&gt; \n  gf_jitter(Tip ~ Party, color = ~Meal) |&gt; \n  gf_labs(title = \"Sketch model 1\")\n\np2 &lt;- tips |&gt; \n  gf_jitter(Tip ~ Party, color = ~Meal) |&gt; \n  gf_labs(title = \"Sketch model 2\")\n\np1 + p2 + plot_layout(guides = \"collect\")"
  },
  {
    "objectID": "ae/exam-01-review.html#footnotes",
    "href": "ae/exam-01-review.html#footnotes",
    "title": "Exam 01 Review/Practice",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.↩︎"
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html",
    "href": "ae/ae-13-mlr-conditions.html",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-13”.\nGo to the Canvas and locate your AE-13 assignment to get started.\nUpload the ae-13.qmd and rail_trail.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment."
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#packages-data",
    "href": "ae/ae-13-mlr-conditions.html#packages-data",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(ggformula)\nlibrary(broom)\nlibrary(knitr)\nlibrary(mosaic)\nlibrary(mosaicData)\n\nrail_trail &lt;- read_csv(\"data/rail_trail.csv\")\n\nThe data for this AE is based on data from the Pioneer Valley Planning Commission (PVPC) and is included in the mosaicData package. The PVPC collected data for ninety days from April 5, 2005 to November 15, 2005. Data collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station. More information can be found here."
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#analysis-goal",
    "href": "ae/ae-13-mlr-conditions.html#analysis-goal",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this activity are to:\n\nDetermine whether the conditions for inference are satisfied in this multi-predictor setting."
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#exercise-0",
    "href": "ae/ae-13-mlr-conditions.html#exercise-0",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Exercise 0",
    "text": "Exercise 0\nFit a linear model to predict volume from ALL of the other predictors. The resulting model is called the “full model”. Hint: if you use the formula volume ~ . in your lm command, it will automatically include all predictors. Once you have fit your model, use tidy to print it out. Have the reporter for you group write down the model on the white board. Does the model include any interaction terms?"
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#exercise-1",
    "href": "ae/ae-13-mlr-conditions.html#exercise-1",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Exercise 1",
    "text": "Exercise 1\nAugment the model you created above using the augment function. Generate a scatter plot of the residuals vs. the fitted values for this model."
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#exercise-2",
    "href": "ae/ae-13-mlr-conditions.html#exercise-2",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Exercise 2",
    "text": "Exercise 2\nMake two plots:\n\nResiduals vs. precip.\nResiduals vs. day_type."
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#exercise-3",
    "href": "ae/ae-13-mlr-conditions.html#exercise-3",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Exercise 3",
    "text": "Exercise 3\nBased on the three plots you’ve made and the four on the screen, do you think the linearity condition is satisfied?"
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#exercise-4",
    "href": "ae/ae-13-mlr-conditions.html#exercise-4",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Exercise 4",
    "text": "Exercise 4\nWhat does it mean for two things to be independent in statistics (feel free to use google)? Do we think our p-values/confidence intervals are independent across variables?"
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#to-submit-the-ae",
    "href": "ae/ae-13-mlr-conditions.html#to-submit-the-ae",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "slides/19-mlr-conditions.html",
    "href": "slides/19-mlr-conditions.html",
    "title": "MLR: Inference and conditions",
    "section": "",
    "text": "Midterm next Friday 10/11 (right before spring break)\nProject proposal also due 10/11 but will accept until 10/14 without penalty"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#announcements",
    "href": "slides/19-mlr-conditions.html#announcements",
    "title": "MLR: Conditions",
    "section": "Announcements",
    "text": "Announcements\n\nProject: EDA Due Wednesday, October 30th\nOral R Quiz (time to start scheduling it)\n\n\n📋 AE 13 - Rail Trails\n\nOpen up AE 13"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#topics",
    "href": "slides/19-mlr-conditions.html#topics",
    "title": "MLR: Conditions",
    "section": "Topics",
    "text": "Topics\n\n\nChecking model conditions"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#computational-setup",
    "href": "slides/19-mlr-conditions.html#computational-setup",
    "title": "MLR: Conditions",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(mosaicData)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#data-credit-cards",
    "href": "slides/19-mlr-conditions.html#data-credit-cards",
    "title": "MLR: Inference and conditions",
    "section": "Data: Credit Cards",
    "text": "Data: Credit Cards\nThe data is from the Credit data set in the ISLR2 R package. It is a simulated data set of 400 credit card customers.\n\n\nRows: 400\nColumns: 11\n$ Income    &lt;dbl&gt; 14.891, 106.025, 104.593, 148.924, 55.882, 80.180, 20.996, 7…\n$ Limit     &lt;dbl&gt; 3606, 6645, 7075, 9504, 4897, 8047, 3388, 7114, 3300, 6819, …\n$ Rating    &lt;dbl&gt; 283, 483, 514, 681, 357, 569, 259, 512, 266, 491, 589, 138, …\n$ Cards     &lt;dbl&gt; 2, 3, 4, 3, 2, 4, 2, 2, 5, 3, 4, 3, 1, 1, 2, 3, 3, 3, 1, 2, …\n$ Age       &lt;dbl&gt; 34, 82, 71, 36, 68, 77, 37, 87, 66, 41, 30, 64, 57, 49, 75, …\n$ Education &lt;dbl&gt; 11, 15, 11, 11, 16, 10, 12, 9, 13, 19, 14, 16, 7, 9, 13, 15,…\n$ Own       &lt;fct&gt; No, Yes, No, Yes, No, No, Yes, No, Yes, Yes, No, No, Yes, No…\n$ Student   &lt;fct&gt; No, Yes, No, No, No, No, No, No, No, Yes, No, No, No, No, No…\n$ Married   &lt;fct&gt; Yes, Yes, No, No, Yes, No, No, No, No, Yes, Yes, No, Yes, Ye…\n$ Region    &lt;fct&gt; South, West, West, West, South, South, East, West, South, Ea…\n$ Balance   &lt;dbl&gt; 333, 903, 580, 964, 331, 1151, 203, 872, 279, 1350, 1407, 0,…"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#variables",
    "href": "slides/19-mlr-conditions.html#variables",
    "title": "MLR: Inference and conditions",
    "section": "Variables",
    "text": "Variables\nFeatures (another name for predictors)\n\nIncome: Annual income (in 1000’s of US dollars)\nRating: Credit Rating\n\nOutcome\n\nLimit: Credit limit"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#review-simple-linear-regression-slr",
    "href": "slides/19-mlr-conditions.html#review-simple-linear-regression-slr",
    "title": "MLR: Inference and conditions",
    "section": "Review: Simple linear regression (SLR)",
    "text": "Review: Simple linear regression (SLR)\n\ngf_point(Limit ~ Rating, data = Credit, alpha = 0.5) |&gt; \n  gf_lm()  |&gt; \n  gf_labs(x = \"Credit Rating\", y = \"Income\") |&gt; \n  gf_refine(scale_y_continuous(labels = dollar_format()),\n            scale_x_continuous(labels = dollar_format()))\n\nWarning: Using the `size` aesthetic with geom_line was deprecated in ggplot2 3.4.0.\nℹ Please use the `linewidth` aesthetic instead."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#slr-model-summary",
    "href": "slides/19-mlr-conditions.html#slr-model-summary",
    "title": "MLR: Inference and conditions",
    "section": "SLR model summary",
    "text": "SLR model summary\n\nincome_slr_fit &lt;- lm(Limit ~ Income, data = Credit)\n\ntidy(income_slr_fit) |&gt; kable()\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2389.86941\n114.828758\n20.81246\n0\n\n\nIncome\n51.87502\n2.003836\n25.88785\n0"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#slr-hypothesis-test",
    "href": "slides/19-mlr-conditions.html#slr-hypothesis-test",
    "title": "MLR: Inference and conditions",
    "section": "SLR hypothesis test",
    "text": "SLR hypothesis test\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n2389.87\n114.83\n20.81\n0\n\n\nIncome\n51.88\n2.00\n25.89\n0\n\n\n\n\n\n\nSet hypotheses: \\(H_0: \\beta_1 = 0\\) vs. \\(H_A: \\beta_1 \\ne 0\\)\n\n. . .\n\nCalculate test statistic and p-value: The test statistic is \\(t= 25.89\\) . The p-value is calculated using a \\(t\\) distribution with 399 degrees of freedom. The p-value is \\(\\approx 0\\) .\n\n. . .\n\nState the conclusion: The p-value is small, so we reject \\(H_0\\). The data provide strong evidence that income is a helpful predictor for a credit card holder’s credit limit, i.e. there is a linear relationship between income and credit limit."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#multiple-linear-regression",
    "href": "slides/19-mlr-conditions.html#multiple-linear-regression",
    "title": "MLR: Inference and conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\n\ncredit_fit &lt;- lm(Limit ~ Rating + Income, data = Credit)\n\ntidy(credit_fit) |&gt; kable(digits = 2)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-532.47\n24.17\n-22.03\n0.00\n\n\nRating\n14.77\n0.10\n153.12\n0.00\n\n\nIncome\n0.56\n0.42\n1.32\n0.19"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#multiple-linear-regression-1",
    "href": "slides/19-mlr-conditions.html#multiple-linear-regression-1",
    "title": "MLR: Inference and conditions",
    "section": "Multiple linear regression",
    "text": "Multiple linear regression\nThe multiple linear regression model assumes \\[Y|X_1, X_2,  \\ldots, X_p \\sim N(\\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\dots + \\beta_p X_p, \\sigma_\\epsilon^2)\\]\n. . .\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots, x_{ip}, y_i)\\), we can rewrite the previous statement as\n\\[y_i = \\beta_0 + \\beta_1 x_{i1} + \\beta_2 x_{i2} + \\dots + \\beta_p x_{ip} + \\epsilon_{i}, \\hspace{10mm} \\epsilon_i \\sim N(0,\\sigma_{\\epsilon}^2)\\]"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#estimating-sigma_epsilon",
    "href": "slides/19-mlr-conditions.html#estimating-sigma_epsilon",
    "title": "MLR: Inference and conditions",
    "section": "Estimating \\(\\sigma_\\epsilon\\)",
    "text": "Estimating \\(\\sigma_\\epsilon\\)\nFor a given observation \\((x_{i1}, x_{i2}, \\ldots,x_{ip}, y_i)\\) the residual is \\[\n\\begin{aligned}\ne_i &= y_{i} - \\hat{y_i}\\\\\n&= y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 x_{i1} + \\hat{\\beta}_{2} x_{i2} + \\dots + \\hat{\\beta}_p x_{ip})\n\\end{aligned}\n\\]\n. . .\nThe estimated value of the regression standard error , \\(\\sigma_{\\epsilon}\\), is\n\\[\\hat{\\sigma}_\\epsilon  = \\sqrt{\\frac{\\sum_{i=1}^ne_i^2}{n-p-1}}\\]\n. . .\nAs with SLR, we use \\(\\hat{\\sigma}_{\\epsilon}\\) to calculate \\(SE_{\\hat{\\beta}_j}\\), the standard error of each coefficient. See Matrix Form of Linear Regression for more detail."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#mlr-hypothesis-test-income",
    "href": "slides/19-mlr-conditions.html#mlr-hypothesis-test-income",
    "title": "MLR: Inference and conditions",
    "section": "MLR hypothesis test: Income",
    "text": "MLR hypothesis test: Income\n\nSet hypotheses: \\(H_0: \\beta_{Income} = 0\\) vs. \\(H_A: \\beta_{Income} \\ne 0\\), given Rating is in the model\n\n. . .\n\nCalculate test statistic and p-value: The test statistic is \\(t = 1.32\\). The p-value is calculated using a \\(t\\) distribution with \\[(n - p - 1) = 400 - 2 -1 = 398\\] degrees of freedom. The p-value is \\(\\approx 0.19\\).\n\n. . .\n\nState the conclusion: The p-value is not small, so we fail to reject \\(H_0\\). The data does not provide convincing evidence that a borrowers income is a useful predictor in a model that already contains credit rating as a predictor for the credit limit of a borrower.\n\n\nComplete Exercises 1-2."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#confidence-interval-for-beta_j-1",
    "href": "slides/19-mlr-conditions.html#confidence-interval-for-beta_j-1",
    "title": "MLR: Inference and conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\nThe \\(C\\%\\) confidence interval for \\(\\beta_j\\) \\[\\hat{\\beta}_j \\pm t^* SE(\\hat{\\beta}_j)\\] where \\(t^*\\) follows a \\(t\\) distribution with \\(n - p - 1\\) degrees of freedom.\nGenerically: We are \\(C\\%\\) confident that the interval LB to UB contains the population coefficient of \\(x_j\\).\nIn context: We are \\(C\\%\\) confident that for every one unit increase in \\(x_j\\), we expect \\(y\\) to change by LB to UB units, holding all else constant.\n\n\nComplete Exercise 3."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#confidence-interval-for-beta_j-2",
    "href": "slides/19-mlr-conditions.html#confidence-interval-for-beta_j-2",
    "title": "MLR: Inference and conditions",
    "section": "Confidence interval for \\(\\beta_j\\)",
    "text": "Confidence interval for \\(\\beta_j\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nconf.low\nconf.high\n\n\n\n\n(Intercept)\n-532.47\n24.17\n-22.03\n0.00\n-579.99\n-484.95\n\n\nRating\n14.77\n0.10\n153.12\n0.00\n14.58\n14.96\n\n\nIncome\n0.56\n0.42\n1.32\n0.19\n-0.28\n1.39"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#large-sample-sizes",
    "href": "slides/19-mlr-conditions.html#large-sample-sizes",
    "title": "MLR: Inference and conditions",
    "section": "Large sample sizes",
    "text": "Large sample sizes\n\n\n\n\n\n\nCaution\n\n\n\nIf the sample size is large enough, the test will likely result in rejecting \\(H_0: \\beta_j = 0\\) even \\(x_j\\) has a very small effect on \\(y\\).\n\n\nConsider the practical significance of the result not just the statistical significance.\nUse the confidence interval to draw conclusions instead of relying only p-values."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#small-sample-sizes",
    "href": "slides/19-mlr-conditions.html#small-sample-sizes",
    "title": "MLR: Inference and conditions",
    "section": "Small sample sizes",
    "text": "Small sample sizes\n\n\n\n\n\n\nCaution\n\n\n\nIf the sample size is small, there may not be enough evidence to reject \\(H_0: \\beta_j=0\\).\n\n\nWhen you fail to reject the null hypothesis, DON’T immediately conclude that the variable has no association with the response.\nThere may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.\n\n\n\n\n\nComplete exercise 4\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#data-rail_trail",
    "href": "slides/19-mlr-conditions.html#data-rail_trail",
    "title": "MLR: Conditions",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n\n# A tibble: 90 × 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ℹ 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#full-model",
    "href": "slides/19-mlr-conditions.html#full-model",
    "title": "MLR: Conditions",
    "section": "Full model",
    "text": "Full model\nComplete Exercise 0 to fit the so-called “full-model”."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#full-model-1",
    "href": "slides/19-mlr-conditions.html#full-model-1",
    "title": "MLR: Conditions",
    "section": "Full model",
    "text": "Full model\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.622161\n76.582860\n0.2301058\n0.8185826\n\n\nhightemp\n7.070528\n2.420523\n2.9210743\n0.0045045\n\n\navgtemp\n-2.036685\n3.142113\n-0.6481896\n0.5186733\n\n\nseasonSpring\n35.914983\n32.992762\n1.0885716\n0.2795319\n\n\nseasonSummer\n24.153571\n52.810486\n0.4573632\n0.6486195\n\n\ncloudcover\n-7.251776\n3.843071\n-1.8869743\n0.0627025\n\n\nprecip\n-95.696525\n42.573359\n-2.2478030\n0.0272735\n\n\nday_typeWeekend\n35.903750\n22.429056\n1.6007696\n0.1132738"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#model-conditions",
    "href": "slides/19-mlr-conditions.html#model-conditions",
    "title": "MLR: Conditions",
    "section": "Model conditions",
    "text": "Model conditions\nOur model conditions are the same as they were with SLR:\n\nLinearity: There is a linear relationship between the response and predictor variables.\nConstant Variance: The variability about the least squares line is generally constant.\nNormality: The distribution of the residuals is approximately normal.\nIndependence: The residuals are independent from each other."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#checking-linearity",
    "href": "slides/19-mlr-conditions.html#checking-linearity",
    "title": "MLR: Conditions",
    "section": "Checking Linearity",
    "text": "Checking Linearity\n\nLook at a plot of the residuals vs. predicted values\nLook at a plot of the residuals vs. each predictor\n\nUse scatter plots for quantitative and boxplots of categorical predictors\n\nLinearity is met if there is no discernible pattern in each of these plots\n\nComplete Exercises 1-4"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#residuals-vs.-predicted-values",
    "href": "slides/19-mlr-conditions.html#residuals-vs.-predicted-values",
    "title": "MLR: Conditions",
    "section": "Residuals vs. predicted values",
    "text": "Residuals vs. predicted values"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#residuals-vs.-each-predictor",
    "href": "slides/19-mlr-conditions.html#residuals-vs.-each-predictor",
    "title": "MLR: Conditions",
    "section": "Residuals vs. each predictor",
    "text": "Residuals vs. each predictor"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#checking-linearity-1",
    "href": "slides/19-mlr-conditions.html#checking-linearity-1",
    "title": "MLR: Conditions",
    "section": "Checking linearity",
    "text": "Checking linearity\n\nThe plot of the residuals vs. predicted values looked OK\nThe plots of residuals vs. hightemp and avgtemp appear to have a parabolic pattern.\nThe linearity condition does not appear to be satisfied given these plots.\n\n\n\nGiven this conclusion, what might be a next step in the analysis?"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#checking-constant-variance",
    "href": "slides/19-mlr-conditions.html#checking-constant-variance",
    "title": "MLR: Conditions",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nDoes the constant variance condition appear to be satisfied?"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#checking-constant-variance-1",
    "href": "slides/19-mlr-conditions.html#checking-constant-variance-1",
    "title": "MLR: Conditions",
    "section": "Checking constant variance",
    "text": "Checking constant variance\n\nThe vertical spread of the residuals is not constant across the plot.\nThe constant variance condition is not satisfied.\n\n\n\nGiven this conclusion, what might be a next step in the analysis?\n\nComplete Exercises 5-6."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#checking-normality",
    "href": "slides/19-mlr-conditions.html#checking-normality",
    "title": "MLR: Conditions",
    "section": "Checking normality",
    "text": "Checking normality\n\nThe distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size 90 is sufficiently large to relax this condition if it was not satisfied."
  },
  {
    "objectID": "slides/19-mlr-conditions.html#checking-independence",
    "href": "slides/19-mlr-conditions.html#checking-independence",
    "title": "MLR: Conditions",
    "section": "Checking independence",
    "text": "Checking independence\n\nWe can often check the independence condition based on the context of the data and how the observations were collected.\nIf the data were collected in a particular order, examine a scatter plot of the residuals versus order in which the data were collected.\nIf there is a grouping variable lurking in the background, check the residuals based on that grouping variable.\nWhy might the independence condition be violated here?"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#checking-independence-1",
    "href": "slides/19-mlr-conditions.html#checking-independence-1",
    "title": "MLR: Conditions",
    "section": "Checking independence",
    "text": "Checking independence\nResiduals vs. order of data collection:\n\n\nCode\ngf_line(.resid ~ 1:nrow(rt_full_aug), data = rt_full_aug) |&gt;\n  gf_point()  |&gt;\n  gf_hline(yintercept = 0, color = \"red\", linetype = \"dashed\")  |&gt;\n  gf_labs(x = \"Order of data collection\", y = \"Residuals\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#checking-independence-2",
    "href": "slides/19-mlr-conditions.html#checking-independence-2",
    "title": "MLR: Conditions",
    "section": "Checking independence",
    "text": "Checking independence\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/19-mlr-conditions.html#what-is-multicollinearity",
    "href": "slides/19-mlr-conditions.html#what-is-multicollinearity",
    "title": "MLR: Conditions",
    "section": "What is multicollinearity",
    "text": "What is multicollinearity\nMulticollinearity is the case when two or more predictor variables are strongly correlated with one another\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#exerise-4",
    "href": "ae/ae-13-mlr-conditions.html#exerise-4",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Exerise 4",
    "text": "Exerise 4\nWe check the constant variance assumption in the same way we do with SLR. Does the constant variance condition seems to be satisfied?"
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#exercise-5",
    "href": "ae/ae-13-mlr-conditions.html#exercise-5",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Exercise 5",
    "text": "Exercise 5\nGenerate a histogram of the residuals. If you have time, also generate a QQ-plot of the residuals. Do you believe the normality condition is satisfied?"
  },
  {
    "objectID": "ae/ae-13-mlr-conditions.html#exercise-6",
    "href": "ae/ae-13-mlr-conditions.html#exercise-6",
    "title": "AE 13: Conditions for Multiple Linear Regression",
    "section": "Exercise 6",
    "text": "Exercise 6\nHow do you think would you go about checking the independence condition?"
  },
  {
    "objectID": "slides/20-multicollinearity.html#announcements",
    "href": "slides/20-multicollinearity.html#announcements",
    "title": "Multicollinearity",
    "section": "Announcements",
    "text": "Announcements\n\nProject propsal due\n\nFriday, October 27 (Tuesday labs)\nSunday, October 29 (Thursday labs)\n\nHW 03 due Wednesday, November 1\n\nreleased after Section 002 lecture"
  },
  {
    "objectID": "slides/20-multicollinearity.html#topics",
    "href": "slides/20-multicollinearity.html#topics",
    "title": "Multicollinearity",
    "section": "Topics",
    "text": "Topics\n\n\nInference for multiple linear regression\nChecking model conditions\nVariable transformations"
  },
  {
    "objectID": "slides/20-multicollinearity.html#computational-setup",
    "href": "slides/20-multicollinearity.html#computational-setup",
    "title": "Multicollinearity",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/20-multicollinearity.html#announcements-1",
    "href": "slides/20-multicollinearity.html#announcements-1",
    "title": "Multicollinearity",
    "section": "Announcements",
    "text": "Announcements\n\nProject: EDA Due Wednesday, October 30th\nOral R Quiz (time to start scheduling it)\n\n\n📋 AE 13 - Rail Trails\n\nOpen up AE 13"
  },
  {
    "objectID": "slides/20-multicollinearity.html#topics-1",
    "href": "slides/20-multicollinearity.html#topics-1",
    "title": "Multicollinearity",
    "section": "Topics",
    "text": "Topics\n\n\nChecking model conditions"
  },
  {
    "objectID": "slides/20-multicollinearity.html#computational-setup-1",
    "href": "slides/20-multicollinearity.html#computational-setup-1",
    "title": "Multicollinearity",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\nlibrary(mosaicData)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/20-multicollinearity.html#data-rail_trail",
    "href": "slides/20-multicollinearity.html#data-rail_trail",
    "title": "Multicollinearity",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n\n# A tibble: 90 × 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ℹ 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/20-multicollinearity.html#full-model",
    "href": "slides/20-multicollinearity.html#full-model",
    "title": "Multicollinearity",
    "section": "Full model",
    "text": "Full model\nComplete Exercise 0 to fit the so-called “full-model”."
  },
  {
    "objectID": "slides/20-multicollinearity.html#full-model-1",
    "href": "slides/20-multicollinearity.html#full-model-1",
    "title": "Multicollinearity",
    "section": "Full model",
    "text": "Full model\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.622161\n76.582860\n0.2301058\n0.8185826\n\n\nhightemp\n7.070528\n2.420523\n2.9210743\n0.0045045\n\n\navgtemp\n-2.036685\n3.142113\n-0.6481896\n0.5186733\n\n\nseasonSpring\n35.914983\n32.992762\n1.0885716\n0.2795319\n\n\nseasonSummer\n24.153571\n52.810486\n0.4573632\n0.6486195\n\n\ncloudcover\n-7.251776\n3.843071\n-1.8869743\n0.0627025\n\n\nprecip\n-95.696525\n42.573359\n-2.2478030\n0.0272735\n\n\nday_typeWeekend\n35.903750\n22.429056\n1.6007696\n0.1132738"
  },
  {
    "objectID": "slides/20-multicollinearity.html#what-is-multicollinearity",
    "href": "slides/20-multicollinearity.html#what-is-multicollinearity",
    "title": "Multicollinearity",
    "section": "What is multicollinearity",
    "text": "What is multicollinearity\nMulticollinearity is the case when two or more predictor variables are strongly correlated with one another"
  },
  {
    "objectID": "slides/20-multicollinearity.html#example",
    "href": "slides/20-multicollinearity.html#example",
    "title": "Multicollinearity",
    "section": "Example",
    "text": "Example\nLet’s assume the true population regression equation is \\(y = 3 + 4x\\)\n\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/20-multicollinearity.html#example-1",
    "href": "slides/20-multicollinearity.html#example-1",
    "title": "Multicollinearity",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the “best” combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)\nIn statistics, we say this model is “unidentifiable” because different parameters combinations can result in the same model"
  },
  {
    "objectID": "slides/20-multicollinearity.html#why-multicollinearity-is-a-problem",
    "href": "slides/20-multicollinearity.html#why-multicollinearity-is-a-problem",
    "title": "Multicollinearity",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have perfect collinearities, we are unable to get estimates for the coefficients\n\nWhen we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference\nIt is also difficult to interpret the model coefficients"
  },
  {
    "objectID": "slides/20-multicollinearity.html#detecting-multicollinearity",
    "href": "slides/20-multicollinearity.html#detecting-multicollinearity",
    "title": "Multicollinearity",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when…\n\nThere are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\nOne (or more) predictor variables is an almost perfect linear combination of the others\nThere are interactions between two or more continuous variables\nThere is a categorical predictor with very few observations in the baseline level"
  },
  {
    "objectID": "slides/20-multicollinearity.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/20-multicollinearity.html#detecting-multicollinearity-in-the-eda",
    "title": "Multicollinearity",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship\n\nLook at the distribution of categorical predictors and avoid setting the baseline to a category with very few observations"
  },
  {
    "objectID": "slides/20-multicollinearity.html#detecting-multicollinearity-vif",
    "href": "slides/20-multicollinearity.html#detecting-multicollinearity-vif",
    "title": "Multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/20-multicollinearity.html#detecting-multicollinearity-vif-1",
    "href": "slides/20-multicollinearity.html#detecting-multicollinearity-vif-1",
    "title": "Multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\n\nTypically \\(VIF &gt; 10\\) indicates concerning multicollinearity\nVariables with similar values of VIF are typically the ones correlated with each other\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/20-multicollinearity.html#vif-for-rail-trail-model",
    "href": "slides/20-multicollinearity.html#vif-for-rail-trail-model",
    "title": "Multicollinearity",
    "section": "VIF for rail trail model",
    "text": "VIF for rail trail model\n\nvif(rt_full_fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\n\nhightemp and avgtemp are correlated. We need to remove one of these variables and refit the model."
  },
  {
    "objectID": "slides/20-multicollinearity.html#model-without-hightemp",
    "href": "slides/20-multicollinearity.html#model-without-hightemp",
    "title": "Multicollinearity",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 &lt;- lm(volume ~ . - hightemp, data = rail_trail)\n  \nm1 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038"
  },
  {
    "objectID": "slides/20-multicollinearity.html#model-without-avgtemp",
    "href": "slides/20-multicollinearity.html#model-without-avgtemp",
    "title": "Multicollinearity",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 &lt;- lm(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/20-multicollinearity.html#choosing-a-model",
    "href": "slides/20-multicollinearity.html#choosing-a-model",
    "title": "Multicollinearity",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel without hightemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel without avgtemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\n\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model without avgtemp is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/20-multicollinearity.html#selected-model-for-now",
    "href": "slides/20-multicollinearity.html#selected-model-for-now",
    "title": "Multicollinearity",
    "section": "Selected model (for now)",
    "text": "Selected model (for now)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100\n\n\n\n\n\n\n\n\n\n🔗 STA 210 - Fall 2023 - Schedule"
  },
  {
    "objectID": "slides/20-comparison.html#announcements",
    "href": "slides/20-comparison.html#announcements",
    "title": "Model comparison",
    "section": "Announcements",
    "text": "Announcements\n\nProject: EDA Due Wednesday, October 30th\nOral R Quiz (time to start scheduling it)\n\n\n📋 AE 14 - Model Comparison\n\nOpen up AE 14 and complete Exercises 0-2"
  },
  {
    "objectID": "slides/20-comparison.html#topics",
    "href": "slides/20-comparison.html#topics",
    "title": "Model comparison",
    "section": "Topics",
    "text": "Topics\n\n\nANOVA for multiple linear regression and sum of squares\nComparing models with \\(R^2\\)"
  },
  {
    "objectID": "slides/20-comparison.html#computational-setup",
    "href": "slides/20-comparison.html#computational-setup",
    "title": "Model comparison",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(yardstick)\nlibrary(ggformula)\nlibrary(supernova)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(janitor)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/20-comparison.html#data-restaurant-tips",
    "href": "slides/20-comparison.html#data-restaurant-tips",
    "title": "Model comparison",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 × 4\n     Tip Party Meal   Age   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# ℹ 159 more rows"
  },
  {
    "objectID": "slides/20-comparison.html#variables",
    "href": "slides/20-comparison.html#variables",
    "title": "Model comparison",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\n\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/20-comparison.html#outcome-tip",
    "href": "slides/20-comparison.html#outcome-tip",
    "title": "Model comparison",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/20-comparison.html#predictors",
    "href": "slides/20-comparison.html#predictors",
    "title": "Model comparison",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/20-comparison.html#relevel-categorical-predictors",
    "href": "slides/20-comparison.html#relevel-categorical-predictors",
    "title": "Model comparison",
    "section": "Relevel categorical predictors",
    "text": "Relevel categorical predictors\n\ntips &lt;- tips |&gt;\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "slides/20-comparison.html#predictors-again",
    "href": "slides/20-comparison.html#predictors-again",
    "title": "Model comparison",
    "section": "Predictors, again",
    "text": "Predictors, again"
  },
  {
    "objectID": "slides/20-comparison.html#outcome-vs.-predictors",
    "href": "slides/20-comparison.html#outcome-vs.-predictors",
    "title": "Model comparison",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/20-comparison.html#fit-and-summarize-model",
    "href": "slides/20-comparison.html#fit-and-summarize-model",
    "title": "Model comparison",
    "section": "Fit and summarize model",
    "text": "Fit and summarize model\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.170\n0.366\n-0.465\n0.643\n\n\nParty\n1.837\n0.124\n14.758\n0.000\n\n\nAgeMiddle\n1.009\n0.408\n2.475\n0.014\n\n\nAgeSenCit\n1.388\n0.485\n2.862\n0.005\n\n\n\n\n\n\n\n\nIs this model good?"
  },
  {
    "objectID": "slides/20-comparison.html#another-model-summary",
    "href": "slides/20-comparison.html#another-model-summary",
    "title": "Model comparison",
    "section": "Another model summary",
    "text": "Another model summary\n\nanova(tip_fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/20-comparison.html#analysis-of-variance-anova-1",
    "href": "slides/20-comparison.html#analysis-of-variance-anova-1",
    "title": "Model comparison",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)"
  },
  {
    "objectID": "slides/20-comparison.html#anova",
    "href": "slides/20-comparison.html#anova",
    "title": "Model comparison",
    "section": "ANOVA",
    "text": "ANOVA\n\nMain Idea: Decompose the total variation of the outcome into:\n\nthe variation that can be explained by the each of the variables in the model\nthe variation that can’t be explained by the model (left in the residuals)\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model, \\(\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2\\)\n\n\nComplete Exercise 3."
  },
  {
    "objectID": "slides/20-comparison.html#anova-output-in-r1",
    "href": "slides/20-comparison.html#anova-output-in-r1",
    "title": "Model comparison",
    "section": "ANOVA output in R1",
    "text": "ANOVA output in R1\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.63588\n1188.635880\n285.711511\n0.000000\n\n\nAge\n2\n38.02783\n19.013916\n4.570361\n0.011699\n\n\nResiduals\n165\n686.44389\n4.160266\nNA\nNA\n\n\n\n\n\nClick here for explanation about the way R calculates sum of squares for each variable."
  },
  {
    "objectID": "slides/20-comparison.html#anova-output-with-totals",
    "href": "slides/20-comparison.html#anova-output-with-totals",
    "title": "Model comparison",
    "section": "ANOVA output, with totals",
    "text": "ANOVA output, with totals\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\n\n\n\n\nTotal\n168\n1913.11"
  },
  {
    "objectID": "slides/20-comparison.html#sum-of-squares",
    "href": "slides/20-comparison.html#sum-of-squares",
    "title": "Model comparison",
    "section": "Sum of squares",
    "text": "Sum of squares\n\n\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model, \\(\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2\\)"
  },
  {
    "objectID": "slides/20-comparison.html#sum-of-squares-ss_total",
    "href": "slides/20-comparison.html#sum-of-squares-ss_total",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Total}\\)",
    "text": "Sum of squares: \\(SS_{Total}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome\n\n\\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\) = 1913.11"
  },
  {
    "objectID": "slides/20-comparison.html#sum-of-squares-ss_error",
    "href": "slides/20-comparison.html#sum-of-squares-ss_error",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Error}\\)",
    "text": "Sum of squares: \\(SS_{Error}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals\n\n\\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\) = 686.44"
  },
  {
    "objectID": "slides/20-comparison.html#sum-of-squares-ss_model",
    "href": "slides/20-comparison.html#sum-of-squares-ss_model",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Model}\\)",
    "text": "Sum of squares: \\(SS_{Model}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Model}\\): Variability explained by the model\n\n\\(\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2 = SS_{Model} = SS_{Total} - SS_{Error} =\\) 1226.67"
  },
  {
    "objectID": "slides/20-comparison.html#r-squared-r2",
    "href": "slides/20-comparison.html#r-squared-r2",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\nRecall: \\(R^2\\) is the proportion of the variation in the response variable explained by the regression model.\n\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\n\\]\nComplete Exercises 4-7."
  },
  {
    "objectID": "slides/20-comparison.html#r-squared-r2-1",
    "href": "slides/20-comparison.html#r-squared-r2-1",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\n\\(R^2\\) will always increase as we add more variables to the model + If we add enough variables, we can always achieve \\(R^2=100\\%\\)\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables"
  },
  {
    "objectID": "slides/20-comparison.html#adjusted-r2",
    "href": "slides/20-comparison.html#adjusted-r2",
    "title": "Model comparison",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model\nDiffers from \\(R^2\\) by using the mean squares (sumsq/df) rather than sums of squares and therefore adjusting for the number of predictor variables"
  },
  {
    "objectID": "slides/20-comparison.html#r2-and-adjusted-r2",
    "href": "slides/20-comparison.html#r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/20-comparison.html#using-r2-and-adjusted-r2",
    "href": "slides/20-comparison.html#using-r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables"
  },
  {
    "objectID": "slides/20-comparison.html#comparing-models-with-r2_adj",
    "href": "slides/20-comparison.html#comparing-models-with-r2_adj",
    "title": "Model comparison",
    "section": "Comparing models with \\(R^2_{adj}\\)",
    "text": "Comparing models with \\(R^2_{adj}\\)\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n    data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.674         0.664\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.683         0.662\n\n\n\n\n\n\nWhy did we not use the full recipe() workflow to fit Model 1 or Model 2?\nWhich model would we choose based on \\(R^2\\)?\nWhich model would we choose based on Adjusted \\(R^2\\)?\nWhich statistic should we use to choose the final model - \\(R^2\\) or Adjusted \\(R^2\\)? Why?\n\nVote on Ed Discussion [10:05am lecture][1:25pm lecture]"
  },
  {
    "objectID": "slides/20-comparison.html#aic-bic",
    "href": "slides/20-comparison.html#aic-bic",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nEstimators of prediction error and relative quality of models:\n\nAkaike’s Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \n\n\nSchwarz’s Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]"
  },
  {
    "objectID": "slides/20-comparison.html#aic-bic-1",
    "href": "slides/20-comparison.html#aic-bic-1",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\n\nFirst Term: Decreases as p increases"
  },
  {
    "objectID": "slides/20-comparison.html#aic-bic-2",
    "href": "slides/20-comparison.html#aic-bic-2",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\nSecond Term: Fixed for a given sample size n"
  },
  {
    "objectID": "slides/20-comparison.html#aic-bic-3",
    "href": "slides/20-comparison.html#aic-bic-3",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nThird Term: Increases as p increases"
  },
  {
    "objectID": "slides/20-comparison.html#using-aic-bic",
    "href": "slides/20-comparison.html#using-aic-bic",
    "title": "Model comparison",
    "section": "Using AIC & BIC",
    "text": "Using AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nChoose model with the smaller value of AIC or BIC\nIf \\(n \\geq 8\\), the penalty for BIC is larger than that of AIC, so BIC tends to favor more parsimonious models (i.e. models with fewer terms)"
  },
  {
    "objectID": "slides/20-comparison.html#comparing-models-with-aic-and-bic",
    "href": "slides/20-comparison.html#comparing-models-with-aic-and-bic",
    "title": "Model comparison",
    "section": "Comparing models with AIC and BIC",
    "text": "Comparing models with AIC and BIC\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  714.  736.\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  720.  757.\n\n\n\n\n\n\nWhich model would we choose based on AIC?\nWhich model would we choose based on BIC?"
  },
  {
    "objectID": "slides/20-comparison.html#commonalities-between-criteria",
    "href": "slides/20-comparison.html#commonalities-between-criteria",
    "title": "Model comparison",
    "section": "Commonalities between criteria",
    "text": "Commonalities between criteria\n\n\\(R^2_{adj}\\), AIC, and BIC all apply a penalty for more predictors\nThe penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\nGoal: Parsimony"
  },
  {
    "objectID": "slides/20-comparison.html#parsimony-and-occams-razor",
    "href": "slides/20-comparison.html#parsimony-and-occams-razor",
    "title": "Model comparison",
    "section": "Parsimony and Occam’s razor",
    "text": "Parsimony and Occam’s razor\n\nThe principle of parsimony is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation1\nCalled Occam’s razor because he “shaved” his explanations down to the bare minimum\nParsimony in modeling:\n\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments relying on few assumptions should be preferred to those relying on many\nmodels should be pared down until they are minimal adequate\nsimple explanations should be preferred to complex explanations\n\n\n\nSource: The R Book by Michael J. Crawley."
  },
  {
    "objectID": "slides/20-comparison.html#in-pursuit-of-occams-razor",
    "href": "slides/20-comparison.html#in-pursuit-of-occams-razor",
    "title": "Model comparison",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\nOccam’s razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\nModel selection follows this principle\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\nIn other words, we prefer the simplest best model, i.e. parsimonious model"
  },
  {
    "objectID": "slides/20-comparison.html#alternate-views",
    "href": "slides/20-comparison.html#alternate-views",
    "title": "Model comparison",
    "section": "Alternate views",
    "text": "Alternate views\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nRadford Neal - Bayesian Learning for Neural Networks1\n\nSuggested blog post: Occam by Andrew Gelman"
  },
  {
    "objectID": "slides/20-comparison.html#other-concerns-with-our-approach",
    "href": "slides/20-comparison.html#other-concerns-with-our-approach",
    "title": "Model comparison",
    "section": "Other concerns with our approach",
    "text": "Other concerns with our approach\n\nAll criteria we considered for model comparison require making predictions for our data and then uses the prediction error (\\(SS_{Error}\\)) somewhere in the formula\nBut we’re making prediction for the data we used to build the model (estimate the coefficients), which can lead to overfitting\nInstead we should\n\nsplit our data into testing and training sets\n“train” the model on the training data and pick a few models we’re genuinely considering as potentially good models\ntest those models on the testing set\n…and repeat this process multiple times"
  },
  {
    "objectID": "slides/20-comparison.html#spending-our-data",
    "href": "slides/20-comparison.html#spending-our-data",
    "title": "Model comparison",
    "section": "Spending our data",
    "text": "Spending our data\n\nWe have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.\nHowever, we usually need to understand the effectiveness of the model before using the test set.\nTypically we can’t decide on which final model to take to the test set without making model assessments.\nRemedy: Resampling to make model assessments on training data in a way that can generalize to new data."
  },
  {
    "objectID": "slides/20-comparison.html#resampling-for-model-assessment",
    "href": "slides/20-comparison.html#resampling-for-model-assessment",
    "title": "Model comparison",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\nResampling is only conducted on the training set. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\nThe model is fit with the analysis set. Model fit statistics such as \\(R^2_{Adj}\\), AIC, and BIC are calculated based on this fit.\nThe model is evaluated with the assessment set."
  },
  {
    "objectID": "slides/20-comparison.html#resampling-for-model-assessment-1",
    "href": "slides/20-comparison.html#resampling-for-model-assessment-1",
    "title": "Model comparison",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\n\n\nImage source: Kuhn and Silge. Tidy modeling with R."
  },
  {
    "objectID": "slides/20-comparison.html#analysis-and-assessment-sets",
    "href": "slides/20-comparison.html#analysis-and-assessment-sets",
    "title": "Model comparison",
    "section": "Analysis and assessment sets",
    "text": "Analysis and assessment sets\n\nAnalysis set is analogous to training set.\nAssessment set is analogous to test set.\nThe terms analysis and assessment avoids confusion with initial split of the data.\nThese data sets are mutually exclusive."
  },
  {
    "objectID": "slides/20-comparison.html#cross-validation-1",
    "href": "slides/20-comparison.html#cross-validation-1",
    "title": "Model comparison",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, v-fold cross validation – commonly used resampling technique:\n\nRandomly split your training data into v partitions\nUse v-1 partitions for analysis, and the remaining 1 partition for analysis (model fit + model fit statistics)\nRepeat v times, updating which partition is used for assessment each time\n\n\nLet’s give an example where v = 3…"
  },
  {
    "objectID": "slides/20-comparison.html#to-get-started",
    "href": "slides/20-comparison.html#to-get-started",
    "title": "Model comparison",
    "section": "To get started…",
    "text": "To get started…\nSplit data into training and test sets\n\nset.seed(345)\n\ntips_split &lt;- initial_split(tips)\ntips_train &lt;- training(tips_split)\ntips_test &lt;- testing(tips_split)"
  },
  {
    "objectID": "slides/20-comparison.html#to-get-started-1",
    "href": "slides/20-comparison.html#to-get-started-1",
    "title": "Model comparison",
    "section": "To get started…",
    "text": "To get started…\nCreate recipe\n\ntips_rec1 &lt;- recipe(Tip ~ Party + Age + Meal, \n                    data = tips_train)\n\ntips_rec1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3"
  },
  {
    "objectID": "slides/20-comparison.html#to-get-started-2",
    "href": "slides/20-comparison.html#to-get-started-2",
    "title": "Model comparison",
    "section": "To get started…",
    "text": "To get started…\nSpecify model\n\ntips_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n\nCreate workflow\n\ntips_wflow1 &lt;- workflow() |&gt;\n  add_model(tips_spec) |&gt;\n  add_recipe(tips_rec1)\n\ntips_wflow1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/20-comparison.html#cross-validation-step-1",
    "href": "slides/20-comparison.html#cross-validation-step-1",
    "title": "Model comparison",
    "section": "Cross validation, step 1",
    "text": "Cross validation, step 1\nRandomly split your training data into 3 partitions:"
  },
  {
    "objectID": "slides/20-comparison.html#tips-split-training-data",
    "href": "slides/20-comparison.html#tips-split-training-data",
    "title": "Model comparison",
    "section": "Tips: Split training data",
    "text": "Tips: Split training data\n\nfolds &lt;- vfold_cv(tips_train, v = 3)\nfolds\n\n#  3-fold cross-validation \n# A tibble: 3 × 2\n  splits          id   \n  &lt;list&gt;          &lt;chr&gt;\n1 &lt;split [84/42]&gt; Fold1\n2 &lt;split [84/42]&gt; Fold2\n3 &lt;split [84/42]&gt; Fold3"
  },
  {
    "objectID": "slides/20-comparison.html#cross-validation-steps-2-and-3",
    "href": "slides/20-comparison.html#cross-validation-steps-2-and-3",
    "title": "Model comparison",
    "section": "Cross validation, steps 2 and 3",
    "text": "Cross validation, steps 2 and 3\n\n\nUse v-1 partitions for analysis, and the remaining 1 partition for assessment\nRepeat v times, updating which partition is used for assessment each time"
  },
  {
    "objectID": "slides/20-comparison.html#tips-fit-resamples",
    "href": "slides/20-comparison.html#tips-fit-resamples",
    "title": "Model comparison",
    "section": "Tips: Fit resamples",
    "text": "Tips: Fit resamples\n\ntips_fit_rs1 &lt;- tips_wflow1 |&gt;\n  fit_resamples(folds)\n\ntips_fit_rs1 \n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 × 4\n  splits          id    .metrics         .notes          \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [84/42]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [84/42]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [84/42]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "slides/20-comparison.html#cross-validation-now-what",
    "href": "slides/20-comparison.html#cross-validation-now-what",
    "title": "Model comparison",
    "section": "Cross validation, now what?",
    "text": "Cross validation, now what?\n\nWe’ve fit a bunch of models\nNow it’s time to use them to collect metrics (e.g., $R^2$, AIC, RMSE, etc. ) on each model and use them to evaluate model fit and how it varies across folds"
  },
  {
    "objectID": "slides/20-comparison.html#collect-r2-and-rmse-from-cv",
    "href": "slides/20-comparison.html#collect-r2-and-rmse-from-cv",
    "title": "Model comparison",
    "section": "Collect \\(R^2\\) and RMSE from CV",
    "text": "Collect \\(R^2\\) and RMSE from CV\n\n# Produces summary across all CV\ncollect_metrics(tips_fit_rs1)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.10      3  0.243  Preprocessor1_Model1\n2 rsq     standard   0.591     3  0.0728 Preprocessor1_Model1\n\n\n\nNote: These are calculated using the assessment data"
  },
  {
    "objectID": "slides/20-comparison.html#deeper-look-into-r2-and-rmse",
    "href": "slides/20-comparison.html#deeper-look-into-r2-and-rmse",
    "title": "Model comparison",
    "section": "Deeper look into \\(R^2\\) and RMSE",
    "text": "Deeper look into \\(R^2\\) and RMSE\n\ncv_metrics1 &lt;- collect_metrics(tips_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n\n# A tibble: 6 × 5\n  id    .metric .estimator .estimate .config             \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Fold1 rmse    standard       2.04  Preprocessor1_Model1\n2 Fold1 rsq     standard       0.736 Preprocessor1_Model1\n3 Fold2 rmse    standard       1.71  Preprocessor1_Model1\n4 Fold2 rsq     standard       0.509 Preprocessor1_Model1\n5 Fold3 rmse    standard       2.54  Preprocessor1_Model1\n6 Fold3 rsq     standard       0.528 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/20-comparison.html#better-tabulation-of-r2-and-rmse-from-cv",
    "href": "slides/20-comparison.html#better-tabulation-of-r2-and-rmse-from-cv",
    "title": "Model comparison",
    "section": "Better tabulation of \\(R^2\\) and RMSE from CV",
    "text": "Better tabulation of \\(R^2\\) and RMSE from CV\n\ncv_metrics1 |&gt;\n  mutate(.estimate = round(.estimate, 3)) |&gt;\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) |&gt;\n  kable(col.names = c(\"Fold\", \"RMSE\", \"R-squared\"))\n\n\n\n\nFold\nRMSE\nR-squared\n\n\n\n\nFold1\n2.036\n0.736\n\n\nFold2\n1.709\n0.509\n\n\nFold3\n2.545\n0.528"
  },
  {
    "objectID": "slides/20-comparison.html#how-does-rmse-compare-to-y",
    "href": "slides/20-comparison.html#how-does-rmse-compare-to-y",
    "title": "Model comparison",
    "section": "How does RMSE compare to y?",
    "text": "How does RMSE compare to y?\n\n\nCross validation RMSE stats:\n\ncv_metrics1 |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )\n\n# A tibble: 1 × 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.71  2.54  2.10 0.421\n\n\n\nTraining data tips:\n\ntips_train |&gt;\n  summarise(\n    min = min(Tip),\n    max = max(Tip),\n    mean = mean(Tip),\n    sd = sd(Tip)\n  )\n\n# A tibble: 1 × 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  19.5  4.87  3.37"
  },
  {
    "objectID": "slides/20-comparison.html#calculate-r2_adj-aic-and-bic-for-each-fold",
    "href": "slides/20-comparison.html#calculate-r2_adj-aic-and-bic-for-each-fold",
    "title": "Model comparison",
    "section": "Calculate \\(R^2_{Adj}\\), AIC, and BIC for each fold",
    "text": "Calculate \\(R^2_{Adj}\\), AIC, and BIC for each fold\n\n# Function to get Adj R-sq, AIC, BIC\ncalc_model_stats &lt;- function(x) {\n  glance(extract_fit_parsnip(x)) |&gt;\n    select(adj.r.squared, AIC, BIC)\n}\n\n# Fit model and calculate statistics for each fold\ntips_fit_rs1 &lt;-tips_wflow1 |&gt;\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))\n\ntips_fit_rs1\n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 × 5\n  splits          id    .metrics         .notes           .extracts       \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [84/42]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n2 &lt;split [84/42]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n3 &lt;split [84/42]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;"
  },
  {
    "objectID": "slides/20-comparison.html#collect-r2_adj-aic-bic-from-cv",
    "href": "slides/20-comparison.html#collect-r2_adj-aic-bic-from-cv",
    "title": "Model comparison",
    "section": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV",
    "text": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV\n\nmap_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) |&gt;\n  bind_cols(Fold = tips_fit_rs1$id)\n\n# A tibble: 3 × 4\n  adj.r.squared   AIC   BIC Fold \n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1         0.550  369.  386. Fold1\n2         0.659  377.  394. Fold2\n3         0.718  337.  354. Fold3\n\n\n\nNote: These are based on the model fit from the analysis data"
  },
  {
    "objectID": "slides/20-comparison.html#cross-validation-in-practice",
    "href": "slides/20-comparison.html#cross-validation-in-practice",
    "title": "Model comparison",
    "section": "Cross validation in practice",
    "text": "Cross validation in practice\n\n\nTo illustrate how CV works, we used v = 3:\n\n\nAnalysis sets are 2/3 of the training set\nEach assessment set is a distinct 1/3\nThe final resampling estimate of performance averages each of the 3 replicates\n\n\nThis was useful for illustrative purposes, but v = 3 is a poor choice in practice\nValues of v are most often 5 or 10; we generally prefer 10-fold cross-validation as a default"
  },
  {
    "objectID": "slides/20-comparison.html#recap",
    "href": "slides/20-comparison.html#recap",
    "title": "Model comparison",
    "section": "Recap",
    "text": "Recap\n\nANOVA for multiple linear regression and sum of squares\n\\(R^2\\) for multiple lienar regression\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "ae/ae-14-comparison.html",
    "href": "ae/ae-14-comparison.html",
    "title": "AE 14: Model Comparison",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-14”.\nGo to the Canvas and locate your AE-14 assignment to get started.\nUpload the ae-14.qmd and tip-data.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment."
  },
  {
    "objectID": "ae/ae-14-comparison.html#packages-data",
    "href": "ae/ae-14-comparison.html#packages-data",
    "title": "AE 14: Model Comparison",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(yardstick)\nlibrary(ggformula)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntips &lt;- read_csv(\"data/tip-data.csv\") \n\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nMeal: which meal this was (Lunch, Dinner, Late Night)\nParty: number of people in the party\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)"
  },
  {
    "objectID": "ae/ae-14-comparison.html#analysis-goal",
    "href": "ae/ae-14-comparison.html#analysis-goal",
    "title": "AE 14: Model Comparison",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this activity are to:\n\nUse ANOVA to determine whether our model is useful as a whole\nBegin thinking about \\(R^2\\) in a multivariate setting"
  },
  {
    "objectID": "ae/ae-14-comparison.html#exercise-0",
    "href": "ae/ae-14-comparison.html#exercise-0",
    "title": "AE 14: Model Comparison",
    "section": "Exercise 0",
    "text": "Exercise 0\nComplete the following to clean and then observe your data:\n\nUse drop_na to remove any rows where Party is missing.\n\n\ntips &lt;- tips |&gt; \n  _______   # drop missing values from party\n\n\nGenerate a bar chart of the variable Meal.\n\n\n# insert code here\n\n\nRun the following code and generate the same bar chart as above. You can even copy and paste your code. What’s the difference between the two plots? What do you think fct_relevel does?\n\n\ntips &lt;- tips |&gt; \n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )\n\n# Generate plot here"
  },
  {
    "objectID": "ae/ae-14-comparison.html#to-submit-the-ae",
    "href": "ae/ae-14-comparison.html#to-submit-the-ae",
    "title": "AE 14: Model Comparison",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-14-comparison.html#footnotes",
    "href": "ae/ae-14-comparison.html#footnotes",
    "title": "AE 14: Model Comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.↩︎"
  },
  {
    "objectID": "slides/21-multicollinearity.html",
    "href": "slides/21-multicollinearity.html",
    "title": "Multicollinearity",
    "section": "",
    "text": "Project propsal due\n\nFriday, October 27 (Tuesday labs)\nSunday, October 29 (Thursday labs)\n\nHW 03 due Wednesday, November 1\n\nreleased after Section 002 lecture"
  },
  {
    "objectID": "slides/21-multicollinearity.html#announcements",
    "href": "slides/21-multicollinearity.html#announcements",
    "title": "Multicollinearity",
    "section": "",
    "text": "Project propsal due\n\nFriday, October 27 (Tuesday labs)\nSunday, October 29 (Thursday labs)\n\nHW 03 due Wednesday, November 1\n\nreleased after Section 002 lecture"
  },
  {
    "objectID": "slides/21-multicollinearity.html#topics",
    "href": "slides/21-multicollinearity.html#topics",
    "title": "Multicollinearity",
    "section": "Topics",
    "text": "Topics\n\n\nInference for multiple linear regression\nChecking model conditions\nVariable transformations"
  },
  {
    "objectID": "slides/21-multicollinearity.html#computational-setup",
    "href": "slides/21-multicollinearity.html#computational-setup",
    "title": "Multicollinearity",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ broom        1.0.6     ✔ rsample      1.2.1\n✔ dials        1.3.0     ✔ tune         1.2.1\n✔ infer        1.0.7     ✔ workflows    1.1.4\n✔ modeldata    1.4.0     ✔ workflowsets 1.1.0\n✔ parsnip      1.2.1     ✔ yardstick    1.3.1\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Search for functions across packages at https://www.tidymodels.org/find/\n\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\nlibrary(countdown)\nlibrary(rms)\n\nLoading required package: Hmisc\n\nAttaching package: 'Hmisc'\n\nThe following object is masked from 'package:parsnip':\n\n    translate\n\nThe following objects are masked from 'package:dplyr':\n\n    src, summarize\n\nThe following objects are masked from 'package:base':\n\n    format.pval, units\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/21-multicollinearity.html#announcements-1",
    "href": "slides/21-multicollinearity.html#announcements-1",
    "title": "Multicollinearity",
    "section": "Announcements",
    "text": "Announcements\n\nProject: EDA Due Wednesday, October 30th\nOral R Quiz (time to start scheduling it)\n\n\n📋 AE 13 - Rail Trails\n\nOpen up AE 13"
  },
  {
    "objectID": "slides/21-multicollinearity.html#topics-1",
    "href": "slides/21-multicollinearity.html#topics-1",
    "title": "Multicollinearity",
    "section": "Topics",
    "text": "Topics\n\n\nChecking model conditions"
  },
  {
    "objectID": "slides/21-multicollinearity.html#computational-setup-1",
    "href": "slides/21-multicollinearity.html#computational-setup-1",
    "title": "Multicollinearity",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(mosaic)\n\nRegistered S3 method overwritten by 'mosaic':\n  method                           from   \n  fortify.SpatialPolygonsDataFrame ggplot2\n\n\n\nThe 'mosaic' package masks several functions from core packages in order to add \nadditional features.  The original behavior of these functions should not be affected by this.\n\n\n\nAttaching package: 'mosaic'\n\n\nThe following object is masked from 'package:Matrix':\n\n    mean\n\n\nThe following objects are masked from 'package:infer':\n\n    prop_test, t_test\n\n\nThe following object is masked from 'package:scales':\n\n    rescale\n\n\nThe following objects are masked from 'package:dplyr':\n\n    count, do, tally\n\n\nThe following object is masked from 'package:purrr':\n\n    cross\n\n\nThe following object is masked from 'package:ggplot2':\n\n    stat\n\n\nThe following objects are masked from 'package:stats':\n\n    binom.test, cor, cor.test, cov, fivenum, IQR, median, prop.test,\n    quantile, sd, t.test, var\n\n\nThe following objects are masked from 'package:base':\n\n    max, mean, min, prod, range, sample, sum\n\nlibrary(mosaicData)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\nlibrary(scales)\nlibrary(countdown)\nlibrary(rms)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))"
  },
  {
    "objectID": "slides/21-multicollinearity.html#data-rail_trail",
    "href": "slides/21-multicollinearity.html#data-rail_trail",
    "title": "Multicollinearity",
    "section": "Data: rail_trail",
    "text": "Data: rail_trail\n\n\nThe Pioneer Valley Planning Commission (PVPC) collected data for ninety days from April 5, 2005 to November 15, 2005.\nData collectors set up a laser sensor, with breaks in the laser beam recording when a rail-trail user passed the data collection station.\n\n\n\n\nRows: 90 Columns: 7\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): season, day_type\ndbl (5): volume, hightemp, avgtemp, cloudcover, precip\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n# A tibble: 90 × 7\n   volume hightemp avgtemp season cloudcover precip day_type\n    &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;   \n 1    501       83    66.5 Summer       7.60 0      Weekday \n 2    419       73    61   Summer       6.30 0.290  Weekday \n 3    397       74    63   Spring       7.5  0.320  Weekday \n 4    385       95    78   Summer       2.60 0      Weekend \n 5    200       44    48   Spring      10    0.140  Weekday \n 6    375       69    61.5 Spring       6.60 0.0200 Weekday \n 7    417       66    52.5 Spring       2.40 0      Weekday \n 8    629       66    52   Spring       0    0      Weekend \n 9    533       80    67.5 Summer       3.80 0      Weekend \n10    547       79    62   Summer       4.10 0      Weekday \n# ℹ 80 more rows\n\n\nSource: Pioneer Valley Planning Commission via the mosaicData package."
  },
  {
    "objectID": "slides/21-multicollinearity.html#full-model",
    "href": "slides/21-multicollinearity.html#full-model",
    "title": "Multicollinearity",
    "section": "Full model",
    "text": "Full model\nComplete Exercise 0 to fit the so-called “full-model”."
  },
  {
    "objectID": "slides/21-multicollinearity.html#full-model-1",
    "href": "slides/21-multicollinearity.html#full-model-1",
    "title": "Multicollinearity",
    "section": "Full model",
    "text": "Full model\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n17.622161\n76.582860\n0.2301058\n0.8185826\n\n\nhightemp\n7.070528\n2.420523\n2.9210743\n0.0045045\n\n\navgtemp\n-2.036685\n3.142113\n-0.6481896\n0.5186733\n\n\nseasonSpring\n35.914983\n32.992762\n1.0885716\n0.2795319\n\n\nseasonSummer\n24.153571\n52.810486\n0.4573632\n0.6486195\n\n\ncloudcover\n-7.251776\n3.843071\n-1.8869743\n0.0627025\n\n\nprecip\n-95.696525\n42.573359\n-2.2478030\n0.0272735\n\n\nday_typeWeekend\n35.903750\n22.429056\n1.6007696\n0.1132738"
  },
  {
    "objectID": "slides/21-multicollinearity.html#what-is-multicollinearity",
    "href": "slides/21-multicollinearity.html#what-is-multicollinearity",
    "title": "Multicollinearity",
    "section": "What is multicollinearity",
    "text": "What is multicollinearity\nMulticollinearity is the case when two or more predictor variables are strongly correlated with one another"
  },
  {
    "objectID": "slides/21-multicollinearity.html#example",
    "href": "slides/21-multicollinearity.html#example",
    "title": "Multicollinearity",
    "section": "Example",
    "text": "Example\nLet’s assume the true population regression equation is \\(y = 3 + 4x\\)\n. . .\nSuppose we try estimating that equation using a model with variables \\(x\\) and \\(z = x/10\\)\n\\[\n\\begin{aligned}\\hat{y}&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2z\\\\\n&= \\hat{\\beta}_0 + \\hat{\\beta}_1x  + \\hat{\\beta}_2\\frac{x}{10}\\\\\n&= \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "slides/21-multicollinearity.html#example-1",
    "href": "slides/21-multicollinearity.html#example-1",
    "title": "Multicollinearity",
    "section": "Example",
    "text": "Example\n\\[\\hat{y} = \\hat{\\beta}_0 + \\bigg(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10}\\bigg)x\\]\n\nWe can set \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\) to any two numbers such that \\(\\hat{\\beta}_1 + \\frac{\\hat{\\beta}_2}{10} = 4\\)\nTherefore, we are unable to choose the “best” combination of \\(\\hat{\\beta}_1\\) and \\(\\hat{\\beta}_2\\)\nIn statistics, we say this model is “unidentifiable” because different parameters combinations can result in the same model"
  },
  {
    "objectID": "slides/21-multicollinearity.html#why-multicollinearity-is-a-problem",
    "href": "slides/21-multicollinearity.html#why-multicollinearity-is-a-problem",
    "title": "Multicollinearity",
    "section": "Why multicollinearity is a problem",
    "text": "Why multicollinearity is a problem\n\nWhen we have perfect collinearities, we are unable to get estimates for the coefficients\n\nWhen we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate\nIn other words, we lose precision in our estimates of the regression coefficients\nThis impedes our ability to use the model for inference\nIt is also difficult to interpret the model coefficients"
  },
  {
    "objectID": "slides/21-multicollinearity.html#detecting-multicollinearity",
    "href": "slides/21-multicollinearity.html#detecting-multicollinearity",
    "title": "Multicollinearity",
    "section": "Detecting Multicollinearity",
    "text": "Detecting Multicollinearity\nMulticollinearity may occur when…\n\nThere are very high correlations \\((r &gt; 0.9)\\) among two or more predictor variables, especially when the sample size is small\nOne (or more) predictor variables is an almost perfect linear combination of the others\nThere are interactions between two or more continuous variables\nThere is a categorical predictor with very few observations in the baseline level"
  },
  {
    "objectID": "slides/21-multicollinearity.html#detecting-multicollinearity-in-the-eda",
    "href": "slides/21-multicollinearity.html#detecting-multicollinearity-in-the-eda",
    "title": "Multicollinearity",
    "section": "Detecting multicollinearity in the EDA",
    "text": "Detecting multicollinearity in the EDA\n\nLook at a correlation matrix of the predictor variables, including all indicator variables\n\nLook out for values close to 1 or -1\n\nLook at a scatterplot matrix of the predictor variables\n\nLook out for plots that show a relatively linear relationship\n\nLook at the distribution of categorical predictors and avoid setting the baseline to a category with very few observations"
  },
  {
    "objectID": "slides/21-multicollinearity.html#detecting-multicollinearity-vif",
    "href": "slides/21-multicollinearity.html#detecting-multicollinearity-vif",
    "title": "Multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\nVariance Inflation Factor (VIF): Measure of multicollinearity in the regression model\n\\[VIF(\\hat{\\beta}_j) = \\frac{1}{1-R^2_{X_j|X_{-j}}}\\]\nwhere \\(R^2_{X_j|X_{-j}}\\) is the proportion of variation \\(X\\) that is explained by the linear combination of the other explanatory variables in the model."
  },
  {
    "objectID": "slides/21-multicollinearity.html#detecting-multicollinearity-vif-1",
    "href": "slides/21-multicollinearity.html#detecting-multicollinearity-vif-1",
    "title": "Multicollinearity",
    "section": "Detecting Multicollinearity (VIF)",
    "text": "Detecting Multicollinearity (VIF)\n\nTypically \\(VIF &gt; 10\\) indicates concerning multicollinearity\nVariables with similar values of VIF are typically the ones correlated with each other\nUse the vif() function in the rms R package to calculate VIF"
  },
  {
    "objectID": "slides/21-multicollinearity.html#vif-for-rail-trail-model",
    "href": "slides/21-multicollinearity.html#vif-for-rail-trail-model",
    "title": "Multicollinearity",
    "section": "VIF for rail trail model",
    "text": "VIF for rail trail model\n\nvif(rt_full_fit)\n\n       hightemp         avgtemp    seasonSpring    seasonSummer      cloudcover \n      10.259978       13.086175        2.751577        5.841985        1.587485 \n         precip day_typeWeekend \n       1.295352        1.125741 \n\n\n\n. . .\nhightemp and avgtemp are correlated. We need to remove one of these variables and refit the model."
  },
  {
    "objectID": "slides/21-multicollinearity.html#model-without-hightemp",
    "href": "slides/21-multicollinearity.html#model-without-hightemp",
    "title": "Multicollinearity",
    "section": "Model without hightemp",
    "text": "Model without hightemp\n\nm1 &lt;- lm(volume ~ . - hightemp, data = rail_trail)\n  \nm1 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n76.071\n77.204\n0.985\n0.327\n\n\navgtemp\n6.003\n1.583\n3.792\n0.000\n\n\nseasonSpring\n34.555\n34.454\n1.003\n0.319\n\n\nseasonSummer\n13.531\n55.024\n0.246\n0.806\n\n\ncloudcover\n-12.807\n3.488\n-3.672\n0.000\n\n\nprecip\n-110.736\n44.137\n-2.509\n0.014\n\n\nday_typeWeekend\n48.420\n22.993\n2.106\n0.038"
  },
  {
    "objectID": "slides/21-multicollinearity.html#model-without-avgtemp",
    "href": "slides/21-multicollinearity.html#model-without-avgtemp",
    "title": "Multicollinearity",
    "section": "Model without avgtemp",
    "text": "Model without avgtemp\n\nm2 &lt;- lm(volume ~ . - avgtemp, data = rail_trail)\n  \nm2 |&gt;\n  tidy() |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/21-multicollinearity.html#choosing-a-model",
    "href": "slides/21-multicollinearity.html#choosing-a-model",
    "title": "Multicollinearity",
    "section": "Choosing a model",
    "text": "Choosing a model\nModel without hightemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.42\n1087.5\n1107.5\n\n\n\n\n\nModel without avgtemp:\n\n\n\n\n\nadj.r.squared\nAIC\nBIC\n\n\n\n\n0.47\n1079.05\n1099.05\n\n\n\n\n\n. . .\nBased on Adjusted \\(R^2\\), AIC, and BIC, the model without avgtemp is a better fit. Therefore, we choose to remove avgtemp from the model and leave hightemp in the model to deal with the multicollinearity."
  },
  {
    "objectID": "slides/21-multicollinearity.html#selected-model-for-now",
    "href": "slides/21-multicollinearity.html#selected-model-for-now",
    "title": "Multicollinearity",
    "section": "Selected model (for now)",
    "text": "Selected model (for now)\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n8.421\n74.992\n0.112\n0.911\n\n\nhightemp\n5.696\n1.164\n4.895\n0.000\n\n\nseasonSpring\n31.239\n32.082\n0.974\n0.333\n\n\nseasonSummer\n9.424\n47.504\n0.198\n0.843\n\n\ncloudcover\n-8.353\n3.435\n-2.431\n0.017\n\n\nprecip\n-98.904\n42.137\n-2.347\n0.021\n\n\nday_typeWeekend\n37.062\n22.280\n1.663\n0.100"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html",
    "href": "slides/20-comparison-cv-old.html",
    "title": "Model comparison",
    "section": "",
    "text": "See Ed Discussion for upcoming events and internship opportunities\nStatistics Experience due Mon, Nov 20 at 11:59pm\nPlease submit mid-semester feedback by Friday\nProf. Tackett office hours Fridays 1:30 - 3:30pm for the rest of the semester"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#announcements",
    "href": "slides/20-comparison-cv-old.html#announcements",
    "title": "Model comparison",
    "section": "",
    "text": "See Ed Discussion for upcoming events and internship opportunities\nStatistics Experience due Mon, Nov 20 at 11:59pm\nPlease submit mid-semester feedback by Friday\nProf. Tackett office hours Fridays 1:30 - 3:30pm for the rest of the semester"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#topics",
    "href": "slides/20-comparison-cv-old.html#topics",
    "title": "Model comparison",
    "section": "Topics",
    "text": "Topics\n\n\nANOVA for multiple linear regression and sum of squares\nComparing models with \\(R^2\\) vs. \\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam’s razor and parsimony"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#computational-setup",
    "href": "slides/20-comparison-cv-old.html#computational-setup",
    "title": "Model comparison",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(broom)\nlibrary(yardstick)\n\n\nAttaching package: 'yardstick'\n\nThe following object is masked from 'package:readr':\n\n    spec\n\nlibrary(ggformula)\n\nLoading required package: scales\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nLoading required package: ggridges\n\nNew to ggformula?  Try the tutorials: \n    learnr::run_tutorial(\"introduction\", package = \"ggformula\")\n    learnr::run_tutorial(\"refining\", package = \"ggformula\")\n\nlibrary(tidymodels)\n\n── Attaching packages ────────────────────────────────────── tidymodels 1.2.0 ──\n✔ dials        1.3.0     ✔ rsample      1.2.1\n✔ infer        1.0.7     ✔ tune         1.2.1\n✔ modeldata    1.4.0     ✔ workflows    1.1.4\n✔ parsnip      1.2.1     ✔ workflowsets 1.1.0\n✔ recipes      1.1.0     \n── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──\n✖ scales::discard() masks purrr::discard()\n✖ dplyr::filter()   masks stats::filter()\n✖ recipes::fixed()  masks stringr::fixed()\n✖ dplyr::lag()      masks stats::lag()\n✖ yardstick::spec() masks readr::spec()\n✖ recipes::step()   masks stats::step()\n• Use tidymodels_prefer() to resolve common conflicts.\n\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#data-restaurant-tips",
    "href": "slides/20-comparison-cv-old.html#data-restaurant-tips",
    "title": "Model comparison",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 × 4\n     Tip Party Meal   Age   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# ℹ 159 more rows"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#variables",
    "href": "slides/20-comparison-cv-old.html#variables",
    "title": "Model comparison",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\n\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#outcome-tip",
    "href": "slides/20-comparison-cv-old.html#outcome-tip",
    "title": "Model comparison",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#predictors",
    "href": "slides/20-comparison-cv-old.html#predictors",
    "title": "Model comparison",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#relevel-categorical-predictors",
    "href": "slides/20-comparison-cv-old.html#relevel-categorical-predictors",
    "title": "Model comparison",
    "section": "Relevel categorical predictors",
    "text": "Relevel categorical predictors\n\ntips &lt;- tips |&gt;\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#predictors-again",
    "href": "slides/20-comparison-cv-old.html#predictors-again",
    "title": "Model comparison",
    "section": "Predictors, again",
    "text": "Predictors, again"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#outcome-vs.-predictors",
    "href": "slides/20-comparison-cv-old.html#outcome-vs.-predictors",
    "title": "Model comparison",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#fit-and-summarize-model",
    "href": "slides/20-comparison-cv-old.html#fit-and-summarize-model",
    "title": "Model comparison",
    "section": "Fit and summarize model",
    "text": "Fit and summarize model\n\ntip_fit &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + Age, data = tips)\n\ntidy(tip_fit) |&gt;\n  kable(digits = 3)\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-0.170\n0.366\n-0.465\n0.643\n\n\nParty\n1.837\n0.124\n14.758\n0.000\n\n\nAgeMiddle\n1.009\n0.408\n2.475\n0.014\n\n\nAgeSenCit\n1.388\n0.485\n2.862\n0.005\n\n\n\n\n\n. . .\n\n\nIs this the best model to explain variation in tips?"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#another-model-summary",
    "href": "slides/20-comparison-cv-old.html#another-model-summary",
    "title": "Model comparison",
    "section": "Another model summary",
    "text": "Another model summary\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#analysis-of-variance-anova-1",
    "href": "slides/20-comparison-cv-old.html#analysis-of-variance-anova-1",
    "title": "Model comparison",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#anova",
    "href": "slides/20-comparison-cv-old.html#anova",
    "title": "Model comparison",
    "section": "ANOVA",
    "text": "ANOVA\n\nMain Idea: Decompose the total variation on the outcome into:\n\nthe variation that can be explained by the each of the variables in the model\nthe variation that can’t be explained by the model (left in the residuals)\n\nIf the variation that can be explained by the variables in the model is greater than the variation in the residuals, this signals that the model might be “valuable” (at least one of the \\(\\beta\\)s not equal to 0)"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#anova-output-in-r1",
    "href": "slides/20-comparison-cv-old.html#anova-output-in-r1",
    "title": "Model comparison",
    "section": "ANOVA output in R1",
    "text": "ANOVA output in R1\n\nanova(tip_fit$fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#anova-output-with-totals",
    "href": "slides/20-comparison-cv-old.html#anova-output-with-totals",
    "title": "Model comparison",
    "section": "ANOVA output, with totals",
    "text": "ANOVA output, with totals\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\n\n\n\n\nTotal\n168\n1913.11"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#sum-of-squares",
    "href": "slides/20-comparison-cv-old.html#sum-of-squares",
    "title": "Model comparison",
    "section": "Sum of squares",
    "text": "Sum of squares\n\n\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#sum-of-squares-ss_total",
    "href": "slides/20-comparison-cv-old.html#sum-of-squares-ss_total",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Total}\\)",
    "text": "Sum of squares: \\(SS_{Total}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome\n\n\\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\) = 1913.11"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#sum-of-squares-ss_error",
    "href": "slides/20-comparison-cv-old.html#sum-of-squares-ss_error",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Error}\\)",
    "text": "Sum of squares: \\(SS_{Error}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals\n\n\\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\) = 686.44"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#sum-of-squares-ss_model",
    "href": "slides/20-comparison-cv-old.html#sum-of-squares-ss_model",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Model}\\)",
    "text": "Sum of squares: \\(SS_{Model}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Model}\\): Variability explained by the model\n\n\\(SS_{Total} - SS_{Error}\\) = 1226.67"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#r-squared-r2",
    "href": "slides/20-comparison-cv-old.html#r-squared-r2",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\nRecall: \\(R^2\\) is the proportion of the variation in the response variable explained by the regression model.\n. . .\n\\[\nR^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}} = 1 - \\frac{686.44}{1913.11} = 0.641\n\\]\n. . .\n\nglance(tip_fit)$r.squared\n\n[1] 0.6411891"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#r-squared-r2-1",
    "href": "slides/20-comparison-cv-old.html#r-squared-r2-1",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\n\\(R^2\\) will always increase as we add more variables to the model + If we add enough variables, we can always achieve \\(R^2=100\\%\\)\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#adjusted-r2",
    "href": "slides/20-comparison-cv-old.html#adjusted-r2",
    "title": "Model comparison",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model\nDiffers from \\(R^2\\) by using the mean squares (sumsq/df) rather than sums of squares and therefore adjusting for the number of predictor variables"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#r2-and-adjusted-r2",
    "href": "slides/20-comparison-cv-old.html#r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\n\n. . .\n\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#using-r2-and-adjusted-r2",
    "href": "slides/20-comparison-cv-old.html#using-r2-and-adjusted-r2",
    "title": "Model comparison",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#comparing-models-with-r2_adj",
    "href": "slides/20-comparison-cv-old.html#comparing-models-with-r2_adj",
    "title": "Model comparison",
    "section": "Comparing models with \\(R^2_{adj}\\)",
    "text": "Comparing models with \\(R^2_{adj}\\)\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n    data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.674         0.664\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(r.squared, adj.r.squared)\n\n# A tibble: 1 × 2\n  r.squared adj.r.squared\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.683         0.662\n\n\n\n\n\n\nWhy did we not use the full recipe() workflow to fit Model 1 or Model 2?\nWhich model would we choose based on \\(R^2\\)?\nWhich model would we choose based on Adjusted \\(R^2\\)?\nWhich statistic should we use to choose the final model - \\(R^2\\) or Adjusted \\(R^2\\)? Why?\n\nVote on Ed Discussion [10:05am lecture][1:25pm lecture]"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#aic-bic",
    "href": "slides/20-comparison-cv-old.html#aic-bic",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nEstimators of prediction error and relative quality of models:\n. . .\nAkaike’s Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \n. . .\nSchwarz’s Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#aic-bic-1",
    "href": "slides/20-comparison-cv-old.html#aic-bic-1",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n. . .\n\nFirst Term: Decreases as p increases"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#aic-bic-2",
    "href": "slides/20-comparison-cv-old.html#aic-bic-2",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\nSecond Term: Fixed for a given sample size n"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#aic-bic-3",
    "href": "slides/20-comparison-cv-old.html#aic-bic-3",
    "title": "Model comparison",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nThird Term: Increases as p increases"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#using-aic-bic",
    "href": "slides/20-comparison-cv-old.html#using-aic-bic",
    "title": "Model comparison",
    "section": "Using AIC & BIC",
    "text": "Using AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nChoose model with the smaller value of AIC or BIC\nIf \\(n \\geq 8\\), the penalty for BIC is larger than that of AIC, so BIC tends to favor more parsimonious models (i.e. models with fewer terms)"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#comparing-models-with-aic-and-bic",
    "href": "slides/20-comparison-cv-old.html#comparing-models-with-aic-and-bic",
    "title": "Model comparison",
    "section": "Comparing models with AIC and BIC",
    "text": "Comparing models with AIC and BIC\n\n\n\ntip_fit_1 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal,\n      \n      data = tips)\n\nglance(tip_fit_1) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  714.  736.\n\n\n\n\ntip_fit_2 &lt;- linear_reg() |&gt;\n  set_engine(\"lm\") |&gt;\n  fit(Tip ~ Party + \n            Age + \n            Meal + \n            Day, \n      data = tips)\n\nglance(tip_fit_2) |&gt; \n  select(AIC, BIC)\n\n# A tibble: 1 × 2\n    AIC   BIC\n  &lt;dbl&gt; &lt;dbl&gt;\n1  720.  757.\n\n\n\n\n\n\nWhich model would we choose based on AIC?\nWhich model would we choose based on BIC?"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#commonalities-between-criteria",
    "href": "slides/20-comparison-cv-old.html#commonalities-between-criteria",
    "title": "Model comparison",
    "section": "Commonalities between criteria",
    "text": "Commonalities between criteria\n\n\\(R^2_{adj}\\), AIC, and BIC all apply a penalty for more predictors\nThe penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\nGoal: Parsimony"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#parsimony-and-occams-razor",
    "href": "slides/20-comparison-cv-old.html#parsimony-and-occams-razor",
    "title": "Model comparison",
    "section": "Parsimony and Occam’s razor",
    "text": "Parsimony and Occam’s razor\n\nThe principle of parsimony is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation2\nCalled Occam’s razor because he “shaved” his explanations down to the bare minimum\nParsimony in modeling:\n\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments relying on few assumptions should be preferred to those relying on many\nmodels should be pared down until they are minimal adequate\nsimple explanations should be preferred to complex explanations"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#in-pursuit-of-occams-razor",
    "href": "slides/20-comparison-cv-old.html#in-pursuit-of-occams-razor",
    "title": "Model comparison",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\nOccam’s razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\nModel selection follows this principle\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\nIn other words, we prefer the simplest best model, i.e. parsimonious model"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#alternate-views",
    "href": "slides/20-comparison-cv-old.html#alternate-views",
    "title": "Model comparison",
    "section": "Alternate views",
    "text": "Alternate views\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nRadford Neal - Bayesian Learning for Neural Networks3"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#other-concerns-with-our-approach",
    "href": "slides/20-comparison-cv-old.html#other-concerns-with-our-approach",
    "title": "Model comparison",
    "section": "Other concerns with our approach",
    "text": "Other concerns with our approach\n\nAll criteria we considered for model comparison require making predictions for our data and then uses the prediction error (\\(SS_{Error}\\)) somewhere in the formula\nBut we’re making prediction for the data we used to build the model (estimate the coefficients), which can lead to overfitting\nInstead we should\n\nsplit our data into testing and training sets\n“train” the model on the training data and pick a few models we’re genuinely considering as potentially good models\ntest those models on the testing set\n…and repeat this process multiple times"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#spending-our-data",
    "href": "slides/20-comparison-cv-old.html#spending-our-data",
    "title": "Model comparison",
    "section": "Spending our data",
    "text": "Spending our data\n\nWe have already established that the idea of data spending where the test set was recommended for obtaining an unbiased estimate of performance.\nHowever, we usually need to understand the effectiveness of the model before using the test set.\nTypically we can’t decide on which final model to take to the test set without making model assessments.\nRemedy: Resampling to make model assessments on training data in a way that can generalize to new data."
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#resampling-for-model-assessment",
    "href": "slides/20-comparison-cv-old.html#resampling-for-model-assessment",
    "title": "Model comparison",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\nResampling is only conducted on the training set. The test set is not involved. For each iteration of resampling, the data are partitioned into two subsamples:\n\nThe model is fit with the analysis set. Model fit statistics such as \\(R^2_{Adj}\\), AIC, and BIC are calculated based on this fit.\nThe model is evaluated with the assessment set."
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#resampling-for-model-assessment-1",
    "href": "slides/20-comparison-cv-old.html#resampling-for-model-assessment-1",
    "title": "Model comparison",
    "section": "Resampling for model assessment",
    "text": "Resampling for model assessment\n\n\n\n\n\n\nImage source: Kuhn and Silge. Tidy modeling with R."
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#analysis-and-assessment-sets",
    "href": "slides/20-comparison-cv-old.html#analysis-and-assessment-sets",
    "title": "Model comparison",
    "section": "Analysis and assessment sets",
    "text": "Analysis and assessment sets\n\nAnalysis set is analogous to training set.\nAssessment set is analogous to test set.\nThe terms analysis and assessment avoids confusion with initial split of the data.\nThese data sets are mutually exclusive."
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#cross-validation-1",
    "href": "slides/20-comparison-cv-old.html#cross-validation-1",
    "title": "Model comparison",
    "section": "Cross validation",
    "text": "Cross validation\nMore specifically, v-fold cross validation – commonly used resampling technique:\n\nRandomly split your training data into v partitions\nUse v-1 partitions for analysis, and the remaining 1 partition for analysis (model fit + model fit statistics)\nRepeat v times, updating which partition is used for assessment each time\n\n. . .\nLet’s give an example where v = 3…"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#to-get-started",
    "href": "slides/20-comparison-cv-old.html#to-get-started",
    "title": "Model comparison",
    "section": "To get started…",
    "text": "To get started…\nSplit data into training and test sets\n\nset.seed(345)\n\ntips_split &lt;- initial_split(tips)\ntips_train &lt;- training(tips_split)\ntips_test &lt;- testing(tips_split)"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#to-get-started-1",
    "href": "slides/20-comparison-cv-old.html#to-get-started-1",
    "title": "Model comparison",
    "section": "To get started…",
    "text": "To get started…\nCreate recipe\n\ntips_rec1 &lt;- recipe(Tip ~ Party + Age + Meal, \n                    data = tips_train)\n\ntips_rec1\n\n\n\n\n── Recipe ──────────────────────────────────────────────────────────────────────\n\n\n\n\n\n── Inputs \n\n\nNumber of variables by role\n\n\noutcome:   1\npredictor: 3"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#to-get-started-2",
    "href": "slides/20-comparison-cv-old.html#to-get-started-2",
    "title": "Model comparison",
    "section": "To get started…",
    "text": "To get started…\nSpecify model\n\ntips_spec &lt;- linear_reg() |&gt;\n  set_engine(\"lm\")\n\n. . .\nCreate workflow\n\ntips_wflow1 &lt;- workflow() |&gt;\n  add_model(tips_spec) |&gt;\n  add_recipe(tips_rec1)\n\ntips_wflow1\n\n══ Workflow ════════════════════════════════════════════════════════════════════\nPreprocessor: Recipe\nModel: linear_reg()\n\n── Preprocessor ────────────────────────────────────────────────────────────────\n0 Recipe Steps\n\n── Model ───────────────────────────────────────────────────────────────────────\nLinear Regression Model Specification (regression)\n\nComputational engine: lm"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#cross-validation-step-1",
    "href": "slides/20-comparison-cv-old.html#cross-validation-step-1",
    "title": "Model comparison",
    "section": "Cross validation, step 1",
    "text": "Cross validation, step 1\nRandomly split your training data into 3 partitions:"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#tips-split-training-data",
    "href": "slides/20-comparison-cv-old.html#tips-split-training-data",
    "title": "Model comparison",
    "section": "Tips: Split training data",
    "text": "Tips: Split training data\n\nfolds &lt;- vfold_cv(tips_train, v = 3)\nfolds\n\n#  3-fold cross-validation \n# A tibble: 3 × 2\n  splits          id   \n  &lt;list&gt;          &lt;chr&gt;\n1 &lt;split [84/42]&gt; Fold1\n2 &lt;split [84/42]&gt; Fold2\n3 &lt;split [84/42]&gt; Fold3"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#cross-validation-steps-2-and-3",
    "href": "slides/20-comparison-cv-old.html#cross-validation-steps-2-and-3",
    "title": "Model comparison",
    "section": "Cross validation, steps 2 and 3",
    "text": "Cross validation, steps 2 and 3\n\n\nUse v-1 partitions for analysis, and the remaining 1 partition for assessment\nRepeat v times, updating which partition is used for assessment each time"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#tips-fit-resamples",
    "href": "slides/20-comparison-cv-old.html#tips-fit-resamples",
    "title": "Model comparison",
    "section": "Tips: Fit resamples",
    "text": "Tips: Fit resamples\n\ntips_fit_rs1 &lt;- tips_wflow1 |&gt;\n  fit_resamples(folds)\n\ntips_fit_rs1 \n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 × 4\n  splits          id    .metrics         .notes          \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [84/42]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n2 &lt;split [84/42]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;\n3 &lt;split [84/42]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt;"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#cross-validation-now-what",
    "href": "slides/20-comparison-cv-old.html#cross-validation-now-what",
    "title": "Model comparison",
    "section": "Cross validation, now what?",
    "text": "Cross validation, now what?\n\nWe’ve fit a bunch of models\nNow it’s time to use them to collect metrics (e.g., $R^2$, AIC, RMSE, etc. ) on each model and use them to evaluate model fit and how it varies across folds"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#collect-r2-and-rmse-from-cv",
    "href": "slides/20-comparison-cv-old.html#collect-r2-and-rmse-from-cv",
    "title": "Model comparison",
    "section": "Collect \\(R^2\\) and RMSE from CV",
    "text": "Collect \\(R^2\\) and RMSE from CV\n\n# Produces summary across all CV\ncollect_metrics(tips_fit_rs1)\n\n# A tibble: 2 × 6\n  .metric .estimator  mean     n std_err .config             \n  &lt;chr&gt;   &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt; &lt;chr&gt;               \n1 rmse    standard   2.10      3  0.243  Preprocessor1_Model1\n2 rsq     standard   0.591     3  0.0728 Preprocessor1_Model1\n\n\n\nNote: These are calculated using the assessment data"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#deeper-look-into-r2-and-rmse",
    "href": "slides/20-comparison-cv-old.html#deeper-look-into-r2-and-rmse",
    "title": "Model comparison",
    "section": "Deeper look into \\(R^2\\) and RMSE",
    "text": "Deeper look into \\(R^2\\) and RMSE\n\ncv_metrics1 &lt;- collect_metrics(tips_fit_rs1, summarize = FALSE) \n\ncv_metrics1\n\n# A tibble: 6 × 5\n  id    .metric .estimator .estimate .config             \n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 Fold1 rmse    standard       2.04  Preprocessor1_Model1\n2 Fold1 rsq     standard       0.736 Preprocessor1_Model1\n3 Fold2 rmse    standard       1.71  Preprocessor1_Model1\n4 Fold2 rsq     standard       0.509 Preprocessor1_Model1\n5 Fold3 rmse    standard       2.54  Preprocessor1_Model1\n6 Fold3 rsq     standard       0.528 Preprocessor1_Model1"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#better-tabulation-of-r2-and-rmse-from-cv",
    "href": "slides/20-comparison-cv-old.html#better-tabulation-of-r2-and-rmse-from-cv",
    "title": "Model comparison",
    "section": "Better tabulation of \\(R^2\\) and RMSE from CV",
    "text": "Better tabulation of \\(R^2\\) and RMSE from CV\n\ncv_metrics1 |&gt;\n  mutate(.estimate = round(.estimate, 3)) |&gt;\n  pivot_wider(id_cols = id, names_from = .metric, values_from = .estimate) |&gt;\n  kable(col.names = c(\"Fold\", \"RMSE\", \"R-squared\"))\n\n\n\n\nFold\nRMSE\nR-squared\n\n\n\n\nFold1\n2.036\n0.736\n\n\nFold2\n1.709\n0.509\n\n\nFold3\n2.545\n0.528"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#how-does-rmse-compare-to-y",
    "href": "slides/20-comparison-cv-old.html#how-does-rmse-compare-to-y",
    "title": "Model comparison",
    "section": "How does RMSE compare to y?",
    "text": "How does RMSE compare to y?\n\n\nCross validation RMSE stats:\n\ncv_metrics1 |&gt;\n  filter(.metric == \"rmse\") |&gt;\n  summarise(\n    min = min(.estimate),\n    max = max(.estimate),\n    mean = mean(.estimate),\n    sd = sd(.estimate)\n  )\n\n# A tibble: 1 × 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1.71  2.54  2.10 0.421\n\n\n\nTraining data tips:\n\ntips_train |&gt;\n  summarise(\n    min = min(Tip),\n    max = max(Tip),\n    mean = mean(Tip),\n    sd = sd(Tip)\n  )\n\n# A tibble: 1 × 4\n    min   max  mean    sd\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0  19.5  4.87  3.37"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#calculate-r2_adj-aic-and-bic-for-each-fold",
    "href": "slides/20-comparison-cv-old.html#calculate-r2_adj-aic-and-bic-for-each-fold",
    "title": "Model comparison",
    "section": "Calculate \\(R^2_{Adj}\\), AIC, and BIC for each fold",
    "text": "Calculate \\(R^2_{Adj}\\), AIC, and BIC for each fold\n\n# Function to get Adj R-sq, AIC, BIC\ncalc_model_stats &lt;- function(x) {\n  glance(extract_fit_parsnip(x)) |&gt;\n    select(adj.r.squared, AIC, BIC)\n}\n\n# Fit model and calculate statistics for each fold\ntips_fit_rs1 &lt;-tips_wflow1 |&gt;\n  fit_resamples(resamples = folds, \n                control = control_resamples(extract = calc_model_stats))\n\ntips_fit_rs1\n\n# Resampling results\n# 3-fold cross-validation \n# A tibble: 3 × 5\n  splits          id    .metrics         .notes           .extracts       \n  &lt;list&gt;          &lt;chr&gt; &lt;list&gt;           &lt;list&gt;           &lt;list&gt;          \n1 &lt;split [84/42]&gt; Fold1 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n2 &lt;split [84/42]&gt; Fold2 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;\n3 &lt;split [84/42]&gt; Fold3 &lt;tibble [2 × 4]&gt; &lt;tibble [0 × 3]&gt; &lt;tibble [1 × 2]&gt;"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#collect-r2_adj-aic-bic-from-cv",
    "href": "slides/20-comparison-cv-old.html#collect-r2_adj-aic-bic-from-cv",
    "title": "Model comparison",
    "section": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV",
    "text": "Collect \\(R^2_{Adj}\\), AIC, BIC from CV\n\nmap_df(tips_fit_rs1$.extracts, ~ .x[[1]][[1]]) |&gt;\n  bind_cols(Fold = tips_fit_rs1$id)\n\n# A tibble: 3 × 4\n  adj.r.squared   AIC   BIC Fold \n          &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;\n1         0.550  369.  386. Fold1\n2         0.659  377.  394. Fold2\n3         0.718  337.  354. Fold3\n\n\n\nNote: These are based on the model fit from the analysis data"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#cross-validation-in-practice",
    "href": "slides/20-comparison-cv-old.html#cross-validation-in-practice",
    "title": "Model comparison",
    "section": "Cross validation in practice",
    "text": "Cross validation in practice\n\n\nTo illustrate how CV works, we used v = 3:\n\n\nAnalysis sets are 2/3 of the training set\nEach assessment set is a distinct 1/3\nThe final resampling estimate of performance averages each of the 3 replicates\n\n\nThis was useful for illustrative purposes, but v = 3 is a poor choice in practice\nValues of v are most often 5 or 10; we generally prefer 10-fold cross-validation as a default"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#recap",
    "href": "slides/20-comparison-cv-old.html#recap",
    "title": "Model comparison",
    "section": "Recap",
    "text": "Recap\n\nANOVA for multiple linear regression and sum of squares\nComparing models with\n\n\\(R^2\\) vs. \\(R^2_{Adj}\\)\nAIC and BIC\n\nOccam’s razor and parsimony\n\n\n\nCross validation for\n\nmodel evaluation\nmodel comparison"
  },
  {
    "objectID": "slides/20-comparison-cv-old.html#footnotes",
    "href": "slides/20-comparison-cv-old.html#footnotes",
    "title": "Model comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClick here for explanation about the way R calculates sum of squares for each variable.↩︎\nSource: The R Book by Michael J. Crawley.↩︎\nSuggested blog post: Occam by Andrew Gelman↩︎"
  },
  {
    "objectID": "slides/20-comparison.html#f-test-testing-the-whole-model-at-once",
    "href": "slides/20-comparison.html#f-test-testing-the-whole-model-at-once",
    "title": "Model comparison",
    "section": "F-Test: Testing the whole model at once",
    "text": "F-Test: Testing the whole model at once\nHypotheses:\n\\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0\\) vs. \\(H_A:\\) at least one \\(\\beta_i \\neq 0\\)\n\nTest statistic: F-statistics\n\\[\nF = \\frac{MSModel}{MSE} = \\frac{SSModel/k}{SSE/(n-k-1)} \\\\\n\\]\n\n\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\n\\text{p-value} = P(F &gt; \\text{test statistic}),\n\\]\ncalculated from an \\(F\\) distribution with \\(k\\) and \\(n - k - 1\\) degrees of freedom."
  },
  {
    "objectID": "slides/20-comparison.html#f-test-in-r",
    "href": "slides/20-comparison.html#f-test-in-r",
    "title": "Model comparison",
    "section": "F-test in R",
    "text": "F-test in R\n\nUse glance function from broom package\n\nstatistic: F-statistic\np.value: p-value from F-test"
  },
  {
    "objectID": "ae/ae-14-comparison.html#exercise-1",
    "href": "ae/ae-14-comparison.html#exercise-1",
    "title": "AE 14: Model Comparison",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit a linear model to predict Tips from Party and Age."
  },
  {
    "objectID": "ae/ae-14-comparison.html#exercise-2",
    "href": "ae/ae-14-comparison.html#exercise-2",
    "title": "AE 14: Model Comparison",
    "section": "Exercise 2",
    "text": "Exercise 2\nPipe the model you generated in the previous Exercise into the function anova."
  },
  {
    "objectID": "ae/ae-14-comparison.html#exercise-3",
    "href": "ae/ae-14-comparison.html#exercise-3",
    "title": "AE 14: Model Comparison",
    "section": "Exercise 3",
    "text": "Exercise 3\nBased on the output above, compute \\(SSTotal\\), \\(SSError\\), and \\(SSModel\\). You should only need to use addition and/or subtraction."
  },
  {
    "objectID": "ae/ae-14-comparison.html#exercise-4",
    "href": "ae/ae-14-comparison.html#exercise-4",
    "title": "AE 14: Model Comparison",
    "section": "Exercise 4",
    "text": "Exercise 4\nPipe your model into the glance function. Identify, the F-statistic and p-value for an F-test of this model. Interpret the outcome of your test in the context of the problem. Be prepared to discuss the difference between this p-value and the p-values from the individual model coefficients."
  },
  {
    "objectID": "ae/ae-14-comparison.html#exercise-5",
    "href": "ae/ae-14-comparison.html#exercise-5",
    "title": "AE 14: Model Comparison",
    "section": "Exercise 5",
    "text": "Exercise 5\nWhat is the \\(R^2\\) value for this model?"
  },
  {
    "objectID": "ae/ae-14-comparison.html#exercise-6",
    "href": "ae/ae-14-comparison.html#exercise-6",
    "title": "AE 14: Model Comparison",
    "section": "Exercise 6",
    "text": "Exercise 6\nFit the full model. What is it’s \\(R^2\\)? Does this model have a higher or lower \\(R^2\\) than the previous model? Does this mean it is a better model? Be prepared to discuss."
  },
  {
    "objectID": "ae/ae-14-comparison.html#exercise-7",
    "href": "ae/ae-14-comparison.html#exercise-7",
    "title": "AE 14: Model Comparison",
    "section": "Exercise 7",
    "text": "Exercise 7\nThe following code converts the numerical variable Bill into a new categorical variable Bill_factor. Essentially, each different number in Bill is treated as it’s own category. Fit a model predicting Tip from Bill_factor. What is your R^2? Think about the implications of this and what it means for the usefullness of \\(R^2\\).\n\ntips &lt;- tips |&gt; \n  mutate(Bill_factor = factor(Bill))"
  },
  {
    "objectID": "slides/21-comparison2.html",
    "href": "slides/21-comparison2.html",
    "title": "Model comparison",
    "section": "",
    "text": "Project: EDA Due Wednesday, October 30th\nOral R Quiz (time to start scheduling it)\n\n\n📋 AE 14 - Model Comparison\n\nOpen up AE 14 and complete Exercises 0-2"
  },
  {
    "objectID": "slides/21-comparison2.html#announcements",
    "href": "slides/21-comparison2.html#announcements",
    "title": "Model comparison + selection",
    "section": "Announcements",
    "text": "Announcements\n\nProject: EDA Due Wednesday, October 30th\nOral R Quiz\n\n\n📋 AE 15 - Model Comparison 2\n\nOpen up AE 15"
  },
  {
    "objectID": "slides/21-comparison2.html#topics",
    "href": "slides/21-comparison2.html#topics",
    "title": "Model comparison + selection",
    "section": "Topics",
    "text": "Topics\n\n\nComparing models with \\(R^2\\) vs. \\(R^2_{adj}\\)\nComparing models with AIC and BIC\nOccam’s razor and parsimony"
  },
  {
    "objectID": "slides/21-comparison2.html#computational-setup",
    "href": "slides/21-comparison2.html#computational-setup",
    "title": "Model comparison + selection",
    "section": "Computational setup",
    "text": "Computational setup\n\n# load packages\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(yardstick)\nlibrary(ggformula)\nlibrary(supernova)\nlibrary(tidymodels)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(janitor)\nlibrary(kableExtra)\n\n# set default theme and larger font size for ggplot2\nggplot2::theme_set(ggplot2::theme_bw(base_size = 20))"
  },
  {
    "objectID": "slides/21-comparison2.html#data-restaurant-tips",
    "href": "slides/21-comparison2.html#data-restaurant-tips",
    "title": "Model comparison + selection",
    "section": "Data: Restaurant tips",
    "text": "Data: Restaurant tips\nWhich variables help us predict the amount customers tip at a restaurant?\n\n\n# A tibble: 169 × 4\n     Tip Party Meal   Age   \n   &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; \n 1  2.99     1 Dinner Yadult\n 2  2        1 Dinner Yadult\n 3  5        1 Dinner SenCit\n 4  4        3 Dinner Middle\n 5 10.3      2 Dinner SenCit\n 6  4.85     2 Dinner Middle\n 7  5        4 Dinner Yadult\n 8  4        3 Dinner Middle\n 9  5        2 Dinner Middle\n10  1.58     1 Dinner SenCit\n# ℹ 159 more rows"
  },
  {
    "objectID": "slides/21-comparison2.html#variables",
    "href": "slides/21-comparison2.html#variables",
    "title": "Model comparison + selection",
    "section": "Variables",
    "text": "Variables\nPredictors:\n\n\nParty: Number of people in the party\nMeal: Time of day (Lunch, Dinner, Late Night)\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)\n\n\nOutcome: Tip: Amount of tip"
  },
  {
    "objectID": "slides/21-comparison2.html#outcome-tip",
    "href": "slides/21-comparison2.html#outcome-tip",
    "title": "Model comparison + selection",
    "section": "Outcome: Tip",
    "text": "Outcome: Tip"
  },
  {
    "objectID": "slides/21-comparison2.html#predictors",
    "href": "slides/21-comparison2.html#predictors",
    "title": "Model comparison + selection",
    "section": "Predictors",
    "text": "Predictors"
  },
  {
    "objectID": "slides/21-comparison2.html#relevel-categorical-predictors",
    "href": "slides/21-comparison2.html#relevel-categorical-predictors",
    "title": "Model comparison",
    "section": "Relevel categorical predictors",
    "text": "Relevel categorical predictors\n\ntips &lt;- tips |&gt;\n  mutate(\n    Meal = fct_relevel(Meal, \"Lunch\", \"Dinner\", \"Late Night\"),\n    Age  = fct_relevel(Age, \"Yadult\", \"Middle\", \"SenCit\")\n  )"
  },
  {
    "objectID": "slides/21-comparison2.html#predictors-again",
    "href": "slides/21-comparison2.html#predictors-again",
    "title": "Model comparison",
    "section": "Predictors, again",
    "text": "Predictors, again"
  },
  {
    "objectID": "slides/21-comparison2.html#outcome-vs.-predictors",
    "href": "slides/21-comparison2.html#outcome-vs.-predictors",
    "title": "Model comparison + selection",
    "section": "Outcome vs. predictors",
    "text": "Outcome vs. predictors"
  },
  {
    "objectID": "slides/21-comparison2.html#fit-and-summarize-model",
    "href": "slides/21-comparison2.html#fit-and-summarize-model",
    "title": "Model comparison + selection",
    "section": "Fit and summarize model",
    "text": "Fit and summarize model\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.838\n0.397\n2.112\n0.036\n\n\nParty\n1.837\n0.124\n14.758\n0.000\n\n\nAgeSenCit\n0.379\n0.410\n0.925\n0.356\n\n\nAgeYadult\n-1.009\n0.408\n-2.475\n0.014\n\n\n\n\n\n\n\n\nIs this model good?"
  },
  {
    "objectID": "slides/21-comparison2.html#another-model-summary",
    "href": "slides/21-comparison2.html#another-model-summary",
    "title": "Model comparison",
    "section": "Another model summary",
    "text": "Another model summary\n\nanova(tip_fit) |&gt;\n  tidy() |&gt;\n  kable(digits = 2)\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0.00\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\nNA\nNA"
  },
  {
    "objectID": "slides/21-comparison2.html#analysis-of-variance-anova-1",
    "href": "slides/21-comparison2.html#analysis-of-variance-anova-1",
    "title": "Model comparison",
    "section": "Analysis of variance (ANOVA)",
    "text": "Analysis of variance (ANOVA)"
  },
  {
    "objectID": "slides/21-comparison2.html#anova",
    "href": "slides/21-comparison2.html#anova",
    "title": "Model comparison",
    "section": "ANOVA",
    "text": "ANOVA\n\nMain Idea: Decompose the total variation on the outcome into:\n\nthe variation that can be explained by the each of the variables in the model\nthe variation that can’t be explained by the model (left in the residuals)\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model, \\(\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2\\)\n\n. . .\nComplete Exercise 3."
  },
  {
    "objectID": "slides/21-comparison2.html#anova-output-in-r1",
    "href": "slides/21-comparison2.html#anova-output-in-r1",
    "title": "Model comparison",
    "section": "ANOVA output in R1",
    "text": "ANOVA output in R1\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.63588\n1188.635880\n285.711511\n0.000000\n\n\nAge\n2\n38.02783\n19.013916\n4.570361\n0.011699\n\n\nResiduals\n165\n686.44389\n4.160266\nNA\nNA"
  },
  {
    "objectID": "slides/21-comparison2.html#anova-output-with-totals",
    "href": "slides/21-comparison2.html#anova-output-with-totals",
    "title": "Model comparison",
    "section": "ANOVA output, with totals",
    "text": "ANOVA output, with totals\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `across(where(is.numeric), round, 2)`.\nCaused by warning:\n! The `...` argument of `across()` is deprecated as of dplyr 1.1.0.\nSupply arguments directly to `.fns` through an anonymous function instead.\n\n  # Previously\n  across(a:b, mean, na.rm = TRUE)\n\n  # Now\n  across(a:b, \\(x) mean(x, na.rm = TRUE))\n\n\n\n\n\nterm\ndf\nsumsq\nmeansq\nstatistic\np.value\n\n\n\n\nParty\n1\n1188.64\n1188.64\n285.71\n0\n\n\nAge\n2\n38.03\n19.01\n4.57\n0.01\n\n\nResiduals\n165\n686.44\n4.16\n\n\n\n\nTotal\n168\n1913.11"
  },
  {
    "objectID": "slides/21-comparison2.html#sum-of-squares",
    "href": "slides/21-comparison2.html#sum-of-squares",
    "title": "Model comparison",
    "section": "Sum of squares",
    "text": "Sum of squares\n\n\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome, \\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\)\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals, \\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\)\n\\(SS_{Model} = SS_{Total} - SS_{Error}\\): Variability explained by the model, \\(\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2\\)"
  },
  {
    "objectID": "slides/21-comparison2.html#sum-of-squares-ss_total",
    "href": "slides/21-comparison2.html#sum-of-squares-ss_total",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Total}\\)",
    "text": "Sum of squares: \\(SS_{Total}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Total}\\): Total sum of squares, variability of outcome\n\n\\(\\sum_{i = 1}^n (y_i - \\bar{y})^2\\) = 1913.11"
  },
  {
    "objectID": "slides/21-comparison2.html#sum-of-squares-ss_error",
    "href": "slides/21-comparison2.html#sum-of-squares-ss_error",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Error}\\)",
    "text": "Sum of squares: \\(SS_{Error}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Error}\\): Residual sum of squares, variability of residuals\n\n\\(\\sum_{i = 1}^n (y_i - \\hat{y}_i)^2\\) = 686.44"
  },
  {
    "objectID": "slides/21-comparison2.html#sum-of-squares-ss_model",
    "href": "slides/21-comparison2.html#sum-of-squares-ss_model",
    "title": "Model comparison",
    "section": "Sum of squares: \\(SS_{Model}\\)",
    "text": "Sum of squares: \\(SS_{Model}\\)\n\n\n\n\n\nterm\ndf\nsumsq\n\n\n\n\nParty\n1\n1188.64\n\n\nAge\n2\n38.03\n\n\nResiduals\n165\n686.44\n\n\nTotal\n168\n1913.11\n\n\n\n\n\n\n\n\n\n\\(SS_{Model}\\): Variability explained by the model\n\n\\(\\sum_{i = 1}^n (\\hat{y}_i - \\bar{y})^2 = SS_{Model} = SS_{Total} - SS_{Error} =\\) 1226.67"
  },
  {
    "objectID": "slides/21-comparison2.html#f-test-testing-the-whole-model-at-once",
    "href": "slides/21-comparison2.html#f-test-testing-the-whole-model-at-once",
    "title": "Model comparison",
    "section": "F-Test: Testing the whole model at once",
    "text": "F-Test: Testing the whole model at once\nHypotheses:\n\\(H_0: \\beta_1 = \\beta_2 = \\cdots = \\beta_k = 0\\) vs. \\(H_A:\\) at least one \\(\\beta_i \\neq 0\\)\n. . .\nTest statistic: F-statistics\n\\[\nF = \\frac{MSModel}{MSE} = \\frac{SSModel/k}{SSE/(n-k-1)} \\\\\n\\]\n. . .\np-value: Probability of observing a test statistic at least as extreme (in the direction of the alternative hypothesis) from the null value as the one observed\n\\[\n\\text{p-value} = P(F &gt; \\text{test statistic}),\n\\]\ncalculated from an \\(F\\) distribution with \\(k\\) and \\(n - k - 1\\) degrees of freedom."
  },
  {
    "objectID": "slides/21-comparison2.html#f-test-in-r",
    "href": "slides/21-comparison2.html#f-test-in-r",
    "title": "Model comparison",
    "section": "F-test in R",
    "text": "F-test in R\n\nUse glance function from broom package\n\nstatistic: F-statistic\np.value: p-value from F-test"
  },
  {
    "objectID": "slides/21-comparison2.html#r-squared-r2",
    "href": "slides/21-comparison2.html#r-squared-r2",
    "title": "Model comparison continued",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\n\\(R^2\\) will always increase as we add more variables to the model\nIf we add enough variables, we can usually achieve \\(R^2=100\\%\\)\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables"
  },
  {
    "objectID": "slides/21-comparison2.html#r-squared-r2-1",
    "href": "slides/21-comparison2.html#r-squared-r2-1",
    "title": "Model comparison",
    "section": "R-squared, \\(R^2\\)",
    "text": "R-squared, \\(R^2\\)\n\n\\(R^2\\) will always increase as we add more variables to the model + If we add enough variables, we can always achieve \\(R^2=100\\%\\)\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choose the model with the most predictor variables"
  },
  {
    "objectID": "slides/21-comparison2.html#adjusted-r2",
    "href": "slides/21-comparison2.html#adjusted-r2",
    "title": "Model comparison + selection",
    "section": "Adjusted \\(R^2\\)",
    "text": "Adjusted \\(R^2\\)\n\nAdjusted \\(R^2\\): measure that includes a penalty for unnecessary predictor variables\nSimilar to \\(R^2\\), it is a measure of the amount of variation in the response that is explained by the regression model\nDiffers from \\(R^2\\) by using the mean squares (sum of squares/degrees of freedom) rather than sums of squares and therefore adjusting for the number of predictor variables"
  },
  {
    "objectID": "slides/21-comparison2.html#r2-and-adjusted-r2",
    "href": "slides/21-comparison2.html#r2-and-adjusted-r2",
    "title": "Model comparison + selection",
    "section": "\\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "\\(R^2\\) and Adjusted \\(R^2\\)\n\\[R^2 = \\frac{SS_{Model}}{SS_{Total}} = 1 - \\frac{SS_{Error}}{SS_{Total}}\\]\n\n\n\\[R^2_{adj} = 1 - \\frac{SS_{Error}/(n-p-1)}{SS_{Total}/(n-1)}\\]\nwhere\n\n\\(n\\) is the number of observations used to fit the model\n\\(p\\) is the number of terms (not including the intercept) in the model"
  },
  {
    "objectID": "slides/21-comparison2.html#using-r2-and-adjusted-r2",
    "href": "slides/21-comparison2.html#using-r2-and-adjusted-r2",
    "title": "Model comparison + selection",
    "section": "Using \\(R^2\\) and Adjusted \\(R^2\\)",
    "text": "Using \\(R^2\\) and Adjusted \\(R^2\\)\n\n\\(R^2_{adj}\\) can be used as a quick assessment to compare the fit of multiple models; however, it should not be the only assessment!\nUse \\(R^2\\) when describing the relationship between the response and predictor variables\n\nComplete Exercises 1-3."
  },
  {
    "objectID": "slides/21-comparison2.html#comparing-models-with-r2_adj",
    "href": "slides/21-comparison2.html#comparing-models-with-r2_adj",
    "title": "Model comparison + selection",
    "section": "Comparing models with \\(R^2_{adj}\\)",
    "text": "Comparing models with \\(R^2_{adj}\\)\n\n\ntip_fit_1:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.6743626\n0.6643738\n1.954983\n67.51136\n0\n5\n-350.0405\n714.0811\n735.9904\n622.9793\n163\n169\n\n\n\n\n\n\ntip_fit_2:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nadj.r.squared\nsigma\nstatistic\np.value\ndf\nlogLik\nAIC\nBIC\ndeviance\ndf.residual\nnobs\n\n\n\n\n0.6825157\n0.6624218\n1.96066\n33.96625\n0\n10\n-347.898\n719.7959\n757.3547\n607.3815\n158\n169\n\n\n\n\n\n\n\n\n\nWhich model would we choose based on \\(R^2\\)?\nWhich model would we choose based on Adjusted \\(R^2\\)?\nWhich statistic should we use to choose the final model - \\(R^2\\) or Adjusted \\(R^2\\)? Why?"
  },
  {
    "objectID": "slides/21-comparison2.html#aic-bic",
    "href": "slides/21-comparison2.html#aic-bic",
    "title": "Model comparison + selection",
    "section": "AIC & BIC",
    "text": "AIC & BIC\nEstimators of prediction error and relative quality of models:\n\nAkaike’s Information Criterion (AIC): \\[AIC = n\\log(SS_\\text{Error}) - n \\log(n) + 2(p+1)\\] \n\n\nSchwarz’s Bayesian Information Criterion (BIC): \\[BIC = n\\log(SS_\\text{Error}) - n\\log(n) + log(n)\\times(p+1)\\]"
  },
  {
    "objectID": "slides/21-comparison2.html#aic-bic-1",
    "href": "slides/21-comparison2.html#aic-bic-1",
    "title": "Model comparison + selection",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = \\color{blue}{n\\log(SS_\\text{Error})} - n \\log(n) + 2(p+1) \\\\\n& BIC = \\color{blue}{n\\log(SS_\\text{Error})} - n\\log(n) + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\n\nFirst Term: Decreases as p increases… why?"
  },
  {
    "objectID": "slides/21-comparison2.html#aic-bic-2",
    "href": "slides/21-comparison2.html#aic-bic-2",
    "title": "Model comparison + selection",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned}\n& AIC = n\\log(SS_\\text{Error}) - \\color{blue}{n \\log(n)} + 2(p+1) \\\\\n& BIC = n\\log(SS_\\text{Error}) - \\color{blue}{n\\log(n)} + \\log(n)\\times(p+1)\n\\end{aligned}\n\\]\n\nSecond Term: Fixed for a given sample size n"
  },
  {
    "objectID": "slides/21-comparison2.html#aic-bic-3",
    "href": "slides/21-comparison2.html#aic-bic-3",
    "title": "Model comparison + selection",
    "section": "AIC & BIC",
    "text": "AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{2(p+1)} \\\\\n& BIC = n\\log(SS_\\text{Error}) - n\\log(n) + \\color{blue}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nThird Term: Increases as p increases"
  },
  {
    "objectID": "slides/21-comparison2.html#using-aic-bic",
    "href": "slides/21-comparison2.html#using-aic-bic",
    "title": "Model comparison + selection",
    "section": "Using AIC & BIC",
    "text": "Using AIC & BIC\n\\[\n\\begin{aligned} & AIC = n\\log(SS_{Error}) - n \\log(n) + \\color{red}{2(p+1)} \\\\\n& BIC = n\\log(SS_{Error}) - n\\log(n) + \\color{red}{\\log(n)\\times(p+1)}\n\\end{aligned}\n\\]\n\nChoose model with the smaller value of AIC or BIC\nIf \\(n \\geq 8\\), the penalty for BIC is larger than that of AIC, so BIC tends to favor more parsimonious models (i.e. models with fewer terms)\n\nComplete Exercise 4."
  },
  {
    "objectID": "slides/21-comparison2.html#comparing-models-with-aic-and-bic",
    "href": "slides/21-comparison2.html#comparing-models-with-aic-and-bic",
    "title": "Model comparison + selection",
    "section": "Comparing models with AIC and BIC",
    "text": "Comparing models with AIC and BIC\n\n\ntip_fit_1\n\n\n\n\n\nAIC\nBIC\n\n\n\n\n714.0811\n735.9904\n\n\n\n\n\n\ntip_fit_2\n\n\n\n\n\nAIC\nBIC\n\n\n\n\n719.7959\n757.3547\n\n\n\n\n\n\n\n\n\nWhich model would we choose based on AIC?\nWhich model would we choose based on BIC?"
  },
  {
    "objectID": "slides/21-comparison2.html#commonalities-between-criteria",
    "href": "slides/21-comparison2.html#commonalities-between-criteria",
    "title": "Model comparison + selection",
    "section": "Commonalities between criteria",
    "text": "Commonalities between criteria\n\n\\(R^2_{adj}\\), AIC, and BIC all apply a penalty for more predictors\nThe penalty for added model complexity attempts to strike a balance between underfitting (too few predictors in the model) and overfitting (too many predictors in the model)\nGoal: Parsimony"
  },
  {
    "objectID": "slides/21-comparison2.html#parsimony-and-occams-razor",
    "href": "slides/21-comparison2.html#parsimony-and-occams-razor",
    "title": "Model comparison + selection",
    "section": "Parsimony and Occam’s razor",
    "text": "Parsimony and Occam’s razor\n\nThe principle of parsimony is attributed to William of Occam (early 14th-century English nominalist philosopher), who insisted that, given a set of equally good explanations for a given phenomenon, the correct explanation is the simplest explanation1\nCalled Occam’s razor because he “shaved” his explanations down to the bare minimum\nParsimony in modeling:\n\n\nmodels should have as few parameters as possible\nlinear models should be preferred to non-linear models\nexperiments relying on few assumptions should be preferred to those relying on many\nmodels should be pared down until they are minimal adequate (i.e. contain the minimum number of predictors required to meet some critereon)\nsimple explanations should be preferred to complex explanations\n\n\n\nSource: The R Book by Michael J. Crawley."
  },
  {
    "objectID": "slides/21-comparison2.html#in-pursuit-of-occams-razor",
    "href": "slides/21-comparison2.html#in-pursuit-of-occams-razor",
    "title": "Model comparison + selection",
    "section": "In pursuit of Occam’s razor",
    "text": "In pursuit of Occam’s razor\n\nOccam’s razor states that among competing hypotheses that predict equally well, the one with the fewest assumptions should be selected\nModel selection follows this principle\nWe only want to add another variable to the model if the addition of that variable brings something valuable in terms of predictive power to the model\nIn other words, we prefer the simplest best model, i.e. parsimonious model"
  },
  {
    "objectID": "slides/21-comparison2.html#alternate-views",
    "href": "slides/21-comparison2.html#alternate-views",
    "title": "Model comparison + selection",
    "section": "Alternate views",
    "text": "Alternate views\n\nSometimes a simple model will outperform a more complex model . . . Nevertheless, I believe that deliberately limiting the complexity of the model is not fruitful when the problem is evidently complex. Instead, if a simple model is found that outperforms some particular complex model, the appropriate response is to define a different complex model that captures whatever aspect of the problem led to the simple model performing well.\n\nRadford Neal - Bayesian Learning for Neural Networks1\n\nSuggested blog post: Occam by Andrew Gelman"
  },
  {
    "objectID": "slides/21-comparison2.html#other-concerns-with-our-approach",
    "href": "slides/21-comparison2.html#other-concerns-with-our-approach",
    "title": "Model comparison + selection",
    "section": "Other concerns with our approach",
    "text": "Other concerns with our approach\n\nAll criteria we considered for model comparison require making predictions for our data and then uses the prediction error (\\(SS_{Error}\\)) somewhere in the formula\nBut we’re making prediction for the data we used to build the model (estimate the coefficients), which can lead to overfitting"
  },
  {
    "objectID": "slides/21-comparison2.html#recap",
    "href": "slides/21-comparison2.html#recap",
    "title": "Model comparison + selection",
    "section": "Recap",
    "text": "Recap\n\nComparing models with\n\n\\(R^2\\) vs. \\(R^2_{Adj}\\)\nAIC and BIC\n\nOccam’s razor and parsimony\nComplete Exercise 5.\n\n\n\n\n\n🔗 MAT 212 - Fall 2024 - Schedule"
  },
  {
    "objectID": "slides/21-comparison2.html#footnotes",
    "href": "slides/21-comparison2.html#footnotes",
    "title": "Model comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nClick here for explanation about the way R calculates sum of squares for each variable.↩︎\nSource: The R Book by Michael J. Crawley.↩︎\nSuggested blog post: Occam by Andrew Gelman↩︎"
  },
  {
    "objectID": "prepare/mlr-comparison-prep.html",
    "href": "prepare/mlr-comparison-prep.html",
    "title": "October 23 lecture",
    "section": "",
    "text": "Stat2: Topic 3.2 Assessing a Multiple Regression Model"
  },
  {
    "objectID": "prepare/mlr-comparison-prep.html#assigned-reading",
    "href": "prepare/mlr-comparison-prep.html#assigned-reading",
    "title": "October 23 lecture",
    "section": "",
    "text": "Stat2: Topic 3.2 Assessing a Multiple Regression Model"
  },
  {
    "objectID": "slides/21-comparison2.html#r-squared-r2-overfitting",
    "href": "slides/21-comparison2.html#r-squared-r2-overfitting",
    "title": "Model comparison + selection",
    "section": "R-squared, \\(R^2\\), Overfitting",
    "text": "R-squared, \\(R^2\\), Overfitting\n\n\\(R^2\\) will always increase as we add more variables to the model\n\nIf we add enough variables, we can usually achieve \\(R^2=100\\%\\)\nEventually our model will over-align to the noise in our data and become worse at predicting new data… this is called overfitting\n\n\nIf we only use \\(R^2\\) to choose a best fit model, we will be prone to choosing the model with the most predictor variables"
  },
  {
    "objectID": "ae/ae-15-comparison2.html",
    "href": "ae/ae-15-comparison2.html",
    "title": "AE 15: Model Comparison",
    "section": "",
    "text": "Important\n\n\n\n\nOpen RStudio and create a subfolder in your AE folder called “AE-15”.\nGo to the Canvas and locate your AE-15 assignment to get started.\nUpload the ae-15.qmd and tip-data.csv files into the folder you just created. The .qmd and PDF responses are due in Canvas. You can check the due date on the Canvas assignment."
  },
  {
    "objectID": "ae/ae-15-comparison2.html#packages-data",
    "href": "ae/ae-15-comparison2.html#packages-data",
    "title": "AE 15: Model Comparison",
    "section": "Packages + data",
    "text": "Packages + data\n\nlibrary(tidyverse)\nlibrary(broom)\nlibrary(yardstick)\nlibrary(ggformula)\nlibrary(patchwork)\nlibrary(knitr)\nlibrary(kableExtra)\n\ntips &lt;- read_csv(\"data/tip-data.csv\") |&gt; \n  drop_na(Party)\n\nWhat factors are associated with the amount customers tip at a restaurant? To answer this question, we will use data collected in 2011 by a student at St. Olaf who worked at a local restaurant.1\nThe variables we’ll focus on for this analysis are\n\nTip: amount of the tip\nMeal: which meal this was (Lunch, Dinner, Late Night)\nParty: number of people in the party\nAge: Age category of person paying the bill (Yadult, Middle, SenCit)"
  },
  {
    "objectID": "ae/ae-15-comparison2.html#analysis-goal",
    "href": "ae/ae-15-comparison2.html#analysis-goal",
    "title": "AE 15: Model Comparison",
    "section": "Analysis goal",
    "text": "Analysis goal\nThe goals of this activity are to comparing models with: - \\(R^2\\) vs. \\(R^2_{Adj}\\) - AIC and BIC"
  },
  {
    "objectID": "ae/ae-15-comparison2.html#exercise-1",
    "href": "ae/ae-15-comparison2.html#exercise-1",
    "title": "AE 15: Model Comparison",
    "section": "Exercise 1",
    "text": "Exercise 1\nFit two models:\n\ntip_fit_1: predict Tip from Party, Age, and Meal\ntip_fit_2: predict Tip from Party, Age, Meal, and Day."
  },
  {
    "objectID": "ae/ae-15-comparison2.html#exercise-2",
    "href": "ae/ae-15-comparison2.html#exercise-2",
    "title": "AE 15: Model Comparison",
    "section": "Exercise 2",
    "text": "Exercise 2\nApply glance to both models above to find the \\(R^2\\) and \\(R^2_{adj}\\) values?"
  },
  {
    "objectID": "ae/ae-15-comparison2.html#exercise-3",
    "href": "ae/ae-15-comparison2.html#exercise-3",
    "title": "AE 15: Model Comparison",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nWhich model would we choose based on \\(R^2\\)?\nWhich model would we choose based on Adjusted \\(R^2\\)?\nWhich statistic should we use to choose the final model - \\(R^2\\) or Adjusted \\(R^2\\)? Why?"
  },
  {
    "objectID": "ae/ae-15-comparison2.html#to-submit-the-ae",
    "href": "ae/ae-15-comparison2.html#to-submit-the-ae",
    "title": "AE 15: Model Comparison",
    "section": "To submit the AE",
    "text": "To submit the AE\n\n\n\n\n\n\nImportant\n\n\n\n\nRender the document to produce the PDF with all of your work from today’s class.\nUpload your QMD and PDF files to the Canvas assignment."
  },
  {
    "objectID": "ae/ae-15-comparison2.html#footnotes",
    "href": "ae/ae-15-comparison2.html#footnotes",
    "title": "AE 15: Model Comparison",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDahlquist, Samantha, and Jin Dong. 2011. “The Effects of Credit Cards on Tipping.” Project for Statistics 212-Statistics for the Sciences, St. Olaf College.↩︎"
  },
  {
    "objectID": "ae/ae-15-comparison2.html#exercise-4",
    "href": "ae/ae-15-comparison2.html#exercise-4",
    "title": "AE 15: Model Comparison",
    "section": "Exercise 4",
    "text": "Exercise 4\nReference the output from Exercise 2.\n\nWhich model would we choose based on AIC?\nWhich model would we choose based on BIC?"
  },
  {
    "objectID": "ae/ae-15-comparison2.html#exercise-5-time-permitting",
    "href": "ae/ae-15-comparison2.html#exercise-5-time-permitting",
    "title": "AE 15: Model Comparison",
    "section": "Exercise 5 (Time Permitting)",
    "text": "Exercise 5 (Time Permitting)\nFind the best model you can! You have until the end of class! Team with the best model gets a seriously great high-five."
  }
]