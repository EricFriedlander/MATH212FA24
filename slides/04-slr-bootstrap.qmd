---
title: "SLR: Simulation-based inference"
subtitle: "Bootstrap confidence intervals for the slope"
author: "Prof. Eric Friedlander"
date: "2024-08-28"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— MAT 212 - Fall 2024 -  Schedule](https://mat212fa24.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
  cache: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r setup}
#| include: false

library(countdown)
library(tidyverse)
library(tidymodels)
library(knitr)
library(priceR)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)
ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))
```


<!-- # AE 03 Follow-up -->

<!-- ## AE 03 Follow-up {.midi} -->

<!-- **Goal:** Use simple linear regression to model the relationship between temperature and daily bike rentals in the **winter** season -->

<!-- ```{r} -->
<!-- #| include: false -->


<!-- winter <- read_csv("../ae/data/dcbikeshare.csv") |> -->
<!--   mutate(season = case_when( -->
<!--     season == 1 ~ "winter",  -->
<!--     season == 2 ~ "spring",  -->
<!--     season == 3 ~ "summer",  -->
<!--     season == 4 ~ "fall" -->
<!--   ),  -->
<!--   season = factor(season)) |> -->
<!--   filter(season == "winter") -->
<!-- ``` -->

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- #| eval: true -->
<!-- #| out-width: "65%" -->

<!-- ggplot(data = winter, aes(x = temp_orig, y = count)) +  -->
<!--   geom_point() +  -->
<!--   labs(x = "Temperature in Celsius",  -->
<!--        y = "Daily bike rentals",  -->
<!--        title = "Capital Bikeshare daily bike rentals vs. temperature",  -->
<!--        subtitle = "in winter months") -->
<!-- ``` -->

<!-- ## AE 03 Follow-up -->

<!-- **Statistical Model**: -->

<!-- $$count = \beta_0 +\beta_1 ~ temp\_orig + \epsilon, \hspace{5mm} \epsilon \sim N(0, \sigma_{\epsilon}^2)$$ -->

<!-- . . . -->

<!-- ```{r} -->
<!-- winter_fit <- linear_reg() |> -->
<!--   set_engine("lm") |> -->
<!--   fit(count ~ temp_orig, data = winter) -->

<!-- tidy(winter_fit) |> kable(digits = 3) -->
<!-- ``` -->

<!-- ## AE 03 Follow-up -->

<!-- Use the output to write out the estimated regression equation. -->

<!-- $$ -->
<!-- \hat{count} =  -111.038 + 222.416 ~temp\_orig -->
<!-- $$ -->

<!-- ::: aside -->
<!-- LaTex: -->

<!-- \\\$\\\$\\hat{count} = -111.038 + 222.416 \~ temp\\\_orig\\\$\\\$ -->
<!-- ::: -->

<!-- ::: callout-note -->
<!-- ## Your turn! -->

<!-- -   Interpret the slope in the context of the data. -->

<!-- -   Why is there no error term in the regression equation? -->
<!-- ::: -->

# Simulation-based inference

Bootstrap confidence intervals

## Topics

-   Find range of plausible values for the slope using bootstrap confidence intervals

## Computational setup

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)   # for data wrangling and visualization
library(ggformula)  # for modeling
library(openintro)   # for Duke Forest dataset
library(scales)      # for pretty axis labels
library(glue)        # for constructing character strings
library(knitr)       # for neatly formatted tables
library(kableExtra)  # also for neatly formatted tablesf


# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))
```

## [Data: San Antonio Income & Organic Food Access]{.r-fit-text}

::: columns
::: {.column width="60%"}
-   Average household income (per zip code) and number of organic vegetable offerings in San Antonio, TX
-   Data from HEB website, compiles by high school student Linda Saucedo, Fall 2019
-   Source: [**Skew The Script**](https://skewthescript.org/3-1)
:::

::: {.column width="40%"}
![](images/HEB-Logo.jpg)
:::
:::

**Goal**: Use the average household income to understand variation in access to organic foods.

## Exploratory data analysis


```{r}
#| code-fold: true

heb <- read_csv("data/HEBIncome.csv") |> 
  mutate(Avg_Income_K = Avg_Household_Income/1000)

gf_point(Number_Organic ~ Avg_Income_K, data = heb, alpha = 0.7) |> 
  gf_labs(
    x = "Average Household Income (in thousands)",
    y = "Number of Organic Vegetables",
  ) |> 
  gf_refine(scale_x_continuous(labels = label_dollar()))
```

## Modeling {.midi}

```{r}
#| echo: true

heb_fit <- lm(Number_Organic ~ Avg_Income_K, data = heb)

tidy(heb_fit) |>
  kable(digits=2) #neatly format table to 2 digits
```

. . .

```{r}
#| echo: false
intercept <- tidy(heb_fit) |> filter(term == "(Intercept)") |> pull(estimate) |> round(digits=2)
slope <- tidy(heb_fit) |> filter(term == "Avg_Income_K") |> pull(estimate) |> round(digits = 2)
```

-   **Intercept:** HEBs in Zip Codes with an average household income of \$0 are expected to have `r intercept` organic vegetable options, on average.
    -   Is this interpretation useful?
-   **Slope:** For each additional \$1,000 in average household income, we expect the number of organic options available at nearby HEBs to increase by `r slope`, on average.

## From sample to population {.midi}

> For each additional $1,000 in average household income, we expect the number of organic options available at nearby HEBs to increase by `r slope`, on average.

<br>

-   Estimate is valid for the single sample of `r nrow(heb)` HEBs
-   What if we're not interested quantifying the relationship between the size and price of a house in this single sample?
-   What if we want to say something about the relationship between these variables for all supermarkets in America?

## Statistical inference

-   **Statistical inference** refers to ideas, methods, and tools for to generalizing the single observed sample to make statements (inferences) about the population it comes from

-   For our inferences to be valid, the sample should be random and representative of the population we're interested in

## Inference for simple linear regression

::: incremental

-   Calculate a confidence interval for the slope, $\beta_1$

-   Conduct a hypothesis test for the slope,$\beta_1$

-   Why not $\beta_0$?

-   We can but it isn't super interesting typically

:::

# Confidence interval for the slope

## Confidence interval {.midi}

::: incremental
-   **Confidence interval**: plausible range of values for a population parameter
-   single point estimate $\implies$ fishing in a murky lake with a spear, confidence interval $\implies$ fishing with a net
    -   We can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish
    -  If we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter
    -   High confidence $\implies$ wider interval (larger net)
-   Remember: single CI $\implies$ either you hit parameter or you don't
    -   [Favorite Visualization](https://rpsychologist.com/d3/ci/)
:::

## Confidence interval for the slope {.midi}

A confidence interval will allow us to make a statement like "*For each \$1K in average income, the model predicts the number of organic vegetables available at local supermarkets to be higher, on average, by `r slope`, plus or minus X options.*"

. . .

-   Should X be 1? 2? 3?

-   If we were to take another sample of `r nrow(heb)` would we expect the slope calculated based on that sample to be exactly `r slope`? Off by 1? 2? 3?

-   The answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is

-   We need a way to quantify the variability of the sample statistic

## Quantify the variability of the slope {.midi}

**for estimation**

::: incremental
-   Two approaches:
    1.  Via simulation (what we'll do today)
    2.  Via mathematical models (what we'll do in the soon)
-   **Bootstrapping** to quantify the variability of the slope for the purpose of estimation:
    -   Generate new samples by sampling with replacement from the original sample
    -   Fit models to each of the new samples and estimate the slope
    -   Use features of the distribution of the bootstrapped slopes to construct a confidence interval
:::

```{r}
#| echo: false
set.seed(119)

heb_boot_samples_5 <- heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  generate(reps = 5, type = "bootstrap")
```

## Original Sample

<!-- Fix the scales for these slides, so the difference in slopes is easier to see-->
```{r}
#| echo: false
#| out.width: "100%"

p_heb_obs <- ggplot(heb, aes(x = Avg_Income_K, y = Number_Organic)) +
  geom_point() +
  labs(
    x = "Average Household Income (in thousands)",
    y = "Number of Organic Vegetables",
  ) +
  scale_x_continuous(labels = label_dollar(), limit = c(30, 130)) +
  scale_y_continuous(labels = label_number(), limits = c(0, 120)) +
  geom_smooth(method = "lm", se = FALSE, fullrange=TRUE)
p_heb_obs
```

## Bootstrap sample 1

```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 1

p_heb_obs +
  geom_point(data = heb_boot_samples_5 |> filter(replicate == replicate_no), mapping = aes(x = Avg_Income_K, y = Number_Organic), shape="o", alpha = 0.3, color = "red", size = 5) +
  geom_line(data = heb_boot_samples_5 |> filter(replicate == replicate_no), stat = "smooth", method = "lm", se = FALSE, alpha = 0.8, color = "red",
            fullrange=TRUE)
```


## Bootstrap sample 2

```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 2

p_heb_obs +
  geom_point(data = heb_boot_samples_5 |> filter(replicate == replicate_no), mapping = aes(x = Avg_Income_K, y = Number_Organic), shape="o", alpha = 0.3, color = "red", size = 5) +
  geom_line(data = heb_boot_samples_5 |> filter(replicate == replicate_no), stat = "smooth", method = "lm", se = FALSE, alpha = 0.8, color = "red",
            fullrange=TRUE)
```

## Bootstrap sample 3

```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 3

p_heb_obs +
  geom_point(data = heb_boot_samples_5 |> filter(replicate == replicate_no), mapping = aes(x = Avg_Income_K, y = Number_Organic), shape="o", alpha = 0.3, color = "red", size = 5) +
  geom_line(data = heb_boot_samples_5 |> filter(replicate == replicate_no), stat = "smooth", method = "lm", se = FALSE, alpha = 0.8, color = "red",
            fullrange=TRUE)
```


## Bootstrap sample 4

```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 4

p_heb_obs +
  geom_point(data = heb_boot_samples_5 |> filter(replicate == replicate_no), mapping = aes(x = Avg_Income_K, y = Number_Organic), shape="o", alpha = 0.3, color = "red", size = 5) +
  geom_line(data = heb_boot_samples_5 |> filter(replicate == replicate_no), stat = "smooth", method = "lm", se = FALSE, alpha = 0.8, color = "red",
            fullrange=TRUE)
```


## Bootstrap sample 5

```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 5

p_heb_obs +
  geom_point(data = heb_boot_samples_5 |> filter(replicate == replicate_no), mapping = aes(x = Avg_Income_K, y = Number_Organic), shape="o", alpha = 0.3, color = "red", size = 5) +
  geom_line(data = heb_boot_samples_5 |> filter(replicate == replicate_no), stat = "smooth", method = "lm", se = FALSE, alpha = 0.8, color = "red",
            fullrange=TRUE)
```

## Bootstrap samples 1 - 5

```{r}
#| echo: false
#| out.width: "100%"

p_heb_obs +
  geom_line(data = heb_boot_samples_5, 
            mapping = aes(x = Avg_Income_K, y = Number_Organic, group = replicate), 
            stat = "smooth", method = "lm", se = FALSE, alpha = 0.8, color = "red",
            fullrange=TRUE)
```

## Bootstrap samples 1 - 100

```{r}
#| echo: false
#| out.width: "100%"
set.seed(119)

heb_boot_samples_100 <- heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  generate(reps = 100, type = "bootstrap")

p_heb_boot_samples_100 <- p_heb_obs +
  geom_line(data = heb_boot_samples_100, 
            mapping = aes(x = Avg_Income_K, y = Number_Organic, group = replicate), 
            stat = "smooth", method = "lm", se = FALSE, alpha = 0.1, color = "red",
            fullrange=TRUE)

p_heb_boot_samples_100
```

## Slopes of bootstrap samples

::: question
**Fill in the blank:** For each additional \$1k in average household income, the model predicts the number of organic vegetables available to be higher, on average, by `r slope`, plus or minus \_\_\_.
:::

```{r}
#| echo: false
p_heb_boot_samples_100
```

## Slopes of bootstrap samples

::: question
**Fill in the blank:** For each additional \$1k in average household income, the model predicts the number of organic vegetables available to be higher, on average, by `r slope`, plus or minus \_\_\_.
:::

```{r}
#| echo: false
heb_boot_samples_100_fit <- heb_boot_samples_100 |>
  fit()

heb_boot_samples_100_hist <- ggplot(heb_boot_samples_100_fit |> filter(term == "Avg_Income_K"), 
                                    aes(x = estimate)) +
  geom_histogram(bins=15, color = "white") +
  geom_vline(xintercept = slope, color = "#8F2D56", size = 1) +
  labs(x = "Slope", y = "Count",
       title = "Slopes of 100 bootstrap samples") +
  scale_x_continuous(labels = label_number())

heb_boot_samples_100_hist
```

## Confidence level

::: question
How confident are you that the true slope is between 0.8 and 1.2? How about 0.9 and 1.0? How about 1.0 and 1.4?
:::

```{r}
#| echo: false
heb_boot_samples_100_hist
```

## 95% confidence interval {.smaller}

```{r}
#| echo: false
#| out.width: "70%"
#| fig-align: "center"
lower <- heb_boot_samples_100_fit |>
  ungroup() |>
  filter(term == "Avg_Income_K") |>
  summarise(quantile(estimate, 0.025)) |>
  pull()

upper <- heb_boot_samples_100_fit |>
  ungroup() |>
  filter(term == "Avg_Income_K") |>
  summarise(quantile(estimate, 0.975)) |>
  pull()

heb_boot_samples_100_hist +
  geom_vline(xintercept = lower, color = "steelblue", size = 1, linetype = "dashed") +
  geom_vline(xintercept = upper, color = "steelblue", size = 1, linetype = "dashed")
```

::: incremental
-   **95% bootstrapped confidence interval**: bounded by the middle 95% of the bootstrap distribution
-   We are 95% confident that for each additional \$1K in average household income, the model predicts the number of organic vegetables options at local supermarkets to be higher, on average, by `r round(lower, 2)` to `r round(upper, 2)`.
:::

<!-- # Application exercise -->

<!-- ::: appex -->
<!-- ðŸ“‹ [AE 04: Bootstrap confidence intervals](https://sta210-fa23.netlify.app/ae/ae-04-bootstrap.html) -->
<!-- ::: -->

## Computing the CI for the slope I

Calculate the observed slope:

```{r}
#| echo: true

library(infer) # package that does Simulation-Based Inference

observed_fit <- heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  fit()

observed_fit
```

## Computing the CI for the slope II {.smaller}

Take `100` bootstrap samples and fit models to each one:

```{r}
#| echo: true
#| code-line-numbers: "1,5,6"

set.seed(1120)

boot_fits <- heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  generate(reps = 100, type = "bootstrap") |>
  fit()

boot_fits
```

## Computing the CI for the slope III

**Percentile method:** Compute the 95% CI as the middle 95% of the bootstrap distribution:

```{r}
#| echo: true
#| code-line-numbers: "5"

get_confidence_interval(
  boot_fits, 
  point_estimate = observed_fit, 
  level = 0.95,
  type = "percentile" #default method
)
```

## Precision vs. accuracy

::: question
If we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?
:::

. . .

![](images/05/garfield.png)

## Precision vs. accuracy

::: question
How can we get best of both worlds -- high precision and high accuracy?
:::

## Changing confidence level

::: question
How would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?
:::

```{r}
#| echo: true
#| code-line-numbers: "|4"

get_confidence_interval(
  boot_fits, 
  point_estimate = observed_fit, 
  level = 0.95,
  type = "percentile"
)
```

## Changing confidence level {.midi}

```{r}
#| echo: true

## confidence level: 90%
get_confidence_interval(
  boot_fits, point_estimate = observed_fit, 
  level = 0.90, type = "percentile"
)

## confidence level: 99%
get_confidence_interval(
  boot_fits, point_estimate = observed_fit, 
  level = 0.99, type = "percentile"
)
```

## Recap {.smaller}

-   **Population:** Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = $N$)

-   **Sample:** Subset of the population, ideally random and representative (sample size = $n$)

-   Sample statistic $\ne$ population parameter, but if the sample is good, it can be a good estimate

-   **Statistical inference:** Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process

-   We report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population

-   Since we can't continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability
