---
title: "MLR: Inference and conditions"
author: "Prof. Eric Friedlander"
date: "2024-10-04"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— MAT 212 - Fall 2024 -  Schedule](https://mat212fa24.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
  cache: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 10, fig.asp = 0.618,
  fig.retina = 3, dpi = 300, fig.align = "center"
)
```

## Announcements

-   Midterm next Friday 10/11 (right before spring break)
-   Project proposal also due 10/11 but will accept until 10/14 without penalty 

## Topics

::: nonincremental
-   Inference for multiple linear regression
-   Checking model conditions
:::

## Computational setup

```{r}
#| echo: true


# load packages
library(tidyverse)
library(broom)
library(mosaic)
library(ISLR2)
library(patchwork)
library(knitr)
library(kableExtra)
library(scales)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_minimal(base_size = 16))
```

# Inference for multiple linear regression


## Data: Credit Cards

The data is from the `Credit` data set in the **ISLR2** R package. It is a simulated data set of 400 credit card customers.

```{r}
#| echo: false
Credit |>  glimpse()
```


## Variables

**Features (another name for predictors)**

-   `Income`: Annual income (in 1000's of US dollars)
-   `Rating`: Credit Rating

**Outcome**

-   `Limit`: Credit limit

# Conduct a hypothesis test for $\beta_j$

## [Review: Simple linear regression (SLR)]{.r-fit-text}

```{r}
gf_point(Limit ~ Rating, data = Credit, alpha = 0.5) |> 
  gf_lm()  |> 
  gf_labs(x = "Credit Rating", y = "Income") |> 
  gf_refine(scale_y_continuous(labels = dollar_format()),
            scale_x_continuous(labels = dollar_format()))
```

## SLR model summary

```{r}
income_slr_fit <- lm(Limit ~ Income, data = Credit)

tidy(income_slr_fit) |> kable()
```


## SLR hypothesis test {.midi}

```{r}
#| echo: false

tidy(income_slr_fit) |> kable(digits = 2)
```

1.  **Set hypotheses:** $H_0: \beta_1 = 0$ vs. $H_A: \beta_1 \ne 0$

. . .

2.  **Calculate test statistic and p-value:** The test statistic is $t= 25.89$ . The p-value is calculated using a $t$ distribution with 399 degrees of freedom. The p-value is $\approx 0$ .

. . .

3.  **State the conclusion:** The p-value is small, so we reject $H_0$. The data provide strong evidence that income is a helpful predictor for a credit card holder's credit limit, i.e. there is a linear relationship between income and credit limit.

## Multiple linear regression

```{r}
credit_fit <- lm(Limit ~ Rating + Income, data = Credit)

tidy(credit_fit) |> kable(digits = 2)
```

## Multiple linear regression

The multiple linear regression model assumes $$Y|X_1, X_2,  \ldots, X_p \sim N(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_p X_p, \sigma_\epsilon^2)$$

. . .

For a given observation $(x_{i1}, x_{i2}, \ldots, x_{ip}, y_i)$, we can rewrite the previous statement as

$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \epsilon_{i}, \hspace{10mm} \epsilon_i \sim N(0,\sigma_{\epsilon}^2)$$

------------------------------------------------------------------------

## Estimating $\sigma_\epsilon$

For a given observation $(x_{i1}, x_{i2}, \ldots,x_{ip}, y_i)$ the residual is
$$
\begin{aligned}
e_i &= y_{i} - \hat{y_i}\\
&= y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_{i1} + \hat{\beta}_{2} x_{i2} + \dots + \hat{\beta}_p x_{ip})
\end{aligned}
$$

. . .

The estimated value of the regression standard error , $\sigma_{\epsilon}$, is

$$\hat{\sigma}_\epsilon  = \sqrt{\frac{\sum_{i=1}^ne_i^2}{n-p-1}}$$

. . .

As with SLR, we use $\hat{\sigma}_{\epsilon}$ to calculate $SE_{\hat{\beta}_j}$, the standard error of each coefficient. See [Matrix Form of Linear Regression](https://github.com/STA210-Sp19/supplemental-notes/blob/master/regression-basics-matrix.pdf) for more detail.

## MLR hypothesis test: Income {.midi}

1.  **Set hypotheses:** $H_0: \beta_{Income} = 0$ vs. $H_A: \beta_{Income} \ne 0$, given `Rating` is in the model

. . .

2.  **Calculate test statistic and p-value:** The test statistic is $t = 1.32$. The p-value is calculated using a $t$ distribution with $$(n - p - 1) = 400 - 2 -1 = 398$$ degrees of freedom. The p-value is $\approx 0.19$.

. . .

3.  **State the conclusion:** The p-value is not small, so we fail to reject $H_0$. The data does not provide convincing evidence that a borrowers income is a useful predictor in a model that already contains credit rating as a predictor for the credit limit of a borrower.

::: appex
Complete Exercises 1-2.
:::


# Confidence interval for $\beta_j$

## Confidence interval for $\beta_j$ {.midi}

-   The $C\%$ confidence interval for $\beta_j$ $$\hat{\beta}_j \pm t^* SE(\hat{\beta}_j)$$ where $t^*$ follows a $t$ distribution with $n - p - 1$ degrees of freedom.

-   **Generically**: We are $C\%$ confident that the interval LB to UB contains the population coefficient of $x_j$.

-   **In context:** We are $C\%$ confident that for every one unit increase in $x_j$, we expect $y$ to change by LB to UB units, holding all else constant.

::: appex
Complete Exercise 3.
:::

## Confidence interval for $\beta_j$

```{r}
#| echo: FALSE
tidy(credit_fit, conf.int = TRUE) |>
  kable(digits = 2)
```


# Inference pitfalls

## Large sample sizes

::: callout-caution
If the sample size is large enough, the test will likely result in rejecting $H_0: \beta_j = 0$ even $x_j$ has a very small effect on $y$.

::: nonincremental
-   Consider the **practical significance** of the result not just the statistical significance.

-   Use the confidence interval to draw conclusions instead of relying only p-values.
:::
:::

## Small sample sizes

::: callout-caution
If the sample size is small, there may not be enough evidence to reject $H_0: \beta_j=0$.

::: nonincremental
-   When you fail to reject the null hypothesis, **DON'T** immediately conclude that the variable has no association with the response.

-   There may be a linear association that is just not strong enough to detect given your data, or there may be a non-linear association.
:::
:::

::: appex
Complete exercise 4
:::


<!-- # Conditions for inference -->

<!-- ## Full model {.smaller} -->

<!-- Including all available predictors -->

<!-- **Fit:** -->

<!-- ```{r} -->
<!-- rt_full_fit <- lm(volume ~ ., data = Credit) -->
<!-- ``` -->

<!-- . . . -->

<!-- **Summarize:** -->

<!-- ```{r} -->
<!-- tidy(rt_full_fit) -->
<!-- ``` -->

<!-- . . . -->

<!-- **Augment:** -->

<!-- ```{r} -->
<!-- rt_full_aug <- augment(rt_full_fit) -->
<!-- ``` -->

<!-- ## Model conditions -->

<!-- 1.  **Linearity:** There is a linear relationship between the response and predictor variables. -->

<!-- 2.  **Constant Variance:** The variability about the least squares line is generally constant. -->

<!-- 3.  **Normality:** The distribution of the residuals is approximately normal. -->

<!-- 4.  **Independence:** The residuals are independent from each other. -->


<!-- ## Checking Linearity -->

<!-- -   Look at a plot of the residuals vs. predicted values -->

<!-- -   Look at a plot of the residuals vs. each predictor -->

<!-- -   Linearity is met if there is no discernible pattern in each of these plots -->

<!-- ## Residuals vs. predicted values -->

<!-- ```{r} -->
<!-- #| label: main_res_pred -->

<!-- gf_point(.resid ~ .fitted, data = rt_full_aug, alpha = 0.7) |>  -->
<!--   gf_hline(yintercept = 0, color = "red", linetype = "dashed") |>  -->
<!--   gf_labs(x = "Predicted values", y = "Residuals") -->
<!-- ``` -->

<!-- ## Residuals vs. each predictor -->

<!-- ```{r} -->
<!-- #| fig.asp: 0.5 -->
<!-- #| echo: false -->

<!-- p1 <- gf_point(.resid ~ hightemp, data = rt_full_aug, alpha = 0.7) |>   -->
<!--   gf_hline(yintercept = 0, color = "red", linetype = "dashed") -->

<!-- p2 <- gf_point(.resid ~ avgtemp, data = rt_full_aug, alpha = 0.7) |>   -->
<!--   gf_hline(yintercept = 0, color = "red", linetype = "dashed") -->

<!-- p3 <- gf_boxplot(.resid ~ season, data = rt_full_aug, alpha = 0.7) |>   -->
<!--   gf_hline(yintercept = 0, color = "red", linetype = "dashed") -->

<!-- p4 <- gf_point(.resid ~ cloudcover, data = rt_full_aug, alpha = 0.7) |>   -->
<!--   gf_hline(yintercept = 0, color = "red", linetype = "dashed") -->

<!-- p5 <- gf_point(.resid ~ precip, data = rt_full_aug, alpha = 0.7) |>   -->
<!--   gf_hline(yintercept = 0, color = "red", linetype = "dashed") -->

<!-- p6 <- gf_boxplot(.resid ~ day_type, data = rt_full_aug, alpha = 0.7) |>   -->
<!--   gf_hline(yintercept = 0, color = "red", linetype = "dashed") -->

<!-- (p1 + p2 + p3) / (p4 + p5 + p6) -->
<!-- ``` -->

<!-- ## Checking linearity -->

<!-- -   The plot of the residuals vs. predicted values looked OK -->

<!-- -   The plots of residuals vs. `hightemp` and `avgtemp` appear to have a parabolic pattern. -->

<!-- -   The linearity condition does not appear to be satisfied given these plots. -->

<!-- . . . -->

<!-- ::: question -->
<!-- Given this conclusion, what might be a next step in the analysis? -->
<!-- ::: -->

<!-- ## Checking constant variance -->

<!-- ::: question -->
<!-- Does the constant variance condition appear to be satisfied? -->
<!-- ::: -->

<!-- ```{r} -->
<!-- #| ref.label: main_res_pred -->
<!-- #| echo: false -->
<!-- ``` -->

<!-- ## Checking constant variance -->

<!-- -   The vertical spread of the residuals is not constant across the plot. -->

<!-- -   The constant variance condition is not satisfied. -->

<!-- . . . -->

<!-- ::: question -->
<!-- We will talk about to address this later in the notes. -->
<!-- ::: -->

<!-- ## Checking normality -->

<!-- ```{r} -->
<!-- #| fig.asp: 0.8 -->
<!-- #| echo: false -->

<!-- resid_hist <- ggplot(rt_full_aug, aes(x = .resid)) + -->
<!--   geom_histogram(binwidth = 25) + -->
<!--   labs(x = "Residuals")   -->

<!-- resid_hist -->
<!-- ``` -->

<!-- The distribution of the residuals is approximately unimodal and symmetric, so the normality condition is satisfied. The sample size 90 is sufficiently large to relax this condition if it was not satisfied. -->

<!-- ## Checking independence -->

<!-- -   We can often check the independence condition based on the context of the data and how the observations were collected. -->

<!-- -   If the data were collected in a particular order, examine a scatterplot of the residuals versus order in which the data were collected. -->

<!-- -   If there is a grouping variable lurking in the background, check the residuals based on that grouping variable. -->

<!-- ## Checking independence {.midi} -->

<!-- Residuals vs. order of data collection: -->

<!-- ```{r} -->
<!-- gf_line(.resid ~ 1:nrow(rt_full_aug), data = rt_full_aug) |>  -->
<!--   gf_point()  |>  -->
<!--   gf_hline(yintercept = 0, color = "red", linetype = "dashed")  |>  -->
<!--   gf_labs(x = "Order of data collection", y = "Residuals") -->
<!-- ``` -->

<!-- ## Checking independence -->

<!-- -   No clear pattern in the residuals vs. order of data collection plot. -->

<!-- -   Independence condition appears to be satisfied, as far as we can evaluate it. -->

<!-- # Multicollinearity -->

<!-- ## What is multicollinearity -->

<!-- **Multicollinearity** is the case when two or more predictor variables are strongly correlated with one another -->

<!-- ## Example -->

<!-- Let's assume the true population regression equation is $y = 3 + 4x$ -->

<!-- . . . -->

<!-- Suppose we try estimating that equation using a model with variables $x$ and $z = x/10$ -->

<!-- $$ -->
<!-- \begin{aligned}\hat{y}&= \hat{\beta}_0 + \hat{\beta}_1x  + \hat{\beta}_2z\\ -->
<!-- &= \hat{\beta}_0 + \hat{\beta}_1x  + \hat{\beta}_2\frac{x}{10}\\ -->
<!-- &= \hat{\beta}_0 + \bigg(\hat{\beta}_1 + \frac{\hat{\beta}_2}{10}\bigg)x -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- ## Example -->

<!-- $$\hat{y} = \hat{\beta}_0 + \bigg(\hat{\beta}_1 + \frac{\hat{\beta}_2}{10}\bigg)x$$ -->

<!-- -   We can set $\hat{\beta}_1$ and $\hat{\beta}_2$ to any two numbers such that $\hat{\beta}_1 + \frac{\hat{\beta}_2}{10} = 4$ -->

<!-- -   Therefore, we are unable to choose the "best" combination of $\hat{\beta}_1$ and $\hat{\beta}_2$ -->

<!-- ## Why multicollinearity is a problem -->

<!-- -   When we have perfect collinearities, we are unable to get estimates for the coefficients -->

<!--     -   When we have almost perfect collinearities (i.e. highly correlated predictor variables), the standard errors for our regression coefficients inflate -->

<!--     -   In other words, we lose precision in our estimates of the regression coefficients -->

<!--     -   This impedes our ability to use the model for inference -->

<!--     -   It is also difficult to interpret the model coefficients -->

<!-- ## Detecting Multicollinearity {.midi} -->

<!-- Multicollinearity may occur when... -->

<!-- -   There are very high correlations $(r > 0.9)$ among two or more predictor variables, especially when the sample size is small -->

<!-- <!-- --> -->

<!-- -   One (or more) predictor variables is an almost perfect linear combination of the others -->

<!-- -   There is a quadratic term in the model without mean-centering the variable first -->

<!-- -   There are interactions between two or more continuous variables -->

<!--     -   Can reduce this by mean-centering the variables first -->

<!-- -   There is a categorical predictor with very few observations in the baseline level -->

<!-- ## Detecting multicollinearity in the EDA -->

<!-- -   Look at a correlation matrix of the predictor variables, including all indicator variables -->
<!--     -   Look out for values close to 1 or -1 -->
<!-- -   Look at a scatterplot matrix of the predictor variables -->
<!--     -   Look out for plots that show a relatively linear relationship -->
<!-- -   Look at the distribution of categorical predictors and avoid setting the baseline to a category with very few observations -->


<!-- ## Detecting Multicollinearity (VIF) -->

<!-- **Variance Inflation Factor (VIF)**: Measure of multicollinearity in the regression model -->

<!-- $$VIF(\hat{\beta}_j) = \frac{1}{1-R^2_{X_j|X_{-j}}}$$ -->

<!-- where $R^2_{X_j|X_{-j}}$ is the proportion of variation $X$ that is explained by the linear combination of the other explanatory variables in the model. -->

<!-- ## Detecting Multicollinearity (VIF) -->

<!-- -   Typically $VIF > 5$ indicates concerning multicollinearity -->

<!-- -   Variables with similar values of VIF are typically the ones correlated with each other -->

<!-- -   Use the `vif()` function in the **rms** R package to calculate VIF -->

<!-- ## VIF for rail trail model -->

<!-- ```{r} -->
<!-- #| echo: true -->

<!-- vif(rt_full_fit) -->
<!-- ``` -->

<!-- <br> -->

<!-- . . . -->

<!-- `hightemp`, `avgtemp`, and `seasonSummer` are correlated.  -->

<!-- ## Solutions for Multicollinearity -->

<!-- 1. Drop some predictors -->
<!-- 2. Combine some predictors -->
<!-- 3. Discount the individual coefficients and t-tests (i.e. predictions are meaningful but coefficients, tests, and confidence intervals are not) -->

<!-- . . . -->

<!-- Let's try removing `hightemp` and `avgtemp` (separately) -->

<!-- ## Model without `hightemp` {.smaller} -->

<!-- ```{r} -->
<!-- m1 <- lm(volume ~ . - hightemp, data = Credit) -->

<!-- m1 |> -->
<!--   tidy() |> -->
<!--   kable(digits = 3) -->
<!-- ``` -->

<!-- ## Model without `avgtemp` {.smaller} -->

<!-- ```{r} -->
<!-- m2 <- lm(volume ~ . - avgtemp, data = Credit) -->

<!-- m2 |> -->
<!--   tidy() |> -->
<!--   kable(digits = 3) -->
<!-- ``` -->

<!-- ## Choosing a model {.midi} -->

<!-- Model without **hightemp**: -->

<!-- ```{r} -->
<!-- #| echo: false -->

<!-- glance(m1) |> -->
<!--   select(adj.r.squared, AIC, BIC) |> kable(digits = 2) -->
<!-- ``` -->

<!-- Model without **avgtemp**: -->

<!-- ```{r echo = F} -->
<!-- #| echo: false -->

<!-- glance(m2) |> -->
<!--   select(adj.r.squared, AIC, BIC) |> kable(digits = 2) -->
<!-- ``` -->

<!-- . . . -->

<!-- Based on Adjusted $R^2$, AIC, and BIC, the model without **avgtemp** is a better fit. Therefore, we choose to remove **avgtemp** from the model and leave **hightemp** in the model to deal with the multicollinearity. -->

<!-- ## Selected model (for now) -->

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- tidy(m2) |> -->
<!--   kable(digits = 3) -->
<!-- ``` -->

<!-- # Variable transformations -->

<!-- ## Topics -->

<!-- -   Log transformation on the response variable -->

<!-- -   Log transformation on the predictor variable -->

<!-- ## Residuals vs. fitted for the selected model {.midi} -->

<!-- ```{r} -->
<!-- #| echo: false -->

<!-- m2_aug <- augment(m2) -->

<!-- resid_orig <- gf_point(.resid ~ .fitted, data = m2_aug, alpha = 0.7) |>  -->
<!--   gf_hline(yintercept = 0, color = "red", linetype = "dashed") |>  -->
<!--   gf_labs(x = "Predicted values", y = "Residuals",  -->
<!--         title="Residuals vs. Predicted",  -->
<!--        subtitle = "Selected model (without avgtemp)") -->

<!-- resid_orig -->

<!-- ``` -->

<!-- The constant variance condition is not satisfied. We can transform the response variable to address the violation in this condition. -->

<!-- # Log transformation on the response variable -->

<!-- ## Identifying a need to transform $Y$ {.midi} -->

<!-- -   Typically, a "fan-shaped" residual plot indicates the need for a transformation of the response variable $Y$ -->
<!--     -   There are multiple ways to transform a variable, e.g., $\sqrt{Y}$, $1/Y$, $\log(Y)$ -->
<!--     -   $\log(Y)$ the most straightforward to interpret, so we use that transformation when possible -->

<!-- . . . -->

<!-- -   When building a model: -->
<!--     -   Choose a transformation and build the model on the transformed data -->
<!--     -   Reassess the residual plots -->
<!--     -   If the residuals plots did not sufficiently improve, try a new transformation! -->

<!-- ## Log transformation on $Y$ -->

<!-- -   If we apply a log transformation to the response variable, we want to estimate the parameters for the statistical model -->

<!-- $$ -->
<!-- \log(Y) = \beta_0+ \beta_1 X_1 + \dots +\beta_pX_p + \epsilon, \hspace{10mm} \epsilon \sim N(0,\sigma^2_\epsilon) -->
<!-- $$ -->

<!-- -   The regression equation is -->

<!-- $$\widehat{\log(Y)} = \hat{\beta}_0+ \hat{\beta}_1 X + \dots + \hat{\beta}_pX_p$$ -->

<!-- ## Log transformation on $Y$ -->

<!-- We want to interpret the model in terms of the original variable $Y$, not $\log(Y)$, so we need to write the regression equation in terms of $Y$ -->

<!-- $$\begin{align}\hat{Y} &= \exp\{\hat{\beta}_0 + \hat{\beta}_1 X + \dots + \hat{\beta}_PX_P\}\\ &= \exp\{\hat{\beta}_0\}\exp\{\hat{\beta}_1X\}\dots\exp\{\hat{\beta}_pX_p\}\end{align}$$ -->

<!-- ::: callout-note -->
<!-- The predicted value $\hat{Y}$ is the predicted *median* of $Y$. Note, when the distribution of $Y|X_1, \ldots, X_p$ is symmetric, then the median equals the mean. See the slides in the [appendix] for more detail. -->
<!-- ::: -->

<!-- ## Model interpretation {.midi} -->

<!-- $$\begin{align}\hat{Y} &= \exp\{\hat{\beta}_0 + \hat{\beta}_1 X + \dots + \hat{\beta}_PX_P\}\\ &= \exp\{\hat{\beta}_0\}\exp\{\hat{\beta}_1X\}\dots\exp\{\hat{\beta}_pX_p\}\end{align}$$ -->

<!-- . . . -->

<!-- -   **Intercept**: When $X_1 = \dots = X_p =0$, $Y$ is expected to be $\exp\{\hat{\beta}_0\}$ -->

<!-- -   **Slope:** For every one unit increase in $X_j$, the $Y$ is expected to multiply by a factor of $\exp\{\hat{\beta}_j\}$, holding all else constant -->

<!-- ::: question -->
<!-- Why is the interpretation in terms of a multiplicative change? -->
<!-- ::: -->

<!-- ## Model for $log(volume)$ {.midi} -->

<!-- ```{r} -->
<!-- #| echo: true -->
<!-- #fit model -->
<!-- log_rt_fit <- lm(log(volume) ~ hightemp + season + cloudcover + precip + day_type, data = Credit) -->

<!-- tidy(log_rt_fit) |> -->
<!--   kable(digits = 3) -->
<!-- ``` -->

<!-- ## Interpretation of model for $\log(volume)$ {.midi} -->

<!-- ```{r} -->
<!-- #| echo: false -->

<!-- tidy(log_rt_fit) |> -->
<!--   kable(digits = 3) -->

<!-- ``` -->

<!-- ::: question -->
<!-- -   Interpret the intercept in terms of (1) `log(volume)` and (2) `volume`. -->

<!-- -   Interpret the coefficient of `hightemp` in terms of (1) `log(volume)` and (2) `volume`. -->
<!-- ::: -->

<!-- ## Residuals for model with $\log(volume)$ -->

<!-- ```{r} -->
<!-- #| echo: false -->
<!-- resp_logy_aug <- augment(log_rt_fit) -->

<!-- resid_logy <- ggplot(data = resp_logy_aug, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point(alpha = 0.7) +  -->
<!--   geom_hline(yintercept=0, color="red") + -->
<!--   labs(x="Predicted values", y="Residuals", -->
<!--        title="Residuals vs. Predicted",  -->
<!--        subtitle = "Log-transformed Y") -->

<!-- resid_logy -->
<!-- ``` -->

<!-- ## Compare residual plots -->

<!-- ```{r} -->
<!-- #| echo: false -->

<!-- resid_orig <- ggplot(data = rt_full_aug, aes(x = .fitted, y = .resid)) + -->
<!--   geom_point(alpha = 0.7) + -->
<!--   geom_hline(yintercept = 0, color = "red", linetype = "dashed") + -->
<!--   labs(x = "Predicted values", y = "Residuals",  -->
<!--         title="Residuals vs. Predicted",  -->
<!--        subtitle = "Original Y") -->


<!-- resid_orig + resid_logy -->
<!-- ``` -->


<!-- # Log transformation on a predictor variable -->

<!-- ## Log Transformation on $X$ -->

<!-- ```{r} -->
<!-- #| echo: FALSE -->
<!-- set.seed(1) -->

<!-- s <- ggplot2::diamonds |> sample_n(100) -->
<!-- p1 <- ggplot(data=s,aes(x=carat,y=log(price)))+ -->
<!--   geom_point(color="blue")+ -->
<!--   ggtitle("Scatterplot")+ -->
<!--   xlab("X")+ -->
<!--   ylab("Y") -->

<!-- mod2 <- lm(log(price) ~ carat, data=s) -->
<!-- s <- s |> mutate(residuals = resid(mod2), predicted = predict(mod2)) -->
<!-- p2 <- ggplot(data=s,aes(x=predicted, y=residuals)) +  -->
<!-- geom_point(alpha = 0.7)+ -->
<!-- geom_hline(yintercept=0,color="red") + -->
<!--   ggtitle("Residual vs. Predicted")+ -->
<!--   xlab("Predicted")+ -->
<!--   ylab("residuals")  -->

<!-- p1 + p2 + plot_annotation(title = "Example data") -->
<!-- ``` -->

<!-- Try a transformation on $X$ if the scatterplot shows some curvature but the variance is constant for all values of $X$ -->

<!-- ## Respiratory Rate vs. Age {.midi} -->

<!-- -   A high respiratory rate can potentially indicate a respiratory infection in children. In order to determine what indicates a "high" rate, we first want to understand the relationship between a child's age and their respiratory rate. -->

<!-- -   The data contain the respiratory rate for 618 children ages 15 days to 3 years. It was obtained from the **Sleuth3** R package and is originally form a 1994 publication "Reference Values for Respiratory Rate in the First 3 Years of Life". -->

<!-- -   **Variables**: -->

<!--     -   `Age`: age in months -->
<!--     -   `Rate`: respiratory rate (breaths per minute) -->

<!-- ## Rate vs. Age -->

<!-- ```{r} -->
<!-- #| echo: false -->

<!-- library(Sleuth3) -->
<!-- ggplot(data=ex0824 ,aes(x=Age,y=Rate)) +  -->
<!--   geom_point(alpha = 0.7)  + -->
<!--   geom_smooth(se = FALSE) + -->
<!--   ggtitle("Respiratory Rate vs. Age") +  -->
<!--   xlab("Age in months")+ -->
<!--   ylab("Respiratory Rate in breaths per minute") -->
<!-- ``` -->

<!-- ## Model with Transformation on $X$ {.midi} -->

<!-- Suppose we have the following regression equation: -->

<!-- $$\hat{Y} = \hat{\beta}_0 + \hat{\beta}_1 \log(X)$$ -->

<!-- . . . -->

<!-- -   **Intercept:** When $X = 1$ $(\log(X) = 0)$, $Y$ is expected to be $\hat{\beta}_0$ (i.e. the mean of $Y$ is $\hat{\beta}_0$) -->

<!-- -   **Slope:** When $X$ is multiplied by a factor of $\mathbf{C}$, the mean of $Y$ is expected to increase by $\boldsymbol{\hat{\beta}_1}\mathbf{\log(C)}$ units -->

<!--     -   **Example**: when $X$ is multiplied by a factor of 2, $Y$ is expected to increase by $\boldsymbol{\hat{\beta}_1}\mathbf{\log(2)}$ units -->

<!-- ## Model interpretation -->

<!-- ```{r} -->
<!-- #| echo: FALSE -->
<!-- resp_logx_fit <- lm(Rate ~ log(Age), data = ex0824) -->

<!-- tidy(resp_logx_fit) |> -->
<!--   kable(digits = 3) -->
<!-- ``` -->

<!-- $$\hat{\text{Rate}} = 50.135 - 5.982 \times \log\text{(Age)}$$ -->

<!-- ::: question -->
<!-- -   Interpret the intercept in the context of the data. -->

<!-- -   Interpret the slope in terms of age multiplying by 2 in the context of the data. -->
<!-- ::: -->

<!-- ## Learn more -->

<!-- See [Log Transformations in Linear Regression](https://github.com/sta210-sp20/supplemental-notes/blob/master/log-transformations.pdf) for more details about interpreting regression models with log-transformed variables. -->

<!-- # Appendix -->

<!-- ## Why $Median(Y|X)$ instead of $\mu_{Y|X}$ -->

<!-- Suppose we have a set of values -->

<!-- ```{r echo = TRUE} -->
<!-- x <- c(3, 5, 6, 8, 10, 14, 19) -->
<!-- ``` -->

<!-- <br> -->

<!-- . . . -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- Let's calculate $\overline{\log(x)}$ -->

<!-- ```{r, echo = TRUE} -->
<!-- log_x <- log(x) -->
<!-- mean(log_x) -->
<!-- ``` -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- Let's calculate $\log(\bar{x})$ -->

<!-- ```{r, echo = TRUE} -->
<!-- xbar <- mean(x) -->
<!-- log(xbar) -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: -->

<!-- <br> -->

<!-- . . . -->

<!-- Note: $\overline{\log(x)} \neq \log(\bar{x})$ -->

<!-- ## Why $Median(Y|X)$ instead of $\mu_{Y|X}$ -->

<!-- ```{r echo = TRUE} -->
<!-- x <- c(3, 5, 6, 8, 10, 14, 19) -->
<!-- ``` -->

<!-- <br> -->

<!-- . . . -->

<!-- ::: columns -->
<!-- ::: {.column width="50%"} -->
<!-- Let's calculate $\text{Median}(\log(x))$ -->

<!-- ```{r , echo = TRUE} -->
<!-- log_x <- log(x) -->
<!-- median(log_x) -->
<!-- ``` -->
<!-- ::: -->

<!-- ::: {.column width="50%"} -->
<!-- Let's calculate $\log(\text{Median}(x))$ -->

<!-- ```{r, echo = TRUE} -->
<!-- median_x <- median(x) -->
<!-- log(median_x) -->
<!-- ``` -->
<!-- ::: -->
<!-- ::: -->

<!-- <br> -->

<!-- . . . -->

<!-- Note: $\text{Median} (\log(x)) = \log(\text{Median}(x))$ -->

<!-- ## Mean, Median, and Log -->

<!-- ```{r} -->
<!-- x <- c(3, 5, 6, 8, 10, 14, 19) -->
<!-- ``` -->

<!-- $$\overline{\log(x)} \neq \log(\bar{x})$$ -->

<!-- ```{r echo = T} -->
<!-- mean(log_x) == log(xbar) -->
<!-- ``` -->

<!-- . . . -->

<!-- $$\text{Median}(\log(x)) = \log(\text{Median}(x))$$ -->

<!-- ```{r echo = T} -->
<!-- median(log_x) == log(median_x) -->
<!-- ``` -->

<!-- ## Mean and median of $\log(Y)$ -->

<!-- -   Recall that $Y = \beta_0 + \beta_1 X$ is the **mean** value of the response at the given value of the predictor $X$. This doesn't hold when we log-transform the response variable. -->

<!-- -   Mathematically, the mean of the logged values is **not** necessarily equal to the log of the mean value. Therefore at a given value of $X$ -->

<!-- . . . -->

<!-- $$ -->
<!-- \begin{aligned}\exp\{\text{Mean}(\log(Y|X))\} \neq \text{Mean}(Y|X) \\[5pt] -->
<!-- \Rightarrow \exp\{\beta_0 + \beta_1 X\} \neq \text{Mean}(Y|X) \end{aligned} -->
<!-- $$ -->

<!-- ## Mean and median of $\log(y)$ -->

<!-- -   However, the median of the logged values **is** equal to the log of the median value. Therefore, -->

<!-- $$\exp\{\text{Median}(\log(Y|X))\} = \text{Median}(Y|X)$$ -->

<!-- . . . -->

<!-- -   If the distribution of $\log(Y)$ is symmetric about the regression line, for a given value $X$, we can expect $Mean(Y)$ and $Median(Y)$ to be approximately equal. -->




