---
title: "SLR: Randomization test for the slope"
author: "Prof. Eric Friedlander"
date: "2024-09-06"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— MAT 212 - Fall 2024 -  Schedule](https://mat212fa24.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
  cache: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

# Application exercise

::: appex
ðŸ“‹ [AE 03: Bootstrap confidence intervals](https://mat212fa24.netlify.app/ae/ae-03-bootstrap)
:::

## Concept Check

:::{.question}
You professor is interested in calculating the average amount of time CofI students spend doing homework.

1. If he collects a set of data and asks 100 students to compute 95% confidence intervals from that data, how many of those would you expect to contain the true average?
2. If, instead, he has each of those 100 students collect their own data and compute 95% confidence intervals from their own data, how many would you expect to contain the true average?
:::

## From last time {.smaller}

-   **Population:** Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = $N$)

-   **Sample:** Subset of the population, ideally random and representative (sample size = $n$)

-   Sample statistic $\ne$ population parameter, but if the sample is good, it can be a good estimate

-   **Statistical inference:** Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process

-   We report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population

-   Since we can't continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability


## Topics

-   Evaluate a claim about the slope using hypothesis testing

-   Define mathematical models to conduct inference for slope

## Computational setup

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)   # for data wrangling and visualization
library(broom)       # for nearly formatting model output
library(ggformula)   # for plotting
library(infer)       # for simulation based inference
library(scales)      # for pretty axis labels
library(knitr)       # for neatly formatted tables

# load my data
heb <- read_csv("data/HEBIncome.csv") |> 
  mutate(Avg_Income_K = Avg_Household_Income/1000)


# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))
```

# Recap of last lecture

## [Data: San Antonio Income & Organic Food Access]{.r-fit-text}

```{r}
#| echo: false
ggplot(heb, aes(x = Avg_Income_K, y = Number_Organic)) +
  geom_point(alpha = 0.7) +
  labs(
    x = "Average Household Income (in thousands)",
    y = "Number of Organic Vegetables",
  ) +
  scale_x_continuous(labels = label_dollar())
```

## The regression model

```{r}
heb_fit <- lm(Number_Organic ~ Avg_Income_K, data = heb)

tidy(heb_fit) |>
  kable(digits = 2)
```

. . .

```{r}
#| echo: false
intercept <- tidy(heb_fit) |> filter(term == "(Intercept)") |> pull(estimate) |> round(digits=2)
slope <- tidy(heb_fit) |> filter(term == "Avg_Income_K") |> pull(estimate) |> round(digits=2)
```

**Slope:** For each additional \$1,000 in average household income, we expect the number of organic options available at nearby HEBs to increase by `r slope`, on average.

## Inference for simple linear regression

-   Calculate a confidence interval for the slope, $\beta_1$

-   Conduct a hypothesis test for the slope, $\beta_1$

## Sampling is natural {.midi}

![](images/05/soup.png){fig-alt="Illustration of a bowl of soup" fig-align="center"}

-   When you taste a spoonful of soup and decide the spoonful you tasted isn't salty enough, that's exploratory analysis
-   If you generalize and conclude that your entire soup needs salt, that's an inference
-   For your inference to be valid, the spoonful you tasted (the sample) needs to be representative of the entire pot (the population)

## Confidence interval via bootstrapping {.midi}

-   Bootstrap new samples from the original sample
-   Fit models to each of the samples and estimate the slope
-   Use features of the distribution of the bootstrapped slopes to construct a confidence interval

## Bootstrapping pipeline I

```{r}
#| echo: true
#| code-line-numbers: "|1|3|4"

set.seed(212)

heb |>
  specify(Number_Organic ~ Avg_Income_K)
```

## Bootstrapping pipeline II

```{r}
#| echo: true
#| code-line-numbers: "|5"

set.seed(212)

heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  generate(reps = 1000, type = "bootstrap")
```

## Bootstrapping pipeline III

```{r}
#| echo: true
#| code-line-numbers: "|6"

set.seed(212)

heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  generate(reps = 1000, type = "bootstrap") |>
  fit()
```

## Bootstrapping pipeline IV

```{r}
#| echo: true
#| code-line-numbers: "|3"

set.seed(212)

boot_dist <- heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  generate(reps = 1000, type = "bootstrap") |>
  fit()
```

## Visualize the bootstrap distribution

```{r}
#| echo: true
#| code-line-numbers: "|2"

boot_dist |>
  filter(term == "Avg_Income_K") |>
  gf_histogram(~estimate, bins = 15)
```

## Compute the CI

```{r}
#| echo: false
boot_dist |>
  filter(term == "Avg_Income_K") |>
  ggplot(aes(x = estimate)) +
  geom_histogram(bins = 15)
```

## But first...

```{r}
#| echo: true

obs_fit <- heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  fit()

obs_fit
```

## Compute 95% confidence interval

```{r}
#| echo: true

boot_dist |>
  get_confidence_interval(
    point_estimate = obs_fit,
    level = 0.95,
    type = "percentile"
  )
```

# Hypothesis test for the slope

## Research question and hypotheses

"Do the data provide sufficient evidence that $\beta_1$ (the true slope for the population) is different from 0?"

. . .

**Null hypothesis**: there is no linear relationship between `Number_Organic` and `Avg_Income_K`

$$
H_0: \beta_1 = 0
$$

. . .

**Alternative hypothesis**: there is a linear relationship between `Number_Organic` and `Avg_Income_K`

$$
H_A: \beta_1 \ne 0
$$

## Hypothesis testing as a court trial

::: incremental
-   **Null hypothesis**, $H_0$: Defendant is innocent
-   **Alternative hypothesis**, $H_A$: Defendant is guilty
-   **Present the evidence:** Collect data
-   **Judge the evidence:** "Could these data plausibly have happened by chance if the null hypothesis were true?"
    -   Yes: Fail to reject $H_0$
    -   No: Reject $H_0$
-   Not guilty $\neq$ innocent $\implies$ why we say "fail to reject the null" rather than "accept the null"
:::

## Hypothesis testing framework {.midi}

::: incremental
-   Start with a null hypothesis, $H_0$ that represents the status quo
-   Set an alternative hypothesis, $H_A$ that represents the research question, i.e. claim we're testing
-   Under the assumption that the null hypothesis is true, calculate a **p-value** (probability of getting outcome or outcome even more favorable to the alternative)
    -   if the test results suggest that the data do not provide convincing evidence for the alternative hypothesis, stick with the null hypothesis
    -   if they do, then reject the null hypothesis in favor of the alternative
:::

## Quantify the variability of the slope {.midi}

**for testing**

::: incremental
-   Two approaches:
    1.  Via simulation
    2.  Via mathematical models
-   Use **Randomization** to quantify the variability of the slope for the purpose of testing, *under the assumption that the null hypothesis is true*:
    -   Simulate new samples from the original sample via permutation
    -   Fit models to each of the samples and estimate the slope
    -   Use features of the distribution of the permuted slopes to conduct a hypothesis test
:::

## Permutation, described {.smaller}

::: columns
::: {.column width="40%"}
-   Use permuting to simulate data under the assumption the null hypothesis is true and measure the natural variability in the data due to sampling, [**not**]{.underline} due to variables being correlated
    -   Permute reponse variable to eliminate any existing relationship with explanatory variable
-   Each `Number_Organic` value is randomly assigned to the `Avg_Household_K`, i.e. `Number_Organic` and `Avg_Household_K` are no longer matched for a given store
:::

::: {.column width="60%"}
```{r}
#| echo: false
set.seed(1234)

heb_rand <- heb |>
  mutate(
    Number_Organic_Original = Number_Organic,
    Number_Organic_Permuted = sample(Number_Organic, size = nrow(heb))
    ) |>
  select(contains("Number_Organic_"), Avg_Income_K)
heb_rand
```
:::
:::

## Permutation, visualized

::: columns
::: {.column width="50%"}
-   Each of the observed values for `area` (and for `price`) exist in both the observed data plot as well as the permuted `price` plot
-   Permuting removes the relationship between `area` and `price`
:::

::: {.column width="50%"}
```{r}
#| out.width: "100%"
#| fig.asp: 1.2
#| echo: false

heb_rand |>
  pivot_longer(cols = contains("Number_Organic_"), names_to = "Number_Organic_Type", names_prefix = "Number_Organic_", values_to = "Number_Organic") |>
  ggplot(aes(x = Avg_Income_K, y = Number_Organic)) +
  geom_point() +
  geom_smooth(aes(color = Number_Organic_Type), method = "lm", se = FALSE, show.legend = FALSE) +
  facet_wrap(~Number_Organic_Type, nrow = 2) +
  scale_color_manual(values = c("#8F2D56", "gray")) +
  scale_x_continuous(labels = label_number()) +
  scale_y_continuous(labels = label_dollar()) +
  labs(x = "Avg_Income_K", y = "Number_Organic") + 
  theme(text = element_text(size = 20))
```
:::
:::

## Permutation, repeated

Repeated permutations allow for quantifying the variability in the slope under the condition that there is no linear relationship (i.e., that the null hypothesis is true)

```{r}
#| echo: false

set.seed(1125)

heb_perms_1000 <- heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute")

ggplot(heb_perms_1000, 
       aes(x = Avg_Income_K, y = Number_Organic, group = replicate)) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.1) +
  labs(
    x = "Average Household Income (in thousands)",
    y = "Number of Organic Vegetables",
  ) +
  scale_x_continuous(labels = label_dollar(), limits = c(range(heb$Avg_Income_K))) +
  scale_y_continuous(labels = label_number(), limits = c(range(heb$Number_Organic))) +
  geom_abline(intercept = intercept, slope = slope, color = "#8F2D56")
```

## Concluding the hypothesis test {.smaller}

::: question
Is the observed slope of $\hat{\beta_1} = 159$ (or an even more extreme slope) a likely outcome under the null hypothesis that $\beta = 0$? What does this mean for our original question: "Do the data provide sufficient evidence that $\beta_1$ (the true slope for the population) is different from 0?"
:::

```{r}
#| out.width: "60%"
#| fig.asp: 0.618
#| echo: false

null_dist <- heb_perms_1000 |>
  fit()

ggplot(null_dist |> filter(term == "Avg_Income_K"),
       aes(x = estimate)) +
  geom_histogram(color = "white") +
  labs(x = "Slope", y = "Count",
       title = "Slopes of 1000 permuted samples") +
  geom_vline(xintercept = slope, color = "#8F2D56", size = 1) +
  geom_vline(xintercept = -1*slope, color = "#8F2D56", size = 1, linetype = "dashed") +
  scale_x_continuous(limits = c(-slope, slope), breaks = seq(-150, 150, 50))
```



## Permutation pipeline I

```{r}
#| echo: true
#| code-line-numbers: "|1|3|4"

set.seed(1218)

heb |>
  specify(Number_Organic ~ Avg_Income_K)
```

## Permutation pipeline II

```{r}
#| echo: true
#| code-line-numbers: "|5"

set.seed(1218)

heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  hypothesize(null = "independence")
```

## Permutation pipeline III

```{r}
#| echo: true
#| code-line-numbers: "|6"

set.seed(1218)

heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute")
```

## Permutation pipeline IV

```{r}
#| echo: true
#| code-line-numbers: "|7"

set.seed(1218)

heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  fit()
```

## Permutation pipeline V

```{r}
#| echo: true
#| code-line-numbers: "|3"

set.seed(1218)

heb |>
  specify(Number_Organic ~ Avg_Income_K) |>
  hypothesize(null = "independence") |>
  generate(reps = 1000, type = "permute") |>
  fit()
```

## Visualize the null distribution

```{r}
#| echo: true
#| code-line-numbers: "|2"

null_dist |>
  filter(term == "Avg_Income_K") |>
  gf_histogram(~estimate, color = "white")
```

## Reason around the p-value {.smaller}

::: question
In a world where the there is no relationship between the the number of organic food options and the nearby average household income ($\beta_1 = 0$), what is the probability that we observe a sample of `r nrow(heb)` stores where the slope fo the model predicting the number of organic options from average household income is 0.96 or even more extreme?
:::

```{r}
#| echo: false

null_dist |>
  filter(term == "area") |>
  ggplot(aes(x = estimate)) +
  geom_histogram(binwidth = 10, color = "white") +
  geom_vline(xintercept = slope, color = "#8F2D56", size = 1) +
  geom_vline(xintercept = -1*slope, color = "#8F2D56", size = 1, linetype = "dashed") +
  scale_x_continuous(limits = c(-slope, slope), breaks = seq(-150, 150, 50))
```

## Compute the p-value

::: question
What does this warning mean?
:::

```{r}
#| echo: true
#| warning: true

get_p_value(
  null_dist,
  obs_stat = obs_fit,
  direction = "two-sided"
)
```

# Application exercise

::: appex
ðŸ“‹ [AE 04: Randomization test for the slope](https://mat212fa24.netlify.app/ae/ae-04-sim-testing)
:::

