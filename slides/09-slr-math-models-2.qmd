---
title: "SLR: Math Models Continued"
author: "Prof. Eric Friedlander"
date: "2024-09-11"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— MAT 212 - Fall 2024 -  Schedule](https://mat212fa24.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
  cache: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r setup}
#| include: false

library(countdown)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)
```

# Questions from last class?

## Computational set up

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)   # for data wrangling and visualization
library(ggformula)   # for plotting using formulas
library(broom)       # for formatting model output
library(scales)      # for pretty axis labels
library(knitr)       # for pretty tables
library(kableExtra)  # also for pretty tables
library(patchwork)   # arrange plots

# HEB Dataset
heb <- read_csv("data/HEBIncome.csv") |> 
  mutate(Avg_Income_K = Avg_Household_Income/1000)

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 20))
```

## Regression model, revisited

```{r}
#| echo: true
heb_fit <- lm(Number_Organic ~ Avg_Income_K, data = heb)

tidy(heb_fit) |>
  kable(digits = 3)
```

## Mathematical representation, visualized {.midi}

$$
Y|X \sim N(\beta_0 + \beta_1 X, \sigma_\epsilon^2)
$$

![Image source: *Introduction to the Practice of Statistics (5th ed)*](images/06/regression.png){fig-align="center"}

## Confidence interval for the slope

$$
\text{Estimate} \pm \text{ (critical value) } \times \text{SE}
$$

. . .

$$
\hat{\beta}_1 \pm t^* \times SE_{\hat{\beta}_1}
$$

where $t^*$ is calculated from a $t$ distribution with $n-2$ degrees of freedom

## Confidence interval: Critical value

::: columns
::: {.column width="60%"}
::: {.fragment fragment-index="1"}
```{r}
#| echo: true

# confidence level: 95%
qt(0.975, df = nrow(heb) - 2)
```
:::

::: {.fragment fragment-index="2"}
```{r}
# confidence level: 90%
qt(0.95, df = nrow(heb) - 2)
```
:::

::: {.fragment fragment-index="3"}
```{r}
# confidence level: 99%
qt(0.995, df = nrow(heb) - 2)
```
:::
:::

::: {.column width="40%"}
```{r}
#| out.width: "100%"
#| echo: false
library(openintro)
normTail(M = c(-2.030108, 2.030108), df = nrow(heb) - 2, col = "#D9E3E4")
text(x = 0, y = 0.04, labels = "95%", cex = 2, col = "#5B888C")
```
:::
:::

## 95% CI for the slope: Calculation

```{r}
#| echo: false
tidy(heb_fit) |> 
  kable(digits = 2) |>
  row_spec(2, background = "#D9E3E4")
```

$$\hat{\beta}_1 = 0.96 \hspace{15mm} t^* = 2.03 \hspace{15mm} SE_{\hat{\beta}_1} = 0.13$$

. . .

$$
0.96 \pm 2.03 \times 0.13 = (0.70, 1.22)
$$

## 95% CI for the slope: Computation

```{r}
#| echo: true

tidy(heb_fit, conf.int = TRUE, conf.level = 0.95) |> 
  kable(digits = 2)
```

# Intervals for predictions

## Intervals for predictions {.smaller} 

-   Question: *"What is the predicted number of organic vegetable options in a neighborhood with an average income of \$70k?"*
-   We said reporting a single estimate for the slope is not wise, and we should report a plausible range instead
-   Similarly, reporting a single prediction for a new value is not wise, and we should report a plausible range instead

```{r}
#| fig.width: 10
#| fig.align: "center"
#| echo: false

x_new <- 70
y_hat_x_new <- predict(heb_fit, newdata = tibble(Avg_Income_K = x_new))

ggplot(heb, aes(x = Avg_Income_K, y = Number_Organic)) +
  geom_segment(
    x = x_new, xend = x_new, y = y_hat_x_new-3, yend = y_hat_x_new+3,
    color = "#CDDBDC", size = 4
  ) +
  geom_segment(
    x = x_new, xend = x_new, y = y_hat_x_new-2, yend = y_hat_x_new+2,
    color = "#ADC3C5", size = 4
  ) +
  geom_segment(
    x = x_new, xend = x_new, y = y_hat_x_new-1, yend = y_hat_x_new+1,
    color = "#7B9FA3", size = 4
  ) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") +
  geom_segment(
    x = x_new, xend = x_new, y = 0, yend = y_hat_x_new,
    linetype = "dashed", color = "#5B888C"
  ) +
  geom_segment(
    x = 0, xend = x_new, y = y_hat_x_new, yend = y_hat_x_new,
    linetype = "dashed", color = "#5B888C"
  ) +
  annotate("point", x = x_new, y = y_hat_x_new, size = 2, color = "magenta") +
  annotate("point", x = x_new, y = y_hat_x_new, size = 5, shape = "circle open", color = "#5B888C", stroke = 2) +
  labs(
    x = "Average Household Income (in thousands)",
    y = "Number of Organic Vegetables",
  ) +
  scale_x_continuous(labels = label_dollar())
```

## Two types of predictions

1.  Prediction for the mean: "What is the average predicted number of organic vegetable options in a neighborhood with an average income of \$70k?"

2.  Prediction for an individual observation: "What is the predicted number of organic vegetable options at a single HEB in a neighborhood with an average income of \$70k?"

. . .

::: question
Which would you expect to be more variable? The average prediction or the prediction for an individual observation? Based on your answer, how would you expect the widths of plausible ranges for these two predictions to compare?
:::

## Uncertainty in predictions

**Confidence interval for the mean outcome:** $$\large{\hat{y} \pm t_{n-2}^* \times \color{purple}{\mathbf{SE}_{\hat{\boldsymbol{\mu}}}}}$$

. . .

**Prediction interval for an individual observation:** $$\large{\hat{y} \pm t_{n-2}^* \times \color{purple}{\mathbf{SE_{\hat{y}}}}}$$

## Standard errors

**Standard error of the mean outcome:** $$SE_{\hat{\mu}} = \hat{\sigma}_\epsilon\sqrt{\frac{1}{n} + \frac{(x-\bar{x})^2}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}}$$

. . .

**Standard error of an individual outcome:** $$SE_{\hat{y}} = \hat{\sigma}_\epsilon\sqrt{1 + \frac{1}{n} + \frac{(x-\bar{x})^2}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}}$$

## Standard errors

**Standard error of the mean outcome:** $$SE_{\hat{\mu}} = \hat{\sigma}_\epsilon\sqrt{\frac{1}{n} + \frac{(x-\bar{x})^2}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}}$$

**Standard error of an individual outcome:** $$SE_{\hat{y}} = \hat{\sigma}_\epsilon\sqrt{\mathbf{\color{purple}{\Large{1}}} + \frac{1}{n} + \frac{(x-\bar{x})^2}{\sum\limits_{i=1}^n(x_i - \bar{x})^2}}$$

## Confidence interval

The 95% **confidence interval** for the *mean* outcome:

```{r}
#| echo: true
new_neighborhood <- tibble(Avg_Income_K = 70)

predict(heb_fit, newdata = new_neighborhood, interval = "confidence", level = 0.95) |>
  kable()
```

```{r}
#| echo: false
new_neighborhood_ci <- as.tibble(predict(heb_fit, newdata = new_neighborhood, interval = "confidence", level = 0.95) )
```

. . .

* We are 95% confident that the *mean* number of organic vegetable options offered by HEB in a neighborhood with an average income of \$70k is between `r new_neighborhood_ci$lwr |> round(digits=2)` and `r new_neighborhood_ci$upr |>  round(digits=2)`.

## Prediction interval

The 95% **prediction interval** for an *individual* outcome:

```{r}
#| echo: true
predict(heb_fit, newdata = new_neighborhood, interval = "prediction", level = 0.95) |>
  kable()
```

```{r}
#| echo: false
new_neighborhood_pi <- as.tibble(predict(heb_fit, newdata = new_neighborhood, interval = "prediction", level = 0.95))
```

. . .

We are 95% confident that the number of organic vegetable options offered by HEB in a neighborhood with an average income of \$70k is between `r new_neighborhood_pi$lwr |> round(digits=2)` and `r new_neighborhood_pi$upr |>  round(digits=2)`.

## Comparing intervals

```{r}
#| out.width: "100%"
#| fig.width: 10
#| echo: false

new_neighborhoods <- tibble(Avg_Income_K = seq(min(heb$Avg_Income_K), max(heb$Avg_Income_K), diff(range(heb$Avg_Income_K))/30))
new_neighborhood_ci <- as.tibble(predict(heb_fit, newdata = new_neighborhoods, interval = "confidence", level = 0.95)) |> 
  mutate(
    Avg_Income_K = new_neighborhoods$Avg_Income_K,
    type = "Confidence interval"
    )
new_neighborhood_pi <- as.tibble(predict(heb_fit, newdata = new_neighborhoods, interval = "prediction", level = 0.95)) |> 
  mutate(
    Avg_Income_K = new_neighborhoods$Avg_Income_K,
    type = "Prediction interval"
    )
new_neighborhoods_int <- bind_rows(new_neighborhood_ci, new_neighborhood_pi)

ggplot(heb, aes(x = Avg_Income_K, y = Number_Organic)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") +
  geom_line(data = new_neighborhoods_int,
            aes(x = Avg_Income_K, y = lwr, linetype = type, color = type),
            size = 1) +
  geom_line(data = new_neighborhoods_int,
            aes(x = Avg_Income_K, y = upr, linetype = type, color = type),
            size = 1) +
  labs(
    x = "Average Household Income (in thousands)",
    y = "Number of Organic Vegetables",
  ) +
  scale_x_continuous(labels = label_dollar()) +
  theme(
    legend.position = c(0.2, 0.85)
  )
```

## Extrapolation

Using the model to predict for values outside the range of the original data is **extrapolation**.

. . .

::: columns
::: {.column width="55%"}
::: question
Calculate the prediction interval for the number of organic options in an extremely wealthy neighborhood with an average household income of \$500k.
:::

[*No, thanks!*]{.fragment}
:::

::: {.column width="45%"}
![](images/07/Dall_ERichPersonVeges.jpg){fig-alt='AI Generated Image of "A very rich person eating organic vegetables"' fig-align="center"}
:::
:::



## Extrapolation

::: question
Why do we want to avoid extrapolation?
:::
