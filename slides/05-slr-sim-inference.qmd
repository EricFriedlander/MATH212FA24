---
title: "SLR: Simulation-based inference"
subtitle: "Bootstrap confidence intervals for the slope"
author: "Prof. Maria Tackett"
date: "2022-09-12"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— Week 03](https://sta210-fa22.netlify.app/weeks/week-03.html)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: true
    incremental: false 
    chalkboard: true
execute:
  freeze: auto
  echo: true
  warning: false
  message: false
knitr:
  opts_chunk: 
    R.options:      
    width: 200
---

```{r setup}
#| include: false

library(countdown)

knitr::opts_chunk$set(
  fig.width = 8,
  fig.asp = 0.618,
  fig.retina = 3,
  dpi = 300,
  out.width = "80%",
  fig.align = "center"
)
```

## Announcements

-   Lab 01 due

    -   TODAY, 11:59pm (Thursday labs)

    -   Tuesday, 11:59pm (Friday labs)

    -   Make sure all work is pushed to GitHub and the PDF is submitted on Gradescope by the deadline

<!-- -->

-   See [Week 03](https://sta210-fa22.netlify.app/weeks/week-03.html) for this week's activities.

## Topics

-   Assess model's predictive importance using data splitting and bootstrapping

-   Find range of plausible values for the slope using bootstrap confidence intervals

## Computational setup

```{r packages}
#| echo: true
#| message: false

# load packages
library(tidyverse)   # for data wrangling and visualization
library(tidymodels)  # for modeling
library(usdata)      # for the county_2019 dataset
library(openintro)   # for Duke Forest dataset
library(scales)      # for pretty axis labels
library(glue)        # for constructing character strings
library(knitr)       # for neatly formatted tables
library(kableExtra)  # also for neatly formatted tablesf


# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 16))
```

# Recap of last class

## Uninsurance vs. HS graduation rates

```{r}
#| echo: false

county_2019_nc <- county_2019 |>
  as_tibble() |>
  filter(state == "North Carolina") |>
  select(name, hs_grad, uninsured)
```

```{r nc-uninsured-hsgrad-scatter-line}
#| code-fold: true
#| echo: true
ggplot(county_2019_nc, aes(x = hs_grad, y = uninsured)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") +
  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  labs(
    x = "High school graduate", y = "Uninsured",
    title = "Uninsurance vs. HS graduation rates",
    subtitle = "North Carolina counties, 2015 - 2019"
  )
```

## Fitting the model

```{r}
#| echo: true

nc_fit <- linear_reg() |>
  set_engine("lm") |>
  fit(uninsured ~ hs_grad, data = county_2019_nc)

tidy(nc_fit)
```

## Augmenting the data

With `augment()` to add columns for predicted values (`.fitted`), residuals (`.resid`), etc.:

```{r}
#| echo: true

nc_aug <- augment(nc_fit$fit)
nc_aug
```

## Statistics for model evaluation {.midi}

::: incremental
-   **R-squared**, $R^2$ : Percentage of variability in the outcome explained by the regression model (in the context of SLR, the predictor)

    $$
    R^2 = Cor(x,y)^2 = Cor(y, \hat{y})^2
    $$

-   **Root mean square error, RMSE**: A measure of the average error (average difference between observed and predicted values of the outcome)

    $$
    RMSE = \sqrt{\frac{\sum_{i = 1}^n (y_i - \hat{y}_i)^2}{n}}
    $$
:::

## Obtaining $R^2$ and RMSE

::: incremental
Use `rsq()` and `rmse()`, respectively

```{r}
#| echo: true
rsq(nc_aug, truth = uninsured, estimate = .fitted)
rmse(nc_aug, truth = uninsured, estimate = .fitted)
```
:::

## Purpose of model evaluation

-   $R^2$ tells us how our model is doing to predict the data we *already have*
-   But generally we are interested in prediction for a new observation, not for one that is already in our sample, i.e. **out-of-sample prediction**
-   We have a couple ways of *simulating* out-of-sample prediction before actually getting new data to evaluate the performance of our models

# Splitting data

## Spending our data

::: incremental
-   There are several steps to create a useful model: parameter estimation, model selection, performance assessment, etc.
-   Doing all of this on the entire data we have available leaves us with no other data to assess our choices
-   We can allocate specific subsets of data for different tasks, as opposed to allocating the largest possible amount to the model parameter estimation only (which is what we've done so far)
:::

## Simulation: data splitting {.smaller}

::: columns
::: {.column width="30%"}
::: nonincremental
-   Take a random sample of 10% of the data and set aside (testing data)
-   Fit a model on the remaining 90% of the data (training data)
-   Use the coefficients from this model to make predictions for the testing data
-   Repeat 10 times
:::
:::

::: {.column width="70%"}
```{r}
#| out.width: "100%"
#| echo: false

set.seed(345)

n_folds <- 10

county_2019_nc_folds <- county_2019_nc |>
  slice_sample(n = nrow(county_2019_nc)) |>
  mutate(fold = rep(1:n_folds, n_folds))|>
  arrange(fold)

predict_folds <- function(i) {
  fit <- lm(uninsured ~ hs_grad, data = county_2019_nc_folds |> filter(fold != i))
  predict(fit, newdata = county_2019_nc_folds |> filter(fold == i)) |>
    bind_cols(county_2019_nc_folds  |>
                filter(fold == i), .fitted = _)
}

nc_fits <- map_df(1:n_folds, predict_folds)

p_nc_fits <- ggplot(nc_fits, aes(x = hs_grad, y = .fitted, group = fold)) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, size = 0.3, alpha = 0.5) +
  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  labs(
    x = "High school graduate", y = "Uninsured",
    title = "Predicted uninsurance rate in NC",
    subtitle = glue("For {n_folds} different testing datasets")
    )

p_nc_fits
```
:::
:::

## Predictive performance {.smaller}

::: columns
::: {.column width="25%"}
::: question
::: nonincremental
-   How consistent are the predictions for different testing datasets?
-   How consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?
:::
:::
:::

::: {.column width="75%"}
```{r}
#| out.width: "100%"
#| echo: false

p_nc_fits
```
:::
:::

# Bootstrapping

## Bootstrapping our data

::: incremental
-   The idea behind bootstrapping is that if a given observation exists in a sample, there may be more like it in the population
-   With bootstrapping, we simulate resampling from the population by resampling from the sample we observed
-   Bootstrap samples are the sampled *with replacement* from the original sample and same size as the original sample
    -   For example, if our sample consists of the observations {A, B, C}, bootstrap samples could be {A, A, B}, {A, C, A}, {B, C, C}, {A, B, C}, etc.
:::

## Simulation: bootstrapping {.smaller}

::: columns
::: {.column width="25%"}
::: nonincremental
-   Take a bootstrap sample -- sample with replacement from the original data, same size as the original data
-   Fit model to the sample and make predictions for that sample
-   Repeat many times
:::
:::

::: {.column width="75%"}
```{r}
#| out.width: "100%"
#| echo: false

n_boot <- 100

predict_boots <- function(i){
  boot <- county_2019_nc |>
    slice_sample(n = nrow(county_2019_nc), replace = TRUE) |>
    mutate(boot_samp = i)
  fit <- lm(uninsured ~ hs_grad, data = boot)
  predict(fit) |> bind_cols(boot, .fitted = _)
}

set.seed(1234)
county_2019_nc_boots <- map_df(1:n_boot, predict_boots)

p_nc_boots <- ggplot(county_2019_nc_boots, aes(x = hs_grad, y = .fitted, group = boot_samp)) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, size = 0.3, alpha = 0.5) +
  scale_x_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  scale_y_continuous(labels = label_percent(scale = 1, accuracy = 1)) +
  labs(
    x = "High school graduate", y = "Uninsured",
    title = "Predicted uninsurance rate in NC",
    subtitle = glue("For {n_boot} bootstrap samples")
    )

p_nc_boots
```
:::
:::

## Predictive performance {.smaller}

::: columns
::: {.column width="25%"}
::: question
::: nonincremental
-   How consistent are the predictions for different bootstrap datasets?
-   How consistent are the predictions for counties with high school graduation rates in the middle of the plot vs. in the edges?
:::
:::
:::

::: {.column width="75%"}
```{r}
#| out.width: "100%"
#| echo: false
p_nc_boots
```
:::
:::

## Recap

-   Motivated the importance of model evaluation

-   Described how $R^2$ and RMSE are used to evaluate models

-   Assessed model's predictive importance using data splitting and bootstrapping

# Simulation-based inference

## Data: Houses in Duke Forest

::: columns
::: {.column width="50%"}
-   Data on houses that were sold in the Duke Forest neighborhood of Durham, NC around November 2020
-   Scraped from Zillow
-   Source: [`openintro::duke_forest`](http://openintrostat.github.io/openintro/reference/duke_forest.html)
:::

::: {.column width="50%"}
![](images/05/duke_forest_home.jpg){fig-alt="Home in Duke Forest"}
:::
:::

**Goal**: Use the area (in square feet) to understand variability in the price of houses in Duke Forest.

## Exploratory data analysis

```{r}
#| code-fold: true
ggplot(duke_forest, aes(x = area, y = price)) +
  geom_point(alpha = 0.7) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = "Price and area of houses in Duke Forest"
  ) +
  scale_y_continuous(labels = label_dollar()) 
```

## Modeling {.midi}

```{r}
#| echo: true
#| code-line-numbers: "|5|6"

df_fit <- linear_reg() |>
  set_engine("lm") |>
  fit(price ~ area, data = duke_forest)

tidy(df_fit) |>
  kable(digits = 2) #neatly format table to 2 digits
```

. . .

<br>

```{r}
#| echo: false
intercept <- tidy(df_fit) |> filter(term == "(Intercept)") |> pull(estimate) |> round()
slope <- tidy(df_fit) |> filter(term == "area") |> pull(estimate) |> round()
```

-   **Intercept:** Duke Forest houses that are 0 square feet are expected to sell, on average, for `r dollar(intercept)`.
-   **Slope:** For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by `r dollar(slope)`.

## From sample to population {.midi}

> For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by `r dollar(slope)`.

<br>

-   This estimate is valid for the single sample of `r nrow(duke_forest)` houses.
-   But what if we're not interested quantifying the relationship between the size and price of a house in this single sample?
-   What if we want to say something about the relationship between these variables for all houses in Duke Forest?

## Statistical inference

-   **Statistical inference** provide methods and tools so we can use the single observed sample to make valid statements (inferences) about the population it comes from

-   For our inferences to be valid, the sample should be random and representative of the population we're interested in

## Inference for simple linear regression

-   Calculate a confidence interval for the slope, $\beta_1$

-   Conduct a hypothesis test for the slope,$\beta_1$

# Confidence interval for the slope

## Confidence interval {.midi}

::: incremental
-   A plausible range of values for a population parameter is called a **confidence interval**
-   Using only a single point estimate is like fishing in a murky lake with a spear, and using a confidence interval is like fishing with a net
    -   We can throw a spear where we saw a fish but we will probably miss, if we toss a net in that area, we have a good chance of catching the fish
    -   Similarly, if we report a point estimate, we probably will not hit the exact population parameter, but if we report a range of plausible values we have a good shot at capturing the parameter
:::

## Confidence interval for the slope {.midi}

A confidence interval will allow us to make a statement like "*For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by `r dollar(slope)`, plus or minus X dollars.*"

. . .

-   Should X be \$10? \$100? \$1000?

-   If we were to take another sample of `r nrow(duke_forest)` would we expect the slope calculated based on that sample to be exactly `r dollar(slope)`? Off by \$10? \$100? \$1000?

-   The answer depends on how variable (from one sample to another sample) the sample statistic (the slope) is

-   We need a way to quantify the variability of the sample statistic

## Quantify the variability of the slope {.midi}

**for estimation**

::: incremental
-   Two approaches:
    1.  Via simulation (what we'll do today)
    2.  Via mathematical models (what we'll do in the next class)
-   **Bootstrapping** to quantify the variability of the slope for the purpose of estimation:
    -   Bootstrap new samples from the original sample
    -   Fit models to each of the samples and estimate the slope
    -   Use features of the distribution of the bootstrapped slopes to construct a confidence interval
:::

```{r}
#| echo: false
set.seed(119)

df_boot_samples_5 <- duke_forest |>
  specify(price ~ area) |>
  generate(reps = 5, type = "bootstrap")
```

## Bootstrap sample 1

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs <- ggplot(duke_forest, aes(x = area, y = price)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE, color = "#8F2D56") +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = "Price and area of houses in Duke Forest"
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 1

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap sample 2

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 2

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap sample 3

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 3

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap sample 4

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 4

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap sample 5

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

replicate_no = 5

ggplot(df_boot_samples_5 |> filter(replicate == replicate_no), 
       aes(x = area, y = price)) +
  geom_point(alpha = 0.5) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.8) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap sample {replicate_no}")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

. . .

*so on and so forth...*

## Bootstrap samples 1 - 5

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

ggplot(df_boot_samples_5, aes(x = area, y = price, group = replicate)) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.5) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap samples 1 - 5")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())
```
:::
:::

## Bootstrap samples 1 - 100

```{r}
#| echo: false
set.seed(119)

df_boot_samples_100 <- duke_forest |>
  specify(price ~ area) |>
  generate(reps = 100, type = "bootstrap")
```

::: columns
::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_obs
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
#| out.width: "100%"

p_df_boot_samples_100 <- ggplot(df_boot_samples_100, aes(x = area, y = price, group = replicate)) +
  geom_line(stat = "smooth", method = "lm", se = FALSE, alpha = 0.05) +
  labs(
    x = "Area (square feet)",
    y = "Sale price (USD)",
    title = glue("Bootstrap samples 1 - 100")
  ) +
  scale_y_continuous(labels = label_dollar()) +
  scale_x_continuous(labels = label_number())

p_df_boot_samples_100
```
:::
:::

## Slopes of bootstrap samples

::: question
**Fill in the blank:** For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by `r dollar(slope)`, plus or minus \_\_\_ dollars.
:::

```{r}
#| echo: false
p_df_boot_samples_100 +
  geom_abline(intercept = intercept, slope = slope, color = "#8F2D56")
```

## Slopes of bootstrap samples

::: question
**Fill in the blank:** For each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by `r dollar(slope)`, plus or minus \_\_\_ dollars.
:::

```{r}
#| echo: false
df_boot_samples_100_fit <- df_boot_samples_100 |>
  fit()

df_boot_samples_100_hist <- ggplot(df_boot_samples_100_fit |> filter(term == "area"), aes(x = estimate)) +
  geom_histogram(binwidth = 10, color = "white") +
  geom_vline(xintercept = slope, color = "#8F2D56", size = 1) +
  labs(x = "Slope", y = "Count",
       title = "Slopes of 100 bootstrap samples") +
  scale_x_continuous(labels = label_dollar())

df_boot_samples_100_hist
```

## Confidence level

::: question
How confident are you that the true slope is between \$0 and \$250? How about \$150 and \$170? How about \$90 and \$210?
:::

```{r}
#| echo: false
df_boot_samples_100_hist
```

## 95% confidence interval {.midi}

```{r}
#| echo: false
lower <- df_boot_samples_100_fit |>
  ungroup() |>
  filter(term == "area") |>
  summarise(quantile(estimate, 0.025)) |>
  pull()

upper <- df_boot_samples_100_fit |>
  ungroup() |>
  filter(term == "area") |>
  summarise(quantile(estimate, 0.975)) |>
  pull()

df_boot_samples_100_hist +
  geom_vline(xintercept = lower, color = "steelblue", size = 1, linetype = "dashed") +
  geom_vline(xintercept = upper, color = "steelblue", size = 1, linetype = "dashed")
```

::: incremental
-   A 95% confidence interval is bounded by the middle 95% of the bootstrap distribution
-   We are 95% confident that for each additional square foot, the model predicts the sale price of Duke Forest houses to be higher, on average, by `r dollar(lower)` to `r dollar(upper)`.
:::

## Application exercise

::: appex
ðŸ“‹ [AE 03: Bootstrap confidence intervals](https://sta210-fa22.netlify.app/ae/ae-03-bootstrap.html)
:::

## Computing the CI for the slope I

Calculate the observed slope:

```{r}
#| echo: true

observed_fit <- duke_forest |>
  specify(price ~ area) |>
  fit()

observed_fit
```

## Computing the CI for the slope II {.smaller}

Take `100` bootstrap samples and fit models to each one:

```{r}
#| echo: true
#| code-line-numbers: "1,5,6"

set.seed(1120)

boot_fits <- duke_forest |>
  specify(price ~ area) |>
  generate(reps = 100, type = "bootstrap") |>
  fit()

boot_fits
```

## Computing the CI for the slope III

**Percentile method:** Compute the 95% CI as the middle 95% of the bootstrap distribution:

```{r}
#| echo: true
#| code-line-numbers: "5"

get_confidence_interval(
  boot_fits, 
  point_estimate = observed_fit, 
  level = 0.95,
  type = "percentile"
)
```

## Computing the CI for the slope IV

**Standard error method:** Alternatively, compute the 95% CI as the point estimate $\pm$ \~2 standard deviations of the bootstrap distribution:

```{r}
#| echo: true
#| code-line-numbers: "5"

get_confidence_interval(
  boot_fits, 
  point_estimate = observed_fit, 
  level = 0.95,
  type = "se"
)
```

## Precision vs. accuracy

::: question
If we want to be very certain that we capture the population parameter, should we use a wider or a narrower interval? What drawbacks are associated with using a wider interval?
:::

. . .

![](images/05/garfield.png)

## Precision vs. accuracy

::: question
How can we get best of both worlds -- high precision and high accuracy?
:::

## Changing confidence level

::: question
How would you modify the following code to calculate a 90% confidence interval? How would you modify it for a 99% confidence interval?
:::

```{r}
#| echo: true
#| code-line-numbers: "|4"

get_confidence_interval(
  boot_fits, 
  point_estimate = observed_fit, 
  level = 0.95,
  type = "percentile"
)
```

## Changing confidence level {.midi}

```{r}
#| echo: true

## confidence level: 90%
get_confidence_interval(
  boot_fits, point_estimate = observed_fit, 
  level = 0.90, type = "percentile"
)

## confidence level: 99%
get_confidence_interval(
  boot_fits, point_estimate = observed_fit, 
  level = 0.99, type = "percentile"
)
```

## Recap {.smaller}

-   **Population:** Complete set of observations of whatever we are studying, e.g., people, tweets, photographs, etc. (population size = $N$)

-   **Sample:** Subset of the population, ideally random and representative (sample size = $n$)

-   Sample statistic $\ne$ population parameter, but if the sample is good, it can be a good estimate

-   **Statistical inference:** Discipline that concerns itself with the development of procedures, methods, and theorems that allow us to extract meaning and information from data that has been generated by stochastic (random) process

-   We report the estimate with a confidence interval, and the width of this interval depends on the variability of sample statistics from different samples from the population

-   Since we can't continue sampling from the population, we bootstrap from the one sample we have to estimate sampling variability
