---
title: "Logistic regression"
subtitle: "Introduction"
author: "Prof. Maria Tackett"
date: "2023-10-30"
date-format: "MMM DD, YYYY"
footer: "[ðŸ”— STA 210 - Fall 2023 -  Schedule](https://sta210-fa23.netlify.app/schedule)"
logo: "../images/logo.png"
format: 
  revealjs:
    theme: slides.scss
    multiplex: false
    transition: fade
    slide-number: false
    incremental: false 
    chalkboard: true
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
execute:
  freeze: auto
  echo: true
knitr:
  opts_chunk: 
    R.options:      
    width: 200
bibliography: references.bib
---

```{r}
#| include: false

# figure options
knitr::opts_chunk$set(
  fig.width = 10, fig.asp = 0.618, out.width = "90%",
  fig.retina = 3, dpi = 300, fig.align = "center"
)

library(countdown)
```

## Announcements

-   HW 03 due Wed, Nov 1 at 11:59pm

-   See Ed Discussion for

    -   [Code](https://edstem.org/us/courses/44523/discussion/3725414) to reformat `Theme` variable in HW 03 data set
    -   [Explanation](https://edstem.org/us/courses/44523/discussion/3754375) on interpreting models with quadratic terms

## Spring 2024 statistics courses {.small}

-   STA 211: Mathematics of Regression
    -   Prereqs: MATH 216/218/221, STA 210
-   STA 230 or STA 240: Probability
    -   Prereqs: MATH 22/112/122/202/212/219/222
-   STA 310: Generalized Linear Models
    -   Prereqs: STA 210 and STA 230/240
-   STA 313: Advanced Data Visualization
    -   Prereqs: STA 198 or STA 199 or STA 210
-   STA 323: Statistical Computing
    -   Prereqs: STA 210 and STA 230/240
-   STA 360: Bayesian Inference and Modern Statistical Methods
    -   Prereqs: STA 210 and STA 230/240 and MATH 202/212/219/222 and CS 101/102/201 and MATH 216/218/221, 211 (co-req)

## Interpreting models with log-transformed variables {.small}

```{r}
#| echo: false
#| warning: false
#| message: false

library(tidyverse)
library(tidymodels)
library(knitr)

rail_trail <- read_csv(here::here("slides", "data/rail_trail.csv"))
log_rt_fit <- linear_reg() |>
  set_engine("lm") |>
  fit(log(volume) ~ hightemp + season + cloudcover + precip + day_type, data = rail_trail)

tidy(log_rt_fit) |>
  kable(digits = 3)
```

::: question
-   Interpret the intercept in terms of (1) `log(volume)` and (2) `volume`.

-   Interpret the coefficient of `hightemp` in terms of (1) `log(volume)` and (2) `volume`.
:::

# Logistic regression

## Topics

-   Logistic regression for binary response variable

-   Relationship between odds and probabilities

-   Use logistic regression model to calculate predicted odds and probabilities

## Computational setup

```{r}
#| warning: false

# load packages
library(tidyverse)
library(tidymodels)
library(knitr)
library(Stat2Data) #contains data set

# set default theme and larger font size for ggplot2
ggplot2::theme_set(ggplot2::theme_bw(base_size = 20))
```

# Predicting categorical outcomes

## Types of outcome variables

**Quantitative outcome variable**:

-   Sales price of a house in Duke Forest
-   **Model**: Expected sales price given the number of bedrooms, lot size, etc.

. . .

**Categorical outcome variable**:

-   Indicator of being high risk of getting coronary heart disease in the next 10 years
-   **Model**: Probability an adult is high risk of heart disease in the next 10 years given their age, total cholesterol, etc.

## Models for categorical outcomes

::: columns
::: {.column width="50%"}
**Logistic regression**

2 Outcomes

1: Yes, 0: No
:::

::: {.column width="50%"}
**Multinomial logistic regression**

3+ Outcomes

1: Democrat, 2: Republican, 3: Independent
:::
:::

## 2022 election forecasts

![](images/19/fivethirtyeight-2022-senate.png){fig-align="center"}

Source: [FiveThirtyEight 2022 Election Forecasts](https://projects.fivethirtyeight.com/2022-election-forecast/)

## 2020 NBA finals predictions

![](images/19/nba-predictions.png){fig-align="center"}

Source: [FiveThirtyEight 2019-20 NBA Predictions](https://projects.fivethirtyeight.com/2020-nba-predictions/games/?ex_cid=rrpromo)

## Do teenagers get 7+ hours of sleep? {.smaller}

::: columns
::: {.column width="40%"}
Students in grades 9 - 12 surveyed about health risk behaviors including whether they usually get 7 or more hours of sleep.

`Sleep7`

1: yes

0: no
:::

::: {.column width="60%"}
```{r}
data(YouthRisk2009) #from Stat2Data package
sleep <- YouthRisk2009 |>
  as_tibble() |>
  filter(!is.na(Age), !is.na(Sleep7))
sleep |>
  relocate(Age, Sleep7)
```
:::
:::

## Plot the data

```{r}
ggplot(sleep, aes(x = Age, y = Sleep7)) +
  geom_point() + 
  labs(y = "Getting 7+ hours of sleep")
```

## Let's fit a linear regression model

**Outcome:** $Y$ = 1: yes, 0: no

```{r}
#| echo: false

ggplot(sleep, aes(x = Age, y = Sleep7)) +
  geom_point() + 
  geom_smooth(method = "lm", se = FALSE) +
  labs(y = "Getting 7+ hours of sleep")
```

## Let's use proportions

**Outcome:** Probability of getting 7+ hours of sleep

```{r}
#| echo: false

sleep_age <- sleep |>
  group_by(Age) |>
  summarise(prop = mean(Sleep7))

ggplot(sleep_age, aes(x = Age, y = prop)) +
  geom_point() + 
  geom_hline(yintercept = c(0,1), lty = 2) + 
  stat_smooth(method = "lm",fullrange = TRUE, se = FALSE) +
  labs(y = "P(7+ hours of sleep)")
```

## What happens if we zoom out?

**Outcome:** Probability of getting 7+ hours of sleep

```{r}
#| echo: false

ggplot(sleep_age, aes(x = Age, y = prop)) +
  geom_point() + 
  geom_hline(yintercept = c(0,1), lty = 2) + 
  stat_smooth(method = "lm",fullrange = TRUE, se = FALSE) +
  labs(y = "P(7+ hours of sleep)") +
  xlim(1, 40) +
  ylim(-1, 2)
```

ðŸ›‘ *This model produces predictions outside of 0 and 1.*

## Let's try another model

```{r}
#| label: logistic-model-plot
#| echo: false

ggplot(sleep_age, aes(x = Age, y = prop)) +
  geom_point() + 
  geom_hline(yintercept = c(0,1), lty = 2) + 
  stat_smooth(method ="glm", method.args = list(family = "binomial"), 
              fullrange = TRUE, se = FALSE) +
  labs(y = "P(7+ hours of sleep)") +
  xlim(1, 40) +
  ylim(-0.5, 1.5)
```

*âœ… This model (called a **logistic regression model**) only produces predictions between 0 and 1.*

## The code

```{r}
#| ref.label: logistic-model-plot
#| echo: true
#| fig-show: hide
```

## Different types of models

| Method                          | Outcome      | Model                                                     |
|-------------------|------------------|-----------------------------------|
| Linear regression               | Quantitative | $Y = \beta_0 + \beta_1~ X$                                |
| Linear regression (transform Y) | Quantitative | $\log(Y) = \beta_0 + \beta_1~ X$                          |
| Logistic regression             | Binary       | $\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1 ~ X$ |

## Linear vs. logistic regression

::: question
State whether a linear regression model or logistic regression model is more appropriate for each scenario.

1.  Use age and education to predict if a randomly selected person will vote in the next election.

2.  Use budget and run time (in minutes) to predict a movie's total revenue.

3.  Use age and sex to calculate the probability a randomly selected adult will visit Duke Health in the next year.

Submit your responses on Ed Discussion \[[10:05am](https://edstem.org/us/courses/44523/discussion/471372)\]\[[1:25pm](https://edstem.org/us/courses/44523/discussion/471377)\]
:::

# Odds and probabilities

## Binary response variable

::: incremental
-   $Y = 1: \text{ yes}, 0: \text{ no}$
-   $\pi$: **probability** that $Y=1$, i.e., $P(Y = 1)$
-   $\frac{\pi}{1-\pi}$: **odds** that $Y = 1$
-   $\log\big(\frac{\pi}{1-\pi}\big)$: **log odds**
-   Go from $\pi$ to $\log\big(\frac{\pi}{1-\pi}\big)$ using the **logit transformation**
:::

## Odds

::: incremental
Suppose there is a **70% chance** it will rain tomorrow

-   Probability it will rain is $\mathbf{p = 0.7}$
-   Probability it won't rain is $\mathbf{1 - p = 0.3}$
-   Odds it will rain are **7 to 3**, **7:3**, $\mathbf{\frac{0.7}{0.3} \approx 2.33}$
:::

## Are teenagers getting enough sleep?

```{r}
sleep |>
  count(Sleep7) |>
  mutate(p = round(n / sum(n), 3))
```

. . .

$P(\text{7+ hours of sleep}) = P(Y = 1) = p = 0.664$

. . .

$P(\text{< 7 hours of sleep}) = P(Y = 0) = 1 - p = 0.336$

. . .

$P(\text{odds of 7+ hours of sleep}) = \frac{0.664}{0.336} = 1.976$

## From odds to probabilities

::: columns
::: {.column width="50%"}
**odds**

$$\omega = \frac{\pi}{1-\pi}$$
:::

::: {.column width="50%"}
**probability**

$$\pi = \frac{\omega}{1 + \omega}$$
:::
:::

# Logistic regression

## From odds to probabilities {.incremental}

(1) **Logistic model**: log odds = $\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1~X$
(2) **Odds =** $\exp\big\{\log\big(\frac{\pi}{1-\pi}\big)\big\} = \frac{\pi}{1-\pi}$
(3) Combining (1) and (2) with what we saw earlier

. . .

$$\text{probability} = \pi = \frac{\exp\{\beta_0 + \beta_1~X\}}{1 + \exp\{\beta_0 + \beta_1~X\}}$$

## Logistic regression model

**Logit form**: $$\log\big(\frac{\pi}{1-\pi}\big) = \beta_0 + \beta_1~X$$

. . .

**Probability form**:

$$
\pi = \frac{\exp\{\beta_0 + \beta_1~X\}}{1 + \exp\{\beta_0 + \beta_1~X\}}
$$

## Risk of coronary heart disease

This dataset is from an ongoing cardiovascular study on residents of the town of Framingham, Massachusetts. We want to use `age` to predict if a randomly selected adult is high risk of having coronary heart disease in the next 10 years.

<br>

`high_risk`:

::: nonincremental
-   1: High risk of having heart disease in next 10 years
-   0: Not high risk of having heart disease in next 10 years
:::

`age`: Age at exam time (in years)

## Data: `heart_disease`

```{r}
#| echo: false
heart_disease <- read_csv(here::here("slides", "data/framingham.csv")) |>
  select(age, TenYearCHD) |>
  drop_na() |>
  mutate(high_risk = as.factor(TenYearCHD)) |>
  select(age, high_risk)

heart_disease
```

## High risk vs. age

```{r}
#| echo: true

ggplot(heart_disease, aes(x = high_risk, y = age)) +
  geom_boxplot(fill = "steelblue") +
  labs(x = "High risk - 1: yes, 0: no",
       y = "Age", 
       title = "Age vs. High risk of heart disease")
```

## Let's fit the model

```{r}
#| echo: true
#| code-line-numbers: "1|2|3"
heart_disease_fit <- logistic_reg() |>
  set_engine("glm") |>
  fit(high_risk ~ age, data = heart_disease, family = "binomial")

tidy(heart_disease_fit) |> kable(digits = 3)
```

## The model

```{r}
#| echo: true

tidy(heart_disease_fit) |> kable(digits = 3)
```

\
$$\log\Big(\frac{\hat{\pi}}{1-\hat{\pi}}\Big) = -5.561 + 0.075 \times \text{age}$$ where $\hat{\pi}$ is the predicted probability of being high risk of having heart disease in the next 10 years

## Predicted log odds {.midi}

```{r}
augment(heart_disease_fit$fit) |> select(.fitted, .resid)
```

. . .

**For observation 1**

$$\text{predicted odds} = \hat{\omega} = \frac{\hat{\pi}}{1-\hat{\pi}} = \exp\{-2.650\} = 0.071$$

## Predicted probabilities {.midi}

```{r}
predict(heart_disease_fit, new_data = heart_disease, type = "prob")
```

. . .

**For observation 1**

$$\text{predicted probability} = \hat{\pi} = \frac{\exp\{-2.650\}}{1 + \exp\{-2.650\}} = 0.066$$

## Predicted classes

```{r}
predict(heart_disease_fit, new_data = heart_disease, type = "class")
```

## Default prediction

For a logistic regression, the default prediction is the `class`.

```{r}
predict(heart_disease_fit, new_data = heart_disease)
```

## Observed vs. predicted

::: question
What does the following table show?
:::

```{r}
predict(heart_disease_fit, new_data = heart_disease) |>
  bind_cols(heart_disease) |>
  count(high_risk, .pred_class)
```

. . .

::: question
The `.pred_class` is the class with the highest predicted probability. What is a limitation to using this method to determine the predicted class?
:::

# Application exercise

::: appex
ðŸ“‹ [AE 12: Logistic Regression Intro](../ae/ae-12-logistic-intro.html)
:::

## Recap

-   Introduced logistic regression for binary response variable

-   Described relationship between odds and probabilities

-   Used logistic regression model to calculate predicted odds and probabilities
